CITATION,Positive,Objective,Negative
we analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences including removing extraneous phrases from an extracted sentence combining a reduced sentence with other sentences syntactic transformation substituting phrases in an extracted sentence with their paraphrases substituting phrases with more general or specific descriptions and reordering the extracted sentences jing and mckeown 1999 jing and mckeown 2000 ,0,1,0
table 3 example compressions compression avglen rating baseline 970 193 bt2step 2206 321 spade 1909 310 humans 2007 383 table 4 mean ratings for automatic compressions nally we added a simple baseline compression algorithm proposed by jing and mckeown 2000 which removed all prepositional phrases clauses toinfinitives and gerunds ,0,1,0
53 related works and discussion our twostep model essentially belongs to the same category as the works of mani et al 1999 and jing and mckeown 2000 ,0,1,0
automatic text summarization approaches have offered reasonably wellperforming approximations for identifiying important sentences lin and hovy 2002 schiffman et al 2002 erkan and radev 2004 mihalcea and tarau 2004 daume iii and marcu 2006 but not surprisingly text regeneration has been a major challange despite some work on subsentential modification jing and mckeown 2000 knight and marcu 2000 barzilay and mckeown 2005 ,0,1,0
al 1994 compression of sentences with automatic translation approaches knight and marcu 2000 hidden markov model jing and mckeown 2000 topic signatures based methods lin and hovy 2000 lacatusu et al 2006 are among the most popular techniques that have been used in the summarization systems of this category ,1,0,0
because of this it is generally accepted that some kind of postprocessing should be performed to improve the final result by shortening fusing or otherwise revising the material grefenstette 1998 mani gates and bloedorn 1999 jing and mckeown 2000 barzilay et al 2000 knight and marcu 2000 ,0,1,0
additionally some research has explored cutting and pasting segments of text from the full document to generate a summary jing and mckeown 2000 ,0,1,0
but in fact the issue of editing in text summarization has usually been neglected notable exceptions being the works by jing and mckeown 2000 and mani gates and bloedorn 1999 ,1,0,0
jing and mckeown 2000 and jing 2000 propose a cutandpaste strategy as a computational process of automatic abstracting and a sentence reduction strategy to produce concise sentences ,0,1,0
jing and mckeown 2000 have proposed a rulebased algorithm for sentence combination but no results have been reported ,0,0,1
as previously observed in the literature mani gates and bloedorn 1999 jing and mckeown 2000 such components include a clause in the clause conjunction relative clauses and some elements within a clause such as adverbs and prepositions ,0,1,0
in addition to sentence fusion compression algorithms chandrasekar doran and bangalore 1996 grefenstette 1998 mani gates and bloedorn 1999 knight and marcu 2002 jing and mckeown 2000 reizler et al 2003 and methods for expansion of a multiparallel corpus pang knight and marcu 2003 are other instances of such methods ,0,1,0
while this approach exploits only syntactic and lexical information jing and mckeown 2000 also rely on cohesion information derived from word distribution in a text phrases that are linked to a local context are retained while phrases that have no such links are dropped ,0,1,0
in addition to reducing the original sentences jing and mckeown 2000 use a number of manually compiled rules to aggregate reduced sentences for example reduced clauses might be conjoined with and ,0,1,0
previous research has addressed revision in singledocument summaries jing mckeown 2000 mani et al 1999 and has suggested that revising summaries can make them more informative and correct errors ,0,1,0
to contrast jing mckeown 2000 concentrated on analyzing humanwritten summaries in order to determine how professionals construct summaries ,0,1,0
the recent approach for editing extracted text spans jing and mckeown 2000 may also produce improvement for our algorithm ,1,0,0
first splitting and merging of sentences jing and mckeown 2000 which seems related to content planning and aggregation ,0,1,0
1 introduction the task of sentence compression or sentence reduction can be defined as summarizing a single sentence by removing information from it jing and mckeown 2000 ,0,1,0
one of the applications is in automatic summarization in order to compress sentences extracted for the summary lin 2003 jing and mckeown 2000 ,0,1,0
in cutandpaste summarization jing and mckeown 2000 sentence combination operations were implemented manually following the study of a set of professionally written abstracts however the particular pasting operation presented here was not implemented ,0,1,0
close to the problem studied here is jing and mckeowns jing and mckeown 2000 cutandpaste method founded on endresniggemeyers observations ,0,1,0
jing and mckeown 1999 2000 found that human summarization can be traced back to six cutandpaste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part ,0,1,0
the specialist minimal commitment parser relies on the specialist lexicon as well as the xerox stochastic tagger cutting et al 1992 ,0,1,0
the two systems we use are engcg karlsson et al 1994 and the xerox tagger cutting et al 1992 ,0,1,0
22 xerox tagger the xerox tagger 1 xt cutting et al 1992 is a statistical tagger made by doug cutting julian kupiec jan pedersen and penelope sibun in xerox parc ,0,1,0
the xerox experiments cutting et al 1992 correspond to something between d1 and d2 and between to and t1 in that there is some initial biasing of the probabilities ,0,1,0
all 8907 articles were tagged by the xerox partofspeech tagger cutting et al 1992 4 ,0,1,0
cutting et al 1992 ,0,1,0
for czech we created a prototype of the first step of this process the partofspeech pos tagger using rank xerox tools tapanainen 1995 cutting et al 1992 ,0,1,0
5 related work cutting introduced grouping of words into equivalence classes based on the set of possible tags to reduce the number of the parameters cutting et al 1992 schmid used tile equivaleuce classes for smoothing ,0,1,0
in tabh 2 the accuracy rate of the nettagger is corollated to that of a trigram lmsed tagger kempe 1993 and a liidden markov model tagger cutting et al 1992 which were ,0,1,0
in this paper a new partofspeech tagging method hased on neural networks nettagger is presented and its performance is compared to that of a llmmtagger cutting et al 1992 and a trigrambased tagger kempe 1993 ,0,1,0
the performance of tle presented tagger is measured and compared to that of two other taggers cutting et al 1992 kempe 1993 ,0,1,0
no documentation of tile construction algorithm of the sulix lexicon in cutting et al 1992 was available ,0,1,0
the tagger used is thus one that does not need tagged and disambiguated material to be trained on namely the xpost originally constructed at xerox parc cutting et al 1992 cutting and pedersen 1993 ,0,1,0
it is usedas tagging mode in english church 1988 cutting et al 1992 and morphological analysis nlodel word segmentation and tagging in japanese nagata 1994 ,0,1,0
derose 1988 cutting et al 1992 merialdo 1994 ,0,1,0
52 assigning complex ambiguity tags in the tagging literature eg cutting et al 1992 an ambiguity class is often composed of the set of every possible tag for a word ,0,1,0
the corpus lines retained are partofspeech tagged cutting et al 1992 ,0,1,0
this text was partofspeech tagged using the xerox hmm tagger cutting et al 1992 ,0,1,0
no pretagged text is necessary for hidden markov models jelinek 1985 cutting et al 1991 kupiec 1992 ,0,1,0
we obtained 47025 50dimensional reduced vectors from the svd and clustered them into 200 classes using the fast clustering algorithm buckshot cutting et al 1992 group average agglomeration applied to a sample ,0,1,0
3 the statistical model we use the xerox partofspeech tagger cutting et al 1992 a statistical tagger made at the xerox palo alto research center ,0,1,0
this corpusbased information typically concerns sequences of 13 tags or words with some wellknown exceptions eg cutting et al 1992 ,0,1,0
as a common strategy pos guessers examine the endings of unknown words cutting et al 1992 along with their capitalization or consider the distribution of unknown words over specific partsofspeech weischedel et al 1993 ,0,1,0
on the other hand according to the datadriven approach a frequencybased language model is acquired from corpora and has the forms of ngrams church 1988 cutting et al 1992 rules hindle 1989 brill 1995 decision trees cardie 1994 daelemans et al 1996 or neural networks schmid 1994 ,0,1,0
pos disambiguation has usually been performed by statistical approaches mainly using the hidden markov model hmm in english research communities cutting et al 1992 kupiec 1992 weischedel et al 1993 ,0,1,0
the main application of these techniques to written input has been in the robust lexical tagging of corpora with partofspeech labels eg garside leech and sampson 1987 de rose 1988 meteer schwartz and weischedel 1991 cutting et al 1992 ,0,1,0
two main approaches have generally been considered rulebased klein and simmons 1963 brodda 1982 paulussen and martin 1992 brill et al 1990 probabilistic bahl and mercer 1976 debili 1977 stolz tannenbaum and carstensen 1965 marshall 1983 leech garside and atwell 1983 derouault and merialdo 1986 derose 1988 church 1989 beale 1988 marcken 1990 merialdo 1991 cutting et al 1992 ,0,1,0
stochastic taggers use both contextual and morphological information and the model parameters are usually defined or updated automatically from tagged texts cerfdanon and e1beze 1991 church 1988 cutting et al 1992 dermatas and kokkinakis 1988 1990 1993 1994 garside leech and sampson 1987 kupiec 1992 maltese department of electrical engineering wire communications laboratory wcl university of patras 265 00 patras greece ,0,1,0
unlike stochastic approaches to partofspeech tagging church 1988 kupiec 1992 cutting et al 1992 merialdo 1990 derose 1988 weischedel et al 1993 up to now the knowledge found in finitestate taggers has been handcrafted and was not automatically acquired ,0,1,0
7 independently cutting et al 1992 quote a performance of 800 words per second for their partofspeech tagger based on hidden markov models ,0,1,0
these methods have reported performance in the range of 9599 correct by word derose 1988 cutting et al 1992 jelinek mercer and roukos 1992 kupiec 1992 ,0,1,0
a number of partofspeech taggers are readily available and widely used all trained and retrainable on text corpora church 1988 cutting et al 1992 brill 1992 weischedel et al 1993 ,1,0,0
partofspeech tagging is an active area of research a great deal of work has been done in this area over the past few years eg jelinek 1985 church 1988 derose 1988 hindle 1989 demarcken 1990 merialdo 1994 brill 1992 black et al 1992 cutting et al 1992 kupiec 1992 charniak et al 1993 weischedel et al 1993 schutze and singer 1994 ,0,1,0
almost all recent work in developing automatically trained partofspeech taggers has been on further exploring markovmodel based tagging jelinek 1985 church 1988 derose 1988 demarcken 1990 merialdo 1994 cutting et al 1992 kupiec 1992 charniak et al 1993 weischedel et al 1993 schutze and singer 1994 ,0,1,0
as the baseline standard we took the endingguessing rule set supplied with the xerox tagger cutting et al 1992 ,0,1,0
the xerox tagger cutting et al 1992 comes with a set of rules that assign an unknown word a set of possible postags ie posclass on the basis of its ending segment ,0,1,0
cutting et al 1992 reported very high results 96 on the brown corpus for unsupervised pos tagging using hidden markov models hmms by exploiting handbuilt tag dictionaries and equivalence classes ,1,0,0
it is also possible to train statistical models using unlabeled data with the expectation maximization algorithm cutting et al 1992 ,0,1,0
for english there are many pos taggers employing machine learning techniques like transformationbased errordriven learning brill 1995 decision trees black et al 1992 markov model cutting et al 1992 maximum entropy methods ratnaparkhi 1996 etc there are also taggers which are hybrid using both stochastic and rulebased approaches such as claws garside and smith 1997 ,0,1,0
in such cases additional information may be coded into the hmm model to achieve higher accuracy cutting et al 1992 ,0,1,0
stochastic models cutting et al 1992 dermatas et al 1995 brants 2000 have been widely used in pos tagging for simplicity and language independence of the models ,1,0,0
this situation is very similar to that involved in training hmm text taggers where joint probabilities are computed that a particular word corresponds to a particular partofspeech and the rest of the words in the sentence are also generated eg cutting et al 1992 ,0,1,0
making such an assumption is reasonable since pos taggers that can achieve accuracy of 96 are readily available to assign pos to unrestricted english sentences brill 1992 cutting et al 1992 ,0,1,0
derose 1988 cutting et al 1992 church 1988 ,0,1,0
there has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques eg church 1988 cutting et al 1992 derose 1988 constraintbased techniques karlsson et al 1995 voutilainen 1995b voutilainen heikkili and anttila 1992 voutilainen and tapanainen 1993 oflazer and kurusz 1994 oflazer and till 1996 and transformationbased techniques brilt 1992 brill 1994 brill 1995 ,0,1,0
second the automatic approach in which the model is automatically obtained from corpora either raw or annotated 1 and consists of ngrams garside et al 1987 cutting et ah 1992 rules hindle 1989 or neural nets schmid 1994 ,0,1,0
cutting et al 1992 local rules eg hindle 1989 and neural networks eg schmid 1994 ,0,1,0
2000 that draws on a stochastic tagger see cutting et al 1992 for details as well as the specialist lexicon5 a large syntactic lexicon of both general and medical english that is distributed with the umls ,0,1,0
the xerox tagger comes with a list of builtin ending guessing rules cutting et al 1992 ,0,1,0
it has been known for some years that good performance can be realized with partial tagging and a hidden markov model cutting et al 1992 ,1,0,0
this analysis depends on the specialist lexicon and the xerox partofspeech tagger cutting et al 1992 and provides simple noun phrases that are mapped to concepts in the umls metathesaurus using metamap aronson 2001 ,0,1,0
the initial phase relies on a parser that draws on the specialist lexicon mccray et al 1994 and the xerox partofspeech tagger cutting et al 1992 to produce an underspecified categorial analysis ,0,1,0
many approaches for pos tagging have been developed in the past including rulebased tagging brill 1995 hmm taggers brants 2000 cutting and others 1992 maximumentropy models rathnaparki 1996 cyclic dependency networks toutanova et al 2003 memorybased learning daelemans et al 1996 etc all of these approaches require either a large amount of annotated training data for supervised tagging or a lexicon listing all possible tags for each word for unsupervised tagging ,0,0,1
brills results demonstrate that this approach can outperform the hidden markov model approaches that are frequently used for partofspeech tagging jelinek 1985 church 1988 derose 1988 cutting et al 1992 weischedel et al 1993 as well as showing promise for other applications ,0,0,1
it is possible to use unsupervised learning to train stochastic taggers without the need for a manually annotated corpus by using the baumwelch algorithm baum 1972 jelinek 1985 cutting et al 1992 kupiec 1992 elworthy 1994 merialdo 1995 ,0,1,0
this method is employed in kupiec 1992 cutting et al 1992 ,0,1,0
almost all of the work in the area of automatically trained taggers has explored markovmodel based part of speech tagging jelinek 1985 church 1988 derose 1988 demarcken 1990 cutting et al 1992 kupiec 1992 charniak et al 1993 weischedel et al 1993 schutze and singer 1994 lin et al 1994 elworthy 1994 merialdo 1995 ,0,1,0
1 introduction in the partofspeech hterature whether taggers are based on a rulebased approach klein and simmons 1963 brill 1992 voutilainen 1993 or on a statistical one bahl and mercer 1976 leech et al 1983 merialdo 1994 derose 1988 church 1989 cutting et al 1992 there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones ,0,1,0
5 comparison with other approaches in some sense this approach is similar to the notion of ambiguity classes explained in kupiec 1992 and cutting et al 1992 where words that belong to the same partofspeech figure together ,0,1,0
most work on statistical methods has used ngram models or hidden markov modelbased taggers eg church 1988 derose 1988 cutting et al 1992 merialdo 1994 etc ,0,1,0
generalized forward backward reestimation generalization of the forward and viterbi algorithm in english part of speech taggers the maximization of equation 1 to get the most likely tag sequence is accomplished by the viterbi algorithm church 1988 and the maximum likelihood estimates of the parameters of equation 2 are obtained from untagged corpus by the forwardbackward algorithm cutting et al 1992 ,0,1,0
the accuracy of the derived model depends heavily on the initial bias but with a good choice results are comparable to those of method three cutting et al 1992 ,0,1,0
cutting et al 1992 feldweg 1995 the tagger for grammatical functions works with lexical and contextual probability measures pq ,0,1,0
in our experiments we used the hidden markov model hmm tagging method described in cutting et al 1992 ,0,1,0
the pos disambiguation has usually been performed by statistical approaches mainly using hidden markov model hmm cutting et al 1992 kupiec ,0,1,0
our statistical tagging model is adjusted from standard bigrams using the viterbisearch cutting et al 1992 plus onthefly extra computing of lexical probabilities for unknown morphemes ,0,1,0
examples of such affinities include synonyms terra and clarke 2003 verb similarities resnik and diab 2000 and word associations rapp 2002 ,0,1,0
several papers have looked at higherorder representations but have not examined the equivalence of synpara distributions when formalized as markov chains schutze and pedersen 1993 lund and burgess 1996 edmonds 1997 rapp 2002 biemann et al 2004 lemaire and denhiere 2006 ,0,0,1
roughly in keeping with rapp 2002 we hereby regard paradigmatic assocations as those based largely on word similarity ie including those typically classed as synonyms antonyms hypernyms hyponyms etc whereas syntagmatic associations are all those words which strongly invoke one another yet which cannot readily be said to be similar ,0,1,0
then by using evaluations similar to those described in baroni et al 2008 and by rapp 2002 we show that the best distancebased measures correlate better overall with human association scores than do the best window based configurations see section 4 and that they also serve as better predictors of the strongest human associations see section 5 ,0,1,0
while choosing an optimum window size for an application is often subject to trial and error there are some generally recognized tradeoffs between small versus large windows such as the impact of datasparseness and the nature of the associations retrieved church and hanks 1989 church and hanks 1991 rapp 2002 measures based on distance between words in the text ,0,1,0
3 methodology similar to rapp 2002 baroni et al 2008 among others we use comparison to human assocation datasets as a test bed for the scores produced by computational association measures ,0,1,0
we use evaluations similar to those used before rapp 2002 pado and lapata 2007 baroni et al 2008 among others ,0,1,0
at the present time given the key role of window size in determining the selection and apparent strength of associations under the conventional cooccurrence model highlighted here and in the works of church et al 1991 rapp 2002 wang 2005 and schulte im walde melinger 2008 we would urge that this is an issue which windowdriven studies continue to conscientiously address at the very least scale is a parameter which findings dependent on distributional phenomena must be qualified in light of ,0,1,0
as rapp 2002 observes choosing a window size involves making a tradeoff between various qualities ,0,1,0
21 scaledependence it has been shown that varying the size of the context considered for a word can impact upon the performance of applications rapp 2002 yarowsky florian 2002 there being no ideal window size for all applications ,0,1,0
22 data sparseness another facet of the general tradeoff identified by rapp 2002 pertains to how limitations in862 herent in the combination of data and cooccurrence retrieval method are manifest ,0,1,0
this is one manifestation of what is commonly referred to as the data sparseness problem and was discussed by rapp 2002 as a sideeffect of specificity ,0,1,0
3 algorithm as in previous work rapp 2002 our computations are based on a partially lemmatized version of the british national corpus bnc which has the function words removed ,0,1,0
we used the procedure described in rapp 2002 with the only modification being the multiplication of the loglikelihood values with a triangular function that depends on the logarithm of a words frequency ,0,1,0
our method is based on the ones described in erkan and radev 2004 mihalcea and tarau 2004 fader et al 2007 the objective of this paper is to dynamically rank speakers or participants in a discussion ,0,1,0
eigenvector centrality in particular has been successfully applied to many different types of networks including hyperlinked web pages brin and page 1998 kleinberg 1998 lexical networks erkan and radev 2004 mihalcea and tarau 2004 kurland and lee 2005 kurland and lee 2006 and semantic networks mihalcea et al 2004 ,1,0,0
still it is in our next plans and part of our future work to embed in our model some of the interesting wsd approaches like knowledgebased sinha and mihalcea 2007 brody et al 2006 corpusbased mihalcea and csomai 2005 mccarthy et al 2004 or combinations with very high accuracy montoyo et al 2005 ,0,1,0
this method was preferred against other related methods like the one introduced in mihalcea et al 2004 since it embeds all the available semantic information existing in wordnet even edges that cross pos thus offering a richer semantic representation ,0,0,1
previous approaches include supervised learning hirao et al 2002 teufel and moens 1997 vectorial similarity computed between an initial abstract and sentences in the given document intradocument similarities salton et al 1997 or graph algorithms mihalcea and tarau 2004 erkan and radev 2004 wolf and gibson 2004 ,0,1,0
ranking algorithms such as kleinbergs hits algorithm kleinberg 1999 or googles pagerank brin and page 1998 have been traditionally and successfully used in weblink analysis brin and page 1998 social networks and more recently in text processing applications mihalcea and tarau 2004 mihalcea et al 2004 erkan and radev 2004 ,0,1,0
inspired by the idea of graph based algorithms to collectively rank and select the best candidate research efforts in the natural language community have applied graphbased approaches on keyword selection mihalcea and tarau 2004 text summarization erkan and radev 2004 mihalcea 2004 word sense disambiguation mihalcea et al 2004 mihalcea 2005 sentiment analysis pang and lee 2004 and sentence retrieval for question answering otterbacher et al 2005 ,0,1,0
such textoriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases to extractive summarization and word sense disambiguation mihalcea et al 2004 ,0,1,0
using dictionaries as network of lexical items or senses has been quite popular for word sense disambiguation veronis and ide 1990 hkozima and furugori 1993 niwa and nitta 1994 before losing ground to statistical approaches even though gaume et al 2004 mihalcea et al 2004 tried a revival of such methods ,0,1,0
although various approaches to smt system combination have been explored including enhanced combination model structure rosti et al 2007 better word alignment between translations ayan et al 2008 he et al 2008 and improved confusion network construction rosti et al 2008 most previous work simply used the ensemble of smt systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way ,0,1,0
most of the work focused on seeking better word alignment for consensusbased confusion network decoding matusov et al 2006 or wordlevel system combination he et al 2008 ayan et al 2008 ,0,1,0
thus we can compute the source dependency lm score in the same way we compute the target side score using a procedure described in shen et al 2008 ,0,1,0
due to the lack of a good arabic parser compatible with the sakhr tokenization that we used on the source side we did not test the source dependency lm for arabictoenglish mt when extracting rules with source dependency structures we applied the same wellformedness constraint on the source side as we did on the target side using a procedure described by shen et al 2008 ,0,1,0
a remedy is to aggressively limit the feature space eg to syntactic labels or a small fraction of the bilingual features available as in chiang et al 2008 chiang et al 2009 but that reduces the benefit of lexical features ,0,1,0
in post and gildea 2008 shen et al 2008 target trees were employed to improve the scoring of translation theories ,0,1,0
a few studies carpuat and wu 2007 ittycheriah and roukos 2007 he et al 2008 hasan et al 2008 addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence ,0,1,0
the other approach is to estimate a single score or likelihood of a translation with rich features for example with the maximum entropy maxent method as in carpuat and wu 2007 ittycheriah and roukos 2007 he et al 2008 ,0,1,0
in he et al 2008 lexical 72 features were limited on each single side due to the feature space problem ,0,1,0
carpuat and wu 2007 and he et al 2008 the specific technique we used by means of a context language model is rather different ,0,1,0
73 122 baseline system and experimental setup we take bbns hierdec a stringtodependency decoder as described in shen et al 2008 as our baseline for the following two reasons it provides a strong baseline which ensures the validity of the improvement we would obtain ,0,1,0
2 linguistic and context features 21 nonterminal labels in the original stringtodependency model shen et al 2008 a translation rule is composed of a string of words and nonterminals on the source side and a wellformed dependency structure on the target side ,0,1,0
previously published approaches to reducing the rule set include enforcing a minimum span of two words per nonterminal lopez 2008 which would reduce our set to 115m rules or a minimum count mincount threshold zollmann et al 2008 which would reduce our set to 78m mincount2 or 57m mincount3 rules ,0,1,0
lopez 2008 explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrasebased systems ,0,1,0
first such a system makes use of lexical information when modeling reordering lopez 2008 which has previously been shown to be useful in germantoenglish translation koehn et al 2008 ,1,0,0
2 models search spaces and errors a translation model consists of two distinct elements an unweighted ruleset and a parameterization lopez 2008a 2009 ,0,1,0
lopez 2008b gives indirect experimental evidence that this difference affects performance ,0,1,0
our hierarchical system is hiero chiang 2007 modified to construct rules from a small sample of occurrences of each source phrase in training as described by lopez 2008b ,0,1,0
in a next step chunk information was added by a rulebased languageindependent chunker macken et al 2008 that contains distituency rules which implies that chunk boundaries are added between two pos codes that cannot occur in the same constituent ,0,1,0
macken et al 2008 showed that the results for frenchenglish were competitive to stateoftheart alignment systems ,1,0,0
then the same system weights are applied to both inchmm and joint decoding based approaches and the feature weights of them are trained using the maxbleu training method proposed by och 2003 and refined by moore and quirk 2008 ,1,0,0
previouswork eg moore and quirk 2008 cer et al 2008 has focusedonimprovingtheperformanceofpowells algorithm ,0,1,0
one such relational reasoning task is the problem of compound noun interpretation which has received a great deal of attention in recent years girju et al 2005 turney 2006 butnariu and veale 2008 ,0,1,0
turney 2008 has recently proposed a simpler svmbased algorithm for analogical classification called pairclass ,0,1,0
turney 2008 argues that many nlp tasks can be formulated in terms of analogical reasoning and he applies his pairclass algorithm to a number of problems including sat verbal analogy tests synonymantonym classification and distinction between semantically similar and semantically associated words ,0,1,0
analternativeembeddingisthatusedbyturney 2008 in his pairclass system see section 6 ,0,1,0
language modeling chen and goodman 1996 nounclustering ravichandran et al 2005 constructing syntactic rules for smt galley et al 2004 and finding analogies turney 2008 are examples of some of the problems where we need to compute relative frequencies ,0,1,0
in nlp community it has been shown that having more data results in better performance ravichandran et al 2005 brants et al 2007 turney 2008 ,0,1,0
in table 6 we report our results together with the stateoftheart from the acl wiki5 and the scores of turney 2008 pairclass and from amac herdagdelens pairspace system that was trained on ukwac ,0,1,0
2 related work turney 2008 recently advocated the need for a uniform approach to corpusbased semantic tasks ,0,1,0
such tasks will require an extension of the current framework of turney 2008 beyond evidence from the direct cooccurrence of target word pairs ,0,1,0
turney 2008 is the first to the best of our knowledge to raise the issue of a unified approach ,0,1,0
we adopt a similar approach to the one used in turney 2008 and consider each question as a separate binary classification problem with one positive training instance and 5 unknown pairs ,0,1,0
they are part of an effort to better integrate a linguistic rulebased system and the statistical correcting layer also illustrated in ueffing et al 2008 ,0,1,0
35 domain adaptation in machine translation within mt there has been a variety of approaches dealing with domain adaption for example wu et al 2008 koehn and schroeder 2007 ,0,1,0
the sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions eg constraints drawn from a wordalignment cherry and lin 2007 zhang et al 2008b or a grammar fixed a priori blunsom et al 1f and e are the input and output sentences respectively ,0,1,0
other linear time algorithms for rank reduction are found in the literature zhang et al 2008 but they are restricted to the case of synchronous contextfree grammars a strict subclass of the lcfrs with f 2 ,0,1,0
in the smt research community the second step has been well studied and many methods have been proposed to speed up the decoding process such as nodebased or spanbased beam search with different pruning strategies liu et al 2006 zhang et al 2008a 2008b and cube pruning huang and chiang 2007 mi et al 2008 ,0,1,0
31 exhaustive search by tree fragments this method generates all possible tree fragments rooted by each node in the source parse tree or forest and then matches all the generated tree fragments against the source parts left hand side of translation rules to extract the useful rules zhang et al 2008a ,0,1,0
1 introduction recently linguisticallymotivated syntaxbased translation method has achieved great success in statistical machine translation smt galley et al 2004 liu et al 2006 2007 zhang et al 2007 2008a mi et al 2008 mi and huang 2008 zhang et al 2009 ,1,0,0
motivated by the fact that nonsyntactic phrases make nontrivial contribution to phrasebased smt the tree sequencebased translation model is proposed liu et al 2007 zhang et al 2008a that uses tree sequence as the basic translation unit rather than using single subtree as in the stsg ,0,1,0
2008a propose a tree sequencebased tree to tree translation model and zhang et al ,0,1,0
therefore structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntaxbased smt zhang et al 2008a mi et al 2008 ,0,1,0
a tree sequence to string rule 174 a treesequence to string translation rule in a forest is a triple l r a where l is the tree sequence in source language r is the string containing words and variables in target language and a is the alignment between the leaf nodes of l and r this definition is similar to that of liu et al 2007 zhang et al 2008a except our treesequence is defined in forest ,0,1,0
to address this issue many syntaxbased approaches yamada and knight 2001 eisner 2003 gildea 2003 ding and palmer 2005 quirk et al 2005 zhang et al 2007 2008a bod 2007 liu et al 2006 2007 hearne and way 2003 tend to integrate more syntactic information to enhance the noncontiguous phrase modeling ,0,1,0
nevertheless the generated rules are strictly required to be derived from the contiguous translational equivalences galley et al 2006 marcu et al 2006 zhang et al 2007 2008a 2008b liu et al 2006 2007 ,0,1,0
2 we illustrate the rule extraction with an example from the treetotree translation model based on tree sequence alignment zhang et al 2008a without losing of generality to most syntactic tree based models ,0,1,0
word alignment is also a required first step in other algorithms such as for learning subsentential phrase pairs lavie et al 2008 or the generation of parallel treebanks zhechev and way 2002 ,0,1,0
previously published approaches to reducing the rule set include enforcing a minimum span of two words per nonterminal lopez 2008 which would reduce our set to 115m rules or a minimum count mincount threshold zollmann et al 2008 which would reduce our set to 78m mincount2 or 57m mincount3 rules ,0,1,0
zollmann et al 2008 ,0,1,0
this is in direct contrast to recent reported results in which other filtering strategies lead to degraded performance shen et al 2008 zollmann et al 2008 ,0,1,0
the fluency models hold promise for actual improvements in machine translation output quality zwarts and dras 2008 ,1,0,0
dolan 1994 and krovetz and croft 1992 claim that finegrained semantic distinctions are unlikely to be of practical value for many applications ,0,1,0
much work has gone into methods for measuring synset similarity early work in this direction includes dolan 1994 which attempted to discover sense similarities between dictionary senses ,0,1,0
recognizing this dolan 1994 proposes a method for ambiguating dictionary senses by combining them to create grosser sense distinctions ,0,1,0
various approaches to word sense division have been proposed in the literature on wsd including 1 sense numbers in everyday dictionaries lesk 1986 cowie guthrie and guthrie 1992 2 automatic or handcrafted clusters of dictionary senses dolan 1994 bruce and wiebe 1995 luk department of computer science national tsing hua university hsinchu 30043 taiwan roc ,0,1,0
82 chen and chang topical clustering dolan 1994 maintains the position that intersense relations are mostly idiosyncratical thereby making it difficult to characterize them in a general way so as to identify them ,0,1,0
however they do not elaborate on how the comparisons are done or on how effective the program is dolan 1994 describes a heuristic approach to forming unlabeled clusters of closely related senses in an mrd ,0,1,0
as noted in dolan 1994 it is possible to run a senseclustering algorithm on several mrds to build an integrated lexical database with more complete coverage of word senses ,0,1,0
these relations are then used for various tasks ranging from the interpretation of a noun sequence vanderwende 1994 or a prepositional phrase ravin 1990 to resolving structural ambiguity jenson and binot 1987 to merging dictionary senses for wsd dolan 1994 ,0,1,0
5 related work dolan 1994 describes a method for clustering word senses with the use of information provided in the electronic version of ldoce textual definitions semantic relations domain labels etc ,0,1,0
this approach took inspiration from the pioneering work by dolan 1994 but it is also fundamentally different because instead of grouping similar senses together the corelex approach groups together words according to all of their senses ,1,0,0
dolan 1994 described a heuristic approach to forming unlabeled clusters of closely related senses in a mrd ,0,1,0
dolan 1994 observed that sense division in mrd is frequently too free for the purpose of wsd ,0,1,0
towards a meaningfull comparison of lexieal resources kenneth c lltkowska cl research 9208 gue road damascus md 20872 kenclres corn httpwww tires tom abstract the mapping from wordnet to hector senses m senseval provides a gold standard against wluch to judge our ability to compare lexlcal resources the gold standard is provided through a word overlap analysis with and without a stop list for flus mapping achieving at most a 36 percent correct mapping inflated by 9 percent from empty assignments an alternauve componenttal analysis of the defimtaons using syntacuc collocatmnal and semantac component and relation identification through the use ofdefimng patterns integrated seamlessly mto the parsing thclaonary provides an almost 41 percent correct mapping with an additaonal 4 percent by recogmzmg semantic components not used in the senseval mapping defimtion sets of the senseval words from three pubhshed thclaonanes and dorrs lextcal knowledge base were added to wordnet and the hector database to exanune the nature of the mapping process between defimtton sets of more and less scoe the tecbauques described here consutute only an maaal implementation of the componenual analysis approach and suggests that considerable further improvements can be aclueved introduction the difficulty of companng lemcal resources long a sgnfficant challenge in computauonal hnguistlcs atlans 1991 came to the fore in the recent senseval competatton iolgarnff 1998 when some systems that relied heavily on the wordnet miller et al 1990 sense inventory were faced with the necessity of using another sense inventory hecto0 a hasty solutaon to the problem was the development of a map between the two inventories but some partcipants expressed concerns that use of flus map may have degraded their performance to an unknown degree although there were disclaimers about the wordnethector map it nonetheless stands as a usable gold standard for efforts to compare lexical resources moreover we have a usable baseline a word overlap method suggested m lesk 1986 against which to compare whether we are able to make improvements m the mapping since flus method has been shown to perform not as well as expected krovetz 1992 we first describe the lextcal resources used m the study hector wordnet other dicuonanes and a lexcal knowledge base first characterizing them in terms ofpolysemy and the types of leracal mformauon each contmns syntacuc properties and features semantac components and relauons and collocauonal properties we then present results of perfornung the word overlap analysis of the 18 verbs used m senseval analyzing the definitions m wordnet and hector we then expand our analysis to include other dictionaries we describe our methods of analysis particularly the methods of parsing defimtaons and identffqng semantic relations semrels based on defimng patterns essentially takang first steps m implementing the program described by atkms and focusmg on the use ofmeamng full mformataon rather than statistical mformauon we identify the results that have been achieved thus far and outline further steps that may add more meanmg to the analysis iall analyses described m this paper were performed automatically using functlonahty incorporated m dimap dictionary maintenance programs available for immediate download at cl research 1999a this includes automatac extracuon of wordnet reformation for the selected words mtegrated m dimap hector defimtlons were uploaded into dimap dicuonanes after use of a conversmn program defimtlons for other 30 the lexical resources tlus analysis focuses on the mmn verb senses used in senseval not ichoms and phrases specifically the followmg amaze band bet bother bury calculate consume derive float hurdle invade promise sack sanction scrap seize shake slight the hector database used in senseval consists of a tree of senses each of which contains defimttons syntactic properties example usages and clues collocational information about the syntactic and semantic enwronment in wluch a word appears in the spectfic sense the wordnet database contmns synonyms synsets perhaps a defimtton or example usages gloss some syntactic mformauon verb frames hypernyms hyponyms and some other semrels entails causes to extend our analysis in order to look at other issues of lexacal resource comparison we have included the defirauons or leracal information from the following additional sources websters 3 ra new international dictionary w3 oxford advanced learners dctlonary oald american hentage dlcuonary aiid dorrs lexacal knowledge base dorr we used only the defimuons from w3 oald and ahd which also contmn sample usages and some collocattonal information m the form of usage notes not used at the present tame dorrs database contains thematic grids wluch characterize the thematic roles of obligatory and optional semanuc components frequently identifying accompanying preposmons olsen et al 1998 the following table identities the number of senses and average overall polysemy for each of these resources dictionaries were entered by hand word amaze band bet bother bury calculate consume denve float hurdle invade pronuse sack sanction scrap seize shake shght average polysemy o o o 1 2 4 2 3 1 ii 4 4 2 5 5 7 6 9 7 12 6 14 5 5 5 10 9 6 6 8 8 6 5 15 5 16 4 41 14 2 1 4 3 6 2 10 5 5 4 7 4 4 4 6 3 2 2 5 2 3 1 3 3 11 6 21 13 8 8 37 17 1 1 6 3 o 1 2 2 4 1 3 4 4 8 1 3 1 3 1 3 2 10 5 1 0 3 1 3 2 2 0 1 1 1 0 7 1 7 12 i 0 57 37 120 62 34 22 word overlap analysis we first estabhsh a baseline for automatic replication of the lexicographers mappmg from wordnet 1 6 to hector using a smple word overlap analysis smular to lesk 1986 the lextcographer mapped the 66 wordnet senses each synset m which a test occurred into 102 hector senses a total of 86 assignments were made 9 wordnet senses were gwen no assignments 40 recewed exactly one and 17 senses received 2 or 3 asssgnments the wordnet senses contained 348 words about half of wluch were common words appeanng on our stop list which contained 165 words mostly preposmons pronouns and conjunctions the hector senses selected m the word overlap analysis contained about 960 words all hector senses contained 1878 words we performed a strict word overlap analysts with and wsthout a stop hst between tile definluons in wordnet and the hector senses that is we did not attempt to ldenttfy root forms of inflected words we took each word m a wordnet sense and determined whether t appeared in a hector sense we selected a hector sense based on the highest percentage of words over all hector senses an 31 empty selection was made ff all the words in the wordnet sense did not appear in any hector sense only content words were considered when the stop hst was used for example for bet wordnet sense 2 stake money on the outcome of an issue mapped into hector sense 4 of a person to risk a sum of money or property m thts way in this case there was an overlap on two words money 039 in the hector defimtlon 0 13 of its 15 words without the stop list when the stop list was invoked there was an overlap of only one word money 0 07 of the hector defimtion in this case the lexicographer had made three assignments hector senses 2 3 and 4 our scoring method treated flus as only 1 out of 3 correct not using the relaxed method employed in senseval of treating flus as completely correct without the stop hst our selections matched the lexicographers in 28 of 86 cases 32 6 using the stop list we were successful in 31 of 86 cases 36 1 the improvement arising when the stop list was used is deceptive where 8 cases were due to empty assignments so that only 23 cases 26 7 were due to matching content words overall only 41 content words were involved in these 23 successes when the stop list was used an average of i 8 content words to summanze the word overlap analysis 1 despite a ncher set of defimtions in hector 9 of 66 wordnet senses 13 6 could not be assigned 2 despite the greater detail in hector senses compared to wordnet senses 2 8 times as many words only 1 8 content words participated in the assignments and 3 therefore the defimng vocabulary between these two definition sets seems to be somewhat divergent although it might appear as if the word overlap analysis does not perform well this is not the case the analysis provides a broad overview of the defimuon companson process between two definmon sets and frames a deeper analysis of the differences moreover it appears that the accuracy of a gold standard mapping is not crucially important the quality of the mapping may help frame the subsequent analysis more precisely but it seems sufficient that any reasonable mapping will suffice this will be discussed further after presenting the results of the componentlal analysis of the defimtlons 32 meaningfull analysis of definitions the deeper analysis of the mapping between two defimtion sets relies primarily on two major steps 1 parsing definitions and using defimng patterns to identify semrels present m the definitions and 2 relaxing values to these relations by allowing synonymic substitution using wordnet thus for example ffwe identify hypernyms or instruments from parsing a defimtion we would say that the defimtions are equal not just ffthe hypernym or instrument is the same word but also lf the hypernyms or instruments are members of the same synset this approach is based on the finding litkowski 1978 that a dictionary induces a semantic network where nodes represent concepts that may be lexicahzed and verbalized in more than one way this finding implies in general the absence of true synonyms and instead the kind of concept embodied in wordnet synsets with several lexical items and phraseologles a slmdar approach parsing defimtlons and relaxing semrel values was followed in dolan 1994 for clnstenng related senses wthin a single dictionary the ideal toward which this approach strives is a complete identification of the meamng components included in a defimtion the meaning components can include syntactic features and charactenstlcs including subcategonzation patterns semantm components realized through identification of semrels selectional restrictions and couocational specifications the first stage of the analysis parses the definitions cl research 1999b litkowski to appear and uses the parse results to extract via defining patterns semrels since definitions have many idiosyncrasies that do not follow ordinary text an important first step in this stage is preprocessmg the definition text to put it into a sentence frame that facilitates the extraction of semrels 2 2note that the stop hst is not applicable to the definition parsing the parser is a fullscale sentence parser where prepositmns and other words on the stop list are necessary for successful parsing moreover inclusion of the prepositions is cmcml to the method since they are the bearers of much semrel information the extractmn of semrels examines the parse results a e a tree whose mtermedaate nodes represent nonternunals and whose leaves represent the lextcal atems that compnse the defimuons where any node may also include annotations such as characterizations of number and tense for all noun or verb defimttons flus includes identification of the head noun with recogmtton ofempty heads or verb for verbs we signal whether the defimtaon contmned any selecttonal restnctmus that as pamcular parenthesazed expressaons for the subject and object we then exanune preposattonal phrases in the defimuon and deterrmne whether we have a defining pattern for the preposauon whach we can use as mdacauve of a partacular semrel we also identify adverbs m the parse tree and look these up in wordnet to adentffy an adjecuve synset from wluch they are derived if one is gwen the defimng pattems are actually part of the dictionary used by the parser that is we do not have to develop specafic routines to look for speclfic patterns a defimng pattern s a regular expressaon that arlaculates a syntactac pattern to be matched thus to recograze a manner semrel we have the fouowmg entry for m mdpat rep0 ldet0 adj manner0 stmanner this allows us to recognize m as possibly gwmg rise to a manner component where we recogmze m the tdde which allows us to specify partacular elements before the m as well vath a noun phrase that consasts of 0 or 1 determiner an adjectwe and the lateral manner the 0 after the detenmner and the hteral mdacate that these words are not copied into the value for a manner role so that the value to the manner semrel becomes only the adjectwe that as recogmzed the second stage of the analysis uses the populated lexacal database to compare senses and make the selectaons this process follows the general methodology used m senseval lltkowska to appear specifically m the defimtaon comparison we first exanune exclusaon cntena to rule out specific mappings these criteria include syntacuc properues e g a verb sense that is only transluve cannot map into one that is only mtransrave and collocataonal propertaes e g a sense that is used with a parucle cannot map into one that uses a different particle at the present tune these are used only rmmmally 33 we next score each viable sense based on rots semrels we increment the score ff the senses have a common hypernym or if a senses hypernyms belong to the same synset as the other senses hypernyms if a parucular sense conns a large number of synonyms that as no differentiae on the hypernym and they overlap consaderably m the synsets they evoke the score can be increased substanually currently we add 5 points for each match 3 we increment the score based on common semrels in tins amtml tmplementauon we have defimng patterns usually qmte nummal for recogmzmg instrument means location purpose source manner hasconstituents hasmembers ispartof locale and goal 4 we increment the score by 2 points when we have a common semrel and then by another 5 points when the value is dentacal or m the same synset after all possable increments to the scores have been made we then select the senses wth the lughest score finally we compare our selecuon with that of the gold standard to assess our mapping over all senses another way an wluch our methodology follows the senseval process as that at proceeds incrementally thus t ms not necessary to have a final perfect parse and mapping rouune we can make conunual refinements at any stage of the process and exarmne the overall effect as m senseval we may make changes to deal wath a particular phenomenon with the result that overall performance dechnes but wth a sounder basis for making subsequent amprovements results of componential analysis the gold standard analysis involves mapping 66 wordnet senses with 348 words into 102 hector senses with 1878 words using the method described above we obtained 35 out of 86 correct 3at the present tame we use wordnet to adentffy semreis we envaslon usmg the full semanlac network created by parsing all a dlcuonarys defimtaons thas would include a richer set of semrels than currently included m wordnet 4the defimng patterns are developed by hand we have onlyjust begun this effort so the current set ms somewhat impoverished mappmgs 407 a shght improvement over the 31 correct assignments usmg the stoplast word overlap techmque however as mentioned above the stophst techmque had aclueved 8 of its successes by matclung null assignments consadered on tlus basins t seems that the componentaal analysis techmque provides substantial mprovement in addition our technique erred on 4 cases by malang assagnments where none were made by the leracographer we suggest that these cases do conn some common elements of meaning and may conceivably not be construed as errors the mapping from wordnet to hector had relatavely few empty mappings senses for wtuch it was not possable to make an assignment these are the cases where at appears that the chetmnanes do not overlap and thus prowde a tentative mdacataon of where two dictionaries may have different coverage the cases of multiple assignments mchcate the degree ofamblgmty m the mapping the average m both darecuons between hector and wordnet were donunated by the mabdaty to obtain good dascnnunatton for the word semze thus tlus method identifies individual words where the scnnunatwe ablhty needs to be further refined perhaps more importantly the componentml analysis method exploits consaderably more wordnet hector mformauon than the word overlap methods whereas the stophst word overlap mapping was based on only 41 content words the componenual approach in the selected mappings had 228 hits in developing ats scores with only a small number of defining patterns comparison of dictionaries tel o 3 03 we next exanuned the nature of the mterrelalaons between parrs of chctaonanes wthout use of a gold standard to assess the process of mapping for tus purpose we mapped m both recttons between the paars wordnet hector w3 oald and w3 ahd we exanune dorrs lexacal knowledge base for the amphcatlons it may have m the mapping process neither wordnet nor hector are properly vewed as chcuonanes since there was no mtenuon to pubhsh them as such wordnet glosses are generally smaller 53 words per sense compared to hector 184 words per sense whach contains many words specff3nng selectmnal restnctons on the subject and object of the verbs hector was used primarily for a largescale sense tagging project the three formal dctmnanes were subject to rigorous pubhslung and style standards the average number of words per sense were 87 oald 7 1 ahd and 9 9 w3 wth an average of 3 4 62 and 120 senses per word each table shows the average number of senses being mapped the average number of assignments m the target dlctmnary the average number of senses for which no assagnment could be made the average number of muluple assignments per word and the average score of the assignments that were made wnhector 37 47 06 17 119 hectorwn 57 64 14 22 113 these points are further emphasized m the mapping between w3 and oald where the disparity between the empty and muluple assagnments indicate that we are mapping between dictionaries qmte disparate this tends to be the case not only for the enure set of words but also is evident for individual words where there is a considerable dspanty m the number of senses wtuch then dominate the overall dlspanty thus for example w3 has 41 defimuons for float while oald has 10 we tend to be unable to find the specific sense m going from w3 to oald because at is likely that we have many more specific defimtlons that are not present in the other direction we are hkely to have considerable ambiguity and multiple assignments w3oald oaldw3 w3 oald 120 78 60 18 99 34 60 07 32 86 34 a between w3 and ahd there ss less overall daspanty between the defimtaon sets although since w3 is tmabndged we stall have a relatavely lugh number of senses m w3 that do not appear to be present m ahd finally it should be noted that the scores for the published dictaonanes tend to be a little lower than for wordnet and hector tlus reflects the hkehhood that we have not extracted as much mformataon as we dad m parsing and analyzmg the defimtaon sets used m senseval w3 ahd oj q o w3ahd 120 115 40 36 90 ahdw3 6 2 9 1 1 2 4 1 9 1 we next considered dorrs lexacal database we first transformed her theta grids to syntactic spectflcataons transttave or lntransmttve and identtficataon of semreis e g where she identified an instr component we added such a semrel to the dimap sense we were able to identify a mappmg from wordnet to her senses for two words float and shake for wluch dorr has several entries however smce she has considerably more semanuc components than we are currently able to recogmze we dad not pursue this avenue any further at flus time more important than just mappmg between two words dorrs data mdacates the posstbday of further exploitation of a richer set of semanuc components spectfically as reported m olsen et al 1998 m descnbmg procedures for automatically acqumng thematic grids for mandann chinese t was noted that verbs that incorporate themauc elements m their meamng would not allow that element to appear m the complement structure thus by usmg dorrs thematic grids when verb are parsed m defimtaons it s possible to dentffy where partacular semantac components are lexicahzed and which others are transnutted through to the themauc grid complement or subcategonzataon pattern for the defimendum the transmisson of semantic components to the thematic gnd s also reflected overtly m many defimtlons for example shake has one definition to bnng to a specified condatton by or as ffby repeated qmck jerky movements we would thus expect that the thematac grid for this defimtaon should include a goal and deed dorrs database has two senses whch reqmre a goal as part of their thematic grid smularly for many defimtaons m the sample set we dentlfied a source defimng pattern based on the word from frequently the object of the preposmon was the word source ttseff mdacatmg that the subcategonzauon properties of the defimendum should elude a source component discussion wlule the improvement m mapping by using the componentaal analysis techmque over the word overlap methods is modest we consider these results qmte slgmficant m wew of the very small number of defimng patterns we have implemented most of the improvement stems from the word substatuuon pnnclple described earlier as ewdenced by the preponderance of 5 point scores this techmque also provides a mechamsm for bnngmg back the stop words wz the preposmons wluch are the careers of mformatmn about semrels the 2 point scores the more general conclusion from the word subsutuuon is that the success arises from no longer considenng a defimtmn m solation the proper context for a word and its defimtions consists not lust of the words that make up the definition but also the total semantac network represented by the dictaonary we have aclueved our results by explomng only a small part of that network we have moved only a few steps to that network beyond the mdawdual words and their definitions we would expect that further expansmn first by the addon of further and mproved semrel defining patterns and second through the identaficataon of more pnmmve semanuc components will add considerably to our abflay to map between lexacal resources we also expect mprovements from consideration of other techniques such as attempts at ontology ahgnment hovy 1998 although tile definition analysis provlded here was performed on definmons with a stogie language the vanous meamng components m m m m m m m m 35 correspond to those used in an interhngua the use of the exuncuon method developed m order to charactenze verbs m another language clunese can frmtfully be applied here as well two further observauons about tlus process can be made the first is that rchance on a wellestablished semantic network such as wordnets not necessary the componenual analysis method rehes on the local neighborhood of words m the defimuons not on the completeness of the network indeed the network tsel can be bootstrapped based on the parsing results the method can work vath any semanuc network or ontology and may be used to refine or flesh out the network or ontology the second observation is that it is not necessary to have a wellestabhshed gold standard any mapping vail do all that is necessary is for any mvesugator lemcographer or not to create a judgmental mappmg the methods employed here can then quanufy ttus mapping based on a word overlap analysis and then further examine tt based on the componenaal analysis the componenual analysis method can then be used to exanune underlying subtleues and nuances tn the defimuous wluch a lemcographer or analyst can then examine m further detail to assess the mapping future work tlus work has marked the first ume that all the necessary mfrastructure has been combmed tn a rudimentary form because of its rudimentary status the opportumues for improvement are quite extensive in addluon there are many opportumues for using the techmques descnbed here m further nlp apphcatlons first the techmques described here have immediate apphcabtllty as part of a lexicographers workstauon when defimuons are parsed and semrels are zdenttfied the resulung data structures can be apphed against a corpus of instances for parucular words as m senseval for improving wordsense disamblguauon the techmques will also permit comparing an entry vath itself to deternune the mterrelattonshtps among ts defimuons and of companng the defimuons of two synonyms to deternune the amount of overlap between them on a defimtlon by defimuon bass although the analyss here has focused on the parsing of defimuous the development of defimng patterns clearly extends to generalized text parsing since the defimng patterns have been incorporated mto the same chcttonary used for parsing free text the patterns can be used threctly to identify the presence of parucular semrels among sentenual consutuents we are working to integrate ths funcuonahty into our wordsense sambiguauon techruques both the defimng patterns and the semrels even further mt seems that matclung defimng patterns in free text can be used for lextcal acquisition textual matenal that contains these patterns could concewably be flagged as providing defimuonal matenal which can then be compared to emstmg defimuons to assess whether their use ts cousstent vath these defimuons and ff not at least to flag the inconsistency the tecluuques descnbed here can be apphed directly to the fields of ontology development and analysis of ternunologlcal databases for ontoiogles vath or wthout defimuons the methods employed can be used to compare entries m daierent ontologles based pnmanly on the relattous m the ontology both luerarclucal and other for ternunologlcal databases the methods descnbed here can be used to exanune the set of conceptual relauons lmphed by the defimtmus the defimuon parsing wall facdtate the development of the termmologca i network tn the pamcular field covered by the database the componenual analysts methods result m a richer semantic network that can be used m other apphcattous thus for example t ts possible to extend the leracal chatmng methods described m green 1997 which are based on the semrels used m wordnet the semrels developed with the componenttal analysis method would provide additional detad available for apphcauon of lexlcal cohesion methods in particular addtuonal relattous would penmt some structunng wmthm the individual leracal chams rather than just consldenng each cham as an amorphous set green 1999 finally we are currently investigating the use of the componenual analysts techmque for mformauon extracuon the techmque identifies from defimtlous slots that can be used as slots or fields m template generataon once these slots are identified we wall be attemptmg to extract slot values from items m large catalog databases mdhons of items 36 in conclusion it would seem that instead of a paucity of tnformation allovang us to compare lexmal resources by bnngmg m the full semantic network of the lexicon we are overwhelmed with a plethora of data acknowledgments i would like to thank bonnie dorr chnstiane fellbaum steve green ed hovy ramesh knshnamurthy bob krovetz thomas potter lucy vanderwende and an anonymous reviewer for their comments on an earlier draft of this paper references atlans b t s 1991 bmldmga lexicon the contribution of lexicography lnternattonal journal of lextcography 43 167204 cl research 1999a cl research demos httpwww clres comdemo html cl research 1999b dmtlonary parsing project httpwww clres comdpp html dolan w b 1994 59 aug word sense amblguation chistenng related senses coling94 the 15th international conference on computational linguistics kyoto japan green s j 1997 automatically generating hypertext by computing semantic smulanty dlss toronto canada umverstty of toronto green s j sjgreenmn mq edu au 1999 1 june rich semantic networks hovy e 1998 may combining and standardizing largescale practical ontologms for machine translation and other uses language resources and evaluation conference granada spam kalgarnff a 1998 senseval home page httpwww itn bton ac ukeventssenseval krovetz r 1992 june senselinking m a machine readable dictionary 30th annual meeting of the association for computational lmgustics newark delaware association for computational lmgtustics lesk m 1986 automatic sense dlsamblguation using machine readable dmttonanes how to tell a pine cone from an ice cream cone proceechngs of sigdoc lttkowski k c 1978 models of the semantic structure of dictionaries american journal of computattonal lmgutsttcs atf 81 2574 lttkowskl k c to appear senseval the cl research expenence computers and the humamttes mtller g a beckwlth r fellbaum c gross d miller k j 1990 introduction to wordnet an onhne lexical database lnternatwnal journal of lexicography 34 235244 olsen m b dorr b j thomas s c 1998 2831 october enhancmg automatic acqulsmon of thematic structure in a largescale lexacon for mandann chinese tlurd conference of the association for machine translation m the americas amta98 langhorne pa ,0,1,0
on the british national corpus bnc using lins 1998 similarity method we retrieve the following neighbors for the first and second sense respectively 1 ,0,1,0
as described in section 3 we retrieved neighbors using lins 1998 similarity measure on a rasp parsed briscoe and carroll 2002 version of the bnc ,0,1,0
the best accuracies are observed when the labelsarecreatedfromdistributionallysimilarwords using lins 1998 dependencybased similarity measure depend ,0,1,0
lins 1998 informationtheoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised wsd mccarthy et al 2004 ,1,0,0
feature comparison measures to convert two feature sets into a scalar value several measures have been proposed such as cosine lins measure lin 1998 kullbackleibler kl divergence and its variants ,0,1,0
lins measure lin 1998 proposed a symmetrical measure par lin s t summationtext ff s f t wsfwtf summationtext ff s wsf summationtext ff t wtf where f s and f t denote sets of features with positive weights for words s and t respectively ,0,1,0
 three kmeans algorithms using different distributional similarity or dissimilarity measures cosine skew divergence lee 1999 4 and lins similarity lin 1998 ,0,1,0
others proposed distributional similarity measures between words hindle 1990 lin 1998 lee 1999 weeds et al 2004 ,0,1,0
405 prf 1 proposed 383 437 408 multinomial mixture 360 374 367 newman 2004 318 353 334 cosine 603 114 192 skew divergence lee 1999 730 155 255 lins similarity lin 1998 691 096 169 cbc lin and pantel 2002 981 060 114 table 3 precision recall and fmeasure ,0,1,0
applications of word clustering include language modeling brown et al 1992 text classification baker and mccallum 1998 thesaurus construction lin 1998 and so on ,0,1,0
within the nlp community nbest list ranking has been looked at carefully in parsing extractive summarization barzilay et al 1999 hovy and lin 1998 and machine translation zhang et al 2006 to name a few ,0,1,0
following lin 1998 we use syntactic dependencies between words to model their semantic properties ,0,1,0
for each word in the ldv we consulted three existing thesauri rogets thesaurus roget 1995 collins cobuild thesaurus collins 2002 and wordnet fellbaum 1998 ,0,1,0
various methods hindle 1990 lin 1998 of automatically acquiring synonyms have been proposed ,0,1,0
41 features we used a dependency structure as the context for words because it is the most widely used and one of the best performing contextual information in the past studies ruge 1997 lin 1998 ,1,0,0
texts are represented by dependency parse trees using the minipar parser lin 1998b and templates by parse subtrees ,0,1,0
among these measures the most important are wu palmers wu and palmer 1994 resniks resnik 1995 and lins lin 1998 ,1,0,0
where pantel and lin use lins 1998 measure we use wu and palmers 1994 measure ,0,1,0
one of the most important is lins 1998 ,1,0,0
4 experiments and results 41 set up we parsed the 3 gb aquaint corpus voorhees 2002 using minipar lin 1998b and collected verbobject and verbsubject frequencies building an empirical mi model from this data ,0,1,0
lin 1998as similar word list for eat misses these but includes sleep ranked 6 and sit ranked 14 because these have similar subjects to eat ,0,1,0
discriminative contextspecific training seems to yield a better set of similar predicates eg the highestranked contexts for dspcooc on the verb join3 lead 142 rejoin 139 form 134 belong to 131 found 131 quit 129 guide 119 induct 119 launch subj 118 work at 114 give a better simsjoin for equation 1 than the top similarities returned by lin 1998a participate 0164 lead 0150 return to 0148 say 0143 rejoin 0142 sign 0142 meet 0142 include 0141 leave 0140 work 0137 other features are also weighted intuitively ,0,1,0
we also test an mi model inspired by erk 2007 misimnv log summationdisplay nsimsn simnn prvn prvprn we gather similar words using lin 1998a mining similar verbs from a comparablesized parsed corpus and collecting similar nouns from a broader 10 gb corpus of english text4 we also use keller and lapata 2003s approach to obtaining webcounts ,0,1,0
erk 2007 compared a number of techniques for creating similarword sets and found that both the jaccard coefficient and lin 1998as informationtheoretic metric work best ,1,0,0
they have been successfully applied in several tasks such as information retrieval salton et al 1975 and harvesting thesauri lin 1998 ,0,1,0
two lus close in the space are likely to be in a paradigmatic relation ie to be close in a isa hierarchy budanitsky and hirst 2006 lin 1998 pado 2007 ,0,1,0
this similarity score is computed as a max over a number of component scoring functions some based on external lexical resources including various string similarity functions of which most are applied to word lemmas measures of synonymy hypernymy antonymy and semantic relatedness including a widelyused measure due to jiang and conrath 1997 based on manually constructed lexical resources such as wordnet and nombank a function based on the wellknown distributional similarity metric of lin 1998 which automatically infers similarity of words and phrases from their distributions in a very large corpus of english text the ability to leverage external lexical resources both manually and automatically constructedis critical to the success of manli ,1,0,0
for each word pair from the antonym set we calculated the distributional distance between each of their senses using mohammad and hirsts 2006 method of concept distance along with the modified form of lins 1998 distributional measure equation 2 ,0,1,0
again we used mohammad and hirsts 2006 method along with lins 1998 distributional measure to determine the distributional closeness of two thesaurus concepts ,0,1,0
curran 2002 and lin 1998 use syntactic features in the vector definition ,0,1,0
accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation resnik 1995 synonym extraction lin 1998a and automatic thesauri generation curran 2002 ,0,1,0
method correlation edgecounting 0664 jiang conrath 1998 0848 lin 1998a 0822 resnik 1995 0745 li et al ,0,1,0
lin 1998b defined the similarity between two concepts as the information that is in common to both concepts and the information contained in each individual concept ,0,1,0
pereira et al1993 curran and moens 2002 and lin 1998 use syntactic features in the vector definition ,0,1,0
wiebe 2000 uses lin 1998a style distributionally similar adjectives in a clusterandlabel process to generate sentiment lexicon of adjectives ,0,1,0
3httpwwwopenofficeorg another corpora based method due to turney and littman 2003 tries to measure the semantic orientation ot for a term t by ot summationdisplay tis pmitti summationdisplay tjs pmittj where s and s are minimal sets of polar terms that contain prototypical positive and negative terms respectively and pmitti is the pointwise mutual information lin 1998b between the terms t and ti ,0,1,0
our approach to stc uses a thesaurus based on corpus statistics lin 1998 for realvalued similarity calculation ,0,1,0
some researchers hindle 1990 grefenstette 1994 lin 1998 classify terms by similarities based on their distributional syntactic patterns ,0,1,0
a wide range of contextual information such as surrounding words lowe and mcdonald 2000 curran and moens 2002a dependency or case structure hindle 1990 ruge 1997 lin 1998 and dependency path lin and pantel 2001 pado and lapata 2007 has been utilized for similarity calculation and achieved considerable success ,0,1,0
for each word in ldv three existing thesauri are consulted rogets thesaurus roget 1995 collins cobuild thesaurus collins 2002 and wordnet fellbaum 1998 ,0,1,0
we adopt the similarity score proposed by lin 1998 as the distributional similarity score and use 50 nearest neighbours in line with mccarthy et al for the random baseline we select one word sense at random for each word token and average the precision over 100 trials ,0,1,0
2 related work thisworkbuildsuponthatofmccarthyetal2004 which acquires predominant senses for target words from a large sample of text using distributional similarity lin 1998 to provide evidence for predominance ,0,1,0
in this approach we extend the denition overlap by considering the distributional similarity lin 1998 rather than identify of the words in the two denitions ,0,1,0
mccarthy et al use a distributional similarity thesaurus acquired from corpus data using the method of lin 1998 for nding the predominant sense of a word where the senses are dened by wordnet ,0,1,0
let w be a target word and nw fn1n2nkg be the ordered set of the top scoring k neighbours of w from the thesaurus with associated distributional similarity scores fdsswn1dsswn2dsswnkg using lin 1998 ,0,1,0
we use the similarity proposed by lin 1998 ,0,1,0
the thesaurus was produced using the metric described by lin 1998 with input from the grammatical relation data extracted using the 90 million words of written english from the british national corpus bnc leech 1992 using the rasp parser briscoe and carroll 2002 ,0,1,0
the common types of features include contextual lin 1998 cooccurrence yang and callan 2008 and syntactic dependency pantel and lin 2002 pantel and ravichandran 2004 ,0,1,0
inspired by the conjunction and appositive structures riloff and shepherd 1997 roark and charniak 1998 used cooccurrence statistics in local context to discover sibling relations ,0,1,0
clusteringbased approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors brown et al 1992 lin 1998 ,0,1,0
the second uses lin dependency similarity a syntacticdependency based distributional word similarity resource described in lin 1998a9 ,0,1,0
while kazama and torisawa used a chunker we parsed the definition sentence using minipar lin 1998b ,0,1,0
semantic dsn the construction of this network is inspired by lin 1998 ,0,1,0
corpora and corpus query tools has been particularly significant in the area of compiling and developing lexicographic materials kilgarriff and rundell 2002 and in the area of creating various kinds of lexical resources such as wordnet fellbaum 1998 and framenet atkins et al 2003 fillmore et al 2003 ,0,1,0
our next steps will be to take a closer look at the following work clustering of similar words lin 1998 topic signatures lin and hovy 2000 and kilgariffs sketch engine kilgarriff et al 2004 ,0,1,0
the earliest work in this direction are those of hindle 1990 lin 1998 dagan et al 1999 chen and chen 2000 geffet and dagan 2004 and weeds and weir 2005 ,0,1,0
lin 1998 proposed a word similarity measure based on the distributio nal pattern of words which allows to construct a thesaurus using a parsed corpus ,0,1,0
2004 we use k 50 and obtain our thesaurus using the distributional similarity metric described by lin 1998 ,0,1,0
thus we rank each sense wsi wsw using prevalence score wsi 11 njnw dssnj wnsswsinj wsiwsw wnsswsinj where the wordnet similarity score wnss is defined as wnsswsinj max nsxnsnj wnsswsinsx 22 building the thesaurus the thesaurus was acquired using the method described by lin 1998 ,0,1,0
concept similarity is often measured by vectors of cooccurrence with context words that are typed with dependency information lin 1998 curran and moens 2002 ,0,1,0
whereas dependency based semantic spaces have been shown to surpass other word space models for a number of problems pad and lapata 2007 lin 1998 for the task of categorisation simple pattern based spaces have been shown to perform equally good if not better poesio and almuhareb 2005b almuhareb and poesio 2005b ,1,0,0
in particular we work with dependency paths that can reach beyond direct dependencies as opposed to lin 1998 but in the line of pado and lapata 2007 ,0,1,0
as a basis mapping function we used a generalisation of the one used by grefenstette 1994 and lin 1998 ,0,1,0
example of such algorithms are pereira et al 1993 and lin 1998 that use syntactic features in the vector definition ,0,1,0
pereira 1993 curran 2002 and lin 1998 use syntactic features in the vector definition ,0,1,0
we used an implementation of mcdonald 2006forcomparisonofresultsclarkeandlapata 2007 ,0,1,0
more recently clarke and lapata 2007 use centering theory grosz et al 1995 and lexical chains morris and hirst 1991 to identify which information to prune ,0,1,0
this framework is 211 commonly used in generation and summarization applications where the selection process is driven by multiple constraints marciniak and strube 2005 clarke and lapata 2007 ,0,1,0
in prior research ilp was used as a postprocessing step to remove redundancy and make other global decisions about parameters mcdonald 2007 marciniak and strube 2005 clarke and lapata 2007 ,0,1,0
clarke and lapata 2007 included discourse level features in their framework to leverage context for enhancing coherence ,0,1,0
41 corpora sentence compression systems have been tested on product review data from the ziffdavis zd henceforth corpus by knight and marcu 2000 general news articles by clarke and lapata cl henceforth corpus 2007 and biomedical articles lin and wilbur 2007 ,0,1,0
in the first set of experiments we compare two settings of our ualign system with other aligners giza union och and ney 2003 and leaf with 2 iterations fraser and marcu 2007 ,0,1,0
besides precision recall and balanced fmeasure we also include an fmeasure variant strongly biased towards recall 0b01 which fraser and marcu 2007 found to be best to tune their leaf aligner for maximum mt accuracy ,0,1,0
1 introduction word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research for example brown et al 1993 ittycheriah and roukos 2005 fraser and marcu 2007 including work leveraging syntactic parse trees eg cherry and lin 2006 denero and klein 2007 fossum et al 2008 ,0,1,0
12 related work recently discriminative methods for alignment have rivaled the quality of ibm model 4 alignments liu et al 2005 ittycheriah and roukos 2005 taskar et al 2005 moore et al 2006 fraser and marcu 2007b ,0,1,0
in contrast to the semisupervised leaf alignment algorithm of fraser and marcu 2007b which requires 15002000 cpu days per iteration to align 84m chineseenglish sentences anonymous pc link deletion requires only 450 cpu hours to realign such a corpus after initial alignment by giza which requires 2024 cpu days ,0,1,0
however fraser and marcu 2007a show that in phrasebased translation improvements in aer or fmeasure do not necessarily correlate with improvements in bleu score ,0,1,0
they propose two modifications to fmeasure varying the precisionrecall tradeoff and fullyconnecting the alignment links before computing fmeasure11 weighted fullyconnected fmeasure given a hypothesized set of alignment links h and a goldstandard set of alignment links g we define h fullyconnecth and g fullyconnectg and then compute fmeasureh 1 precisionh 1 recallh for phrasebased chineseenglish and arabicenglish translation tasks fraser and marcu 2007a obtain the closest correlation between weighted fullyconnected alignment fmeasure and bleu score using 05 and 01 respectively ,0,1,0
probabilistic generative models like ibm 15 brown et al 1993 hmm vogel et al 1996 itg wu 1997 and leaf fraser and marcu 2007 define formulas for pf e or pe f with okvoon ororok sprok atvoon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat okdrubel okvoon anok plok sprok atdrubel atvoon pippat rrat dat okvoon anok drok brok jok atvoon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok okyurp totat nnat quat oloat atyurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat figure 1 word alignment exercise knight 1997 ,0,1,0
carpuat and wu 2007b integrated a wsd system into a phrasebased smt system pharaoh koehn 2004a ,0,1,0
furthermore they extended wsd to phrase sense disambiguation psd carpuat and wu 2007a ,0,1,0
carpuat and wu 2007b and chan et al ,0,1,0
similar to wsd carpuat and wu 2007a used contextual information to solve the ambiguity problem for phrases ,1,0,0
recently wordsense disambiguation wsd methods have been shown to improve translation quality chan et al 2007 carpuat and wu 2007 ,1,0,0
wsd is one of the fundamental problems in natural language processing and is important for applications such as machine translation mt chan et al 2007a carpuat and wu 2007 information retrieval ir etc wsd is typically viewed as a classification problem where each ambiguous word is assigned a sense label from a predefined sense inventory during the disambiguation process ,1,0,0
another wsd approach incorporating contextdependent phrasal translation lexicons is given in carpuat and wu 2007 and has been evaluated on several translation tasks ,0,1,0
second instead of disambiguating phrase senses as in carpuat and wu 2007 we model word selection independently of the phrases used in the mt models ,0,1,0
the senses are 1 material from cellulose 2 report 3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object inventory is suitable for which application other than crosslingual applications where the inventory can be determined from parallel data carpuat and wu 2007 chan et al 2007 ,0,1,0
there has been considerable skepticism over whether wsd will actually improve performance of applications but we are now starting to see improvement in performance due to wsd in crosslingual information retrieval clough and stevenson 2004 vossen et al 2006 and machine translation carpuat and wu 2007 chan et al 2007 and we hope that other applications such as questionanswering text simplication and summarisation might also benet as wsd methods improve ,1,0,0
promising features might include those over source side reordering rules wang et al 2007 or source context features carpuat and wu 2007 ,1,0,0
on the other hand integrating an additional component into a baseline smt system is notoriously tricky as evident in the research on integrating word sense disambiguation wsd into smt systems different ways of integration lead to conflicting conclusions on whether wsd helps mt performance chan et al 2007 carpuat and wu 2007 ,1,0,0
carpuat and wu 2007 approached the issue as a word sense disambiguation problem ,0,1,0
maximum entropy estimation for translation of individual words dates back to berger et al 1996 and the idea of using multiclass classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroducedundertherubricofwordsensedisambiguation and generalized to substrings chan et al 2007 carpuat and wu 2007a carpuat and wu 2007b ,0,1,0
in statistical machine translation smt recent work shows that wsd helps translation quality when the wsd system directly uses translation candidates as sense inventories carpuat and wu 2007 chan et al 2007 gimenez and marquez 2007 ,1,0,0
even the recent generation of smt models that explicitly use wsd modeling to perform lexical choice rely on sentence context rather than wider document context and translate sentences in isolation carpuat and wu 2007 chan et al 2007 gimenez and marquez 2007 stroppa et al 2007 specia et al 2008 ,0,1,0
we are starting to see the beginnings of a positive effect of wsd in nlp applications such as machine translation carpuat and wu 2007 chan et al 2007 ,1,0,0
unlike a full blown machine translation task carpuat and wu 2007 annotators and systems will not be required to translate the whole context but just the target word ,0,1,0
several studies have demonstrated that for instance statistical machine translation smt benefits from incorporating a dedicated wsd module chan et al 2007 carpuat and wu 2007 ,1,0,0
a simple example is shown in figure 1 where the arc between a and hat indicates that hat is the head of a current statistical dependency parsers perform better if the dependency lengthes are shorter mcdonald and nivre 2007 ,0,1,0
the corresponding unlabeled figures are 733 and 3343 this confirms the results of previous studies showing that the pseudoprojective parsing technique used by maltparser tends to give high precision given that nonprojective dependencies are among the most difficult to parse correctly but rather low recall mcdonald and nivre 2007 ,0,1,0
3 maltparser maltparser nivre et al 2007b is a languageindependent system for datadriven dependency parsing based on a transitionbased parsing model mcdonald and nivre 2007 ,0,1,0
we then describe the two main paradigms for learning and inference in this years shared task as well as in last years which we call transitionbased parsers section 52 and graphbased parsers section 53 adopting the terminology of mcdonald and nivre 20075 finally we give an overview of the domain adaptation methods that were used section 54 ,0,1,0
as shown by mcdonald and nivre 2007 the single malt parser tends to suffer from two problems error propagation due to the deterministic parsing strategy typicallyaffectinglongdependenciesmorethan short ones and low precision on dependencies originating in the artificial root node due to fragmented parses9 the question is which of these problems is alleviatedbythemultipleviewsgivenbythecomponent parsers in the blended system ,0,1,0
a solution that leverages the complementary strengths of these two approachesdescribed in detail by mcdonald and nivre 2007was recently and successfully explored by nivre and mcdonald 2008 ,1,0,0
however they make different types of errors which can be seen as a reflection of their theoretical differences mcdonald and nivre 2007 ,0,1,0
2007 and nivre and mcdonald 2008 can be seen as methods to combine separately defined models ,0,1,0
the terms graphbased and transitionbased were used by mcdonald and nivre 2007 to describe the difference between mstparser mcdonald and pereira 2006 which is a graphbased parser with an exhaustive search decoder and maltparser nivre et al 2006 which is a transitionbased parser with a greedy search decoder ,0,1,0
mcdonald and nivre 2007 showed that the mstparser and maltparser produce different errors ,0,1,0
in the field of parsing mcdonald and nivre 2007 compared parsing errors between graphbased and transitionbased parsers ,0,1,0
in examining the combination of the two types of parsing mcdonald and nivre 2007 utilized similar approaches to our empirical analysis ,0,1,0
also mcdonald and nivre 2007 ,0,1,0
the reason may be that shorter dependencies are often modifier of nouns such as determiners or adjectives or pronouns modifying their direct neighbors while longer dependencies typically represent modifiers of the root or the main verb in a sentencemcdonald and nivre 2007 ,0,1,0
sentence length the longer the sentence is the poorer the parser performs mcdonald and nivre 2007 ,0,1,0
dependency lengths longdistance dependencies exhibit bad performance mcdonald and nivre 2007 ,0,1,0
looking rst at learning times it is obvious that learning time depends primarily on the number of training instances which is why we can observe a difference of several orders of magnitude in learning time between the biggest training set czech and the smallest training set slovene 14 this is shown by nivre and scholz 2004 in comparison to the iterative arcstandard algorithm of yamada and matsumoto 2003 and by mcdonald and nivre 2007 in comparison to the spanning tree algorithm of mcdonald lerman and pereira 2006 ,0,1,0
practically all datadriven models that have been proposed for dependency parsing in recent years can be described as either graphbased or transitionbased mcdonald and nivre 2007 ,0,1,0
in order to get a better understanding of these matters we replicate parts of the error analysis presented by mcdonald and nivre 2007 where parsing errors are related to different structural properties of sentences and their dependency graphs ,0,1,0
as expected malt and mst have very similar accuracy for short sentences but malt degrades more rapidly with increasing sentence length because of error propagation mcdonald and nivre 2007 ,0,1,0
both models have been used to achieve stateoftheart accuracy for a wide range of languages as shown in the conll shared tasks on dependency parsing buchholz and marsi 2006 nivre et al 2007 but mcdonald and nivre 2007 showed that a detailed error analysis reveals important differences in the distribution of errors associated with the two models ,0,1,0
this difference was highlighted in the 3httpw3msivxusejhamaltparser studyofmcdonaldandnivre2007 whichshowed that the difference is reflected directly in the error distributions of the parsers ,0,1,0
7an alternative framework that formally describes some dependency parsers is that of transition systems mcdonald and nivre 2007 ,0,1,0
the experimental results in mcdonald and nivre 2007 show a negative impact on the parsing accuracy from too long dependency relation ,0,1,0
4 dependency parsing baseline 41 learning model and features according to mcdonald and nivre 2007 all datadriven models for dependency parsing that have been proposed in recent years can be described as either graphbased or transitionbased ,0,1,0
the majority of these systems used models belonging to one of the twodominantapproachesindatadrivendependency parsinginrecentyearsmcdonaldandnivre2007 in graphbased models every possible dependency graph for a given input sentence is given a score that decomposes into scores for the arcs of the graph ,0,1,0
there are also attempts at a more finegrained analysis of accuracy targeting specific linguistic constructions or grammatical functions carroll and briscoe 2002 kubler and prokic 2006 mcdonald and nivre 2007 ,0,1,0
second mcdonald and satta 2007 propose an on5 algorithm for computing the marginals as opposed to the on3 matrixinversion approach used by smith and smith 2007 and ourselves ,0,1,0
for example both papers propose minimumrisk decoding and mcdonald and satta 2007 discuss unsupervised learning and language modeling while smith and smith 2007 define hiddenvariable models based on spanning trees ,0,1,0
similar adaptations of the matrixtree theorem have been developed independently and simultaneouslybysmithandsmith2007andmcdonaldand satta 2007 see section 5 for more discussion ,0,1,0
for nonprojective parsing the analogy to the inside algorithm is the on3 matrixtree algorithm which is dominated asymptotically by a matrix determinant smith and smith 2007 koo et al 2007 mcdonald and satta 2007 ,0,1,0
2007 and smith and smith 2007 show how to employ the matrixtree theorem ,0,1,0
the approach has been shown to give improvements over the map classifier in many areas of natural language processing including automatic speech recognition goel and byrne 2000 machine translation kumar and byrne 2004 zhang and gildea 2008 bilingual word alignment kumar and byrne 2002 andparsinggoodman 1996 titovandhenderson 2006 smith and smith 2007 ,1,0,0
it is often straightforward to obtain large amounts of unlabeled data making semisupervised approaches appealing previous work on semisupervised methods for dependency parsing includes smith and eisner 2007 koo et al 2008 wang et al 2008 ,0,1,0
we used a nonprojective model trained using an application of the matrixtree theorem koo et al 2007 smith and smith 2007 mcdonald and satta 2007 for the firstorder czech models and projective parsers for all other models ,0,1,0
note that it is straightforward to calculate these expected counts using a variant of the insideoutside algorithm baker 1979 applied to the eisner 1996 dependencyparsing data structures paskin 2001 for projective dependency structures or the matrixtree theorem koo et al 2007 smith and smith 2007 mcdonald and satta 2007 for nonprojective dependency structures ,0,1,0
smith and smith 2007 ,0,1,0
this weak supervision has been encoded using priors and initializations klein and manning 2004 smith 2006 specialized models klein and manning 2004 seginer 2007 bod 2006 and implicit negative evidence smith 2006 ,0,1,0
in this paper we use a nonprojective dependency tree crf smith and smith 2007 ,0,1,0
smith and eisner 2007 apply entropy regularization to dependency parsing ,0,1,0
mcdonald and satta 2007 smith and smith 2007 ,0,1,0
smith and smith 2007 describe a more efficient algorithm that can compute all edge expectations in on3 time using the inverse of the kirchoff matrix k1 ,1,0,0
minimizing risk has been shown to improve performance for mt kumar and byrne 2004 as well as other language processing tasks goodman 1996 goel and byrne 2000 kumar and byrne 2002 titov and henderson 2006 smith and smith 2007 ,1,0,0
followingtheworkofkooetal2007andsmith and smith 2007 it is possible to compute all expectations in on3 ln2 through matrix inversion ,0,1,0
2007 and smith and smith 2007 showed that the matrixtree theorem can be used to train edgefactored loglinearmodelsofdependencyparsing ,0,1,0
it is often straightforward to obtain large amounts of unlabeled data making semisupervised approaches appealing previous work on semisupervised methods for dependency parsing includes smith and eisner 2007 koo et al 2008 wang et al 2008 ,0,1,0
we used a nonprojective model trained using an application of the matrixtree theorem koo et al 2007 smith and smith 2007 mcdonald and satta 2007 for the firstorder czech models and projective parsers for all other models ,0,1,0
note that it is straightforward to calculate these expected counts using a variant of the insideoutside algorithm baker 1979 applied to the eisner 1996 dependencyparsing data structures paskin 2001 for projective dependency structures or the matrixtree theorem koo et al 2007 smith and smith 2007 mcdonald and satta 2007 for nonprojective dependency structures ,0,1,0
for example the topics sport and education are important cues for differentiating mentions of michael jordan which may refer to a basketball player a computer science professor etc second as noted in the top weps run chen and martin 2007 feature development is important in achieving good coreference performance ,0,1,0
mann and yarowsky 2003 chen and martin 2007 baron and freedman 2008 ,0,1,0
we base our work partly on previous work done by bagga and baldwin bagga and baldwin 1998 which has also been used in later work chen and martin 2007 ,0,1,0
chen and martin 2007 explored the use of a range of syntactic and semantic features in unsupervised clustering of documents ,0,1,0
for more detail see chen martin 2007 ,0,1,0
chen martin 2007 introduced one of those similarity schemes twolevel softtfidf ,0,1,0
standard sequence prediction models are highly effective for supertagging including hidden markov models bangalore and joshi 1999 nielsen 2002 maximum entropy markov models clark 2002 hockenmaier et al 2004 clark and curran 2007 and conditional random fields blunsom and baldwin 2006 ,0,1,0
recentworkconsidersadamagedtagdictionary by assuming that tags are known only for words that occur more than once or twice toutanova and johnson 2007 ,0,1,0
other work aims to do truly unsupervised learning of taggers such as goldwater and griffiths 2007 and johnson 2007 ,0,1,0
dirichlet priors can be used to bias hmms toward more skewed distributions goldwater and griffiths 2007 johnson 2007 which is especially useful in the weakly supervised setting consideredhere ,0,1,0
followingjohnson2007iusevariational bayes em beal 2003 during the mstep for the transition distribution l1ji fenij ifen i ci 3 fv expv 4 60 v braceleftbigg gv 1 2 ifv 7 v 1 1v ow ,0,1,0
for an hmm with a set of states t and a set of output symbols v t t t dir1t 1 t t t dir1v 2 titi1 ti1 multiti1 3 witi ti multiti 4 one advantage of the bayesian approach is that the prior allows us to bias learning toward sparser structures by setting the dirichlet hyperparameters to a value less than one johnson 2007 goldwater and griffiths 2007 ,0,1,0
there is evidence that this leads to better performance on some partofspeech induction metrics johnson 2007 goldwater and griffiths 2007 ,0,1,0
johnson 2007 evaluates both estimation techniques on the bayesian bitag model goldwater and griffiths 2007 emphasize the advantage in the mcmc approach of integrating out the hmm parameters in a tritag model yielding a tagging supported by many different parameter settings ,0,1,0
following the setup in johnson 2007 we initialize the transition and emission distributions to be uniform with a small amount of noise and run em and vb for 1000 iterations ,0,1,0
one option is what johnson 2007 calls manytoone mto1 accuracy in which each induced tag is labeled with its most frequent gold tag ,0,1,0
finally following haghighi and klein 2006 and johnson 2007 we can instead insist that at most one hmm state can be mapped to any partofspeech tag ,0,1,0
the studies presented by goldwater and griffiths 2007 and johnson 2007 differed in the number of states that they used ,0,1,0
goldwater and griffiths 2007 evaluated against the reduced tag set of 17 tags developed by smith and eisner 2005 while johnson 2007 evaluated against the full penn treebank tag set ,0,1,0
the largest corpus that goldwater and griffiths 2007 studied contained 96000 words while johnson 2007 used all of the 1173766 words in the full penn wsj treebank ,0,1,0
we ran each estimator with the eight different combinations of values for the hyperparameters and prime listed below which include the optimal values for the hyperparameters found by johnson 2007 and report results for the best combination for each estimator below 1 ,0,1,0
 prime 1 1 1 05 05 1 05 05 01 01 01 00001 00001 01 00001 00001 further we ran each setting of each estimator at least 10 times from randomly jittered initial starting points for at least 1000 iterations as johnson 2007 showed that some estimators require many iterations to converge ,0,1,0
expectation maximization does surprisingly well on larger data sets and is competitive with the bayesian estimators at least in terms of crossvalidation accuracy confirming the results reported by johnson 2007 ,0,1,0
monte carlo sampling methods and variational bayes are two kinds of approximate inference methods that have been applied to bayesian inference of unsupervised hmm pos taggers goldwater and griffiths 2007 johnson 2007 ,0,1,0
johnson 2007 compared two bayesian inference algorithms variational bayes and what we call here a pointwise collapsed gibbs sampler and found that variational bayes produced the best solution and that the gibbs sampler was extremely slow to converge and produced a worse solution than em ,0,1,0
the samplers that goldwater and griffiths 2007 and johnson 2007 describe are pointwise collapsed gibbs samplers ,0,1,0
recent advances in these approaches include the use of a fully bayesian hmm johnson 2007 goldwater and griffiths 2007 ,0,1,0
recent work johnson 2007 goldwater and griffiths 2007 gao and johnson 2008 explored the task of partofspeech tagging pos using unsupervised hidden markov models hmms with encouraging results ,0,1,0
recent work goldwater and griffiths 2007 johnson 2007 gao and johnson 2008 on this task explored a variety of methodologies to address this issue ,0,1,0
johnson 2007 and gao johnson 2008 assume that words are generated by a hidden markov model and find that the resulting states strongly correlate with pos tags ,0,1,0
the fact that different authors use different versions of the same gold standard to evaluate similar experiments eg goldwater griffiths 2007 versus johnson 2007 supports this claim ,0,1,0
johnson 2007 reports results for different numbers of hidden states but it is unclear how to make this choice a priori while goldwater griffiths 2007 leave this question as future work ,0,1,0
given the parameterspi0pikof the hmm the joint distribution over hidden states s and observationsy can be written with s0 0 psypi0pik tproductdisplay t1 pstst1pytst as johnson 2007 clearly explained training the hmm with em leads to poor results in pos tagging ,1,0,0
41 variational bayes beal 2003 and johnson 2007 describe variational bayes for hidden markov model in detail which can be directly applied to our bilingual model ,0,1,0
johnson 2007 and zhang et al ,0,1,0
importantly this bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories johnson 2007 ,0,1,0
similar to goldwater and griffiths 2007 and johnson 2007 toutanova and johnson 2007 also use bayesian inference for pos tagging ,0,1,0
nevertheless em sometimes fails to find good parameter values2 the reason is that em tries to assign roughly the same number of word tokens to each of the hidden states johnson 2007 ,0,1,0
there has been an increased interest recently in employing bayesian modeling for probabilistic grammars in different settings ranging from putting priors over grammar probabilities johnson et al 2007 to putting nonparametric priors over derivations johnson et al 2006 to learning the set of states in a grammar finkel et al 2007 liang et al 2007 ,0,1,0
mostcommonlyvariational johnson 2007 kurihara and sato 2006 or sampling techniques are applied johnson et al 2006 ,0,1,0
they are most commonly used for parsing and linguistic analysis charniak and johnson 2005 collins 2003 but are now commonly seen in applications like machine translation wu 1997 and question answering wang et al 2007 ,0,1,0
for example if we make a meanfield assumption with respect to hidden structure and weights the variationalalgorithmforapproximatelyinferringthe distribution over and trees y resembles the traditional em algorithm very closely johnson 2007 ,0,1,0
for instance on unsupervised partofspeech tagging em requires over 100 iterations to reach its peak performance on the wallstreet journal johnson 2007 ,0,1,0
unlike johnson 2007 who found optimal performance when was approximately 104 we observed monotonic increases in performance as dropped ,0,0,1
3 variational bayes for itg goldwater and griffiths 2007 and johnson 2007 show that modifying an hmm to include a sparse prior over its parameters and using bayesian estimation leads to improved accuracy for unsupervised partofspeech tagging ,0,1,0
however in experiments in unsupervised pos tag learning using hmm structured models johnson 2007 shows that vb is more effective than gibbs sampling in approaching distributions that agree with the zipfs law which is prominent in natural languages ,0,1,0
as pointed out by johnson 2007 in effect this expression adds to c a small value that asymptotically approaches 05 as c approaches and 0 as c approaches 0 ,0,1,0
bayesian approaches can also improve performance goldwater and griffiths 2007 johnson 2007 kurihara and sato 2006 ,0,1,0
recent projects in semisupervised toutanova and johnson 2007 and unsupervised biemann et al 2007 smith and eisner 2005 tagging also show significant progress ,1,0,0
6 smaller tagset and incomplete dictionaries previously researchers working on this task have also reported results for unsupervised tagging with a smaller tagset smith and eisner 2005 goldwater and griffiths 2007 toutanova and johnson 2008 goldberg et al 2008 ,0,1,0
the overall pos tag distribution learnt by em is relatively uniform as noted by johnson 2007 and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed ,0,1,0
this is also the main reason why most summarization systems applied to news articles do not outperform a simple baseline that just uses the first 100 words of an article svore et al 2007 nenkova 2005 ,1,0,0
5since the test data of svore et al 2007 is not publicly available we were unable to carry out a more detailed comparison ,0,0,1
our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of svore et al 2007 without requiring their thirdparty data resources ,0,0,1
they are not used in ln but they are known to be useful for wsd tanaka et al 2007 magnini et al 2002 ,0,1,0
although to a lesser extent measures of word relatedness have also been applied on other languages including german zesch et al 2007 zesch et al 2008 mohammad et al 2007 chinese wang et al 2008 dutch heylen et al 2008 and others ,0,1,0
also related are the areas of word alignment for machine translation och and ney 2000 induction of translation lexicons schafer and yarowsky 2002 and crosslanguage annotation projections to a second language riloff et al 2002 hwa et al 2002 mohammad et al 2007 ,0,1,0
measures of crosslanguage relatedness are useful for a large number of applications including crosslanguage information retrieval nie et al 1999 monz and dorr 2005 crosslanguage text classification gliozzo and strapparava 2006 lexical choice in machine translation och and ney 2000 bangalore et al 2007 induction of translation lexicons schafer and yarowsky 2002 crosslanguage annotation and resource projections to a second language riloff et al 2002 hwa et al 2002 mohammad et al 2007 ,0,1,0
2 related work the most commonly used similarity measures are based on the wordnet lexical database eg budanitsky and hirst 2006 hughes and ramage 2007 and a number of such measures have been made publicly available pedersen etal 2004 ,0,1,0
et al 2004 collinsthompson and callan 2005 hughes and ramage 2007 ,0,1,0
0 500 1000 1500 2000 5000 10000 15000 20000 25000 30000 number of interlanguage links vector length aren ares arro enes enro esro figure 5 number of interlanguage links vs vector length for the millercharles data set 0 500 1000 1500 2000 2500 3000 3500 4000 5000 10000 15000 20000 25000 30000 number of interlanguage links vector length aren ares arro enes enro esro figure 6 number of interlanguage links vs vector length for the wordsimilarity353 data set edge bases lesk 1986 wu and palmer 1994 resnik 1995 jiang and conrath 1997 hughes and ramage 2007 or on large corpora salton et al 1997 landauer et al 1998 turney 2001 gabrilovich and markovitch 2007 ,0,1,0
the dataset is available only in english and has been widely used in previous semantic relatedness evaluations eg resnik 1995 hughes and ramage 2007 zesch et al 2008 ,0,1,0
method source spearman strube and ponzetto 2006 wikipedia 019048 jarmasz 2003 wordnet 033035 jarmasz 2003 rogets 055 hughes and ramage 2007 wordnet 055 finkelstein et al 2002 web corpus wn 056 gabrilovich and markovitch 2007 odp 065 gabrilovich and markovitch 2007 wikipedia 075 svm web corpus wn 078 table 9 comparison with previous work for wordsim353 ,0,1,0
we want to note that our wordnetbased method outperforms that of hughes and ramage 2007 which uses a similar method ,0,0,1
our similarity method is similar but simpler to that used by hughes and ramage 2007 which report very good results on similarity datasets ,1,0,0
the techniques used to solve this problem can be roughly classified into two main categories those relying on preexisting knowledge resources thesauri semantic networks taxonomies or encyclopedias alvarez and lim 2007 yang and powers 2005 hughes and ramage 2007 and those inducing distributional properties of words from corpora sahami and heilman 2006 chen et al 2006 bollegala et al 2007 ,0,1,0
hughes and ramage 2007 present a lexical similarity model based on random walks on graphs derived from wordnet rao et al ,0,1,0
this is similartothegraphconstructionmethodofhughes and ramage 2007 and rao et al ,0,1,0
713 similarity via pagerank pagerank page et al 1998 is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization erkan and radev 2004 to opinion mining esuli and sebastiani 2007 to our task of lexical relatedness hughes and ramage 2007 ,0,1,0
hughes and ramage 2007 described the use of a biased pagerank over the wordnet graph to compute word pair semantic relatedness using the divergence of the probability values over the graph created by each word ,0,1,0
4 semantic class induction from wikipedia wikipedia has recently been used as a knowledge source for various language processing tasks including taxonomy construction ponzetto and strube 2007a coreference resolution ponzetto and strube 2007b and english ner eg bunescu and pasca 2006 cucerzan 2007 kazama and torisawa 2007 watanabe et al ,0,1,0
it is often straightforward to obtain large amounts of unlabeled data making semisupervised approaches appealing previous work on semisupervised methods for dependency parsing includes smith and eisner 2007 koo et al 2008 wang et al 2008 ,0,1,0
note that it is straightforward to calculate these expected counts using a variant of the insideoutside algorithm baker 1979 applied to the eisner 1996 dependencyparsing data structures paskin 2001 for projective dependency structures or the matrixtree theorem koo et al 2007 smith and smith 2007 mcdonald and satta 2007 for nonprojective dependency structures ,0,1,0
one option would be to leverage unannotated text mcclosky et al 2006 smith and eisner 2007 ,0,1,0
this generates tens of millions features so we prune those features that occur fewer than 10 total times as in smith and eisner 2007 ,0,1,0
finally recent work has explored learning to map sentences to lambdacalculus meaning representations wong and mooney 2007 zettlemoyer and collins 2005 zettlemoyer and collins 2007 ,0,1,0
recently a number of machine learning approaches have been proposed zettlemoyer and collins 2005 mooney 2007 ,0,1,0
for example when applying their approach to a different domain with somewhat less rigid syntax zettlemoyer and collins 2007 need to introduce new combinators and new forms of candidate lexical entries ,0,1,0
there has thus been a trend recently towards robust widecoverage semantic construction eg bos et al 2004 zettlemoyer and collins 2007 ,0,1,0
it has been used for a variety of tasks such as widecoverage parsing hockenmaier and steedman 2002 clark and curran 2007 sentence realization white 2006 learning semantic parsers zettlemoyer and collins 2007 dialog systems kruijff et al 2007 grammar engineering beavers 2004 baldridge et al 2007 and modeling syntactic priming reitter et al 2006 ,0,1,0
available scissor ge and mooney 2005 an integrated syntacticsemantic parser krisp kate and mooney 2006 an svmbased parser using string kernels wasp wong and mooney 2006 wong and mooney 2007 a system based on synchronous grammars zc zettlemoyer and collins 20073 a probabilistic parser based on relaxed ccg grammars and lu lu et al 2008 a generative model with discriminative reranking ,0,1,0
1 introduction recently researchers have developed algorithms that learn to map natural language sentences to representations of their underlying meaning he and young 2006 wong and mooney 2007 zettlemoyer and collins 2005 ,0,1,0
albeit simple the algorithm has proven to be very efficient and accurate for the task of parse selection collins and roark 2004 collins 2004 zettlemoyer and collins 2005 zettlemoyer and collins 2007 ,1,0,0
the parameters of the refined productions ax by cz where ax is a subcategory of a by of b and cz of c can then be estimated in various ways past work has included both generative matsuzaki et al 2005 liang et al 2007 and discriminative approaches petrov and klein 2008 ,0,1,0
there has been an increased interest recently in employing bayesian modeling for probabilistic grammars in different settings ranging from putting priors over grammar probabilities johnson et al 2007 to putting nonparametric priors over derivations johnson et al 2006 to learning the set of states in a grammar finkel et al 2007 liang et al 2007 ,0,1,0
in addition to the block sampler used by bhattacharya and getoor 2006 we are investigating generalpurpose splitmerge samplers jain and neal 2000 and the permutation sampler liang et al 2007a ,0,1,0
general grammars with infinite numbers of nonterminals were studied by liang et al 2007b ,0,1,0
our work differs from these previous approaches in that we explicitly model a prior over grammars within a bayesian framework4 models of grammar refinement petrov et al 2006 liang et al 2007 finkel et al 2007 also aim to automatically learn latent structure underlying treebanked data ,0,1,0
wed like to learn the number of paradigm classes from the data but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive statesplitting found in the ihmm and ipcfg liang et al 2007 ,0,1,0
first we can construct an infinite number of more specialized pcfgs by splitting or refining the pcfgs nonterminals into increasingly finer states this leads to the ipcfg or infinite pcfg liang et al 2007 ,0,1,0
recently methods from nonparametric bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks including language modeling word and morpheme segmentation parsing and machine translation teh et al 2006 goldwater et al 2006a goldwater et al 2006b liang et al 2007 finkel et al 2007 denero et al 2008 ,0,1,0
first we can let the number of nonterminals grow unboundedly as in the infinite pcfg where the nonterminals of the grammar can be indefinitely refined versions of a base pcfg liang et al 2007 ,0,1,0
4 semantic class induction from wikipedia wikipedia has recently been used as a knowledge source for various language processing tasks including taxonomy construction ponzetto and strube 2007a coreference resolution ponzetto and strube 2007b and english ner eg bunescu and pasca 2006 cucerzan 2007 kazama and torisawa 2007 watanabe et al ,0,1,0
wikipedia first sentence wikifs kazama and torisawa 2007 used wikipedia as an external knowledge to improve named entity recognition ,1,0,0
recently wikipedia is emerging as a source for extracting semantic relationships suchanek et al 2007 kazama and torisawa 2007 ,0,1,0
kazama and torisawa 2007 improve their fscore by 3 by including a wikipediabased feature in their machine learner ,1,0,0
41 extraction from definition sentences definition sentences in the wikipedia article were used for acquiring hyponymy relations by kazama and torisawa 2007 for named entity recognition ,0,1,0
hyponymy relations were extracted from definition sentences herbelot and copestake 2006 kazama and torisawa 2007 ,0,1,0
the most relevant to our work are kazama and torisawa 2007 toral and muoz 2006 and cucerzan 2007 ,0,1,0
similarly kazama and torisawa 2007 used wikipedia particularly the first sentence of each article to create lists of entities ,0,1,0
the small differences from their work are 1 we used characters as the unit as we described above 2 while kazama and torisawa 2007 checked only the word sequences that start with a capitalized word and thus exploitedthecharacteristicsofenglishlanguage we checked the matching at every character 3 we used a trie to make the lookup efcient ,0,1,0
for instance kazama and torisawa 2007 used the hyponymy relations extracted from wikipedia for the english ner and reported improved accuracies with such a gazetteer ,1,0,0
first the wikipedia gazetteer improved the accuracy as expected ie it reproduced the result of kazama and torisawa 2007 for japanese ner ,0,1,0
6 related work and discussion there are several studies that used automatically extracted gazetteers for ner shinzato et al 2006 talukdar et al 2006 nadeau et al 2006 kazama and torisawa 2007 ,0,1,0
on the other hand kazama and torisawa 2007 extracted hyponymy relations which are independent of the ne categories from wikipedia and utilized it as a gazetteer ,0,1,0
the previous studies with the exception of kazama and torisawa 2007 used smaller gazetteers than ours ,0,1,0
we also compared the cluster gazetteers with the wikipedia gazetteer constructed by following the method of kazama and torisawa 2007 ,0,1,0
kazama and torisawa 2007 extracted hyponymyrelationsfromtherstsentencesie dening sentences of wikipedia articles and then used them as a gazetteer for ner ,0,1,0
the method described by kazama and torisawa 2007 is to rst extract the rst base noun phrase after the rst is was are or were in the rst sentence of a wikipedia article ,0,1,0
some regarded wikipedia as the corpora and applied handcrafted or machinelearned rules to acquire semantic relations herbelot and copestake 2006 kazama and torisawa 2007 ruizcasado et al 2005 nastase and strube 2008 sumida et al 2008 suchanek et al 2007 ,0,1,0
the second baseline is our implementation of the relevant part of the wikipedia extraction in kazama and torisawa 2007 taking the first noun after a be verb in the definition sentence denoted as wikibl ,0,1,0
kazama and torisawa 2007 explores the first sentence of an article and identifies the first noun phrase following the verb be as a label for the article title ,0,1,0
as the most concise definition we take the first sentence of each article following kazama and torisawa 2007 ,0,1,0
becomp following the general idea in kazama and torisawa 2007 we identify the isa pattern in the definition sentence by extracting nominal complements of the verb be taking 451 no ,0,1,0
it turns out that while problems of coverage and ambiguity prevent straightforward lookup injection of gazetteer matches as features in machinelearning based approaches is critical for good performance cohen 2004 kazama and torisawa 2007a toral and munoz 2006 florian et al 2003 ,0,1,0
for example the entry about the microsoft in wikipedia has the following categories companies listed on nasdaq cloud computing vendors etc both toral and munoz 2006 and kazama and torisawa 2007a used the freetext description of the wikipedia entity to reason about the entity type ,0,1,0
ner proves to be a knowledgeintensive task and it was reassuring to observe that system resources used f1 lbjner wikipedia nonlocal features wordclass model 9080 suzuki and isozaki 2008 semisupervised on 1gword unlabeled data 8992 ando and zhang 2005 semisupervised on 27mword unlabeled data 8931 kazama and torisawa 2007a wikipedia 8802 krishnan and manning 2006 nonlocal features 8724 kazama and torisawa 2007b nonlocal features 8717 finkel et al 2005 nonlocal features 8686 table 7 results for conll03 data reported in the literature ,0,1,0
systems based on perceptron have been shown to be competitive in ner and text chunking kazama and torisawa 2007b punyakanok and roth 2001 carreras et al 2003 we specify the model and the features with the lbj rizzolo and roth 2007 modeling language ,0,1,0
we perform term disambiguation on each document using an entity extractor cucerzan 2007 ,0,1,0
even if the idea of using wikipedia links for disambiguation is not novel cucerzan 2007 it is applied for the first time to framenet lexical units considering a frame as a sense definition ,0,1,0
some researchers cucerzan 2007 nguyen and cao 2008 have explored the use of wikipedia information to improve the disambiguation process ,0,1,0
however most of them do not build a nes resource but exploit external gazetteers bunescu and pasca 2006 cucerzan 2007 ,0,1,0
4 semantic class induction from wikipedia wikipedia has recently been used as a knowledge source for various language processing tasks including taxonomy construction ponzetto and strube 2007a coreference resolution ponzetto and strube 2007b and english ner eg bunescu and pasca 2006 cucerzan 2007 kazama and torisawa 2007 watanabe et al ,0,1,0
ca 2006 and cucerzan 2007 in mining relationships between named entities or in extracting useful facet terms from news articles eg dakka and ipeirotis 2008 ,0,1,0
some of these have been previously employed for various tasks by gabrilovich and markovitch 2006 overell and ruger 2006 cucerzan 2007 and suchanek et al ,0,1,0
much later work evans 2003 etzioni et al 2005 cucerzan 2007 pasca 2004 relies on the use of extremely large corpora which allow very precise but sparse features ,1,0,0
the most relevant to our work are kazama and torisawa 2007 toral and muoz 2006 and cucerzan 2007 ,0,1,0
cucerzan 2007 by contrast to the above used wikipedia primarily for named entity disambiguation following the path of bunescu and paca 2006 ,0,1,0
wu and weld 2007 and cucerzan 2007 calculate the overlap between contexts of named entities and candidate articles from wikipedia using overlap ratios or similarity scores in a vector space model respectively ,0,1,0
crf baseline 9718 9721 table 7 pos tagging results of the previous top systems for ptb iii data evaluated by label accuracy system test additional resources jesscm crfhmm 9515 1gword unlabeled data 9467 15mword unlabeled data ando and zhang 2005 9439 15mword unlabeled data suzuki et al 2007 9436 17mword unlabeled data zhang et al 2002 9417 full parser output kudo and matsumoto 2001 9391 supervised crf baseline 9388 table 8 syntactic chunking results of the previous top systems for conll00 shared task data f1 score 3031 aug 1996 and 67 dec 1996 reuters news articles respectively ,0,1,0
as our approach for incorporating unlabeled data we basically follow the idea proposed in suzuki et al 2007 ,0,1,0
following this idea there have been introduced a parameter estimation approach for nongenerative approaches that can effectively incorporate unlabeled data suzuki et al 2007 ,0,1,0
in addition the calculation cost for estimating parameters of embedded joint pms hmms is independent of the number of hmms j that we used suzuki et al 2007 ,0,1,0
24 comparison with hybrid model ssl based on a hybrid generativediscriminative approach proposed in suzuki et al 2007 has been defined as a loglinear model that discriminatively combines several discriminative models pdi and generative models pgj such that ryx producttext i p di yxii producttext j p gj xjyjj summationtext y producttext i p di yxii producttext j p gj xjyjj where iii1 and iii1jijji1 ,0,1,0
as a solution a given amount of labeled training data is divided into two distinct sets ie 45 for estimating and the 667 remaining 15 for estimating suzuki et al 2007 ,0,1,0
surprisingly although jesscm is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure jesscm provides fscores of 9445 and 8803 for conll00 and 03 data respectively which are 015 and 083 points higher than those reported in suzuki et al 2007 for the same configurations ,0,0,1
networks toutanova et al 2003 9724 svm gimenez and marquez 2003 9705 me based a bidirectional inference tsuruoka and tsujii 2005 9715 guided learning for bidirectional sequence classification shen et al 2007 9733 adaboostsdf with candidate features 21100 wdist 9732 adaboostsdf with candidate features 21010 fdist 9732 svm with candidate features c01 d2 9732 text chunking f1 regularized winnow full parser output zhang et al 2001 9417 svmvoting kudo and matsumoto 2001 9391 aso unlabeled data ando and zhang 2005 9439 crfrerankingkudo et al 2005 9412 me based a bidirectional inference tsuruoka and tsujii 2005 9370 laso approximate large margin update daume iii and marcu 2005 944 hysol suzuki et al 2007 9436 adaboostsdf with candidate featuers 21 wdist 9432 adaboostsdf with candidate featuers 21010wdist 9430 svm with candidate features c1 d2 9431 one of the reasons that boostingbased classifiers realize faster classification speed is sparseness of rules ,0,1,0
these records are also known as field books and reference sets in literature canisius and sporleder 2007 michelson and knoblock 2008 ,0,1,0
both agichtein and ganti 2004 and canisius and sporleder 2007 train a language model for each database column ,0,1,0
5gram word language models in english are trained on a variety of monolingual corpora brants et al 2007 ,0,1,0
for instance word alignment models are often trained using the giza toolkit och and ney 2003 error minimizing training criteria such as the minimum error rate training och 2003 are employed in order to learn feature function weights for loglinear models and translation candidates are produced using phrasebased decoders koehn et al 2003 in combination with ngram language models brants et al 2007 ,0,1,0
of course many applications require smoothing of the estimated distributionsthis problem also has known solutions in mapreduce brants et al 2007 ,0,1,0
since that time however increasingly large amounts of language model training data have become available ranging from approximately one billion words the gigaword corpora from the linguistic data consortium to trillions of words brants et al 2007 ,0,1,0
all the tblms and orlms were unpruned 5gram models and used stupidbackoff smoothing brants et al 2007 2 with the backoff parameter set to 04 as suggested ,0,1,0
2007 looked at golomb coding and brants et al ,0,1,0
this was expected as it has been observed before that very simple smoothing techniques can perform well on large data sets such as web data brants et al 2007 ,0,1,0
we build sentencespecific zerocutoff stupidbackoff brants et al 2007 5gram language models estimated using 47b words of english newswire text and apply them to rescore each 10000best list ,0,1,0
a recent trend is to store the lm in a distributed cluster of machines which are queried via network requests brants et al 2007 emami et al 2007 ,0,1,0
in nlp community it has been shown that having more data results in better performance ravichandran et al 2005 brants et al 2007 turney 2008 ,0,1,0
brants et al 2007 emami et al 2007 built 5gram lms over web using distributed cluster of machines and queried them via network requests ,0,1,0
1 introduction very large corpora obtained from the web have been successfully utilized for many natural languageprocessingnlpapplications suchasprepositional phrase pp attachment otheranaphora resolution spellingcorrection confusablewordsetdisambiguation and machine translation volk 2001 modjeska et al 2003 lapata and keller 2005 atterer and schutze 2006 brants et al 2007 ,1,0,0
to scale lms to larger corpora with higherorder dependencies researchers work completed while this author was at google inc have considered alternative parameterizations such as classbased models brown et al 1992 model reduction techniques such as entropybased pruning stolcke 1998 novel represention schemes such as suffix arrays emami et al 2007 golomb coding church et al 2007 and distributed language models that scale more readily brants et al 2007 ,1,0,0
previous work brants et al 2007 has shown it to be appropriate to largescale language modeling ,0,1,0
we use the distributed training and application infrastructure described in brants et al 2007 with modifications to allow the training of predictive classbased models and their application in the decoder of the machine translation system ,0,1,0
759 for all models used in our experiments both wordand classbased the smoothing method used was stupid backoff brants et al 2007 ,0,1,0
classbased ngram models have also been shown to benefit from their reduced number of parameters when scaling to higherorder ngrams goodman and gao 2000 and even despite the increasing size and decreasing sparsity of language model training corpora brants et al 2007 classbased ngram models might lead to improvements when increasing the ngram order ,0,1,0
indeed researchers have shown that gigantic language models are key to stateoftheart performance brants et al 2007 and the ability of phrasebased decoders to handle largesize highorder language models with no consequence on asymptotic running time during decoding presents a compelling advantage over ckydecoderswhosetimecomplexitygrowsprohibitively large with higherorder language models ,1,0,0
either pruning stolcke 1998 church et al 2007 or lossy randomizing approaches talbot and brants 2008 may result in a compact representation for the application runtime ,0,1,0
to support distributed computation brants et al 2007 we further split the ngram data into shards by hash values of the first bigram ,0,1,0
we implemented an ngram indexerestimator using mpi inspired by the mapreduce implementation of ngram language model indexingestimation pipeline brants et al 2007 ,0,1,0
for our contrast submission we rescore the firstpass translation lattices with a large zerocutoff stupidbackoff brants et al 2007 language model estimated over approximately five billion words of newswire text ,0,1,0
it is therefore desirable to have dedicated servers to load parts of the lm3 an idea that has been exploited by zhang et al 2006 emami et al 2007 brants et al 2007 ,0,1,0
these findings are somehow surprising since it was eventually believed by the community that adding large amounts of bitexts should improve the translation model as it is usually observed for the language model brants et al 2007 ,0,1,0
we have also used tpts to encode ngram count databases such as the google 1t web ngram database brants and franz 2006 but are not able to provide detailed results within the space limitations of this paper4 51 perplexity computation with 5gram language models we compared the performance of tptencoded language models against three other language model implementations the sri language modeling toolkit stolcke 2002 irstlm federico and cettolo 2007 and the language model implementation currently used in the portage smt system badr et al 2007 which uses a pointerbased implementation but is able to perform fast lm filtering at load time ,0,1,0
in this study we use the google web 1t 5gram corpus brants et al 2007 ,0,1,0
2 related work there have been various efforts to integrate linguistic knowledge into smt systems either from the target side marcu et al 2006 hassan et al 2007 zollmann and venugopal 2006 the source side quirk et al 2005 liu et al 2006 huang et al 2006 or both sides eisner 2003 ding et al 2005 koehn and hoang 2007 just to name a few ,0,1,0
decoding is carriedout using the moses decoder koehn and hoang 2007 ,0,1,0
in koehn and hoang 2007 shallow syntactic analysis such as pos tagging and morphological analysis were incorporated in a phrasal decoder ,0,1,0
koehn and hoang 2007 propose factored translation models which extend phrasebased statistical machine translation by allowing the integration of additional morphological features at the word level ,0,1,0
24 factor model decomposition factored translation models koehn and hoang 2007 extend the phrasebased model by integrating word level factors into the decoding process ,0,1,0
koehn and hoang 2007 describes various strategies for the decomposition of the decoding into multiple translation models using the moses decoder ,0,1,0
41 training the training procedure is identical to the factored phrasebased training described in koehn and hoang 2007 ,0,1,0
in a factored translation model other factors than surface form can be used such as lemma or partofspeech koehn and hoang 2007 ,0,1,0
recent work by koehn and hoang 2007 pro514 poses factored translation models that combine feature functions to handle syntactic morphological and other linguistic information in a loglinear model ,0,1,0
the publicly available moses4 decoder is used for training and decoding koehn and hoang 2007 ,0,1,0
factored models are introduced in koehn and hoang 2007 for better integration of morphosyntactic information ,0,1,0
decoding is carriedout using the moses decoder koehn and hoang 2007 ,0,1,0
in recent work koehn and hoang 2007 proposed a general framework for including morphological features in a phrasebased smt system by factoring the representation of words into a vector of morphological features and allowing a phrasebased mt system to work on any of the factored representations which is implemented in the moses system ,0,1,0
though our motivation is similar to that of koehn and hoang 2007 we chose to build an independent component for inflection prediction in isolation rather than folding morphological information into the main translation model ,0,1,0
in their presentation of the factored smt models koehn and hoang 2007 describe experiments for translating from english to german spanish and czech using morphology tags added on the morphologically rich side along with pos tags ,0,1,0
koehn and hoang 2007 present factored translation models as an extension to phrasebased statistical machine translation models ,0,1,0
41 overview in this work factored models koehn and hoang 2007 are experimented with three factors the surface form the lemma and the part of speech pos ,0,1,0
therefore including a model based on surface forms as suggested koehn and hoang 2007 is also necessary ,0,1,0
 truecase 207 04 278 02 table 2 impact of truecasing on casesensitive bleu in a more integrated approach factored translation models koehn and hoang 2007 allow us to consider grammatical coherence in form of partofspeech language models ,0,1,0
321 factored treelet translation labels of nodes at the tlayer are not atomic but consist of more than 20 attributes representing various linguistic features3 we can consider the attributes as individual factors koehn and hoang 2007 ,0,1,0
in order to generate a value for each targetside factor we use a sequence of mapping steps similar to koehn and hoang 2007 ,0,1,0
furthermore the bleu score performance suggests that our model is not very powerful but some interesting hints can be found in table 3 when we compare our method with a 5gram language model to a stateoftheart system moses koehn and hoang 2007 based on various evaluation metrics including bleu score nist score doddington 2002 meteor banerjee and lavie 2005 ter snover et al 2006 wer and per ,1,0,0
for example factored translation models koehn and hoang 2007 retain the simplicity of phrasebased smt while adding the ability to incorporate additional features ,1,0,0
some research into factored machine translation has been published by koehn and hoang 2007 ,0,1,0
we believe that other kinds of translationunit such as ngram jos et al 2006factoredphrasaltranslationkoehn and hoang 2007 or treelet quirk et al 2005 can be used in this method ,0,1,0
a tight integration of morphosyntactic information into the translation model was proposed by koehn and hoang 2007 where lemma and morphological information are translated separately and this information is combined on the output side to generate the translation ,0,1,0
unlike with factored models koehn and hoang 2007 or additional translation lexicons schwenk et al 2008 we do not generate the surface form back from the lemma translation which means that tense gender and number information are 151 newsdev2009a representation oov meteor bleu nist baseline surface form only 224 4905 2045 6135 decoding lemma backoff 213 4912 2044 6143 word alignment lemmapos for all 224 4887 2036 6145 lemmapos for adj 225 4894 2046 6131 lemmapos for verbs 221 4905 2047 6137 decoding alignment backoff all 210 4897 2036 6147 backoff adj 212 4905 2048 6140 backoff verbs 208 4915 2050 6148 newsdev2009b representation oov meteor bleu nist baseline surface form only 252 4960 2110 6211 decoding lemma backoff 243 4966 2102 6210 word alignment lemmapos for all 253 4956 2103 6199 lemmapos for adj 252 4974 2100 6213 lemmapos for verbs 247 4973 2110 6217 decodingalignment backoff all 244 4959 2092 6194 backoff adj 243 4980 2103 6217 backoff verbs 239 4980 2103 6217 table 2 evaluation of the decoding backoff strategy the modified word alignment strategy and their combination input meme sil demissionnait la situation ne changerait pas ,0,1,0
many strategies have been proposed to integrate morphology information in smt including factored translation models koehn and hoang 2007 adding a translation dictionary containing inflected forms to the training data schwenk et al 2008 entirely replacing surface forms by representations built on lemmas and pos tags popovic and ney 2004 morphemes learned in an unsupervised manner virpojia et al 2007 and using porter stems and even 4letter prefixes for word alignment watanabe et al 2006 ,0,1,0
partofspeech language model we use factored translation models koehn and hoang 2007 to also output partofspeech tags with each word in a single phrase mapping and run a second ngram model over them ,0,1,0
without specific knowledge of the target domains annotation standards significant improvement can not be madedredze et al 2007 ,0,1,0
instead of assigning head and deprel in a single step some systems use a twostage approach for attaching and labeling dependencies chen et al 2007 dredze et al 2007 ,0,1,0
in order to calculate a global score or probability for a transition sequence two systems used a markov chain approach duan et al 2007 sagae and tsujii 2007 ,0,1,0
54 domain adaptation 541 featurebased approaches onewayofadaptingalearnertoanewdomainwithout using any unlabeled data is to only include features that are expected to transfer well dredze et al 2007 ,0,1,0
as with many domain adaptation problems it is quite helpful to have some annotated target data especially when annotation styles vary dredze et al 2007 ,0,1,0
however based on annotation differences in the datasets dredze et al 2007 and a bug in their system shimizu and nakagawa 2007 their results are inconclusive1 thus the effectiveness of scl is rather unexplored for parsing ,0,1,0
8412 only ptb baseline 8358 1st sagae and tsujii 2007 8342 2nd dredze et al 2007 8338 3rd attardi et al 2007 8308 third row lists the three highest scores of the domain adaptation track of the conll 2007 shared task ,0,1,0
this was a difcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data dredze et al 2007 ,0,1,0
dredze et al yielded the second highest score1 in the domain adaptation track dredze et al 2007 ,0,1,0
dredze et al also indicated that unlabeled dependency parsing is not robust to domain adaptation dredze et al 2007 ,0,1,0
it is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes since the data has huge variability in terms of quality style genres domains etc and domain adaptation for the nlp tasks involved is still an open problem dredze et al 2007 ,0,1,0
based on annotation differences in the datasets dredze et al 2007 and a bug in their system shimizu and nakagawa 2007 their results are inconclusive ,0,1,0
as well as the sentiment expressions leading to evaluations there are many semantic aspects to be extracted from documents which contain writers opinions such as subjectivity wiebe and mihalcea 2006 comparative sentences jindal and liu 2006 or predictive expressions kim and hovy 2007 ,0,1,0
opinion forecasting differs from that of opinion analysis such as extracting opinions evaluating sentiment and extracting predictions kim and hovy 2007 ,0,1,0
an application of the idea of alternative targets can be seen in kim and hovys 2007 work on election prediction ,0,1,0
examples of the latter include providing suggestions from a machine labeler and using extremely cheap human labelers eg with the amazon mechanical turk snow et al 2008 ,0,1,0
while this is certainly a daunting task it is possible that for annotation studies that do not require expert annotators and extensive annotator training the newly available access to a large pool of inexpensive annotators such as the amazon mechanical turk scheme snow et al 20084 or embedding the task in an online game played by volunteers poesio et al 2008 von ahn 2006 could provide some solutions ,1,0,0
previous work has shown that data collected through the mechanical turk service is reliable and comparable in quality with trusted sources snow et al 2008 ,0,1,0
several recent papers have studied the use of annotations obtained from amazon mechanical turk a marketplace for recruiting online workers su et al 2007 kaisser et al 2008 kittur et al 2008 sheng et al 2008 snow et al 2008 sorokin and forsyth 2008 ,0,1,0
a few studies carpuat and wu 2007 ittycheriah and roukos 2007 he et al 2008 hasan et al 2008 addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence ,0,1,0
we chose this inverse direction since it can be integrated directly into the decoder and thus does not rely on a twopass approach using reranking as it is the case for hasan et al 2008 ,0,1,0
2005 ponzetto and strube 2006 and the exploitation of advanced techniques that involve joint learning eg daume iii and marcu 2005 and joint inference eg denis and baldridge 2007 for coreference resolution and a related extraction task ,0,1,0
while early machine learning approaches for the task relied on local discriminative classifiers soon et al 2001 ng and cardie 2002b morton 2000 kehler et al 2004 more recent approaches use joint andor global models mccallum and wellner 2004 ng 2004 daume iii and marcu 2005 denis and baldridge 2007a ,0,1,0
the most similar work to ours is daume iii and marcu 2005 in which two most common synsets from wordnet for all words in an np and their hypernyms are extracted as features ,0,1,0
other similar work includes the mention detection md task florian et al 2006 and joint probabilistic model of coreference daume iii and marcu 2005 ,0,1,0
see luo and zitouni 2005 and daume iii and marcu 2005 ,0,1,0
7 related work there has been a recent interest in training methods that enable the use of firstorder features paskin 2002 daume iii and marcu 2005b richardson and domingos 2006 ,0,1,0
perhaps the most related is 86 learning as search optimization laso daume iii and marcu 2005b daume iii and marcu 2005a ,0,1,0
in contrast globally optimized clustering decisions were reported in luo et al 2004 and daumeiii and marcu 2005a where all clustering possibilities are considered by searching on a bell tree representation or by using the learning as search optimization laso framework daumeiii and marcu 2005b respectively but the first search is partial and driven by heuristics and the second one only looks back in text ,0,1,0
daumeiii and marcu 2005a use the learning as search optimization framework to take into account the nonlocality behavior of the coreference features ,0,1,0
we have already shown in section 3 how to solve a here we avoid b by maximizing conditional likelihood marginalizing out the hidden variable denotedz max vector summationdisplay xy pxylog summationdisplay z pvectoryz x 17 this sort of conditional training with hidden variables was carried out by koo and collins 2005 for example in reranking it is related to the information bottleneck method tishby et al 1999 and contrastive estimation smith and eisner 2005 ,0,1,0
our method is based on the ones described in erkan and radev 2004 mihalcea and tarau 2004 fader et al 2007 the objective of this paper is to dynamically rank speakers or participants in a discussion ,0,1,0
1113 recursive dp equations for summing over t and a alignments are treated as a hidden variable to be marginalized out10 optimization problems of this form are by now widely known in nlp koo and collins 2005 and have recently been used for machinetranslationaswellblunsometal2008 ,0,1,0
meanwhile some learning algorithms like maximum likelihood for conditional loglinear models lafferty et al 2001 unsupervised models pereira and schabes 1992 and models with hidden variables koo and collins 2005 wang et al 2007 blunsom et al 2008 require summing over the scores of many structures to calculate marginals ,0,1,0
a reranking parser see also koo and collins 2005 is a layered model the base layer is a generative statistical pcfg parser that creates a ranked list of k parses say 50 and the second layer is a reranker that reorders these parses using more detailed features ,0,1,0
1 introduction state of the art statistical parsers collins 1999 charniak 2000 koo and collins 2005 charniak and johnson 2005 are trained on manually annotated treebanks that are highly expensive to create ,1,0,0
5 related work there has not been much previous work on graphical models for full parsing although recently several latent variable models for parsing have been proposed koo and collins 2005 matsuzaki et al 2005 riezler et al 2002 ,0,1,0
in koo and collins 2005 an undirected graphical model is used for parse reranking ,0,1,0
previous research in this area includes several models which incorporate hidden variables matsuzaki et al 2005 koo and collins 2005 petrov et al 2006 titov and henderson 2007 ,0,1,0
use of probability estimates is not a serious limitation of this approach because in practice candidates are normally provided by some probabilistic model and its probability estimates are used as additional features in the reranker collins and koo 2005 shen and joshi 2003 henderson and titov 2005 ,0,1,0
1 introduction the reranking approach is widely used in parsing collins and koo 2005 koo and collins 2005 henderson and titov 2005 shen and joshi 2003 as well as in other structured classification problems ,0,1,0
in syntactic parse reranking supersenses have been used to build useful latent semantic features koo and collins 2005 ,1,0,0
matsuzaki et al 2005 koo and collins 2005 ,0,1,0
in koo and collins 2005 an undirected graphical model for constituent parse reranking uses dependency relations to define the edges ,0,1,0
some researchers lappin and leass 1994 kennedy and boguraev 1996 use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun while others use features determined over the parse tree in a machinelearning approach aone and bennett 1995 yang et al 2004 luo and zitouni 2005 ,0,1,0
2007 and luo and zitouni 2005 ,0,1,0
for example syntactic features ng and cardie 2002b luo and zitouni 2005 can be computed this way and are used in our system ,0,1,0
see luo and zitouni 2005 and daume iii and marcu 2005 ,0,1,0
the coreference resolution system employs a variety of lexical semantic distance and syntactic featuresluo et al 2004 luo and zitouni 2005 ,0,1,0
these features are calculated by mining the parse trees and then could be used for resolution by using manually designed rules lappin and leass 1994 kennedy and boguraev 1996 mitkov 1998 or using machinelearning methods aone and bennett 1995 yang et al 2004 luo and zitouni 2005 ,0,1,0
we also tested the flat syntactic feature set proposed in luo and zitouni 2005s work ,0,1,0
in line with the reports in luo and zitouni 2005 we do observe the performance improvement against the baseline norm for all the domains ,0,1,0
luo and zitouni 2005 proposed a coreference resolution approach which also explores the information from the syntactic parse trees ,0,1,0
to estimate combination weights we extend the f 1 score maximization training algorithm for lrm described in jansche 2005 ,0,1,0
2 f 1 score maximization training of lrm we first review the f 1 score maximization training method for linear models using a logistic function described in jansche 2005 ,0,1,0
by contrast in the training method proposed by jansche 2005 the discriminative function fxw is estimated to maximize the f 1 score of training dataset d this training method employs an approximate form of the f 1 score obtained by using a logistic function ,0,1,0
c a and b are computed for training dataset d as c summationtext m m1 y m y m a summationtext m m1 y m and b summationtext m m1 y m in jansche 2005 y m was approximated by using the discriminative and logistic functions shown in eqs ,0,1,0
moreover an fscore optimization method for logistic regression has also been proposed jansche 2005 ,0,1,0
for example the constrained optimization method of mozer et al 2001 relies on approximations of sensitivity which they call ca and specificity2 their cr related techniques gao et al 2006 jansche 2005 rely on approximations of true positives false positives and false negatives and indirectly recall and precision ,0,1,0
in his analysis of yarowsky 1995 abney 2004 formulates several variants of bootstrapping ,0,1,0
abney 2004 presented a thorough discussion on the yarowsky algorithm ,0,1,0
this approach however does not have a theoretical guarantee on optimality unless certain nontrivial conditions are satisfied abney 2004 ,0,1,0
51 comparison to selftraining for completeness we also compared our results to the selflearning algorithm which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of yarowsky in word sense disambiguation abney 2004 yarowsky 1995 ,0,1,0
under certain precise conditions as described in abney 2004 we can analyze algorithm 1 as minimizing the entropy of the distribution over translations of u however this is true only when the functions estimate score and select have very prescribed definitions ,0,1,0
although a rich literature covers bootstrapping methods applied to natural language problems yarowsky 1995 riloff 1996 collins and singer 1999 yangarber et al 2000 yangarber 2003 abney 2004 several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition ,0,1,0
previous studies called the class of algorithms illustrated in figure 2 cautious or sequential because in each iteration they acquire 1 or a small set of rules abney 2004 collins and singer 1999 ,0,1,0
thispaperfocusesontheframeworkintroduced in figure 2 for two reasons a cautious al50 gorithms were shown to perform best for several nlp problems including acquisition of ie patterns and b it has nice theoretical properties abney 2004 showed that regardless of the selection procedure sequential bootstrapping algorithms converge to a local minimum of k where k is an upper bound of the negative log likelihood of the data ,0,1,0
many 649 similarity measures and weighting functions have been proposed for distributional vectors comparative studies include lee 1999 curran 2003 and weeds and weir 2005 ,0,1,0
the linear kernel derived from the l1 distance is the same as the differenceweighted tokenbased similarity measure of weeds and weir 2005 ,0,1,0
weeds and weir 2005 discuss the influence of bias towards highor lowfrequency items for different tasks correlation with wordnetderived neighbour sets and pseudoword disambiguation and it would not be surprising if the different highfrequency bias were leading to different results ,0,1,0
see weeds and weir 2005 for an overview of other measures ,0,1,0
a variety of other measures of semantic relatedness have been proposed including distributional similarity measures based on cooccurrence in a body of text see weeds and weir 2005 for a survey ,0,1,0
further it has been shown weeds et al 2005 weeds and weir 2005 that performance of lins distributional similarity score decreases more significantly than other measures for low frequency nouns ,0,1,0
this is important when large cutoff 0 5 100 naive 541721 184493 35617 sash 10599 8796 6231 index 5844 13187 32663 table 4 average number of comparisons per term considering that different tasks may require different weights and measures weeds and weir 2005 ,0,1,0
following initial work by sparck jones 1964 and grefenstette 1994 an early online distributional thesaurus presented in lin 1998 has been widely used and cited and numerous authors since have explored thesaurus properties and parameters see survey component of weeds and weir 2005 ,0,1,0
it is explored extensively in curran 2004 weeds and weir 2005 ,1,0,0
2 evaluating sr measures various approaches for computing semantic relatedness of words or concepts have been proposed eg dictionarybased lesk 1986 ontologybased wu and palmer 1994 leacock and chodorow 1998 informationbased resnik 1995 jiang and conrath 1997 or distributional weeds and weir 2005 ,0,1,0
they generally perform less well on lowfrequency words weeds and weir 2005 van der plas 2008 ,0,1,0
this upper bound is consistent with the upper limit of 50 found by daume iii and marcu 2005 which takes into account stemming differences ,0,1,0
the closest work is that of jing and mckeown 1999 and daume iii and marcu 2005 in which multiple sentences are processed with fragments within them being recycled to generate the novel generated text ,0,1,0
daume iii and marcu 2005 propose a model that encodes how likely it is that different sized spans of text are skipped to reach words and phrases to recycle ,0,1,0
the best previous result is an accuracy of 561 turney 2006 ,1,0,0
the average senior high school student achieves 57 correct turney 2006 ,0,1,0
turney 2006 later addressed the same problem using 8000 automatically generated patterns ,0,1,0
pairclass is most similar to the algorithm of turney 2006 but it differs in the following ways pairclass does not use a lexicon to find synonyms for the input word pairs ,0,1,0
 pairclass generates probability estimates whereas turney 2006 uses a cosine measure of similarity ,0,1,0
 the automatically generated patterns in pairclass are slightly more general than the patterns of turney 2006 ,0,0,1
the template we use here is similar to turney 2006 but we have added extra context words before the x and after the y our morphological processing also differs from turney 2006 ,0,1,0
turney 2006 also selects patterns based on the number of pairs that generate them but the number of selected patterns is a constant 8000 independent of the number of input word pairs ,0,1,0
one such relational reasoning task is the problem of compound noun interpretation which has received a great deal of attention in recent years girju et al 2005 turney 2006 butnariu and veale 2008 ,0,1,0
turney 2006 describes a method latent relational analysis that extracts subsequence patterns for noun pairs from a large corpus using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space ,0,1,0
the distinction between lexical and relational similarity for word pair comparison is recognised byturney2006hecallstheformer attributional similarity though the methods he presents focus on relational similarity ,0,1,0
in at with use teacher school 11894470201 289 00 teacher handbook 25 00 32 101 soldier gun 28 103 1059 410 table 5 a fragment of the ccxl space we use this space to measure relational similarity turney 2006 of concept pairs eg finding that the relation between teachers and handbooks is more similar to the one between soldiers and guns than to the one between teachers and schools ,0,1,0
the attr cells summarize the performance of the 6 models on the wiki table that are based on attributional similarity only turney 2006 ,0,1,0
in particular we need to develop a backoff strategy for unseen pairs in the relational similarity tasks that following turney 2006 could be based on constructing surrogate pairs of taxonomically similar words found in the cxlc space ,0,1,0
we solve sat analogies with a simplified version of the method of turney 2006 ,0,1,0
1 introduction corpusderived distributional semantic spaces have proved valuable in tackling a variety of tasks ranging from concept categorization to relation extraction to many others sahlgren 2006 turney 2006 pado and lapata 2007 ,0,1,0
1 introduction cooccurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts sahlgren 2006 turney 2006 ,1,0,0
in computational linguistics our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs hearst 1998 chklovski and pantel 2004 etzioni et al 2004 turney 2006 davidov and rappoport 2008 inter alia ,0,1,0
the results of these studies have important applications in lexicography to detect lexicosyntactic regularities church and hanks 19901 calzolari and bindi1990 such as for example support verbs eg makedecision prepositional verbs eg relyupon idioms semantic relations eg partof and fixed expressions eg kick the bucket ,0,1,0
in calzolari and bindi 1990 church and hanks 1990 the significance of an association xy is measured by the mutual information ixy ie the probability of observing x and y together compared with the probability of observing x and y independently ,0,1,0
in particular mutual information church and hanks 1990 wu and su 1993 and other statistical methods such as smadja 1993 and frequencybased methods such as justeson and katz 1993 exclude infrequent phrases because they tend to introduce too much noise ,0,1,0
previous research in automatic acquisition focuses primarily on the use of statistical techniques such as bilingual alignment church and hanks 1990 klavans and tzoukermann 1995 wu and xia 1995 or extraction of syntactic constructions from online dictionaries and corpora brent 1993 ,0,1,0
church and hanks 1990 introduced a statistical measurement called mutual information for extracting strongly associated or collocated words ,0,1,0
word association norms based on cooccurrence information have been proposed by church and hanks 1990 ,0,1,0
213 correlation analysis as a correlation measure between terms we use mutual information church and hanks 1990 ,0,1,0
a large corpus is vahmble as a source of such nouns church and hanks 1990 brown et al 1992 ,0,1,0
collocations were extracted according to the method described in church and hanks 1990 by moving a window on texts ,0,1,0
mutual information mixy is defined as following church and hanks 1990 log log 22 yfxf yxfn ypxp yxp yxmi 4 where fx and fy are frequency of term x and term y respectively ,0,1,0
one way of resolving query ambiguities is to use the statistics such as mutual information church and hanks 1990 to measure associations of query terms on the basis of existing corpora jang et al 1999 ,0,1,0
the mutual information of a pair of words is defined in terms of their cooccurrence frequency and respective occurrence frequencies church and hanks 1990 ,0,1,0
5 related work although there have been many studies on collocation extraction and mining using only statistical approaches church and hanks 1990 ikehara et al 1996 there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations ,0,1,0
the initial vectors to be clustered are adapted with pointwise mutual information church and hanks 1990 ,0,1,0
in zernik 1990 calzolari and bindi 1990 smadja 1989 church and hanks 1990 associations are detected in a 5 window ,0,1,0
in the field of eomputationa1 linguistics mutual information brown et al 1988 2 church and hanks 1990 or a likelihood ratio test dunning 199a are suggested ,0,1,0
previous research in automatic acquisition focuscs primarily on the use of statistical techniques such as bilingual alignment church and hanks 1990 klavans and tzoukermann 1996 wu and xia 1995 or extraction of syntactic constructions from online dictionaries and corpora brant 1993 dorr garman and weinberg 1995 ,0,1,0
hindle uses the observed frequencies within a specific syntactic pattern subjectverb and verbobject to derive a cooccu rence score which is an estimate of mutual information church and hanks 1990 ,0,1,0
in the past five years important research on the automatic acquisition of word classes based on lexical distribution has been published church and hanks 1990 hindle 1990 smadja 1993 greinstette 1994 grishman and sterling 1994 ,1,0,0
church hanks 1990p24 merkel nilsson ahrenberg 1994 have constructed a system that uses frequency of recurrent segments to determine long phrases ,0,1,0
in the field of statistical analysis of natural language data it is common to use measures of lexical association such as the informationtheoretic measure of mutual information to extract useful relationships between words eg church and hanks 1990 ,0,1,0
more rare words rather than common words are found even in standard dictionaries church and hanks 1990 ,0,1,0
pointwise mutual information church and hanks 1990 3 ,0,1,0
we measure this association using pointwise mutual information mi church and hanks 1990 ,0,1,0
this task is quite common in corpus linguistics and provides the starting point to many other algorithms eg for computing statistics such as pointwise mutual information church and hanks 1990 for unsupervised sense clustering schutze 1998 and more generally a large body of work in lexical semantics based on distributional profiles dating back to firth 1957 and harris 1968 ,0,1,0
many studies on collocation extraction are carried out based on cooccurring frequencies of the word pairs in texts choueka et al 1983 church and hanks 1990 smadja 1993 dunning 1993 pearce 2002 evert 2004 ,0,1,0
 1989 eg lexicography church and hanks 1990 information retrieval salton 1986a text input yamashina and obashi 1988 etc this paper will touch on its feasibility in topic identification ,0,1,0
2mutual information though potentially of interest as a measure of collocational status was not tested due to its wellknown property of overemphasising the significance of rare events church and hanks 1990 ,0,0,1
ic function is a derivative of fanos mutual information formula recently used by church and hanks 1990 to compute word cooccurrence patterns in a 44 million word corpus of associated press news stories ,0,1,0
using techniques described in church and hindle 1990 church and hanks 1990 and hindle and rooth 1991 below are some examples of the most frequent vo pairs from the ap corpus ,0,1,0
church k and hanks p 1990 word association norms mutual information and lexicography computational linguistics vol ,0,1,0
while we have observed reasonable results with both g 2 and fishers exact test we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information mi measure church and hanks 1990 ixy log 2 pxy 4 pxpy in 4 y is the seed term and x a potential target word ,0,1,0
orgpubscitations j ournalstoms1986122p154meht a mutual information given the definition of mutual information church and hanks 1990 ixy log 2 pxy pxpy we consider the distribution of a window word according to the contingency table a in table 4 ,0,1,0
equation 10 is of interest because the ratio pc v rpc r can be interpreted as a measure of association between the verb v and class c this ratio is similar to pointwise mutual information church and hanks 1990 and also forms part of resniks association score which will be introduced in section 6 ,0,1,0
7 this discussion could also be cast in an information theoretic framework using the notion of mutual information fano 1961 estimating the variance of the degree of match in order to find a frequencythreshold see church and hanks 1990 ,0,1,0
using techniques described in church and hindle 1990 church and hanks 1990 and hindle and rooth 1991 figure 4 shows some examples of the most frequent vo pairs from the ap corpus ,0,1,0
for example church and hanks 1990 describe the use of the mutual information index for this purpose cf ,0,1,0
these tools are important in that the strongest collocational associations often represent different word senses and thus they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags church and hanks 1990 p 28 ,0,1,0
statistics on cooccurrence of words in a local context were used recently for monolingual word sense disambiguation gale church and yarowsky 1992b 1993 sch6tze 1992 1993 see section 7 for more details and church and hanks 1990 smadja 1993 for other applications of these statistics ,0,1,0
the window size may vary church and hanks 1990 used windows of size 2 and 5 ,0,1,0
211 pointwise mutual information this measure for word similarity was first used in this context by church and hanks 1990 ,0,1,0
1 introduction many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts dunning 1993 church and hanks 1990 dagan et al 1999 ,0,1,0
the value of fj is calculated by mutual information church and hanks 1990 between xi and fj ,0,1,0
strength of association between subject i and verb j is measured using mutual information church and hanks 1990 ln ji ij tftf tfnjimi here tfij is the maximum frequency of subjectverb pair ij in the reuters corpus tfi is the frequency of subject head noun i in the corpus tfj is the frequency of verb j in the corpus and n is the number of terms in the corpus ,0,1,0
the most widely used association weight function is pointwise mutual information mi church and hanks 1990 lin 1998 dagan 2000 weeds et al 2004 ,1,0,0
metric formula frequency guiliano 1964 x yf pointwise mutual information pmi church hanks 1990 xy x y2log p p p true mutual information tmi manning 1999 xy 2 xy x ylog p p p p chisquared 2 church and gale 1991 2 i x x y y i j i j i j j f tscore church hanks 1990 1 2 2 2 1 2 1 2 x x s s n n cvalues4 frantzi anadiou mima 2000 2 is not nested 2 log log 1 a a b t a f f f b p t where is the candidate string f is its frequency in the corpus t is the set of candidate terms that contain pt is the number of these candidate terms 609 1700 of the threeword phrases are attested in the lexile corpus ,0,1,0
to this end we follow the method introduced by church and hanks 1990 ie by sliding a window of a given size over some texts ,0,1,0
the information content of this set is defined as mutual information ifw church and hanks 1990 ,0,1,0
one can also examine the distribution of character or word ngrams eg language modeling croft and lafferty 2003 phrases church and hanks 1990 lewis 1992 and so on ,0,1,0
to extract such word clusters we used suffix arrays proposed in yamamoto and church 2001 and the pointwise mutual information measure see church and hanks 1990 ,0,1,0
to examine the effects of including some known ams on the performance the following ams had a 50 chance of being included in the initial population pointwise mutual information church and hanks 1990 the dice coefficient and the heuristic measure defined in petrovic et al 2006 habc 2log fabcfafc if posb x log fabcfafbfc otherwise ,0,1,0
4 using vectorbased models of semantic representation to account for the systematic variances in neural activity 41 lexical semantic representation computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly cooccurs church and hanks 1990 ,0,1,0
the use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research church and hanks 1990 zernik and jacobs 1990 hindle 1990 ,0,1,0
in this case it is possible to perform the correct selection if we used only statistics about the cooccurrences of corruption with either investigator or researcher without looking for any syntactic relation as in church and hanks 1990 ,0,1,0
introduction word associations have been studied for some time in the fields of psycholinguistics by testing human subjects on words linguistics where meaning is often based on how words cooccur with each other and more recently by researchers in natural language processing church and hanks 1990 hindle and rooth 1990 dagan 1990 mcdonald et al 1990 wilks et al 1990 using statistical measures to identify sets of associated words for use in various natural language processing tasks ,0,1,0
three recent papers in this area are church and hanks 1990 hindle 1990 and smadja and mckeown 1990 ,0,1,0
in addition ic is stable even for relatively low frequency words which can be contrasted with fanos mutual information formula recently used by church and hanks 1990 to compute word cooccurrence patterns in a 44 million word corpus of associated press news stories ,0,1,0
researchers such as evans et al 1991 and church and hanks 1990 have applied robust grammars and statistical techniques over large corpora to extract interesting noun phrases and subjectverb verbobject pairs ,1,0,0
the mutual information of a cooccurrence pair which measures the degree of association between the two words church and hanks 1990 is defined as fano 1961 pxly ixy log 2 pxy log 2 1 pxpy px log 2 pyx py where px and py are the probabilities of the events x and y occurrences of words in our case and px y is the probability of the joint event a cooccurrence pair ,0,1,0
in the results we describe here we use mutual information fano 1961 2728 church and hanks 1990 as the metric for neighbourhood pruning pruning which occurs as the network is being generated ,0,1,0
thus given a hyponym definition o and a set of candidate hypernym definitions this method selects the candidate hypernym definition e which returns the maximum score given by formula 1 sco e e cwwi wj i wieoawj6e the cooccurrence weight cw between two words can be given by cooccurrence frequency mutual information church and hanks 1990 or association ratio resnik 1992 ,0,1,0
the collocations have been calculated according to the method described in church and hanks 1990 by moving a window on the texts ,0,1,0
the cohesion between two words is measured as in church and hanks 1990 by an estimation of the mutual information based on their collocation frequency ,0,1,0
collocation collocations were extracted from a seven million word sample of the longman english language corpus using the association ratio church and hanks 1990 and outputted to a lexicon ,0,1,0
for instance church and hanks 1990 calculated sa in terms of mutual information between two words wl and w2 n fwlw2 iwl w2 log2 1 fwlfw2 here n is the size of the corpus used in the estimation fwl w2 is the frequency of the cooccurrence fwl and fw2 that of each word ,0,1,0
arguably the most widely used is the mutual information hindle 1990 church and hanks 1990 dagan et al 1995 luk 1995 d lin 1998a ,1,0,0
we then propose a relatively simple yet effective method for resolving translation disambiguation using mutual information mi church and hanks 1990 statistics obtained only from the target document collection ,0,1,0
the mutual information mlxy is defined as the following formula church and hanks 1990 ,0,1,0
we preferred the loglikelihood ratio to other statistical scores such as the association ratio church and hanks 1990 or 2 since it adequately takes into account the frequency of the cooccurring words and is less sensitive to rare events and corpussize dunning 1993 daille 1996 ,0,1,0
for mutual information mi we use two different equations one for twoelement compound nouns church and hanks 1990 and the other for threeelement compound nouns suet al 1994 ,0,1,0
the association relationship between two words can be indicated by their mutual information which can be further used to discover phrases church hanks 1990 ,0,1,0
we then rankorder the p xy mi xy m z pr zy mi zy g092log p x p y p x p y f y p xy p xy f xy p xy p xy f xy m ig13xx jg13yy f ij g09 ij 2 ij f xy g09 xy xy 1g09 xy n f xy g09 xy f xy 1g09f xy n table 1 probabilistic approaches method formula frequency guiliano 1964 f xy pointwise mutual information mi fano 1961 church and hanks 1990 log p pp 2xy xy selectional association resnik 1996 symmetric conditional probability ferreira and pereira 1999 p pp xy x y 2 dice formula dice 1945 2 f f f xy x y loglikelihood dunning 1993 daille 1996 ,0,1,0
since we need knowledgepoor daille 1996 induction we cannot use humansuggested filtering chisquared g24 2 church and gale 1991 zscore smadja 1993 fontenelle et al 1994 students tscore church and hanks 1990 ngram list in accordance to each probabilistic algorithm ,0,1,0
study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction church and hanks 1990 smadja 1993 choueka 1993 lin 1998 ,0,1,0
church and hanks church and hanks 1990 employed mutual information to extract both adjacent and distant bigrams that tend to cooccur within a fixedsize window ,0,1,0
the typical problems like doctornurse church and hanks 1990 could be avoided by using such information ,0,1,0
there are several basic methods for evaluating associations between words based on frequency counts choueka 1988 wettler and rapp 1993 information theoretic church and hanks 1990 and statistical significance smadja 1993 ,0,1,0
after building the chunker students were asked to 4 choose a verb and then analyze verbargument structure they were provided with two relevant papers church and hanks 1990 chklovski and pantel 2004 ,0,1,0
in the following sections we will use 2 statistics to measure the the mutual translation likelihood church and hanks 1990 ,0,1,0
pmi church and hanks 1990 between two phrases is de ned as log2 probph1 is near ph2probph 1 probph2 pmi is positive when two phrases tend to cooccur and negative when they tend to be in a complementary distribution ,0,1,0
usually in 1 in our experiments we set negative pmi values to 0 because church and hanks 1990 in their seminal paper on word association ratio show that negative pmi values are not expected to be accurate unless cooccurrence counts are made from an extremely large corpus ,1,0,0
to compute the degree of interaction between two proteins d4 bd and d4 be we use the informationtheoretic measure of pointwise mutual information church and hanks 1990 manning and schutze 1999 which is computed based on the following quantities 1 ,0,1,0
pointwise mutual information fano 1961 was used to measure strength of selection restrictions for instance by church and hanks 1990 ,0,1,0
indeed as sinopalnikova and pavel 2004 note deese 1965 was the first to conduct linguistic analyses of word association norms such as measurements of semantic similarity based on his convictions that similar words evoke similar word association responsesan approach that is somewhat reminiscent of church and hanks 1990 notion of mutual information ,0,1,0
mutual informatio n church and hanks 1990 discussed the use of the mutual information statistics as a way to identify a variety of interesting linguistic phenomena ranging from semanti c relations of the doctornurse type content wordcontent word to lexicosyntactic cooccurrence preferences between verbs and prepositions content wordfunction word ,0,1,0
the tensor has been adapted with a straightforward extension of pointwise mutual information church and hanks 1990 for threeway cooccurrences following equation 4 ,0,1,0
the first adaptation includes theswapoperationwagnerandlowrance1975 whilethesecondadaptationincludesphoneticsegment distances which are generated by applying an iterative pointwise mutual information pmi procedurechurchand hanks 1990 ,0,1,0
we used pointwise mutual information pmi church and hanks 1990 to obtain these distances ,0,1,0
a broad view of the possible scope of lexical semantics would thus be one which tries to chart out the systematic generalizable aspects of word meanings and of the relations between words drawing on readily accessible sources of lexical knowledge such as machine readable dictionaries encyclopedias and representative corpora coupled with the kind of analytic apparatus that is needed to fruitfully explore such sources for instance custombuilt parsers to cope with dictionary definitions vossen 1990 statistical programs to deal with the distributional properties of lexical items in large corpora church hanks 1990 etc at the same time this kind of massive dataacquisition should be made sensitive to the borders between perceptual experience lexical knowledge and expert knowledge ,0,1,0
in our approach we take into account both the relative positions of the nearby context words as well as the mutual information church hanks 1990 associated with the occurrence of a particular context word ,0,1,0
1989 wettler rapp 1989 and church hanks 1990 describe algorithms which do this ,0,1,0
other representative collocation research can be found in church and hanks 1990 and smadja 1993 ,0,1,0
unlike church and hanks 1990 smadja 1993 goes beyond the twoword limitation and deals with collocations of arbitrary length ,0,0,1
following church and hanks 1990 they use mutual information to select significant twoword patterns but at the same time a lexical inductive process is incorporated which as they claim can improve the collection of domainspecific terms ,0,1,0
we used th3 following a very rough rule of thumb used for wordbased mutual information in church and hanks 1990 ,0,1,0
most of the previously proposed methods to extract compounds or to measure word association using mutual information mi either ignore or penalize items with low cooccurrence counts church and hanks 1990 su wu and chang 1994 because mi becomes unstable when the cooccurrence counts are very small ,0,1,0
previous research in automatic acquisition focuses primarily on the use of statistical techniques such as bilingual alignment church and hanks 1990 klavans and tzoukermann 1996 wu and xia 1995 or extraction of syntactic constructions from online dictionaries and corpora brent 1993 dorr garman and weinberg 1995 ,0,1,0
the classifier uses mutual information mi scores rather than the raw frequences of the occurring patterns church and hanks 1990 ,0,1,0
s e the window to consider when extracting words related to word w should span from postttuon w5 to w5 maarek also defines the resolwng power of a parr m a document d as p pd log pc where pd is the observed probabshty of appearance of the pan m document d pc the observed probabdny of the pmr recorpus and log pc the quantity of mformauon assocmted to the pmr it is easdy seen that p wall be hgher the higher the frequency of the pmr m the document and the lower sts frequency m the corpus which agrees wlth the sdea presented at the begmnmg of this sectton church and hanks 1990 propose the apphcatlon of the concept of mutual mformatton exy xy hog2 ecxey 51 to the retrieval ro a corpus of pairs of lextcally related words they alsoconslder a word span of e5 words and observe that roterestrog pmr s generally present a mutual mformatxon above 3 salton andallan 1995 focas on paragraph level each paragraph is represented by a weighed vector where each element is a term typically ,0,1,0
of the works of kuplec pedersen and chen 1995 and brandow mltze and ran 1995 and advances summarmatlon technology by applynag corpusbased statistical nlp teehmques robust information extraction and readily avaalable onhne resources our prehxmnary experiments with combining different summarization features have been reported and our current effort to learn to combine these features to produce the best summaries has been described the features derived by these robust nlp techmques were also utihzed m presentmg multiple summaryvtews to the user m a novel way references advanced research projects agency 1995 proceedrigs of szth message understanding conference muc6 morgan kanfmann pubhshers brandow ron karl mltze and lisa ran 1995 automatic condensation of electromc pubhcatlous by sentence selection information processing and management 31 forthcoming bull eric 1993 a compsbased approach to language learning ph d thesm umverslty of pennsylvania church kenneth and patrick hanks 1990 word aesoclatlon norrns mutual information and lexicography computational lmgmstscs 161 church kenneth w 1995 one term or two 9 in proceedings of the 17th annual international sigir conference on research and development in informatzon retrzeral pages 310318 edmundson h p 1969 new methods m automatic abstracting journal of the acm 162 264228 fum dando glovanm gmda and carlo tasso 1985 evalutatmg importance a step towards text surnmarlzatlon in i3cai85 pages 840844ijcai aaai hahn udo 1990 topic parsing accounting for text macro structures m fulltext analysm in formaton processing and management 261135170 harman donna 1991 how effective is suttixang journal of the amerlcan sotcry for informatwn scence 421 715 harman donna 1996 overview of the fifth text retrieval conference tree5 in trec5 conference proceedings jmg y and b croft 1994 an assocatwn thesaurns for informatzon retrseval umass techmcal report 94i7 center for intelligent information retrieval university of massachusetts johnson f c c d prate w j black and a p neal 1993 ,0,1,0
ridf is like mi but different references church k and p hanks 1990word association norms mutual information and lexicography computational linguistics 161 pp ,0,1,0
lxy log pxy exey mi has been used to identify a variety of interesting linguistic phenomena ranging from semantic relations of the doctornurse type to lexicosyntactic cooccurrence preferences of the savefrom type church and hanks 1990 ,0,1,0
a standard solution is to use a weighted linear mixture of ngram models 1 n n brown et al 1992 ,0,1,0
previous studies have shed light on the predictability of the next unix command that a user will enter motoda and yoshida 1997 davison and hirsch 1998 the next keystrokes on a small input device such as a pda darragh and witten 1992 and of the translation that a human translator will choose for a given foreign sentence nepveu et al 2004 ,0,1,0
this is due to the reason that telugu entropy15625 bits per character bharati et al 1998 is comparitively a high entropy language than english brown and pietra 1992 ,0,1,0
as a result the empirical approach has been adopted by almost all contemporary partofspeech programs bahl and mercer 1976 leech garside and atwell 1983 jelinek 1985 deroualt and merialdo 1986 garside leech and sampson 1987 church 1988 derose 1988 hindle 1989 kupiec 1989 1992 ayuso et al ,0,1,0
illustrative clusterings of this type can also be found in pereira tishby and lee 1993 brown della pietra mercer della pietra and lai 1992 kneser and ney 1993 and brill et al ,0,1,0
successful approaches aimed at trying to overcome the sparse data limitation include backoff katz 1987 turinggood variants good 1953 church and gale 1991 interpolation jelinek 1985 deleted estimation jelinek 1985 church and gale 1991 similaritybased models dagan pereira and lee 1994 essen and steinbiss 1992 poslanguage models derouault and merialdo 1986 and decision tree models bahl et al 1989 black garside and leech 1993 magerman 1994 ,0,1,0
much research has been carried out recently in this area hughes and atwell 1994 finch and chater 1994 redington chater and finch 1993 brill et al 1990 kiss 1973 pereira and tishby 1992 resnik 1993 ney essen and kneser 1994 matsukawa 1993 ,0,1,0
introduction many applications that process natural language can be enhanced by incorporating information about the probabilities of word strings that is by using statistical language model information church et al 1991 church and mercer 1993 gale church and yarowsky 1992 liddy and paik 1992 ,0,1,0
dependency models rosenfeld 2000 use the parsed dependency structure of sentences to build the language model as in grammatical trigrams lafferty et al 1992 structured language models chelba and jelinek 2000 and dependency language models chelba et al 1997 ,0,1,0
in order to estimate the entropy of english brown et al 1992 approximated pkiunk by a poisson distribution whose parameter is the average word length a in the training corpus and pcz cklk unk by the product of character zerogram probabilities ,0,1,0
it is sometimes assumed that estimates of entropy eg shannons estimate that english is 75 redundant brown et als 1992 upper bound of 175 bits per character for printed english are directly 3there are some cases where words are deliberately misspelled in order to get better output from the synthesizer such as coyote spelled kiote ,0,1,0
work at the university of dundee eg aim et al 1992 todman and alm this volume has shown that the extensive use of fixed text for sequences such as greetings and prestored narratives is beneficial in aac ,0,1,0
methods that use bigrams brown et al 1992 or trigrams martin et al 1998 cluster words considering as a words context the one or two immediately adjacent words and employ as clustering criteria the minimal loss of average 836 nmtual information and the perplexity improvement respectively ,0,1,0
most approaches brown et al 1992 li abe 1997 inherently extract semantic knowledge in the abstracted form of semantic clusters ,0,1,0
previous approaches to processing lnetonymy have used handconstructed ontologies or semantic networks ass 1988 iverson and hehnreich 1992 bmaud et al 1996 fass 1997 ,0,1,0
a large corpus is vahmble as a source of such nouns church and hanks 1990 brown et al 1992 ,0,1,0
brown et al 1992 put forward and discussed ngram models based on classes of words ,0,1,0
among all the language modeling approaches ngram models have been most widely used in speech recognition jelinek 1990 gale and church 1990 brown et al 1992 yang et al 1996 and other applications ,0,1,0
similaritybased smoothing brown et al 1992 dagan et al 1999 is an intuitively appealing approach to this problem where probabilities of unseen cooccurrences are estimated from probabilities of seen cooccurrences of distributionally similar events ,1,0,0
1992 describe one application of mi to identify word collocations kashioka et al ,0,1,0
we believe the benefit to limiting the size of n is connected to brown et als 1992 470 observation that as n increases the accuracy of an ngram model increases but the reliability of our parameter estimates drawn as they must be from a limited training text decreases ,0,1,0
applications of word clustering include language modeling brown et al 1992 text classification baker and mccallum 1998 thesaurus construction lin 1998 and so on ,0,1,0
we have used a stateoftheart chinese handwriting recognizer li et al 1992 developed by atc ccl itri taiwan as the basis of our experiments ,0,1,0
for a class bigram model find v c to maximize t ili pwi iwlpwilwi1 alternatively perplexity jardino an d adda 1993 or average mutual information brown et al 1992 can be used as the characteristic value for optimization ,0,1,0
introduction classbased language models brown et al 1992have been proposed for dealing with two problems confronted by the wellknown word ngram language models 1 data sparseness the amount of training data is insufficient for estimating the huge number of parameters and 2 domain robustness the model is not adaptable to new application domains ,0,1,0
have been proposed hindle 1990 brown et al 1992 pereira et al 1993 tokunaga et al 1995 ,0,1,0
language models such as ngram class models brown et al 1992 and ergodic hidden markov models kuhn el al 1994 were proposed and used in applications such as syntactic class pos tagging for english cutting et al 1992 clustering and scoring of recognizer sentence hypotheses ,0,1,0
brown et al proposed a classbased ngram model which generalizes the ngram model to predict a word from previous words in a text brown et al 1992 ,0,1,0
of particular relevance are classbased language models eg saul and pereira 1997 brown et al 1992 ,0,1,0
2008 who employ clusters of related words constructed by the brown clustering algorithm brown et al 1992 for syntactic processing of texts ,0,1,0
this method was shown to outperform the class based model proposed in brown et al 1992 and can thus be expected to discover better clusters of words ,0,0,1
models of this type include brown et al 1992 zitouni 2007 which use semantic word clustering and bahl et al 1990 which uses variablelength context ,0,1,0
introduction there has been considerable recent interest in the use of statistical methods for grouping words in large online corpora into categories which capture some of our intuitions about the reference of the words we use and the relationships between them eg brown et al 1992 schiitze 1993 ,0,1,0
various clustering techniques have been proposed brown et al 1992 jardino and adda 1993 martin et al 1998 which perform automatic word clustering optimizing a maximumlikelihood criterion with iterative clustering algorithms ,0,1,0
decades like ngram backoff word models katz 1987 class models brown et al 1992 structured language models chelba and jelinek 2000 or maximum entropy language models rosenfeld 1996 ,0,1,0
more specifically we use a classbased bigram model from brown et al 1992 11 iiiiii ccpcwpwwp 3 in equation 3 c i is the class of the word w i which could be a syntactic class or a semantic class ,0,1,0
this is in contrast to purely statistical systems eg brown et al 1992 which are difficult to inspect and modify ,0,0,1
the smoothing methods proposed in the literature overviews are provided by dagan lee and pereira 1999 and lee 1999 can be generally divided into three types discounting katz 1987 classbased smoothing resnik 1993 brown et al 1992 364 computational linguistics volume 28 number 3 pereira tishby and lee 1993 and distanceweighted averaging grishman and sterling 1994 dagan lee and pereira 1999 ,0,1,0
classes can be induced directly from the corpus using distributional clustering pereira tishby and lee 1993 brown et al 1992 lee and pereira 1999 or taken from a manually crafted taxonomy resnik 1993 ,0,1,0
for this purpose we present a datadriven beam search algorithm similar to the one used in speech recognition search algorithms ney et al 1992 ,0,1,0
52 pseudodisambiguation task pseudodisambiguation tasks have become a standard evaluation technique gale church and yarowsky 1992 sch utze 1992 pereira tishby and lee 1993 sch utze 1998 lee 1999 dagan lee and pereira 1999 golding and roth 1999 rooth et al 1999 evenzohar and roth 2000 lee 2001 clark and weir 2002 and in the current setting we may use a nouns neighbors to decide which of two cooccurrences is the most likely ,0,1,0
in addition explicitly using the left context symbols allows easy use of smoothing techniques such as deleted interpolation bahl jelinek and mercer 1983 clustering techniques brown et al 1992 and model refinement techniques lin chiang and su 1994 to estimate the probabilities more reliably by changing the window sizes of the context and weighting the various estimates dynamically ,0,1,0
previous work has demonstrated that this scoring function is able to provide high discrimination power for a variety of applications su chiang and lin 1992 chen et al 1991 su and chang 1990 ,0,1,0
this scoring function has been successfully applied to resolve ambiguity problems in an englishtochinese machine translation system behaviortran chen et al 1991 and a spoken language processing system su chiang and lin 1991 1992 ,0,1,0
illustrative clusterings of this type can also be found in pereira tishby and lee 1993 brown della pietra mercer della pietra and lai 1992 kneser and ney 1993 and brill et al ,0,1,0
successful approaches aimed at trying to overcome the sparse data limitation include backoff katz 1987 turinggood variants good 1953 church and gale 1991 interpolation jelinek 1985 deleted estimation jelinek 1985 church and gale 1991 similaritybased models dagan pereira and lee 1994 essen and steinbiss 1992 poslanguage models derouault and merialdo 1986 and decision tree models bahl et al 1989 black garside and leech 1993 magerman 1994 ,0,1,0
introduction many applications that process natural language can be enhanced by incorporating information about the probabilities of word strings that is by using statistical language model information church et al 1991 church and mercer 1993 gale church and yarowsky 1992 liddy and paik 1992 ,0,1,0
furthermore our model is not necessarily nativist these biases may be innate but they may also be the product of some other earlier learning algorithm as the results of ellison 1992 and brown et al ,0,1,0
notice that most incontext and dictionary translations of source words are bounded within the same category in a typical thesaurus such as the lloce mcarthur 1992 and cilin mei et al 1993 ,0,1,0
several authors for example krovetz and croft 1989 guthrie et al 1991 slator 1992 cowie guthrie and guthrie 1992 janssen 1992 bradenharder 1993 liddy and paik 1993 have attempted to improve results by using supplementary fields of information in the electronic version of the longman dictionary of contemporary english ldoce in particular the box codes and subject codes provided for each sense ,0,1,0
since then supervised learning from sensetagged corpora has since been used by several researchers zernik 1990 1991 hearst 1991 leacock towell and voorhees 1993 gale church and yarowsky 1992d 1993 bruce and wiebe 1994 miller et al ,0,1,0
this set of context vectors is then clustered into a predetermined number of coherent clusters or context groups using buckshot cutting et al 1992 a combination of the em algorithm and agglomerative clustering ,0,1,0
regardless of whether it takes the form of dictionaries lesk 1986 guthrie et al 1991 dagan itai and schwall 1991 karov and edelman 1996 thesauri yarowsky 1992 walker and amsler 1986 bilingual corpora brown et al 1991 church and gale 1991 or handlabeled training sets hearst 1991 leacock towell and voorhees 1993 niwa and nitta 1994 bruce and wiebe 1994 providing information for sense definitions can be a considerable burden ,0,1,0
another body of related work is the literature on word clustering in computational linguistics brown et al 1992 finch 1993 pereira tishby and lee 1993 grefenstette 1994a and document clustering in information retrieval van rijsbergen 1979 willett 1988 sparckjones 1991 cutting et al 1992 ,0,1,0
the second approach sekine et al 1992 chang luo and su 1992 resnik 1993a grishman and sterling 1994 alshawi and carter 1994 takes triples verb prep noun2 and nounl prep noun2 like those in table 10 as training data for acquiring semantic knowledge and performs ppattachment disambiguation on quadruples ,0,1,0
3 24 intonation annotations for our intonation annotation we have annotated the intonational phrase boundaries using the tobi tones and break indices definition silverman et al 1992 ,0,1,0
1992 and magerman 1994 used the clustering algorithm of brown et al ,0,1,0
for handling word identities one could follow the approach used for handling the pos tags eg black et al 1992 magerman 1994 and view the pos tags and word identities as two separate sources of information ,0,1,0
in information retrieval word similarity can be used to identify terms for pseudorelevance feedback harman 1992 buckley et al 1995 xu and croft 2000 vechtomova and robertson 2000 ,0,1,0
a re nement of this model is the classbased ngram where the words are partitioned into equivalence classes brown et al 1992 ,0,1,0
in addition we developed a word clustering procedure based on a standard approach brown et al 1992 that optimizes conditional word clusters ,0,1,0
table 2 three types of classbased mslms on switchboardi swbd and icsi meeting mr corpora of swbd mr classes brown mmi mcmi brown mmi mcmi 100 689 03 684 03 682 03 789 30 773 28 768 28 500 689 03 683 03 679 03 787 31 771 28 767 28 1000 689 03 682 03 679 03 790 31 772 27 769 28 1500 690 03 682 03 680 03 796 31 774 27 774 27 2000 690 03 683 03 680 03 801 31 776 27 779 27 jv j 685 03 783 27 table 3 classbased mslm on switchboard eval2003 size 100 500 1000 1500 2000 jv j 3gram 4gram ppl 658 655 656 657 661 679 721 763 reduction 86 89 88 87 83 58 0 58 classbased language models brown et al 1992 whittaker and woodland 2003 yield great bene ts when data sparseness abounds ,0,1,0
srilm stolcke 2002 can produce classes to maximize the mutual information between the classes icwtcwt 1 as described in brown et al 1992 ,0,1,0
413 alternative paraphrasing techniques to investigate the effect of paraphrase quality on automatic evaluation we consider two alternative paraphrasing resources latent semantic analysis lsa and brown clustering brown et al 1992 ,0,1,0
this can also be interpreted as a generalization of standard classbased models brown et al 1992 ,0,1,0
4 can be used to motivate a novel classbased language model and a regularized version of minimum discrimination information mdi models della pietra et al 1992 ,0,1,0
we consider three class models models s m and l defined as pscjc1cj1w1wj1pngcjcj2cj1 pswjc1cjw1wj1pngwjcj pmcjc1cj1w1wj1pngcjcj2cj1wj2wj1 pmwjc1cjw1wj1pngwjwj2wj1cj plcjc1cj1w1wj1pngcjwj2cj2wj1cj1 plwjc1cjw1wj1pngwjwj2cj2wj1cj1cj model s is an exponential version of the classbased ngram model from brown et al 1992 model m is a novel model introduced in chen 2009 and model l is an exponential version of the model indexpredict from goodman 2001 ,0,1,0
42 models with prior distributions minimum discrimination information models della pietra et al 1992 are exponential models with a prior distribution qyx pyx qyxexp summationtextf i1 ifixy zx 14 the central issue in performance prediction for mdi models is whether qyx needs to be accounted for ,0,1,0
the most popular nondatasplitting methods for predicting test set crossentropy or likelihood are aic and variants such as aicc quasiaic qaic and qaicc akaike 1973 hurvich and tsai 1989 lebreton et al 1992 ,0,1,0
while we can only compare class models with word models on the largest training set for this training set model m outperforms the baseline katzsmoothed word trigram model by 19 absolute6 4 domain adaptation in this section we introduce another heuristic for improving exponential models and show how this heuristic can be used to motivate a regularized version of minimum discrimination information mdi models della pietra et al 1992 ,0,1,0
1999 and lee 1999 can be generally divided into three types discounting katz 1987 classbased smoothing resnik 1993 brown et al 1992 pereira et al 1993 and distanceweighted averaging grishman and sterling 1994 dagan et al 1999 ,0,1,0
classes can be induced directly from the corpus pereira et al 1993 brown et al 1992 or taken from a manually crafted taxonomy resnik 1993 ,0,1,0
and we consider that word pairs that have a small distance between vectors also have similar word neighboring characteristics brown et al 1992 bai et al 1998 ,0,1,0
words are encoded through an automatic clustering algorithm brown et al 1992 while tags labels and extensions are normally encoded using diagonal bits ,0,1,0
recent research yamamoto et al 2001 shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models which use the same clusters for both words brown et al 1992 ,0,1,0
2 related work a large amount of previous research on clustering has been focused on how to find the best clusters brown et al 1992 kneser and ney 1993 yamamoto and sagisaka 1999 ueberla 1996 pereira et al 1993 bellegarda et al 1996 bai et al 1998 ,0,1,0
many traditional clustering techniques brown et al 1992 attempt to maximize the average mutual information of adjacent clusters 21 2 12 2121 log ww wp wwp wwpwwi 2 where the same clusters are used for both predicted and conditional words ,0,1,0
proceedings of the 40th annual meeting of the association for cently semantic resources have also been used in collocation discovery pearce 2001 smoothing and model estimation brown et al 1992 clark and weir 2001 and text classi cation baker and mccallum 1998 ,0,1,0
most systems extract cooccurrence and syntactic information from the words surrounding the target term which is then converted into a vectorspace representation of the contexts that each target term appears in brown et al 1992 pereira et al 1993 ruge 1997 lin 1998b ,0,1,0
in many applications it is natural and convenient to construct classbased language models that is models based on classes of words brown et al 1992 ,0,1,0
agglomerative clustering eg brown et al 1992 li 1996 can produce hierarchical word categories from an unannotated corpus ,0,1,0
in classbased ngram modeling brown et al 1992 for example classbased ngrams are used to determine the probability of occurrence of a pos class given its preceding classes and the probability of a particular word given its own pos class ,0,1,0
distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired schutze 1995 clark 2000 finch et al 1995 probabilistic models have been used to find classes that can improve smoothing and reduce perplexity brown et al 1992 saul and pereira 1997 ,0,1,0
for example we can use automatically extracted hyponymy relations hearst 1992 shinzato and torisawa 2004 or automatically induced mn clusters rooth et al 1999 torisawa 2001 ,0,1,0
they constructed word clusters by using hmms or browns clustering algorithm brown et al 1992 which utilize only information from neighboring words ,0,1,0
to scale lms to larger corpora with higherorder dependencies researchers work completed while this author was at google inc have considered alternative parameterizations such as classbased models brown et al 1992 model reduction techniques such as entropybased pruning stolcke 1998 novel represention schemes such as suffix arrays emami et al 2007 golomb coding church et al 2007 and distributed language models that scale more readily brants et al 2007 ,0,1,0
22 brown clustering algorithm in order to provide word clusters for our experiments we used the brown clustering algorithm brown et al 1992 ,0,1,0
to group the letters into classes we employ a hierarchical clustering algorithm brown et al 1992 ,0,1,0
129 5 active learning whereas a passive supervised learning algorithm is provided with a collection of training examples that are typically drawn at random an active learner has control over the labelled data that it obtains cohn et al 1992 ,0,1,0
we have 11 hypernym patterns based on patterns proposed by hearst 1992 and snow et al 2005 12 sibling patterns which are basically conjunctions and 13 partof patterns based on patterns proposed by girju et al 2003 and cimiano and wenderoth 2007 ,0,1,0
53 performance of taxonomy induction in this section we compare the following automatic taxonomy induction systems he the system by hearst 1992 with 6 hypernym patterns gi the system by girju et al ,0,1,0
patternbased approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen either manually berland and charniak 1999 kozareva et al 2008 or via automatic bootstrapping hearst 1992 widdows and dorow 2002 girju et al 2003 ,0,1,0
clusteringbased approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors brown et al 1992 lin 1998 ,0,1,0
agglomerative clustering brown et al 1992 caraballo 1999 rosenfeld and feldman 2007 yang and callan 2008 iteratively merges the most similar clusters into bigger clusters which need to be labeled ,0,1,0
previous approaches eg miller et al 2004 and koo et al 2008 have all used the brown algorithm for clustering brown et al 1992 ,0,1,0
class based models brown et al pereira et al 1993 hirschman 1986 resnik 1992 distinguish between unobserved cooccurrences using classes of similar words ,0,1,0
however only recently has work been done on the automatic computation of such relationships from text quantifying similarity between words and clustering them brown et al 1992 pereira et al 1993 ,0,1,0
one other published model for grouping semantically related words brown et al 1992 is based on a statistical model of bigrams and trigrams and produces word groups using no linguistic knowledge but no evaluation of the results is reported ,0,1,0
brown et al 1992 where the same idea of improving generalization and accuracy by looking at word classes instead of individual words is used ,0,1,0
the notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in brown et al 1992 ,1,0,0
272 similaritybased estimation was first used for language modeling in the cooccurrence smoothing method of essen and steinbiss 1992 derived from work on acoustic model smoothing by sugawara et al ,0,1,0
the classbased approaches brown et al 1992 resnik 1992 pereira et al 1993 calculate cooccurrence data of words belonging to different classes rather than individual words to enhance the cooccurrence data collected and to cover words which have low occurrence frequencies ,0,1,0
these 30 questions are determined by growing a classification tree on the word vocabulary as described in brown et al 1992 ,0,1,0
given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues brown et al 1992 pereira et al 1993 hatzivassiloglou and mckeown 1993 identifying the semantic orientation of words would allow a system to further refine the retrieved semantic similarity relationships extracting antonyms ,0,1,0
wang and hirschberg 1992 wightman and ostendorf 1994 stolcke and shriberg 1996a kompe et al 1994 mast et al 1996 and on speech repair detection and correction eg ,0,1,0
since there is no wellagreed to definition of what an utterance is we instead focus on intonational phrases silverman et al 1992 which end with an acoustically signaled boundary lone ,0,1,0
black et al 1992 materman 1995 we treat the word identities as a further refinement of the pos tags thus we build a word classification tree for each pos tag ,0,1,0
an exception is the use of similarity for alleviating the sparse data problem in language modeling essen steinbiss 1992 brown et al 1992 dagan et al 1994 ,0,1,0
in our experiments the class assignment is performed by maximizing the mutual information between adjacent phrases following the line described in brown 301 et al 1992 with only the modification that candidates to clustering are phrases instead of words ,0,1,0
in some cases class or part of speech ngrams are used instead of word ngramsbrown et al 1992 chang and chen 1996 ,0,1,0
there have been a number of methods proposed in the literature to address the word clustering problem eg brown et al 1992 pereira et al 1993 li and abe 1996 ,0,1,0
our method is a natural extension of those proposed in brown et al 1992 and li and abe 1996 and overcomes their drawbacks while retaining their advantages ,0,0,1
to cope with this problem we 898 use the concept of class proposed for a word ngram model brown et al 1992 ,0,1,0
to avoid this problem we use the concept of class proposed for a word ngram model brown et al 1992 ,0,1,0
syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts eg brown et al 1992 yarowsky 1992 as well as in similar predicateargument structure contexts eg grishman and sterling 1994 ,0,1,0
the mutual information clustering algorithmbrown et al 1992 were used for this ,0,1,0
furthermore early work on classbased language models was inconclusive brown et al 1992 ,0,1,0
in this spirit we introduce a generalization of the classic kgram models widely used for string processing brown et al 1992 ney et al 1995 to the case of trees ,0,1,0
brown et al 1992 ,0,1,0
for better probability estimation the model was extended to work with hidden word classes brown et al 1992 ward and issar 1996 ,0,1,0
in the literature approaches to construction of taxonomies of concepts have been proposed brown et al 1992 mcmahon and smith 1996 sanderson and croft 1999 ,0,1,0
proceedings of the conference on empirical methods in natural 2 automatic thesaurus extraction the development of large thesauri and semantic resources such as wordnet fellbaum 1998 has allowed lexical semantic information to be leveraged to solve nlp tasks including collocation discovery pearce 2001 model estimation brown et al 1992 clark and weir 2001 and text classi cation baker and mccallum 1998 ,0,1,0
the clusters were found automatically by attempting to minimize perplexity brown et al 1992 ,0,1,0
the second type has clear interpretation as a probability model but no criteria to determine the number of clusters brown et al 1992 kneser and ney 1993 ,0,1,0
examples have been classbased d2gram models brown et al 1992 kneser and ney 1993 smoothing techniques for structural disambiguation li and abe 1998 and word sense disambiguation shutze 1998 ,0,1,0
among various language modeling approaches ngram modeling has been widely used in many applications such as speech recognition machine translation katz 1987 jelinek 1989 gale and church 1990 brown et al 1992 yang et al 1996 bai et al 1998 zhou et al 1999 rosenfeld 2000 gao et al 2002 ,0,1,0
brown et al 1992 is one of the first works to use statistical methods of distributional analysis to induce clusters of words ,0,1,0
while we have shown an increase in performance over a purely syntactic baseline model the algorithm of brown et al 1992 there are a number of avenues to pursue in extending this work ,0,0,1
the corpus used for training our models was on the order of 100000 words whereas that used by brown et al 1992 was around 1000 times this size ,0,1,0
since there is no practical way of determining the classification a0 which maximizes this quantity for a given corpus brown et al 1992 use a greedy algorithm which proceeds from the initial classification performing the merge which results in the least loss in mutual information at each stage ,0,1,0
the observation probabilities for a given state representing a certain word class are determined by the relative frequencies of words belonging to that class as determined by the algorithm of brown et al 1992 the probabilities of other words are set to a small initial value ,0,1,0
since these morphological generalizations are based on the initial categorization provided by the algorithm of brown et al 1992 we hope that they will foster speedy convergence of hnn training ,0,1,0
for example the classbased language model of brown et al 1992 is defined as pw2w1 pw2c2pc2c1 1 this helps solve the sparse data problem since the number of classes is usually much smaller than the number of words ,1,0,0
1994 uses the mutual information clustering algorithm described in brown et al 1992 ,0,1,0
it has been known for some years that good performance can be realized with partial tagging and a hidden markov model cutting et al 1992 ,0,1,0
fortunately using distributional characteristics of term contexts it is feasible to induce partofspeech categories directly from a corpus of suf cient size as several papers have made clear brown et al 1992 schcurrency1utze 1993 clark 2000 ,0,1,0
our approach to inducing syntactic clusters is closely related to that described in brown et al 1992 which is one of the earliest papers on the subject ,0,1,0
clark 2000 reports results on a corpus containing 12 million terms schcurrency1utze 1993 on one containing 25 million terms and brown et al 1992 on one containing 365 million terms ,0,1,0
this paper is heavily indebted to prior work on unsupervised learning of position categories such as brown et al 1992 schtze 1997 higgins 2002 and others cited there ,1,0,0
a key example is that of classbased language models brown et al 1992 dagan et al 1999 where clustering approaches are used in order to partition words determined to be similar into sets ,0,1,0
this approach to term clustering is closely related to others from the literature brown et al 1992 clark 20002 recall that the mutual information between random variables a0 and a1 can be written a2a4a3a6a5a8a7a10a9a11a13a12a15a14a17a16a19a18a21a20a23a22a25a24a27a26a29a28 a14a17a16a19a18a21a20a23a22a25a24 a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24 1 here a0 and a1 correspond to term and context clusters respectively each event a18 and a22 the observation of some term and contextual term in the corpus ,0,1,0
many approaches for pos tagging have been developed in the past including rulebased tagging brill 1995 hmm taggers brants 2000 cutting and others 1992 maximumentropy models rathnaparki 1996 cyclic dependency networks toutanova et al 2003 memorybased learning daelemans et al 1996 etc all of these approaches require either a large amount of annotated training data for supervised tagging or a lexicon listing all possible tags for each word for unsupervised tagging ,0,1,0
these problems include collocation discovery pearce 2001 smoothing and estimation brown et al 1992 clark and weir 2001 and question answering pasca and harabagiu 2001 ,0,1,0
there are many choices for modeling cooccurrence data brown et al 1992 pereira et al 1993 blei et al 2003 ,0,1,0
this merging of contexts is different than clustering words eg clark 2000 brown et al 1992 but is applicable as word clustering relies on knowing which contexts identify the same category ,0,1,0
the technique is based on word class models pioneered by brown et al 1992 which hierarchically 151 conll03 conll03 muc7 muc7 web component test data dev data dev test pages 1 baseline 8365 8925 7472 7128 7141 2 1 gazetteer match 8722 9161 8583 8043 7446 3 1 word class model 8682 9085 8025 7988 7226 4 all external knowledge 8855 9249 8450 8323 7444 table 4 utility of external knowledge ,0,1,0
the approach is related but not identical to distributional similarity for details see brown et al 1992 and liang 2005 ,0,1,0
brown et al 1992 peter f brown vincent j della pietra petere v desouza jenifer c lai and robert l mercer ,0,1,0
a number of knowledgerich jacobs and rau 1990 calzolari and bindi 1990 mauldin 1991 and knowledgepoor brown et al 1992 hindle 1990 ruge 1991 grefenstette 1992 methods have been proposed for recognizing when words are similar ,0,1,0
other researchers have also reported similar problems of excessive resource demands with the collect all neighbors model gale et al 1992 ,0,1,0
other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word classbrown et al 1992 or do not exploit as much linguistic knowledge as we do pereira et al 1993 ,0,0,1
it has been used for diverse problems such as machine translation and sense disambiguation gale et al 1992 schiltze 1992 ,0,1,0
31 distributionally derived groupings distributional cluster brown et al 1992 head body hands eye voice arm seat hair mouth word head 17 alternatives 00000 crown peak summit head top subconceptofupperbound 00000 principal school principal head teacher head educator who has executive authority 00000 head chief top dog subeoncept of leader 00000 head a user of usually soft drugs 01983 head the head of the page the head of the fist 01983 beginning head origin root source the point or place where something begins 00000 pass head straits a difficult juncture a pretty pass 00000 headway head subconcept of progress progression advance 00903 point hod a vshaped mark at one end of an arrow pointer 00000 heading head a line of text serving to indicate what the passage below it is about 00000 mind head intellect psyche that which is responsible for your thoughts and feelings 05428 head the upper or front part of the body that contains the faee and brains 00000 toilet lavatory can head facility john privy bathroom 00000 head the striking part of a tool hammerhead 01685 head a part that projects out from the rest the head of the nail pinhead 00000 drumhead head stretched taut 00000 oral sex head oralgenital stimulation word body 8 alternatives 00000 body an individual 3dimensional object that has mass 00000 gathering assemblage assembly body confluence group of people together in one place 00000 body people associated by some common tie or occupation 00000 body the centralmessage of a communication 09178 torso trunk body subconcept of body part member 00000 body organic structure the entire physical structure of an animal or human being 60 00822 consistency consistence body subeoncept of property 00000 fuselage body the central portion of an airplane word hands 00000 00653 00653 00000 00000 00000 02151 07196 00000 00000 10 alternatives hand subconeept of linear unit hired hand hand hired man a hired laborer on a farm or ranch bridge player hand we need a 4th hand for bridge hand deal the cards held in a card game by a given player at any given time hand a round of applause to signify approval give the little lady a great big hand handwriting cursive hand script something written by hand hand ability he wanted to try his hand at singing hand manus hook mauler mitt paw the distal extremity of the superior limb hand subconcept of pointer hand physical assistance give me a hand with the chores word eye 4 alternatives 01479 center centre middle heart eye approximately central within some region 01547 eye good judgment she has an eye for fresh talent 06432 eye eyeball oculus optic peeper organ of sight 00542 eye a sanall hole or loop as in a needle word voice 7 alternatives 00000 01414 01122 02029 03895 00000 01539 voice the relation of the subject of a verb to the action that the verb denotes spokesperson spokesman interpreter representative mouthpiece voice voice vocalization the sound made by the vibration of vocal folds articulation voice expressing in coherent verbal form i gave voice to my feelings part voice the melody carried by a particular voice or instrument in polyphonic music voice the ability to speak he lost his voice voice the distinctive sound of a persons speech i recognized her voice word arm 6 alternatives 00000 branch subdivision arm an administrative division a branch of congress 06131 arm eornrnonly used to refer to the whole superior limb 00346 weapon arm weapon system used in fighting or hunting 02265 sleeve arm attached at armhole 01950 arm any projtion that is thought to resemble an arm the arm of the record player 00346 arm the part of an armchair that supports the elbow and forearm of a seated person word seat 6 alternatives 00000 seat a city from which authority is exercised 00000 seat place a space reserved for sitting 07369 buttocks arse butt backside burn buns can 02631 seat covers the buttocks 00402 seat designed for sitting on 00402 seat where one sits word hair 5 00323 02313 10000 10000 10000 alternatives hair pilus threadlike keratinous filaments growing from the skin of mammals hair tomentum filamentous hairlike growth on a plant hair follicular growth subeoncept of externalbody part hair mane head of hair hair on the head hair hairy covering of an animal or body part word mouth 5 alternatives 00000 mouth the point where a stream issues into a larger body of water 00000 mouth an opening that resembles a mouth as of a cave or a gorge 00613 sass sassing baektalk lip mouth an impudent or insolent rejoinder 09387 mouth oral cavity subconcept of cavity body cavity bodily cavity 09387 mouth trap hole maw yap muzzle suout list includes informal terms for mouth this group was among classes handselected by brown et al as particularly interesting ,0,1,0
distributional cluster brown et al 1992 cost expense risk profitability deferral earmarks capstone cardinality mintage reseller word cost 2 alternatives 05426 cost price terms damage the amount of money paid for something 04574 monetary value price cost the amount of money it would bring if sold word expense 2 alternatives 10000 expense expenditure outlay outgo spending disbursal disbursement 00000 expense a detriment or sacrifice at the expense of word risk 2 alternatives 06267 hazard jeopardy peril risk subconeept of danger 03733 risk peril danger subeonceptofventure word profitability 1 alternatives 10000 profitableness profitability subconcept of advantage benefit usefulness word deferral 3 alternatives 06267 abeyance deferral recess subconcept of inaction inactivity inactiveness 03733 postponement deferment deferral moratorium an agreed suspension of activity 03733 deferral subconeeptofpause wait word earmarks 2 alternatives 02898 earmark identification mark on the ear of a domestic animal 07102 hallmak trademark earmark a distinguishing characteristic or attribute word capstone 1 alternatives 10000 capstone coping stone stretcher used at top of wall word eardinality not in wordnet word mintage 1 alternatives 62 10000 coinage mintage specie metal money subconcept of cash word reseller not in wordnet this cluster was one presented by brown et al as a randomlyselected class rather than one handpicked for its coherence ,0,1,0
5 conclusions and future work the results of the evaluation are exlremely encouraging especially considering that disambiguating word senses to the level of finegrainedness found in wordnet is quite a bit more difficult than disambiguation to the level of homographs hearst 1991 cowie et al 1992 ,0,1,0
bensch and savitch 1992 brill 1991 brown et al 1992 grefenstette 1994 mckcown and hatzivassiloglou 1993 pereira et al 1993 schtltze 1993 ,0,1,0
2 hierarchical clustering of words several algorithms have been proposed for automatically clustering words based on a large corpus jardino and adda 91 brown et al 1992 kneser and ney 1993 martin et al 1995 ueberla 1995 ,0,1,0
the reader is referred to ushioda 1996 and brown et al 1992 for details of mi clustering but we will first briefly summarize the mi clustering and then describe our hierarchical clustering algorithm ,0,1,0
in all other respects our work departs from previous research on broadcoverage 16 i t i i i i i i i i i i i i i i i i i i i i i 1 i i i i i i i 1 i i i i probabilistic parsing which either attempts to learn to predict grrarntical structure of test data directly from a training treebank brill 1993 collins 1996 eisner 1996 jelinek et al 1994 magerman 1995 skine and orishman 1995 sharman et al 1990 or employs a grammar and sometimes a dictionary to capture linguistic expertise directly black et al 1993a grinberg et al 1995 schabes 1992 but arguably at a less detailed and informative level than in the research reported here ,0,1,0
for example the sets of tags and rule labels have been clustered by our team grmmtrian while a vocabulary of about 60000 words has been clustered by machine brown et al 1992 ushioda 1996a ushioda 1996b ,0,1,0
many authors claim that classbased methods are more robust against data sparseness problems dagan1994 pereira 1993 brown et al 1992 ,0,1,0
brown et al 1992 ,0,1,0
in contrast approaches to wsd attempt to take advantage of many different sources of information eg see mcroy 1992 ng and lee 1996 bruce and wiebe 1994 it seems possible to obtain benefit from sources ranging from local collocational clues yarowsky 1993 to membership in semantically or topically related word classes arowsky 1992 resnik 1993 to consistency of word usages within a discourse gale et al 1992 and disambignation seems highly lexically sensitive in effect requiring specialized disamhignators for each polysemous word ,0,1,0
their weights are calculated by deleted interpolation brown et al 1992 ,0,1,0
cutting et al 1992 feldweg 1995 the tagger for grammatical functions works with lexical and contextual probability measures pq ,0,1,0
aggregate models based on higherorder ngrams brown et al 1992 might be able to capture multiword structures such as noun phrases ,0,1,0
in section 2 we examine aggregate markov models or classbased bigram models brown et al 1992 in which the mapping from words to classes 81 is probabilistic ,0,1,0
though several algorithms brown et al 1992 pereira tishby and lee 1993 have been proposed 100 9o 80 4o 20 1000 goo 80 41111 2 5 10 15 20 25 30 5 10 15 20 25 30 iteration of em iteration of em a b figure 1 plots of a training and b test perplexity versus number of iterations of the em algorithm for the aggregate markov model with c 32 classes ,0,1,0
our approach differs in important ways from the use of hidden markov models hmms for classbased language modeling jelinek et al 1992 ,0,1,0
in practice texts contain an enormous number of word sequences brown et al 1992 only a tiny fraction of which are nccs and it takes considerable computational effort to induce each translation model ,0,1,0
various methods are based on mutual information between classes see brown et al 1992 mcmahon and smith 1996 kneser and ney 1993 jardino and adda 1993 martin liermann and ney 1995 ueberla 1995 ,0,1,0
another application of hard clustering methods in particular bottomup variants is that they can also produce a binary tree which can be used for decisiontree based systems such as the spatter parser magerman 1995 or the atr decisiontree partofspeech tagger black et al 1992 ushioda 1996 ,0,1,0
as with similar work eg brown et al 1992 the size of the corpus makes preprocessing such as lemmatization pos tagging or partial parsing too costly ,0,0,1
while schiitze and pedersen 1993 brown et al 1992 and futrelle and gauch 1993 all demonstrate the ability of their systems to identify word similarity using clustering on the most frequently occurring words in their corpus only grefenstette 1992 demonstrates his system by generating word similarities with respect to a set of target words ,0,1,0
this is in contrast to work by researchers such as schiitze and pedersen 1992 brown et al 1992 and futrelle and gauch 1995 where it is often the most frequent words in the lexicon which are clustered predominantly with the purpose of determining their grammatical classes ,0,1,0
while previous researchers have used agglomerative nesting clustering eg brown et al 1992 futrelle and gauch 1993 comparisons with our work are difficult to draw due to their use of the 1000 commonest words from their respective corpora ,0,1,0
in brown et al 1992 the authors provide some sample subtrees resulting from such a 1000word clustering ,0,1,0
precursors to this work include pereira et al 1993 brown et al 1992 brill kapur 1993 jelinek 1990 and brill et al 1990 and as applied to child language acquisition finch chater 1992 ,0,1,0
clustering algorithms have been previously shown to work fairly well for the classification of words into syntactic and semantic classes brown et al 1992 but determining the optimum number of classes for a hierarchical cluster tree is an ongoing difficult problem particularly without prior knowledge of the item classification ,0,0,1
the fact that information consisting of nothing more than bigrams can capture syntactic information about english has already been noted by brown et al 1992 ,0,1,0
in the link grammar framework lagerty et al 1992 della pietra et al 1994 strictly local contexts are naturally combined with longdistance information coming from longrange trigrams ,0,1,0
most clustering schemes etal 1992 kneser and ney 1993 pereira et al 1993 mccandless and glass 1993 bellegarda et al 1996 saul and pereira 1997 use the average entropy reduction to decide when two words fall into the same cluster ,0,1,0
cutting et al 1992 feldweg 1995 the tagger for grammatical functions works with lexical 1 selbst besucht adv vvpp himself visited hat peter sabine vafin ne ne has peter sabine peter never visited sabine himself l hie adv never figure 2 example sentence and contextual probability measures po depending on the category of a mother node q ,0,1,0
black et al 1992 magerman 1994 and view the pos tags and word identities as two separate sources of information ,0,1,0
7another related measure is dunning 1993s likelihood ratio tests for binomial and multinomial distributions which are claimed to be effective even with very much smaller volumes of text than is necessary for other tests based on assumed normal distributions ,1,0,0
for example in the context of syntactic disambiguation black 1993 and magerman 1995 proposed statistical parsing models basedon decisiontree learning techniques which incorporated not only syntactic but also lexicalsemantic information in the decisiontrees ,0,1,0
e max fl f e 23 11 y now the problem of learning probabilistic subcategorization preference is stated as for every verbnoun collocation e in c estimating the probability distribution pfl 6resnik 1993 applys the idea of the kl distance to measuring the association of a verb v and its object noun class c our definition of ekt corresponds to an extension of resniks association score which considers dependencies of more than one casemarkers in a subcategorization frame ,0,1,0
toiletbathroom since the word facility is the subject of employ and is modified by new in 3 we retrieve other words that appeared in the same contexts and obtain the following two groups of selectors the log a column shows the likelihood ratios dunning 1993 of these words in the local contexts subjects of employ with top20 highest likelihood ratios word freq iogk word freq org 64 504 plant 14 310 company 27 286 operation 8 230 industry 9 146 firm 8 135 pirate 2 121 unit 9 932 shift 3 848 postal service 2 773 machine 3 656 corporation 3 647 manufacturer 3 621 insurance company 2 606 aerospace 2 581 memory device 1 579 department 3 555 foreign office 1 541 enterprise 2 539 pilot 2 537 org includes all proper names recognized as organizations 18 modifiees of new with top20 highest likelihood ratios word freq logk post 432 9529 issue 805 9028 product 675 8886 rule 459 8758 law 356 5415 technology 237 3827 generation 150 3232 model 207 3193 job 260 2692 system 318 2518 word freq log bonds 223 2454 capital 178 2418 order 228 2365 version 158 2237 position 236 2073 high 152 2012 contract 279 1981 bill 208 1949 venture 123 1937 program 283 1838 since the similarity between sense 1 of facility and the selectors is greater than that of other senses the word facility in 3 is tagged sense the key innovation of our algorithm is that a polysemous word is disambiguated with past usages of other words ,0,1,0
the problem of choosing an appropriate level in the hierarchy at which to represent a particular noun sense given a predicate and argument position has been investigated by resnik 1993 li and abe 1998 and llibas 1995 ,0,1,0
for instance mutual information church ct al 1990 and the loglikelihood dunning 1993 methods for extracting word bigrams have been widely used ,1,0,0
we adopted loglikelihood ratio danning 1993 which gave the best pertbrmance among crude noniterative methods in our test experiments 6 ,0,1,0
mutual infornaation involves a problem in that it is overestimated for lowfrequency terms iunning 1993 ,0,1,0
several techniques and results have been reported on learning subcategorization frames sfs from text corpora webster and marcus 1989 brent 1991 brent 1993 brent 1994 ushioda et al 1993 manning 1993 ersan and charniak 1996 briscoe and carroll 1997 carroll and minnen 1998 carroll and rooth 1998 ,0,1,0
using the values computed above pl 7tl k2 p2 77 2 kl k2 p 7z 1 it 2 taking these probabilities to be binomially distributed the log likelihood statistic dunning 1993 is given by 2 log a 2log lpt kl rtl log lp2 k2 rl2 log lp kl n2 log lp k2 n2 where log lp n k k logp z klog1 p according to this statistic tile greater the value of 2 log a for a particular pair of observed frame and verb the more likely that frame is to be valid sf of the verb ,0,1,0
5 comparison with related work preliminary work on sf extraction from coqora was done by brent 1991 brunt 1993 brent 1994 and webster and marcus 1989 ushioda et al 1993 ,0,1,0
however dunning 1993 pointed out that for the purpose of corpus statistics where the sparseness of data is an important issue it is better to use the loglikelihood ratio ,0,1,0
we describe the experiment in greater detail 2the particular verbs selected were looked up in levin 1993 and the class for each verb in the classification system defined in stevenson and merlo 1997 was selected with some discussion with linguists ,0,1,0
for example in john saw mary yesterday at the station only john and mary are required arguments while the other constituents are optional adjuncts3 the problem of sf identification using statistical methods has had a rich discussion in the literature ushioda et al 1993 manning 1993 briscoe and carroll 1997 brent 1994 also see the refences cited in sarkar and zeman 2000 ,0,1,0
for instance the mutual information church et al 1990 and loglikelihood ratio dunning 1993 cohen 1995 have been widely used for extracting word bigrams ,1,0,0
in order to avoid this problem we implemented a simple bootstrapping procedure in which a seed data set of 100 instances of each of the eight categories was hand tagged and used to generate a decision list classifier using the c45 algorithm quinlan 1993 with the word frequency and topic signature features described below ,0,1,0
the topic signatures are automatically generated for each specific term by computing the likelihood ratio score between two hypotheses dunning 1993 ,0,1,0
the scores were then weighted by the inverse of their height in the tree and then summed together similarly to the procedure in resnik 1993 ,0,1,0
methods 41 experiment 1 held out data to examine the generalizability of classifiers trained on the automatically generated data a c45 decision tree classifier quinlan 1993 was trained and tested on the held out test set described above ,0,1,0
each word i in the context vector of w is then weighted with a measure of its association with w we chose the loglikelihood ratio test dunning 1993 to measure this association the context vectors of the target words are then translated with our general bilingual dictionary leaving the weights unchanged when several translations are proposed by the dictionary we consider all of them with the same weight the similarity of each source word s for each target word t is computed on the basis of the cosine measure the similarities are then normalized to yield a probabilistic translation lexicon pts ,0,1,0
31 the likelihood ratio we adopted a method for collocation discovery based on the likelihood ratio dunning 1993 ,0,1,0
its distribution is asymptotic to a 2 distribution and can hence be used as a test statistic dunning 1993 ,0,1,0
3 0 log 2 log a lh lh 1 problems for an unscaled log approach although log identifies collocations much better than competing approaches dunning 1993 in terms of its recall it suffers from its relatively poor precision rates ,1,0,0
we have begun experimenting with log likelihood ratio dunning 1993 as a thresholding technique ,0,1,0
we apply the log likelihood principle dunning 1993 to compute this score ,0,1,0
the measure of predictiveness we employed is log likelihood ratio with respect to the target variable dunning 1993 ,0,1,0
the candidates were then ranked according to the scores assigned by four association measures the loglikelihood ratio g2 dunning 1993 pearsons chisquared statistic x2 manning and schutze 1999 169172 the tscore statistic t church et al 1991 and mere cooccurrence frequency f4 tps were identified according to the definition of krenn 2000 ,0,1,0
smadja 1993 which is the classic work on collocation extraction uses a twostage filtering model in which in the first step ngram statistics determine possible collocations and in the second step these candidates are submitted to a syntactic valida7of course lexical material is always at least partially dependent on the domain in question ,0,1,0
almost all of these measures can be grouped into one of the following three categories a0 frequencybased measures eg based on absolute and relative cooccurrence frequencies a0 informationtheoretic measures eg mutual information entropy a0 statistical measures eg chisquare ttest loglikelihood dices coefficient the corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties dunning 1993 manning and schutze 1999 and their suitability for the task of collocation extraction see evert and krenn 2001 and krenn and evert 2001 for recent evaluations ,0,1,0
22 the choice of cooccurrence qeasure and matrix distance there c many alternatives to measure cooccurrence between two words x and y church 1990 dunning 1993 ,0,1,0
as association measure we apply loglikelihood ratio dunning 1993 to normalized frequency ,0,1,0
beside simple cooccurrence counts within sliding windows other soa measures include functions based on tfidf fung and yee 1998 mutual information pmi lin 1998 conditional probabilities schuetze and pedersen 1997 chisquare test and the loglikelihood ratio dunning 1993 ,0,1,0
many studies on collocation extraction are carried out based on cooccurring frequencies of the word pairs in texts choueka et al 1983 church and hanks 1990 smadja 1993 dunning 1993 pearce 2002 evert 2004 ,0,1,0
thus the alignment set is denoted as 1 ialiaia ii we adapt the bilingual word alignment model ibm model 3 brown et al 1993 to monolingual word alignment ,0,1,0
one popular and statistically appealing such measure is loglikelihood ll dunning 1993 ,1,0,0
for example in this work we use loglikelihood ratio dunning 1993 to determine the soa between a word sense and cooccurring words and cosine to determine the distance between two dpwss log likelihood vectors mcdonald 2000 ,0,1,0
this further supports the claim by dunning 1993 that loglikelihood ratio is much less sensitive than pmi to low counts ,1,0,0
we then scored each query pair q1q2 in this subset using the loglikelihood ratio llr dunning 1993 between q1 and q2 which measures the mutual dependence within the context of web search queries jones et al 2006a ,0,1,0
the significance values are obtained using the loglikelihood measure assuming a binomial distribution for the unrelatedness hypothesis dunning 1993 ,0,1,0
as a result we can use collocation measures like pointwise mutual information church and hanks 1989 or the loglikelihood ratio dunning 1993 to predict the strong association for a given cue ,0,1,0
by default the loglikelihood ratio measure llr is proposed since it was shown to be particularly suited to language data dunning 1993 ,1,0,0
collocation map that is first suggested in itan 1993 is a sigmoid belief network with words as probabilistic variables ,0,1,0
the problem is due to the assumption of normality in naive frequency based statistics according to dunning 1993 ,0,1,0
proceedings of eacl 99 determinants of adjectivenoun plausibility maria lapata and scott mcdonald and frank keller school of cognitive science division of informatics university of edinburgh 2 buccleuch place edinburgh eh8 9lw uk mlap scottm keller cogsciedacuk abstract this paper explores the determinants of adjectivenoun plausibility by using correlation analysis to compare judgements elicited from human subjects with five corpusbased variables cooccurrence frequency of the adjectivenoun pair noun frequency conditional probability of the noun given the adjective the loglikelihood ratio and resniks 1993 selectional association measure ,0,1,0
conditional probability the loglikelihood ratio and resniks 1993 selectional association measure were also significantly correlated with plausibility ratings ,0,1,0
the research presented in this paper is similar in motivation to resniks 1993 work on selectional restrictions ,0,1,0
we employ the loglikelihood ratio as a measure of the collocational status of the adjectivenoun pair dunning 1993 daille 1996 ,0,1,0
we estimated the probabilities pc i pi and pc similarly to resnik 1993 by using relative frequencies from the bnc together with wordnet miller et al 1990 as a source of taxonomic semantic class information ,0,1,0
many studies focus on rare words dunning 1993 moore 2004 butterflies are more interesting than moths ,0,1,0
several other measures like loglikelihood dunning 1993 pearsons a2a4a3 church et al 1991 zscore church et al 1991 cubic association ratio mi3 etc have been also proposed ,0,1,0
5httpclcsokayamauacjprsc jacabit a4a6a5 which gathers the set of cooccurrence units a7 associated with the number of times that a7 and a2 occur together a8a6a9a10a9 a5 a11 in order to identify speci c words in the lexical context and to reduce wordfrequency effects we normalize context vectors using an association score such as mutual information fano 1961 or loglikelihood dunning 1993 ,0,1,0
4 pattern switching the compositional translation presents problems which have been reported by baldwin and tanaka 2004 brown et al 1993 fertility swts and mwts are not translated by a term of a same length ,0,1,0
all the enumerated segment pairs are listed in the following table feature xy feature xy am11 c1 c0 am21 c2c1 c0 am12 c1 c0c1 am22 c2c1 c0c1 am13 c1 c0c1c2 am31 c3c2c1 c0 we use dunnings method dunning 1993 because it does not depend on the assumption of normality and it allows comparisons to be made between the signiflcance of the occurrences of both rare and common phenomenon ,1,0,0
choueka 1988 regarded mwe as connected collocations a sequence of neighboring words whose exact meaning cannot be derived from the meaning or connotation of its components which means that mwes also have low st as some pioneers provide mwe identiflcation methods which are based on association metrics am such as likelihood ratio dunning 1993 ,0,1,0
due to the parameter interdependencies introduced by the onetoone assumption we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in brown et al 1993b equation 26 ,0,1,0
bilingual lexicographers can work with bilingual concordancing software that can point them to instances of any link type induced from a bitext and display these instances sorted by their contexts eg simard foster and perrault 1993 ,0,1,0
the performance of crosslanguage information retrieval with a uniform t is likely to be limited in the same way as the performance of conventional information retrieval without termfrequency information ie where the system knows which terms occur in which documents but not how often buckley 1993 ,0,1,0
1993b this model is symmetric because both word bags are generated together from a joint probability distribution ,0,1,0
dunning 1993 has called attention to the loglikelihood ratio g 2 as appropriate for the analysis of such contingency tables especially when such contingency tables concern very low frequency words ,0,1,0
dunning 1993 argues for the use of g 2 rather than x 2 based on an analysis of the sampling distributions of g 2 and x 2 and results obtained when using the statistics to acquire highly associated bigrams ,0,1,0
8 an alternative formula for g 2 is given in dunning 1993 but the two are equivalent ,0,1,0
dunning 1993 argues for the use of g 2 rather than x 2 based on the claim that the sampling distribution of g 2 approaches the true chisquare distribution quicker than the sampling distribution of x 2 however agresti 1996 page 34 makes the opposite claim the sampling distributions of x 2 and g 2 get closer to chisquared as the sample size n increasesthe convergence is quicker for x 2 than g 2 in addition pedersen 2001 questions whether one statistic should be preferred over the other for the bigram acquisition task and cites cressie and read 1984 who argue that there are some cases where the pearson statistic is more reliable than the loglikelihood statistic ,0,1,0
alternative classbased estimation methods the approaches used for comparison are that of resnik 1993 1998 subsequently developed by ribas 1995 and that of li and abe 1998 which has been adopted by mccarthy 2000 ,0,1,0
the example paper we use throughout the article is f pereira n tishby and l lees distributional clustering of english words acl1993 cmp lg9408011 it was chosen because it is the paper most often cited within our collection ,0,1,0
we measured associations using the loglikelihood measure dunning 1993 for each combination of target category and semantic class by converting each cell of the contingency into a 22 contingency table ,0,1,0
there are good reasons for using such a handcrafted genrespecific verb lexicon instead of a general resource such as wordnet or levins 1993 classes many verbs used in the domain of scientific argumentation have assumed a specialized meaning which our lexicon readily encodes ,0,1,0
5 the semcor collection miller et al 1993 is a subset of the brown corpus and consists of 352 news articles distributed into three sets in which the nouns verbs adverbs and adjectives have been manually tagged with their corresponding wordnet senses and partofspeech tags using brills tagger 1995 ,0,1,0
introduction the automated analysis of large corpora has many useful applications church and mercer 1993 ,0,1,0
in the usual case considered by dunning 1993 and discussed by manning and sch utze 1999 the righthand side of the equation is larger than the lefthand side ,0,1,0
a period should therefore be interpreted as an abbreviation marker and not as a sentence boundary marker if the two tokens surrounding it can indeed be considered as a collocation according to dunnings 1993 original loglikelihood ratio amended with the onesidedness constraint introduced in section 22 ,0,1,0
for english we have used sections 0306 of the wsj portion of the penn treebank marcus santorini and marcinkiewicz 1993 distributed by the linguistic data consortium ldc which have frequently been used to evaluate sentence boundary detection systems before compare section 7 ,0,1,0
the usefulness of likelihood ratios for collocation detection has been made explicit by dunning 1993 and has been confirmed by an evaluation of various collocation detection methods carried out by evert and krenn 2001 ,0,1,0
21 likelihood ratios in the typebased stage the loglikelihood ratio by dunning 1993 tests whether the probability of a word is dependent on the occurrence of the preceding word type ,0,1,0
one could use the estimated cooccurrences from a small sample to compute the test statistics most commonly pearsons chisquared test the likelihood ratio test fishers exact test cosine similarity or resemblance jaccard coefficient dunning 1993 manning and schutze 1999 agresti 2002 moore 2004 ,0,1,0
the third function is an original variant of the second the fourth is original and the fifth is prompted by the arguments of dunning 1993 ,0,1,0
lexical collocation functions especially those determined statistically have recently attracted considerable attention in computational linguistics calzolari and bindi 1990 church and hanks 1990 sekine et al 1992 hindle and rooth 1993 mainly though not exclusively for use in disambiguation ,0,1,0
dunning 1993 also used windows of size 2 which corresponds to word bigrams ,0,1,0
33 syntax based approach an alternative to the window and documentoriented approach is to use syntactical information grefenstette 1993 ,0,1,0
dunning 1993 used a likelihood ratio to test word similarity under the assumption that the words in text have a binomial distribution ,0,1,0
1 introduction many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts dunning 1993 church and hanks 1990 dagan et al 1999 ,0,1,0
the chunker is trained on the answer side of the training corpus in order to learn 2 and 3word collocations defined using the likelihood ratio of dunning 1993 ,0,1,0
for comparing the sentence generator sample to the english sample we compute loglikelihood statistics dunning 1993 on neighboring words that at least cooccur twice ,0,1,0
we use the loglikelihood ratio for determining significance as in dunning 1993 but other measures are possible as well ,0,1,0
schtze 1993 is not suited to highly skewed distributions omnipresent in natural language ,0,1,0
throughout the likelihood ratio dunning 1993 is used as significance measure because of its stable performance in various evaluations yet many more measures are possible ,1,0,0
we then ranked the collected query pairs using loglikelihoodratiollrdunning 1993 whichmeasures the dependence between q1 and q2 within the context of web queries jones et al 2006b ,0,1,0
the remarks on the a3 a4 measure in dunning 1993 ,0,1,0
substituting the probabilities in the pmi formula with the previously introduced web statistics we obtain a15a17a16a25a18a26a11a22a21 qspa49a6a50a22a51a6a52 aspa24 a15a17a16a25a18a26a11a22a21 qspa24a56a55a57a15a33a16a19a18a26a11a6a21 aspa24 a55 a38 a1a6a39a17a34a40a1a8a41a45a43a46a11 maximal likelihood ratio mlhr is also used for word cooccurrence mining dunning 1993 ,0,1,0
proceedings of the 40th annual meeting of the association for in a key step for locating important sentences neats computes the likelihood ratio dunning 1993 to identify key concepts in unigrams bigrams and trigrams1 using the ontopic document collection as the relevant set and the offtopic document collection as the irrelevant set ,0,1,0
these methods often involve using a statistic such as 2 gale and church 1991 or the log likelihood ratio dunning 1993 to create a score to measure the strength of correlation between source and target words ,0,1,0
these constraints tie words in such a way that the space of alignments cannot be enumerated as in ibm models 1 and 2 brown et al 1993 ,0,1,0
each element of the resulting vector was replaced with its loglikelihood value see definition 10 in section 23 which can be considered as an estimate of how surprising or distinctive a cooccurrence pair is dunning 1993 ,0,1,0
our approach was to identify a parallel corpus of manually and automatically transcribed documents the tdt2 corpus and then use a statistical approach dunning 1993 to identify tokens with significantly table 5 impact of recall and precision enhancing devices ,0,1,0
neats computes the likelihood ratio dunning 1993 to identify key concepts in unigrams bigrams and trigrams and clusters these concepts in order to identify major subtopics within the main topic ,0,1,0
we have 21 21 trictrictric trictritri erpercpercp ecrcpecp 6 assumption 2 for an english triple tri e assume that i c only depends on 12 i i e and c r only depends on e r equation 6 is rewritten as 2211 21 ec trietrictrictritri rrpecpecp erpercpercpecp 7 notice that 11 ecp and 22 ecp are translation probabilities within triples they are different from the unrestricted probabilities such as the ones in ibm models brown et al 1993 ,0,1,0
to test whether a better set of initial parameter estimates can improve model 1 alignment accuracy we use a heuristic model based on the loglikelihoodratio llr statistic recommended by dunning 1993 ,0,1,0
in order to filter some noise caused by the error alignment links we only retain those translation pairs whose translation probabilities are above a threshold 1 d 1 or cooccurring frequencies are above a threshold 2 when we train the ibm statistical word alignment model with a limited bilingual corpus in the specific domain we build another translation dictionary with the same method as for the dictionary but we adopt a different filtering strategy for the translation dictionary we use loglikelihood ratio to estimate the association strength of each translation pair because dunning 1993 proved that loglikelihood ratio performed very well on smallscale data ,1,0,0
smadja 1993 also detailed techniques for collocation extraction and developed a program called xtract which is capable of computing flexible collocations based on elaborated statistical calculation ,0,1,0
2 statistical word alignment according to the ibm models brown et al 1993 the statistical word alignment model can be generally represented as in equation 1 ,0,1,0
 m aj j m j aj l i i l i ii m j j mlajdeft en pp m ap 01 11 1 2 0 0 0 pr 00 eef 3 1 a cept is defined as the set of target words connected to a source word brown et al 1993 ,0,1,0
in order to filter the noise caused by the error alignment links we only retain those translation pairs whose loglikelihood ratio scores dunning 1993 are above a threshold ,0,1,0
a variety of methods have been applied ranging from simple frequency justeson katz 1995 modified frequency measures such as cvalues frantzi anadiou mima 2000 maynard anadiou 2000 and standard statistical significance tests such as the ttest the chisquared test and loglikelihood church and hanks 1990 dunning 1993 and informationbased methods eg pointwise mutual information church hanks 1990 ,0,1,0
dunning 1993 or else as with mutual information eschew significance testing in favor of a generic informationtheoretic approach ,0,1,0
22 using loglikelihoodratios to estimate word translation probabilities our method for computing the probabilistic translation lexicon llrlex is based on the the log2httpwwwfjochcomgizahtml likelihoodratio llr statistic dunning 1993 which has also been used by moore 2004a 2004b and melamed 2000 as a measure of word association ,0,1,0
finally the loglikelihood ratios test henceforth llr dunning 1993 is applied on each set of pairs ,0,1,0
this metric tests the hypothesis that the probability of phrase is the same whether phrase has been seen or not by calculating the likelihood of the observed data under a binomial distribution using probabilities derived using each hypothesis dunning 1993 ,0,1,0
this method uses mutual information and loglikelihood which dunning 1993 used to calculate the dependency value between words ,0,1,0
to identify these termsweusetheloglikelihoodstatisticsuggested by dunning dunning 1993 and first used in summarization by lin and hovy hovy and lin 2000 ,0,1,0
partitioning 2 medium and low frequency words as noted in dunning 1993 loglikelihood statistics are able to capture word bigram regularities ,0,1,0
ii apply some statistical tests such as the binomial hypothesis test brent 1993 and loglikelihood ratio score dunning 1993 to sccs to filter out false sccs on the basis of their reliability and likelihood ,0,1,0
such measures as mutual information turney 2001 latent semantic analysis landauer et al 1998 loglikelihood ratio dunning 1993 have been proposed to evaluate word semantic similarity based on the cooccurrence information on a large corpus ,0,1,0
loglikelihood ratio g2 dunning 1993 with respect to a large reference corpus web 1t 5gram corpus brants and franz 2006 is used to capture the contextually relevant nouns ,0,1,0
we compute loglikelihood significance between features and target nouns as in dunning 1993 and keep only the most significant 200 features per target word ,0,1,0
it forms a baseline for performance evaluations but is prone to sparse data problems dunning 1993 ,0,1,0
64 table 1 subjects of employ with highest likelihood ratio word freq loga word freq loga brg 64 504 plant 14 310 company 27 286 operation 8 230 industry 9 146 firm 8 135 pirate 2 121 unit 9 932 shift 3 848 postal service 2 773 machine 3 656 corporation 3 647 manufacturer 3 621 insurance company 2 606 aerospace 2 581 memory device 1 579 department 3 555 foreign office 1 541 enterprise 2 539 pilot 2 537 org includes all proper names recognized as organizations the loga column are their likelihood ratios dunning 1993 ,0,1,0
the likelihood ratio is obtained by treating word and ic as a bigram and computed with the formula in dunning 1993 ,0,1,0
1990 1993 these models have nonuniform linguistically motivated structure at present coded by hand ,0,1,0
5 effectiveness comparison 51 englishchinese atis models both the transfer and transducer systems were trained and evaluated on englishtomandarin chinese translation of transcribed utterances from the atis corpus hirschman et al 1993 ,0,1,0
macklovitch 1994 melamed 1996b concordancing for bilingual lexicography catizone et al 1993 gale church 1991 computerassisted language learning corpus linguistics melby ,0,1,0
the cooccurrence relation can also be based on distance in a bitext space which is a more general representations of bitext correspondence dagan et al 1993 resnik melamed 1997 or it can be restricted to words pairs that satisfy some matching predicate which can be extrinsic to the model melamed 1995 melamed 1997 ,0,1,0
for each cooccurring pair of word types u and v these likelihoods are initially set proportional to their cooccurrence frequency nuv and inversely proportional to their marginal frequencies nu and nv z following dunning 1993 2 ,0,1,0
table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications including crummy mt on the world wide web church iiovy 1993 certain machineassisted translation tools eg ,0,1,0
1 introduction early works gale and church 1993 brown et al 1993 and to a certain extent kay and r6scheisen 1993 presented methods to exct biigua ,0,1,0
in this work model fit is reported in terms of the likelihood ratio statistic g 2 and its significance read and cressie 1988 dunning 1993 ,0,1,0
it is clear that appendix b contains far fewer true noncompositional phrases than appendix a 7 related work there have been numerous previous research on extracting collocations from corpus eg choueka 1988 and smadja 1993 ,0,1,0
we parsed a 125million word newspaper corpus with minipar 1 a descendent of principar lin 1993 lin 1994 and extracted dependency relationships from the parsed corpus ,0,1,0
the frequency counts of dependency relationships are filtered with the loglikelihood ratio dunning 1993 ,0,1,0
not only many combinations are found in the corpus many of them have very similar mutual information values to that of 318 table 2 economic impact verb economic financial political social budgetary ecological economic economic economic economic economic economic economic economic economic object impact impact impact impact impact impact effect implication consequence significance fallout repercussion potential ramification risk mutual freq info 171 185 127 172 46 050 15 094 8 320 4 259 84 070 17 080 59 188 10 084 7 166 7 184 27 124 8 219 17 033 nomial distribution can be accurately approximated by a normal distribution dunning 1993 ,0,1,0
a total of 216 collocations were extracted shown in appendix a we compared the collocations in appendix a with the entries for the above 10 words in the ntcs english idioms dictionary henceforth ntceid spears and kirkpatrick 1993 which contains approximately 6000 definitions of idioms ,0,1,0
levin 1993 assumes that the syntactic realization of a verbs arguments is directly correlated with its meaning cf ,0,1,0
we also experimented with a method suggested by brent 1993 which applies the binomial test on frame frequency data ,0,1,0
for instance the topp frame is poorly represented in the syntactically annotated version of the penn treebank marcus et al 1993 ,0,1,0
however in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case see also grefenstette 1993 so we did not try to include these methods in our system ,0,1,0
they were based on mutual information church hanks 1989 conditional probabilities rapp 1996 or on some standard statistical tests such as the chisquare test or the loglikelihood ratio dunning 1993 ,1,0,0
dunning 1993 reports that we should not rely on the assumption of a normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions is a better alternative for smaller texts ,0,1,0
the different approaches eg brent 991 1993 ushioda et al 1993 briscoe and carroll 1997 manning 1993 carroll and rooth 1998 gahl 1998 lapata 1999 sarkar and zeman 2000 vary largely according to the methods used and the number of scfs being extracted ,0,1,0
according to one account briscoe and carroll 1997 the majority of errors arise because of the statistical filtering process which is reported to be particularly unreliable for low frequency scfs brent 1993 briscoe and carroll 1997 manning 1993 manning and schiitze 1999 ,0,1,0
adopting the scf acquisition system of briscoe and carroll we have experimented with an alternative hypothesis test the binomial loglikelihood ratio llr test dunning 1993 ,0,1,0
brent 1993 estimated the error probabilities for each scf experimentally from the behaviour of his scf extractor which detected simple morphosyntactic cues in the corpus data ,0,1,0
222 the binomial log likelihood ratio as a statistical filter dunning 1993 demonstrates the benefits of the llr statistic compared to pearsons chisquared on the task of ranking bigram data ,1,0,0
we then rankorder the p xy mi xy m z pr zy mi zy g092log p x p y p x p y f y p xy p xy f xy p xy p xy f xy m ig13xx jg13yy f ij g09 ij 2 ij f xy g09 xy xy 1g09 xy n f xy g09 xy f xy 1g09f xy n table 1 probabilistic approaches method formula frequency guiliano 1964 f xy pointwise mutual information mi fano 1961 church and hanks 1990 log p pp 2xy xy selectional association resnik 1996 symmetric conditional probability ferreira and pereira 1999 p pp xy x y 2 dice formula dice 1945 2 f f f xy x y loglikelihood dunning 1993 daille 1996 ,0,1,0
since we need knowledgepoor daille 1996 induction we cannot use humansuggested filtering chisquared g24 2 church and gale 1991 zscore smadja 1993 fontenelle et al 1994 students tscore church and hanks 1990 ngram list in accordance to each probabilistic algorithm ,0,1,0
in particular we use a randomlyselected corpus the first five columns as informationlike consisting of a 67 million word subset of the trec similarly since the last four columns share databases darpa 19931997 ,0,1,0
as a measure of association we use the loglikelihoodratio statistic recommended by dunning 1993 which is the same statistic used by melamed to initialize his models ,0,1,0
in the latter case we use an unsupervised attachment disambiguation method based on the loglikelihood ratio llr dunning 1993 ,0,1,0
one aspect of vpcs that makes them dicult to extract cited in eg smadja 1993 is that the verb and particle can be noncontiguous eg hand the paper in and battle right on ,0,1,0
one of the earliest attempts at extracting interrupted collocations ie noncontiguous collocations including vpcs was that of smadja 1993 ,0,1,0
22 corpus occurrence in order to get a feel for the relative frequency of vpcs in the corpus targeted for extraction namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 vpc types corpus frequency figure 1 frequency distribution of vpcs in the wsj tagger correctextracted prec rec ffl1 brill 135135 1000 0177 0301 penn 667800 0834 0565 0673 table 1 posbased extraction results the wsj section of the penn treebank we took a random sample of 200 vpcs from the alvey natural language tools grammar grover et al 1993 and did a manual corpus search for each ,0,1,0
4 method2 simple chunkbased extraction to overcome the shortcomings of the brill tagger in identifying particles we next look to full chunk 2note this is the same as the maximum span length of 5 used by smadja 1993 and above the maximum attested np length of 3 from our corpus study see section 22 ,0,1,0
likelihood ratios are particularly useful when comparing common and rare events dunning 1993 plaunt and norgard 1998 making them natural here given the rareness of most question categories and the frequency of contributions ,0,1,0
for this present work we use dunnings loglikelihood ratio statistics dunning 1993 defined as follows sim alogablogbclogcdlogd ablogabaclogac bdlogbdcdlogcd abcdlogabcd for each bilingual pattern eijj we compute its similarity score and qualify it as a bilingual sequencetosequence correspondence if no equally strong or stronger association for monolingual constituent is found ,0,1,0
neats computes the likelihood ratio dunning 1993 to identify key concepts in unigrams bigrams and trigrams and clusters these concepts in order to identify major subtopics within the main topic ,0,1,0
first word frequencies context word frequencies in surrounding positions here threewords window are computed following a statisticsbased metrics the loglikelihood ratio dunning 1993 ,0,1,0
to make sense tagging more precise it is advisable to place constraint on the translation counterpart c of w swat considers only those translations c that has been linked with w based the competitive linking algorithm melamed 1997 and logarithmic likelihood ratio dunning 1993 ,0,1,0
there is potential of developing sense definition model to identify and represent semantic and stylistic differentiation reflected in the mrd glosses pointed out in dimarco hirst and stede 1993 ,0,1,0
the approach is in the spirit of smadja 1993 on retrieving collocations from text corpora but is more integrated with parsing ,0,1,0
to prune away those pairs we used the loglikelihoodratio algorithm dunning 1993 to compute the degree of association between the verb and the noun in each pair ,0,1,0
a list of pilot terms ranked from the most representative of the corpus to the least thanks to the loglikelihood coefficient introduced by dunning 1993 ,1,0,0
1 minority report 2 box office 3 scooby doo 4 sixth sense 5 national guard 6 bourne identity 7 air national guard 8 united states 9 phantom menace 10 special effects 11 hotel room 12 comic book 13 blair witch project 14 short story 15 real life 16 jude law 17 iron giant 18 bin laden 19 black people 20 opening weekend 21 bad guy 22 country bears 23 mans man 24 long time 25 spoiler space 26 empire strikes back 27 top ten 28 politically correct 29 white people 30 tv show 31 bad guys 32 freddie prinze jr 33 monsters ball 34 good thing 35 evil minions 36 big screen 37 political correctness 38 martial arts 39 supreme court 40 beautiful mind figure 7 result of reranking output from the phrase extension module 64 revisiting unigram informativeness an alternative approach to calculate informativeness from the foreground lm and the background lm is just to take the ratio of likelihood scores a11 fga9a54a86 a15 a23 a11 bga9a54a86 a15 this is a smoothed version of relative frequency ratio which is commonly used to find subjectspecific terms damerau 1993 ,0,1,0
3 related work word collocation various collocation metrics have been proposed including mean and variance smadja 1994 the ttest church et al 1991 the chisquare test pointwise mutual information mi church and hanks 1990 and binomial loglikelihood ratio test blrt dunning 1993 ,0,1,0
for our baseline we have selected the method based on binomial loglikelihood ratio test blrt described in dunning 1993 ,0,1,0
for that purpose syntactical didier bourigault 1993 statistical frank smadja 1993 ted dunning 1993 gal dias 2002 and hybrid syntaxicostatistical methodologies batrice daille 1996 jeanphilippe goldman et al 2001 have been proposed ,0,1,0
alpha 0 01 02 03 04 05 freq2 13555 13093 12235 11061 10803 10458 freq3 4203 3953 3616 3118 2753 2384 freq4 1952 1839 1649 1350 1166 960 freq5 1091 1019 917 743 608 511 freq2 2869 2699 2488 2070 1666 1307 total 23670 22603 20905 18342 16996 15620 alpha 06 07 08 09 10 freq2 10011 9631 9596 9554 9031 freq3 2088 1858 1730 1685 1678 freq4 766 617 524 485 468 freq5 392 276 232 202 189 freq2 1000 796 627 517 439 total 14257 13178 12709 12443 11805 table 7 number of extracted mwus by frequency 62 qualitative analysis as many authors assess frank smadja 1993 john justeson and slava katz 1995 deciding whether a sequence of words is a multiword unit or not is a tricky problem ,0,1,0
on the other hand purely statistical systems frank smadja 1993 ted dunning 1993 gal dias 2002 extract discriminating mwus from text corpora by means of association measure regularities ,0,1,0
many statistical metrics have been proposed including pointwise mutual information mi church et al 1990 mean and variance hypothesis testing ttest chisquare test etc loglikelihood ratio lr dunning 1993 statistic language model tomokiyo et al 2003 and so on ,0,1,0
relative frequency ratio rfr of terms between two different corpora can also be used to discover domainoriented multiword terms that are characteristic of a corpus when compared with another damerau 1993 ,0,1,0
3 candidates extraction on suffix array suffix array also known as string patarraymanber et al 1993 is a compact data structure to handle arbitrarylength strings and performs much powerful online string search operations such as the ones supported by pattree but has less space overhead ,0,1,0
candidate term segment result of gpws for one sentence in which term appears table 2 examples of candidates eliminated by gpws 5 relative frequency ratio against background corpus relative frequency ratio rfr is a useful method to be used to discover characteristic linguistic phenomena of a corpus when compared with another damerau 1993 ,0,1,0
information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge eg craven and kumlien 1999 hull and gomez 1993 or require humanannotated training data with relation information for each domain craven et al 1998 ,0,1,0
we use the log likelihood ratio llr dunning 1993 given by 2log 2 h o pk 1n 1k 2n 2 h a p 1p 2 n 1k 1n 2k 2 llr measures the extent to which a hypothesized model of the distribution of cell counts h a differs from the null hypothesis h o namely that the percentage of documents containing this term is the same in both corpora ,0,1,0
the statistical significance often evaluate whether two words are independant using hypothesis tests such as tscore church et al 1991 the x2 the loglikelihood dunning 1993 and fishers exact test pedersen 1996 ,0,1,0
a second pass aligns the sentences in a way similar1 to the algorithm described by gale and church 1993 but where the search space is constrained to be close to the one delimited by the word alignment ,0,1,0
first we considered single sentences as documents and tokens as sentences we define a token as a sequence of characters delimited by 1in our case the score we seek to globally maximize by dynamic programming is not only taking into account the length criteria described in gale and church 1993 but also a cognatebased one similar to simard et al 1992 ,0,1,0
in our case we computed a likelihood ratio score dunning 1993 for all pairs of english tokens and inuktitut substrings of length ranging from 3 to 10 characters ,0,1,0
when efficient techniques have been proposed brown et al 1993 och and ney 2003 they have been mostly evaluated on safe pairs of languages where the notion of word is rather clear ,0,1,0
since in these lvcs the complement is a predicative noun in stem form identical to a verb we form development and test expressions by combining give or take with verbs from selected semantic classes of levin 1993 taken from stevenson et al ,0,1,0
1pmi is subject to overestimation for low frequency items dunning 1993 thus we require a minimum frequency of occurrence for the expressions under study ,0,1,0
the sets obtainedare then ranked usingthe loglikelihoodratiostestdunning1993 ,0,1,0
32 german germanis thesecondmostinvestigatedlanguage thanks to the early work of breidt 1993 and morerecently to thatof krennandevertsuch as krennand evert 2001 evert and krenn2001 evert2004centeredonevaluation ,0,1,0
breidt1993 alsopointedouta coupleof problemsthatmakes extractionfor germanmoredifficultthanfor english the stronginflectionfor verbsthe variable wordorderandthepositionalambiguityoftheargumentssheshowsthatevendistinguishingsubjectsfromobjectsisverydifficultwithoutparsing ,0,1,0
such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the british 3 linguist jr firth you shall know a word by the company it keeps firth 1957 p 11 context similarity has been used as a means of extracting collocations from corpora eg by church hanks 1990 and by dunning 1993 of identifying word senses eg by yarowski 1995 and by schutze 1998 of clustering verb classes eg by schulte im walde 2003 and of inducing selectional restrictions of verbs eg by resnik 1993 by abe li 1996 by rooth et al ,0,1,0
2 related work the issue of mwe processing has attracted much attention from the natural language processing nlp community including smadja 1993 dagan and church 1994 daille 1995 1995 mcenery et al 1997 wu 1997 michiels and dufour 1998 maynard and ananiadou 2000 merkel and andersson 2000 piao and mcenery 2001 sag et al 2001 tanaka and baldwin 2003 dias 2003 baldwin et al 2003 nivre and nilsson 2004 pereira et al ,0,1,0
for each candidate triple the loglikelihood dunning 1993 and salience kilgarriff and tugwell 2001 scores were calculated ,0,1,0
the other 5 have been suggested for dutch by hollebrandse 1993 ,0,1,0
it is known that pmi gives undue importance to low frequency events dunning 1993 therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus ,0,1,0
we use the likelihood ratio for a binomial distribution dunning 1993 which tests the hypothesis whether the term occurs independently in texts of biographical nature given a large corpus of biographical and nonbiographical texts ,0,1,0
in this case we use the loglikelihood measure as described in dunning 1993 ,0,1,0
the outcomes of cw resemble those of mincut wu leahy 1993 dense regions in the graph are grouped into one cluster while sparsely connected regions are separated ,0,1,0
to model aspects of cooccurrence association that might be obscured by raw frequency the loglikelihood ratio g2 dunning 1993 was also used to transform the feature space ,0,1,0
in this study we have concentrated on the npsterm extraction which comprises the focus of interest in several studies jacquemin 2001 justeson katz 1995 voutanen 1993 ,0,1,0
generative word alignment models initially developed at ibm brown et al 1993 and then augmented by an hmmbased model vogel et al 1996 have provided powerful modeling capability for word alignment ,0,1,0
prcj1aj1ei1 pjii 1j jproductdisplay j1 pcjeaj 8 312 loglikelihood ratio the loglikelihood ratio statistic has been found to be accurate for modeling the associations between rare events dunning 1993 ,1,0,0
it can be expected that the loglikelihood ratio produces an accurate ranking of word pairs that highly correlates with human judgment dunning 1993 although there are other measures which come close in performance eg rapp 1998 ,1,0,0
corpus dunning 1993 scott 1997 rayson et al 2004 ,0,1,0
21 keywords as our starting point we calculated the keywords of the belgian corpus with respect to the netherlandic corpus both on the basis of a chisquare test with yates continuity correction scott 1997 and the loglikelihood ratio dunning 1993 ,0,1,0
the most obvious comparison takes on the form of a keyword analysis which looks for the words that are significantly more frequent in the one corpus as compared to the other dunning 1993 scott 1997 rayson et al 2004 ,0,1,0
we worked with an implementation of the log likelihood ratio gscore as proposed by dunning 1993 and two variants of the tscore one considering all values tscore and one where only positive values tscore are kept following the results of curran and moens 2002 ,0,1,0
rapp 1999 dunning 1993 but using cosine rather than cityblock distance to measure profile similarity ,0,1,0
given a contextual word cw that occurs in the paragraphs of bc a loglikelihood ratio g2 test is employed dunning 1993 which checks if the distribution of cw in bc is similar to the distribution of cw in rc pcwbc pcwrc null hypothesis ,0,1,0
the algorithm is based on the machine learning method for word categorisation inspired by the well known study on basiclevel categories rosch 1978 presented in basili et al 1993a ,0,1,0
pustejovsky confronted with the problem of automatic acquisition more extensively in pustejovsky et al 1993 ,0,1,0
4 related work the automatic extraction of english subcategorization frames has been considered in brent 1991 brent 1993 where a procedure is presented that takes untamed text as input and generates a list of verbal subcategorization frames ,0,1,0
this statistic is given by 2 log a 2log lp1 kl hi log lp2 k2 n2log lp kl r1log lp k2 n2 where log lco k n k logp n klog1 p and pl p2 p for a detailed description of the statistic used see dunning 1993 ,0,1,0
the logllkelihood ratio g 2 is a mathematically wellgrounded and accurate method for calculating how surprising an event is dunning 1993 ,0,1,0
dunning 1993 and pedersen 1996 shows how some of the methods which have been used in the past particularly mutual information scores are invalid for rare events and introduce accurate measures of how surprising rare events are ,1,0,0
its roots are the same as computational linguistics cl but it has been largely ignored in cl until recently dunning 1993 carletta 1996 kilgarriff 1996 ,0,1,0
this set of words rooted primarily in the verbs of the set corresponds to the levin 1993 characterize class 292 declare 294 admire 312 and judgment verbs 33 and hence may have particular syntactic and semantic patterning ,0,1,0
31 the genderanimaticity statistics after we have identified the correct antecedents it is a simple counting procedure to compute ppwa where wa is in the correct antecedent for the pronoun p note the pronouns are grouped by their gender wain the antecedent for p ppl o when there are multiple relevant words in the antecedent we apply the likelihood test designed by dunning 1993 on all the words in the candidate np ,0,1,0
c c c pcvr is just the probability of the disjunction of the concepts in c that is zpclv r cec in order to see how pclvr relates to the input data note that given a concept c verb v and argument position r a noun can be generated according to the distribution pnc v r where pnlc v r 1 nesync now we have a model for the input data pn v r pvrpnivr pvr pclv rlpntc vr cecnn note that for c cnn pnlc v r o the association norm and similar measures such as the mutual information score have been criticised dunning 1993 because these scores can be greatly overestimated when frequency counts are low ,0,1,0
although this approach can give inaccurate estimates the counts given to the incorrect senses will disperse randomly throughout the hierarchy as noise and by accumulating counts up the hierarchy we will tend to gather counts from the correct senses of related words yarowsky 1992 resnik 1993 ,0,1,0
this example is adapted from resnik 1993 ,0,1,0
have been used in statistical machine translation brown et al 1990 terminology research and translation aids isabelle 1992 ogden and gonzales 1993 van der eijk 1993 bilingual lexicography klavans and tzoukermann 1990 smadja 1992 wordsense disambiguation brown et al 1991b gale et al 1992 and information retrieval in a multilingual environment landauer and littman 1990 ,0,1,0
some methods use sentence alignment and additional statistics to find candidate translations of terms smadja 1992 van der eijk 1993 ,0,1,0
34 related work and issues for future research smadja 1992 and van der eijk 1993 describe term translation methods that use bilingual texts that were aligned at the sentence level ,0,1,0
smadjafrank1993 ,0,1,0
daille 1996 smadja 1993 less prior work exists for bilingual acquisition of domainspecific translations ,0,1,0
for instance one might be interested in frequencies of cooccurences of a word with other words and phrases collocations smadja 1993 or one might be interested in inducing wordclasses from the text by collecting frequencies of the left and right context words for a word in focus finchchater 1993 ,0,1,0
for colnparison we refer here to smadjas method 1993 because this method and the proposed method have much in connnon ,0,1,0
smadja 1993 1 ,0,1,0
algorithms for the computation of firstorder associations have been used in lexicography for the extraction of collocations smadja 1993 and in cognitive psychology for the simulation of associative learning wettler rapp 1993 ,0,1,0
sometimes the notion of collocation is defined in terms of syntax by possible partofspeech patterns or in terms of semantics requiring collocations to exhibit noncompositional meaning smadja 1993 ,0,1,0
future work will include i applying the method to retrieve other types of collocations smadja 1993 and ii evaluating the method using internet directories ,0,1,0
one example of the 450 latter problem is the following in smadja 1993 the nature of a syntactic link between two associated words is detected a posteriori ,0,1,0
of acl 1990 smadja 1993 f smadja retrieving collocations ficma text xtract 1993 ,0,1,0
we propose a corpusbased method biber1993 nagao1993 smadja1993 which generates noun classifier associations nca to overcome the problems in classifier assignment and semantic construction of noun phrase ,0,1,0
unlike smadja 1993 the kevord rnay be part of a chinese word ,0,1,0
further enhancement of these utilities include compiling collocation statistics smadja 1993 and semiautomatic gloassary construction tong 1993 ,0,1,0
for instance there is a substantial body of papers on the extraction of frequently cooccurring words from corpora using statistical methods eg choueka et al 1983 church and hanks 1989 smadja 1993 to list only a few ,0,1,0
iegarling lhis lypu of olloeation the approaches till ilow could be diviled inlo two groups those thai do uo refer to sttbstrings of colloco l ions as a lartiular problem church and llanks t99 kim and cho 1993 nagao and mori 1994 and those that do kita et al t994 smadja 1993 lkchara et al 1995 kjelhner 11994 ,0,1,0
from the extracted ngrams those with a flequcncy of 3 or more were kept other approaches get rid of ngrams of such low frequencies smadja 1993 ,0,1,0
smadja 1993 extracts uninterrupted as well as interrupted collocations predicative relations rigid noun phrases and phrasal templates ,0,1,0
the colllilloil poinis regarding collocations appear to be as smadja 1993 suggestsl they are mbilrary it is nol clear why to bill through means to fail thy are domaindependent interest rate stock market they are recurrenl and cohesive loxical clusters the presence of one of the ,0,1,0
smadja 1993 kits et al 1994 ikehara et al 1995 mention about substrings of collocations ,0,1,0
some papers fung wu 1994 wang et al 1994 based on smadjas paradigm 1993 learned an aided dictionary from a corpus to reduce the possibility of unknown words ,0,1,0
in the past five years important research on the automatic acquisition of word classes based on lexical distribution has been published church and hanks 1990 hindle 1990 smadja 1993 greinstette 1994 grishman and sterling 1994 ,1,0,0
they first extract english collocations using the xtract systetn smadja 1993 and theu look for french coutlterparts ,0,1,0
there are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words church and hanks 1990 the distance between words smadja and makeown 1990 and the number of combined words and frequency of appearance kita 1993 1994 ,0,1,0
thus conventional methods had to introduce some kinds of restrictions such as the limitation of the kind of chains or the length of chains to be extracted smadja 1993 shinnou and isahara 1995 ,0,1,0
smadja 1993finds significant bigrams using an estimate of zscore deviation from an expected mean ,0,1,0
smadja 1993p168 kita al ,0,1,0
baron and hirst 2004 extracted collocations with xtract smadja 1993 and classified the collocations using the orientations of the words in the neighboring sentences ,0,1,0
most previous work on compositionality of mwes either treat them as collocations smadja 1993 or examine the distributional similarity between the expression and its constituents mccarthy et al 2003 baldwin et al 2003 bannard et al 2003 ,0,1,0
these measures have in fact been used previously in measuring term recognition smadja 1993 bourigault 1994 lauriston 1994 ,0,1,0
one of the best efforts to quantify the performance of a termrecognition system smadja 1993 does so only for one processing stage leaving unassessed the texttooutput performance of the system ,1,0,0
in smadjas collocation algorithm xtract the lowestfrequency words are effectively discarded as well smadja 1993 ,0,1,0
the use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research church and hanks 1990 zernik and jacobs 1990 hindle 1990 smadja 1993 ,0,1,0
statistics on cooccurrence of words in a local context were used recently for monolingual word sense disambiguation gale church and yarowsky 1992b 1993 sch6tze 1992 1993 see section 7 for more details and church and hanks 1990 smadja 1993 for other applications of these statistics ,0,1,0
for the extraction problem there have been various methods proposed to date which are quite adequate hindle and rooth 1991 grishman and sterling 1992 manning 1992 utsuro matsumoto and nagao 1992 brent 1993 smadja 1993 grefenstette 1994 briscoe and carroll 1997 ,1,0,0
as we remarked earlier however the input data required by our method triples could be generated automatically from unparsed corpora making use of existing heuristic rules brent 1993 smadja 1993 although for the experiments we report here we used a parsed corpus ,0,1,0
some studies have been done for acquiring collocation translations using parallel corpora smadja et al 1996 kupiec 1993 echizenya et al 2003 ,0,1,0
the former extracts collocations within a fixed window church and hanks 1990 smadja 1993 ,0,1,0
these range from twoword to multiword with or without syntactic structure smadja 1993 lin 1998 pearce 2001 seretan et al 2003 ,0,1,0
smadja 1993 also detailed techniques for collocation extraction and developed a program called xtract which is capable of computing flexible collocations based on elaborated statistical calculation ,0,1,0
it is true that various term extraction systems have been developed such as xtract smadja 1993 termight dagan church 1994 and terms justeson katz 1995 among others cf ,0,1,0
parsing has been also used after extraction smadja 1993 for filtering out invalid results ,0,1,0
this fact is being seriously challenged by current research and might not be true in the near future smadja 1993 151 ,0,1,0
similarly smadja 1993 uses a six content word window to extract significant collocations ,0,1,0
finally knowledge of polarity can be combined with corpusbased collocation extraction methods smadja 1993 to automatically produce entries for the lexical functions used in meaningtext theory meluk and pertsov 1987 for text generation ,0,1,0
smadja 1993 proposed a method to retrieve collocations by combining bigrams whose cooccurrences are greater than a given threshold 3 ,0,1,0
however morphosyntactic features alone cannot verify the terminological status of the units extracted since they can also select non terms see smadja 1993 ,0,1,0
in smadja 1993 automatically extracted collocations are judged by a lexicographer ,0,1,0
other classes such as the ones below can be extracted using lexicostatistical tools such as in smadja 1993 and then checked by a human ,0,1,0
it seems nevertheless that all 2church and hanks 1989 smadja 1993 use statistics in their algorithms to extract collocations from texts ,0,1,0
for the correct identification of phrases in a korean query it would help to identify the lexical relations and produce statistical information on pairs of words in a text corpus as in smadja 1993 ,0,1,0
it is clear that appendix b contains far fewer true noncompositional phrases than appendix a 7 related work there have been numerous previous research on extracting collocations from corpus eg choueka 1988 and smadja 1993 ,0,1,0
cooccurrence statistics is collected from either bilingual parallel and 334 nonparallel corpora smadja et al 1996 kupiec 1993 wu 1995 tanaka and iwasaki 1996 fung and lo 1998 or monolingual corpora smadja 1993 fung and wu 1994 liu and li 1997 shiitze 1992 yarowsky 1995 ,0,1,0
the recurrence property had been utilized to extract keywords or keyphrases from text chien 1999 fung 1998 smadja 1993 ,0,1,0
since we need knowledgepoor daille 1996 induction we cannot use humansuggested filtering chisquared g24 2 church and gale 1991 zscore smadja 1993 fontenelle et al 1994 students tscore church and hanks 1990 ngram list in accordance to each probabilistic algorithm ,0,1,0
the second method considers the means and variance of the distance between two words and can compute flexible collocations smadja 1993 ,0,1,0
one aspect of vpcs that makes them dicult to extract cited in eg smadja 1993 is that the verb and particle can be noncontiguous eg hand the paper in and battle right on ,0,1,0
4 method2 simple chunkbased extraction to overcome the shortcomings of the brill tagger in identifying particles we next look to full chunk 2note this is the same as the maximum span length of 5 used by smadja 1993 and above the maximum attested np length of 3 from our corpus study see section 22 ,0,1,0
there have been many statistical measures which estimate cooccurrence and the degree of association in previous researches such as mutual information church 1990 sporat 1990 tscore church 1991 dice matrix smadja 1993 1996 ,0,1,0
the approach is in the spirit of smadja 1993 on retrieving collocations from text corpora but is more integrated with parsing ,0,1,0
3 related work word collocation various collocation metrics have been proposed including mean and variance smadja 1994 the ttest church et al 1991 the chisquare test pointwise mutual information mi church and hanks 1990 and binomial loglikelihood ratio test blrt dunning 1993 ,0,1,0
for that purpose syntactical didier bourigault 1993 statistical frank smadja 1993 ted dunning 1993 gal dias 2002 and hybrid syntaxicostatistical methodologies batrice daille 1996 jeanphilippe goldman et al 2001 have been proposed ,0,1,0
alpha 0 01 02 03 04 05 freq2 13555 13093 12235 11061 10803 10458 freq3 4203 3953 3616 3118 2753 2384 freq4 1952 1839 1649 1350 1166 960 freq5 1091 1019 917 743 608 511 freq2 2869 2699 2488 2070 1666 1307 total 23670 22603 20905 18342 16996 15620 alpha 06 07 08 09 10 freq2 10011 9631 9596 9554 9031 freq3 2088 1858 1730 1685 1678 freq4 766 617 524 485 468 freq5 392 276 232 202 189 freq2 1000 796 627 517 439 total 14257 13178 12709 12443 11805 table 7 number of extracted mwus by frequency 62 qualitative analysis as many authors assess frank smadja 1993 john justeson and slava katz 1995 deciding whether a sequence of words is a multiword unit or not is a tricky problem ,0,1,0
on the other hand purely statistical systems frank smadja 1993 ted dunning 1993 gal dias 2002 extract discriminating mwus from text corpora by means of association measure regularities ,0,1,0
for example smadja 1993 suggests a basic characteristic of collocations and multiword units is recurrent domaindependent and cohesive lexical clusters ,0,1,0
related works generally speaking approaches to mwe extraction proposed so far can be divided into three categories a statistical approaches based on frequency and cooccurrence affinity b knowledgebased or symbolic approaches using parsers lexicons and language filters and c hybrid approaches combining different methods smadja 1993 dagan and church 1994 daille 1995 mcenery et al 1997 wu 1997 wermter et al 1997 michiels and dufour 1998 merkel and andersson 2000 piao and mcenery 2001 sag et al 2001a 2001b biber et al 2003 ,0,1,0
in his xtract system smadja 1993 first extracted significant pairs of words that consistently cooccur within a single syntactic structure using statistical scores called distance strength and spread and then examined concordances of the bigrams to find longer frequent multiword units ,0,1,0
the group of collocations and compounds should be delimited using statistical approaches such as xtract smadja 1993 or localmax silva et al 1999 so that only the most relevantthose of higher frequency are included in the database ,0,1,0
many efficient techniques exist to extract multiword expressions collocations lexical units and idioms church and hanks 1989 smadja 1993 dias et al 2000 dias 2003 ,1,0,0
study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction church and hanks 1990 smadja 1993 choueka 1993 lin 1998 ,1,0,0
the precision rate using the lexical statistics approach can reach around 60 if both word bigram extraction and ngram extractions are taking into account smadja 1993 lin 1997 and lu et al 2003 ,0,1,0
smadja smadja 1993 proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength ,0,1,0
there are several basic methods for evaluating associations between words based on frequency counts choueka 1988 wettler and rapp 1993 information theoretic church and hanks 1990 and statistical significance smadja 1993 ,0,1,0
in the work of smadja 1993 on extracting collocations preference was given to constructions whose constituents appear in a fixed order a similar and more generally implemented version of our assumption here that asymmetric constructions are more idiomatic than symmetric ones ,0,1,0
we can mentionhere only part of this work berryrogghe 1973 church et al 1989 smadja1993lin1998krennandevert2001 for monolingualextraction and kupiec 1993 wu1994smadjaetal ,0,1,0
morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults breidt 1993 smadja 1993 zajac et al 2003 ,0,1,0
morphologicaltoolssuch as lemmatizers andpostaggersarebeingcommonlyusedin extractionsystemsthey areemployedbothfordealingwithtext variationandfor validatingthe candidatepairs combinationsof functionwordsare typicallyruledout justesonand katz 1995as are the ungrammaticalcombinationsin the systemsthatmake useofparserschurchandhanks 1990smadja1993basilietal ,0,1,0
given the motivations for performing a linguisticallyinformedextraction whichwere also put forth among others by church and hanks199025 smadja1993151 and heid 1994 and given the recent developmentof linguisticanalysistoolsitseemsplausiblethatthe linguisticstructurewill be more and more taken intoaccountbycollocationextractionsystems ,0,1,0
3 overviewofextractionwork 31 english as one mightexpectthe bulk of the collocation extractionwork concernsthe english language choueka1988churchet al 1989churchand hanks1990 smadja1993 justesonand katz 1995kjellmer 1994sinclair 1995lin1998 amongmany others1 ,0,1,0
smadja1993employsthezscoreinconjunction with several heuristicseg the systematic occurrenceof two lexical items at the same distanceintextandextractspredicativecollocations 1egfrantziet al 2000pearce2001goldmanet al 2001zaiuinkpenandhirst2002dias2003seretanetal ,0,1,0
we argue that linguistic knowledge could not only improve results krenn 2000b smadja 1993 but is essential when extracting collocations from certain languages this knowledge provides other applications or a lexicon user respectively with a negrained description of how the extracted collocations are to be used in context ,0,1,0
there are some existing corpus linguistic researches on automatic extraction of collocations from electronic text smadja 1993 lin 1998 xu and lu 2006 ,0,1,0
lastly collocations are domaindependent smadja 1993 and languagedependent ,0,1,0
the problem is that with such a definition of collocations even when improved one identifies not only collocations but freecombining pairs frequently appearing together such as lawyerclient doctorhospital as pointed out by smadja 1993 ,0,1,0
while several methods have been proposed to automatically extract compounds smadja 1993 suet al 1994 we know of no successful attempt to automatically make classes of compounds ,0,0,1
therefore sublanguage techniques such as sager 1981 and smadja 1993 do not work ,0,0,1
mi is defined in general as follows y i ix y log2 px py we can use this definition to derive an estimate of the connectedness between words in terms of collocations smadja 1993 but also in terms of phrases and grammatical relations hindle 1990 ,0,1,0
while bound compositions are not predictable ie their reasonableness cannot be derived from the syntactic and semantic properties of the words in themsmadja 1993 ,0,1,0
now with the availability of largescale corpus automatic acquisition of word compositions especially word collocations from them have been extensively studiedeg choueka et al 1988 church and hanks 1989 smadja 1993 ,0,1,0
based on this assumption smadja 1993 stored all bigrams of words along with their relative position p 5 p 5 ,0,1,0
f c r cr 2 continue explanations we begin by mentioning the xtrgct tool by smadja smadja 1993 ,0,1,0
a number of alignment techniques have been proposed varying from statistical methods brown et al 1991 gale and church 1991 to lexical methods kay and rsscheisen 1993 chen 1993 ,0,1,0
21 the evaluator the evaluator is a function ptt s which assigns to each targettext unit t an estimate of its probability given a source text s and the tokens t which precede t in the current translation of s 1 our approach to modeling this distribution is based to a large extent on that of the ibm group brown et al 1993 but it differs in one significant aspect whereas the ibm model involves a noisy channel decomposition we use a linear combination of separate predictions from a language model ptlt and a translation model ptls ,0,1,0
furthermore the underlying decoding strategies are too time consuming for our application we therefore use a translation model based on the simple linear interpolation given in equation 2 which combines predictions of two translation models ms and m both based on ibmlike model 2brown et al 1993 ,0,1,0
32 mapping mapping the identified units tokens or sequences to their equivalents in the other language was achieved by training a new translation model ibm 2 using the em algorithm as described in brown et al 1993 ,0,1,0
3 bilingual task an application for word alignment 31 sentence and word alignment bilingual alignment methods warwick et al 1990 brown et al 1991a brown et al 1993 gale and church 1991b gale and church 1991a kay and roscheisen 1993 simard et al 1992 church 1993 kupiec 1993a matsumoto et al 1993 dagan et al 1993 ,0,1,0
have been used in statistical machine translation brown et al 1990 terminology research and translation aids isabelle 1992 ogden and gonzales 1993 van der eijk 1993 bilingual lexicography klavans and tzoukermann 1990 smadja 1992 wordsense disambiguation brown et al 1991b gale et al 1992 and information retrieval in a multilingual environment landauer and littman 1990 ,0,1,0
we have been using the output of wordalign a robust alignment program that proved useful for bilingual concordancing of noisy texts dagan et al 1993 ,0,1,0
partofspeech taggers are used in a few applications such as speech synthesis sproat et al 1992 and question answering kupiec 1993b ,0,1,0
word alignment is newer found only in a few places gale and church 1991a brown et al 1993 dagan et al 1993 ,0,1,0
unlike probabilistic parsing proposed by fujisaki et al 1989 briscoe and carroll 1993 also a staff member of matsushita electric industrial co ltd shinagawa tokyo japan ,0,1,0
the computation mechanism of gp and lp bears a resemblance to the em algorithmdempster et al 1977 brown et al 1993 which iteratively computes maximum likelihood estimates from incomplete data ,0,1,0
on the other end of the spectrum characterbased bitext mapping algorithms church 1993 davis et al 1995 are limited to language pairs where cognates are common in addition they may easily be misled by superficial differences in formatting and page layout and must sacrifice precision to be computationally tractable ,0,1,0
in statistical machine translation ibm 15 models brown et al 1993 based on the sourcechmmel model have been widely used and revised for many language donmins and applications ,1,0,0
thus a lot of alignment techniques have been suggested at the sentence gale et al 1993 phrase shin et al 1996 nomt thrase kupiec 1993 word brown et al 1993 berger et al 1996 melamed 1997 collocation smadja et al 1996 and terminology level ,0,1,0
the model is often further restricted so that each source word is assigned to exactly one target word brown et al 1993 ney et al 2000 ,0,1,0
many existing systems tbr smt wang and waibel 1997 niefien et al 198 och and weber 1998 make use of a special way of structuring the string translation model brown et al 1993 lhe correspondence between the words in the source and the target string is described by aligmuents that assign one target word position to each source word position ,0,1,0
in this paper we will describe extensions to tile hiddenmarkov alignment model froln vogel et al 1996 and compare tlmse to models 1 4 of brown et al 1993 ,0,1,0
3 model 1 and model 2 lcllacing the ltendence on ajl in the hmm alignment mom iy a delendence on j we olltain a model wlfich an lie seen as a zeroorder hidlmmarkov model which is similar to model 2 1rotoset ty brown et al 1993 ,0,1,0
1087 model 3 of brown et al 1993 is a zeroorder alignment model like model 2 including in addition fertility paranmters ,0,1,0
model 4 of brown et al 1993 is also a firstorder alignment model along the source positions like the hmm trot includes also fertilities ,0,1,0
tile full description of model 4 brown et al 1993 is rather complicated as there have to be considered tile cases that english words have fertility larger than one and that english words have fertility zero ,0,1,0
therefore the viterbi alignment is comlmted only approximately using the method described in brown et al 1993 ,0,1,0
as in tile hmm we easily can extend the dependencies in the alignment model of model 4 easily using the word class of the previous english word e gci or the word class of the french word f gij brown et al 1993 ,0,1,0
most smt models brown et al 1993 vogel et al 1996 try to model wordtoword correslondences between source and target words using an alignment nmpling from source losition j to target position i aj ,0,1,0
the assumptions we made were the following a lexical token in one half of the translation unit tu corresponds to at most one nonempty lexical unit in the other half of the tu this is the 11 mapping assumption which underlines the work of many other researchers ahrenberg et al 2000 brew and mckelvie 1996 hiemstra 1996 kay and rscheisen 1993 tiedmann 1998 melamed 2001 etc a polysemous lexical token if used several times in the same tu is used with the same meaning this assumption is explicitly used by gale and church 1991 melamed 2001 and implicitly by all the previously mentioned authors a lexical token in one part of a tu can be aligned to a lexical token in the other part of the tu only if the two tokens have compatible types partofspeech in most cases compatibility reduces to the same pos but it is also possible to define other compatibility mappings eg participles or gerunds in english are quite often translated as adjectives or nouns in romanian and viceversa although the word order is not an invariant of translation it is not random either ahrenberg et al 2000 when two or more candidate translation pairs are equally scored the one containing tokens which are closer in relative position are preferred ,0,1,0
another kind of popular approaches to dealing with query translation based on corpusbased techniques uses a parallel corpus containing aligned sentences whose translation pairs are corresponding to each other brown et al 1993 dagan et al 1993 smadja et al 1996 ,1,0,0
related work 21 translation with nonparallel corpora a straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora eg brown et al 1993 ,0,1,0
according to the bayes rule the problem is transformed into the noisy channel model paradigm where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text brown et al 1993 ,0,1,0
for example the statistical word alignment in ibm translation models brown et al 1993 can only handle word to word and multiword to word alignments ,0,0,1
2 statistical word alignment statistical translation models brown et al 1993 only allow word to word and multiword to word alignments ,0,0,1
1 introduction bilingual word alignment is first introduced as an intermediate result in statistical machine translation smt brown et al 1993 ,0,1,0
besides being used in smt it is also used in translation lexicon building melamed 1996 transfer rule learning menezes and richardson 2001 examplebased machine translation somers 1999 etc in previous alignment methods some researches modeled the alignments as hidden parameters in a statistical translation model brown et al 1993 och and ney 2000 or directly modeled them given the sentence pairs cherry and lin 2003 ,0,1,0
word alignment models were first introduced in statistical machine translation brown et al 1993 ,0,1,0
using the ibm translation models ibm1 to ibm5 brown et al 1993 as well as the hiddenmarkov alignment model vogel et al 1996 we can produce alignments of good quality ,1,0,0
6 related work the popular ibm models for statistical machine translation are described in brown et al 1993 ,1,0,0
on the other hand statistical mt employing ibm models brown et al 1993 translates an input sentence by the combination of word transfer and word reordering ,0,1,0
estimated clues are derived from the parallel data using for example measures of cooccurrence eg the dice coefficient smadja et al 1996 statistical alignment models eg ibm models from statistical machine translation brown et al 1993 or string similarity measures eg the longest common subsequence ratio melamed 1995 ,0,1,0
brown et al 1993 vogel et al 1996 garcavarea et al 2002 ahrenberg et al 1998 tiedemann 1999 tufis and barbu 2002 melamed 2000 ,0,1,0
word alignment models were first introduced in statistical machine translation brown et al 1993 ,0,1,0
using the ibm translation models ibm1 to ibm5 brown et al 1993 as well as the hiddenmarkov alignment model vogel et al 1996 we can produce alignments of good quality ,1,0,0
they are based on the sourcechannel approach to statistical machine translation brown et al 1993 ,0,1,0
2 related work the popular ibm models for statistical machine translation are described in brown et al 1993 and the hmmbased alignment model was introduced in vogel et al 1996 ,1,0,0
for scoring mt outputs the proposed rscm uses a score based on a translation model called ibm4 brown et al 1993 tmscore and a score based on a language model for the translation target language lmscore ,0,1,0
statistical machine translation is based on the noisy channel model where the translation hypothesis is searched over the space defined by a translation model and a target language brown et al 1993 ,0,1,0
1 introduction decoding is one of the three fundamental problems in classical smt translation model and language model being the other two as proposed by ibm in the early 1990s brown et al 1993 ,0,1,0
2 decoding the decoding problem in smt is one of finding the most probable translation e in the target language of a given source language sentence f in accordance with the fundamental equation of smt brown et al 1993 e argmaxe prfepre ,0,1,0
in each iteration of local search we look in the neighborhood of the current best alignment for a better alignment brown et al 1993 ,0,1,0
according to the statistical machine translation formalism brown et al 1993 the translation process is to search for the best sentence be such that be arg max e pejj arg maxe pjjepe where pjje is a translation model characterizing the correspondence between e and j pe the english language model probability ,0,1,0
by introducing the hidden word alignment variable a brown et al 1993 the optimal translation can be searched for based on the following criterion 1 arg max m mm m ea eh efa 1 where is a string of phrases in the target language e f fa is the source language string of phrases he are feature functions weights m m are typically optimized to maximize the scoring function och 2003 ,0,1,0
given a manually compiled lexicon containing words and their relative frequencies psfprimej the best segmentationfj1 is the one that maximizes the joint probability of all words in the sentence with the assumption that words are independent of each other1 fj1 argmax fprimejprime1 prfprimejprime1 ck1 argmax fprimejprime1 jprimeproductdisplay j1 psfprimej where the maximization is taken over chinese word sequences whose character sequence is ck1 22 translation system once we have segmented the chinese sentences into words we train standard alignment models in both directions with giza och and ney 2002 using models of ibm1 brown et al 1993 hmm vogel et al 1996 and ibm4 brown et al 1993 ,0,1,0
in the context of statistical machine translation brown et al 1993 we may interprete as an english sentence f its translation in french and a a representation of how the words correspond to each other in the two sentences ,0,1,0
based on these grammars a great number of smt models have been recently proposed including stringtostring model synchronous fsg brown et al 1993 koehn et al 2003 treetostring model tsgstring huang et al 2006 liu et al 2006 liu et al 2007 stringtotree model stringcfgtsg yamada and knight 2001 galley et al 2006 marcu et al 2006 treetotree model synchronous cfgtsg dataoriented translation chiang 2005 cowan et al 2006 eisner 2003 ding and palmer 2005 zhang et al 2007 bod 2007 quirk wt al 2005 poutsma 2000 hearne and way 2003 and so on ,0,1,0
for example sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters brown et al 1991 gale and church 1993 or by statistically estimating word level correspondences chen 1993 kay and rsscheisen 1993 ,0,1,0
the statistical approach involves the following alignment of bilingual texts at the sentence level nsing statistical techniques eg brown lai and mercer 1991 gale and church 1993 chen 1993 and kay and rsscheisen 1993 statistical machine translation models eg brown cooke pietra pietra et al ,0,1,0
in previous work church et al 1993 we have reported some preliminary success in aligning the english and japanese versions of the awk manual aho kernighan weinberger 1980 using charalign church 1993 a method that looks for character sequences that are the same in both the source and target ,0,1,0
this estimate could be used as a starting point for a more detailed alignment algorithm such as wordalign dagan et al 1993 ,0,1,0
these tables were computed from a small fragment of the canadian hansards that has been used in a number of other studies church 1993 and simard et al 1992 ,0,1,0
motivation there have been quite a number of recent papers on parallel text brown et al 1990 1991 1993 chen 1993 church 1993 church et al 1993 dagan et al 1993 gale and church 1991 1993 isabelle 1992 kay and rgsenschein 1993 klavans and tzoukermann 1990 kupiec 1993 matsumoto 1991 ogden and gonzales 1993 shemtov 1993 simard et al 1992 warwickarmstrong and russell 1990 wu to appear ,0,1,0
the resolution of alignment can vat3 from low to high section paragraph sentence phrase and word gale and church 1993 matsumoto et al 1993 ,0,1,0
mcarthur 1992 mei et al 1993 classification allows a word to align with a target word using the collective translation tendency of words in the same class ,0,1,0
machine translation brown et al 1993 but also in other applications such as word sense disanabiguation brown et al 1991 and bilingnal lexicography klavans and tzoukermann 1990 ,0,1,0
of the position infer marion of words at ltlathillg pairs of selltelces which turned out useful brown et al 1993 ,1,0,0
 t he em algorit hnt brown et al 1993ietrttstcr et al 1977 ,0,1,0
iiowever dagan et al 1993 have shown that knowledge of targettext length is not crucial to the models iertbrmanee ,0,1,0
a sinfilar approach has been chosen by dagan et al 1993 ,0,1,0
in the recent years there have been a number of papers considering this or similar problems brown et al 1990 dagan et al 1993 kay et al 1993 fung et al 1993 ,0,1,0
this sort of problem can be solved in principle by conditional variants of the expectationmaximization algorithm baum et al 1970 dempster et al 1977 meng and rubin 1993 jebara and pentland 1999 ,0,1,0
similarly murdock and croft 2005 adopted a simple translation model from ibm model 1 brown et al 1990 brown et al 1993 and applied it to qa ,0,1,0
the tree is produced by a stateoftheart dependency parser mcdonald et al 2005 trained on the wall street journal penn treebank marcus et al 1993 ,0,1,0
2 word alignment framework a statistical translation model brown et al 1993 och and ney 2003 describes the relationship between a pair of sentences in the source and target languages f fj1e ei1 using a translation probability pfe ,0,1,0
given any word alignment model posterior probabilities can be computed as brown et al 1993 paj ief summationdisplay a pafeiaj 1 where i 01i ,0,1,0
2 we note that these posterior probabilities can be computed efficiently for some alignment models such as the hmm vogel et al 1996 och and ney 2003 models 1 and 2 brown et al 1993 ,1,0,0
5 previous work the leaf model is inspired by the literature on generative modeling for statistical word alignment and particularly by model 4 brown et al 1993 ,0,1,0
22 unsupervised parameter estimation we can perform maximum likelihood estimation of the parameters of this model in a similar fashion to that of model 4 brown et al 1993 described thoroughly in och and ney 2003 ,0,1,0
we use viterbi training brown et al 1993 but neighborhood estimation alonaizan et al 1999 och and ney 2003 or pegging brown et al 1993 could also be used ,0,1,0
brown et al 1993 defined two local search operations for their 1ton alignment models 3 4 and 5 ,0,1,0
however searching the space of all possible alignments is intractable for em so in practice the procedure is bootstrapped by models with narrower search space such as ibm model 1 brown et al 1993 or aachen hmm vogel et al 1996 ,0,1,0
approaches include word substitution systems brown et al 1993 phrase substitution systems koehn et al 2003 och and ney 2004 and synchronous contextfree grammar systems wu and wong 1998 chiang 2005 all of which train on string pairs and seek to establish connections between source and target strings ,0,1,0
1 introduction given a sourcelanguage eg french sentence f the problem of machine translation is to automatically produce a targetlanguage eg english translation e the mathematics of the problem were formalized by brown et al 1993 and reformulated by och and ney 2004 in terms of the optimization e arg maxe msummationdisplay m1 mhmef 1 where fhmefg is a set of m feature functions and fmg a set of weights ,0,1,0
the words with the highest association probabilities are chosen as acquired words for entity e 41 base model i using the translation model i brown et al 1993 where each word is equally likely to be aligned with each entity we have pwe 1l 1m mproductdisplay j1 lsummationdisplay i0 pwjei 1 where l and m are the lengths of entity and word sequences respectively ,0,1,0
42 base model ii using the translation model ii brown et al 1993 where alignments are dependent on wordentity positions and wordentity sequence lengths we have pwe mproductdisplay j1 lsummationdisplay i0 paj ijmlpwjei 2 where aj i means that wj is aligned with ei ,0,1,0
one of the simplest models that can be seen in the context of lexical triggers is the ibm model 1 brown et al 1993 which captures lexical dependencies between source and target words ,0,1,0
3 model as an extension to commonly used lexical word pair probabilities pfe as introduced in brown et al 1993 we define our model to operate on word triplets ,0,1,0
the resulting training procedure is analogous to the one presented in brown et al 1993 and tillmann and ney 1997 ,0,1,0
compared with clean parallel corpora such as hansard brown et al 1993 which consists of 505 frenchenglish translations of political debates in the canadian parliament texts from the web are far more diverse and noisy ,1,0,0
1 introduction sentencealigned parallel bilingual corpora have been essential resources for statistical machine translation brown et al 1993 and many other multilingual natural language processing applications ,0,1,0
91 training methodology given a training set we first run a variant of ibm alignment model 1 brown et al 1993 for 100 iterations and then initialize model i with the learned parameter values ,0,1,0
although we have argued section 2 that this is unlikely to succeed to our knowledge we are the first to investigate the matter empirically11 the bestknown mt aligner is undoubtedly giza och and ney 2003 which contains implementations of various ibm models brown et al 1993 as well as the hmm model of vogel et al ,0,1,0
the mt community has developed not only an extensive literature on alignment brown et al 1993 vogel et al 1996 marcu and wong 2002 denero et al 2006 but also standard proven alignment tools such as giza och and ney 2003 ,0,1,0
we use giza och and ney 2003 to train generative directed alignment models hmm and ibm model4 brown et al 1993 from training recordtext pairs ,0,1,0
traditionally generative word alignment models have been trained on massive parallel corpora brown et al 1993 ,0,1,0
the structure of the graphical model resembles ibm model 1 brown et al 1993 in which each target record word is assigned one or more source text words ,0,1,0
furthermore we provide a 638 error reduction compared to ibm model 4 brown et al 1993 ,0,0,1
in this work we propose two models that can be categorized as extensions of standard word lexicons a discriminative word lexicon that uses global ie sentencelevel source information to predict the target words using a statistical classifier and a triggerbased lexicon model that extends the wellknown ibm model 1 brown et al 1993 with a second trigger allowing for a more finegrained lexical choice of target words ,1,0,0
there are three major types of models heuristic models as in melamed 2000 generative models as the ibm models brown et al 1993 and discriminative models varea et al 2001 bangalore et al 2006 ,0,1,0
one of the simplest models in the context of lexical triggers is the ibm model 1 brown et al 1993 which captures lexical dependencies between source and target words ,0,1,0
this is a problem with other direct translation models such as ibm model 1 used as a direct model rather than a channel model brown et al 1993 ,0,0,1
the giza aligner is based on ibm model 4 brown et al 1993 ,0,1,0
yamada and knight 2001 follow brown et al 1993 in using the noisy channel model by decomposing the translation decisions modeled by the translation model into different types and inducing probability distributions via maximum likelihood estimation over each decision type ,0,1,0
previous smt systems eg brown et al 1993 used a wordbased translation model which assumes that a sentence can be translated into other languages by translating each word into one or more words in the target language ,0,1,0
however since we are interested in the word counts that correlate to w we adopt the concept of the translation model proposed by brown et al 1993 ,0,1,0
many studies on collocation extraction are carried out based on cooccurring frequencies of the word pairs in texts choueka et al 1983 church and hanks 1990 smadja 1993 dunning 1993 pearce 2002 evert 2004 ,0,1,0
the difference between mwa and bilingual word alignment brown et al 1993 is that the mwa method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment ,0,1,0
thus the alignment set is denoted as 1 ialiaia ii we adapt the bilingual word alignment model ibm model 3 brown et al 1993 to monolingual word alignment ,0,1,0
in the early statistical translation model work at ibm these representations were called cepts short for concepts brown et al 1993 ,0,1,0
generative methods brown et al 1993 vogel and ney 1996 treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization em algorithm ,0,1,0
numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true for example figure in the first row and first column means that for 989 percent of the language pairs the bleu score for the bidirectional decoder was better than that of the forward decoder proach brown et al 1993 ,0,0,1
their experiments were performed using a decoder based on ibm model 4 using the translation techniques developed at ibm brown et al 1993 ,0,1,0
becausesuchapproachesdirectly learn a generative model over phrase pairs they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the manytoone wordlevel alignments produced by the ibm series models brown et al 1993 or the hidden markov model hmm vogel et al 1996 ,0,1,0
we then built separate directed word alignments for englishx andxenglish xindonesian spanish using ibm model 4 brown et al 1993 combined them using the intersectgrow heuristic och and ney 2003 and extracted phraselevel translation pairs of maximum length seven using the alignment template approach och and ney 2004 ,0,1,0
increasingly parallel corpora are becoming available for many language pairs and smt systems have been built for frenchenglish germanenglish arabicenglish chineseenglish hindienglish and other language pairs brown et al 1993 alonaizan et al 1999 udupa 2004 ,0,1,0
an open question in smt is whether there existsclosed formexpressions whoserepresentation is polynomial in the size of the input for p fe and the counts in the em iterations for models 35 brown et al 1993 ,0,1,0
for a detailed introduction to ibm translation models please see brown et al 1993 ,0,1,0
expectation evaluation is the soul of parameter estimation brown et al 1993 alonaizan et al 1999 ,0,1,0
exact decoding is the original decoding problem as defined in brown et al 1993 and relaxed decoding is the relaxation of the decoding problem typically used in practice ,0,1,0
in their seminal paper on smt brownand his colleagues highlighted the problems weface aswe go from ibm models 12 to 35brown et al 1993 3 asweprogress from model1tomodel5 evaluating the expectations that gives us counts becomes increasingly difficult ,1,0,0
1 introduction statistical machine translation is a data driven machine translation technique which uses probabilistic models of natural language for automatic translation brown et al 1993 alonaizan et al 1999 ,0,1,0
we use the ibm model 1 brown et al 1993 uniform distribution and the hidden markov model hmm firstorder dependency vogel et al 1996 to estimate the alignment model ,0,1,0
alignment spaces can emerge from generative stories brown et al 1993 from syntactic notions wu 1997 or they can be imposed to create competition between links melamed 2000 ,0,1,0
the task originally emerged as an intermediate result of training the ibm translation models brown et al 1993 ,0,1,0
the implementation of meba was strongly influenced by the notorious five ibm models described in brown et al 1993 ,1,0,0
first a parsingbased approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise in order to construct a partial semantic representation dowding et al 1993 allen et al 2001 ward 1991 ,0,1,0
to this end we adopt techniques from statistical machine translation brown et al 1993 och and ney 2003 and use statistical alignment to learn the edit patterns ,0,1,0
we adopt an approach similar to ciaramella 1993 boros et al 1996 in which the meaning representation in our case xml is transformed into a sorted flat list of attributevalue pairs indicating the core contentful concepts of each command ,0,1,0
while em has worked quite well for a few tasks notably machine translations starting with the ibm models 15 brown et al 1993 it has not had success in most others such as partofspeech tagging merialdo 1991 namedentity recognition collins and singer 1999 and contextfreegrammar induction numerous attempts too many to mention ,1,0,0
287 system train base test base 1 baseline 8789 8789 2 contrastive 8870 082 8845 056 5 trialsfold 3 contrastive 8882 093 8855 066 greedy selection table 1 average f1 of 7way crossvalidation to generate the alignments we used model 4 brown et al 1993 as implemented in giza och and ney 2003 ,0,1,0
according to this model when translating a stringf in the source language into the target language a string e is chosen out of all target language strings e if it has the maximal probability given f brown et al 1993 e arg maxe pref arg maxe prfepre where prfe is the translation model and pre is the target language model ,0,1,0
one approach to translate terms consists in using a domainspecific parallel corpus with standard alignment techniques brown et al 1993 to mine new translations ,0,1,0
alignment is often used in training both generative and discriminative models brown et al 1993 blunsom et al 2008 liang et al 2006 ,0,1,0
each item is associated with a stack whose signa12specifically a bhypergraph equivalent to an andor graph gallo et al 1993 or contextfree grammar nederhof 2003 ,0,1,0
logics for the ibm models brown et al 1993 would be similar to our logics for phrasebased models ,0,1,0
to model pfjle8t we assume the existence of an alignment a j we assume that every word fj is produced by the word ej at position aj in the training corpus with the probability pflei j pf lc 1 pl icon jl 7 the word alignment a j is trained automatically using statistical translation models as described in brown et al 1993 vogel et al 1996 ,0,1,0
the ibm models 15 brown et al 1993 produce word alignments with increasing algorithmic complexity and performance ,1,0,0
the 1000best lists are augmented with ibm model1 brown et al 1993 scores and then rescored with a second set of met parameters ,0,1,0
the ibm translation models brown et al 1993 describe word reordering via a distortion model defined over word positions within sentence pairs ,0,1,0
2 the wfst reordering model the translation template model ttm is a generative model of phrasebased translation brown et al 1993 ,0,1,0
word alignments traditionally are based on ibm models 15 brown et al 1993 or on hmms vogel et al 1996 ,0,1,0
2 related work one of the major problems with the ibm models brown et al 1993 and the hmm models vogel et al 1996 is that they are restricted to the alignment of each sourcelanguage word to at most one targetlanguage word ,0,0,1
further details are in the original paper brown et al 1993 ,0,1,0
the ibm models have shown good performance in machine translation and especially so within certain families of languages for example in translating between french and english or between sinhalese and tamil brown et al 1993 weerasinghe 2004 ,1,0,0
this is a common technique in machine translation for which the ibm translation models are popular methods brown et al 1993 ,1,0,0
in the first of our methods we align manual transcripts and asr sentences using the ibm translation model brown et al 1993 to obtain a probabilistic dictionary ,0,1,0
32 details to learn alignments translation probabilities etc in the first method we used work that has been done in statistical machine translation brown et al 1993 where the translation process is considered to be equivalent to a corruption of the source language text to the target language text due to a noisy channel ,0,1,0
757 hbps strong tendency to overestimate the probability of rare biphrases it is computed as in equation 2 except that biphrase probabilities are computed based on individual word translation probabilities somewhat as in ibm model 1 brown et al 1993 prts 1st productdisplay tt summationdisplay ss prts the target language feature function htl this is based on a ngram language model of the target language ,0,1,0
therefore we determine the maximal translation probability of the target word e over the source sentence words pibm1efj1 maxj0j pefj 9 where f0 is the empty source word brown et al 1993 ,0,1,0
for the give source text s it finds the most probable alignment set a and target text t aa satpstp 1 brown brown et al 1993 proposed five alignment models called ibm model for an englishfrench alignment task based on equa68 tion 1 ,0,1,0
in this paper we propose an alignment algorithm between english and korean conceptual units or between english and korean term constituents in englishkorean technical term pairs based on ibm model brown et al 1993 ,0,1,0
2 overview 21 the word segmentation problem as statistical machine translation systems basically rely on the notion of words through their lexicon models brown et al 1993 they are usually capable of outputting sentences already segmented into words when they translate into languages like chinese or japanese ,0,1,0
one promising approach extends standard statistical machine translation smt techniques eg brown et al 1993 och ney 2000 2003 to the problems of monolingual paraphrase identification and generation ,0,1,0
4 pattern switching the compositional translation presents problems which have been reported by baldwin and tanaka 2004 brown et al 1993 fertility swts and mwts are not translated by a term of a same length ,0,1,0
these probabilities are estimated with ibm model 1 brown et al 1993 on parallel corpora ,0,1,0
most existing methods treat word tokens as basic alignment units brown et al 1993 vogel et al 1996 deng and byrne 2005 however many languages have no explicit word boundary markers such as chinese and japanese ,0,1,0
512 learning translation model according to the standard statistical translation model brown et al 1993 we can find the optimal model m by maximizing the probability of generating queries from documents or m argmax m ny i1 pqijdim 524 qw dw pqwjdwu journal kdd 00176 journal conference 00123 journal journal 00176 journal sigkdd 00088 journal discovery 00211 journal mining 00017 journal acm 00088 music music 00375 music purchase 00090 music mp3 00090 music listen 00180 music mp3com 00450 music free 00008 table 1 sample user profile to find the optimal word translation probabilities pqwjdwm we can use the em algorithm ,0,1,0
the details of the algorithm can be found in the literature for statistical translation models such as brown et al 1993 ,0,1,0
in the proposed method the statistical machine translation smt brown et al 1993 is deeply incorporated into the question answering process instead of using the smt as the preprocessing before the monolingual qa process as in the previous work ,0,1,0
most current transliteration systems use a generative model for transliteration such as freely available giza1 och and ney 2000an implementation of the ibm alignment models brown et al 1993 ,0,1,0
at the same time we believe our method has advantages over the approach developed initially at ibm brown et al 1990 brown et al 1993 for training translation systems automatically ,0,0,1
dagan church and gale 1993 expanded on this idea by replacing brown et als 1988 word alignment parameters which were based on absolute word positions in aligned segments with a much smaller set of relative offset parameters ,0,1,0
choosing the most advantageous hiemstra has published parts of the translational distributions of certain words induced using both his method and brown et als 1993b model 1 from the same training bitext ,0,1,0
due to the parameter interdependencies introduced by the onetoone assumption we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in brown et al 1993b equation 26 ,0,1,0
brown et al 1993 ,0,1,0
evaluation 61 evaluation at the token level this section compares translation model estimation methods a b and c to each other and to brown et als 1993b model 1 ,0,1,0
until now translation models have been evaluated either subjectively eg white and oconnell 1993 or using relative metrics such as perplexity with respect to other models brown et al 1993b ,0,1,0
an analysis of the alignments shows that smoothing the fertility probabilities significantly reduces the frequently occurring problem of rare words forming garbage collectors in that they tend to align with too many words in the other language brown della pietra della pietra goldsmith et al 1993 ,0,1,0
the fertility for the null word is treated specially for details see brown et al 1993 ,0,1,0
for placing the head the center function centeri brown et al 1993 uses the notation circledot i is used the average position of the source words with which the target word e i1 is aligned ,0,1,0
many existing systems for statistical machine translation garcavarea and casacuberta 2001 germann et al 2001 nieen et al 1998 och tillmann and ney 1999 implement models presented by brown della pietra della pietra and mercer 1993 the correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position ,0,1,0
the translation models they presented in various papers between 1988 and 1993 brown et al 1988 brown et al 1990 brown della pietra della pietra and mercer 1993 are commonly referred to as ibm models 15 based on the numbering in brown della pietra della pietra and mercer 1993 ,0,1,0
these results were achieved using the statistical alignments provided by model 5 brown et al 1993 och and ney 2000 and smoothed 11grams and 6grams respectively ,0,1,0
in the following section we show how this drawback can be overcome using statistical alignments brown et al 1993 ,0,1,0
yet the modeling training and search methods have also improved since the field of statistical machine translation was pioneered by ibm in the late 1980s and early 1990s brown et al 1990 brown et al 1993 berger et al 1994 ,1,0,0
as an alternative to the often used sourcechannel approach brown et al 1993 we directly model the posterior probability pre i 1 f j 1 och and ney 2002 ,0,1,0
knight and marcu 2000 treat reduction as a translation process using a noisychannel model brown et al 1993 ,0,1,0
one such model is the ibm model 1 brown et al 1993 ,0,1,0
we compare against several competing systems the first of which is based on the original ibm model 4 for machine translation brown et al 1993 and the hmm machine translation alignment model vogel ney and tillmann 1996 as implemented in the giza package och and ney 2003 ,0,1,0
our system outperforms competing approaches including the standard machine translation alignment models brown et al 1993 vogel ney and tillmann 1996 and the stateoftheart cut and paste summary alignment technique jing 2002 ,0,0,1
one obvious first approach would be to run a simpler model for the first iteration for example model 1 from machine translation brown et al 1993 which tends to be very recall oriented and use this to see subsequent iterations of the more complex model ,0,1,0
this feature is implemented by using the ibm1 lexical parameters brown et al 1993 och et al 2004 ,0,1,0
more specifically the latter system uses the ibm1 lexical parameters brown et al 1993 for computing the translation probabilities of two possible new tuples the one resulting when the nullalignedword is attached to table 6 evaluation results for experiments on ngram size incidence ,0,1,0
according to our experience the best performance is achieved when the union of the sourcetotarget and targettosource alignment sets ibm models brown et al 1993 is used for tuple extraction some experimental results regarding this issue are presented in section 422 ,1,0,0
the basic phrasebased model is an instance of the noisychannel approach brown et al 1993 ,0,1,0
introduction automatic word alignment brown et al 1993 is a vital component of all statistical machine translation smt approaches ,1,0,0
different approaches have been proposed for modeling prsat in equation 8 zeroorder models such as model 1 model 2andmodel 3 brown et al 1993 and the rstorder models such as model 4 model 5 brown et al 1993 hidden markov model ney et al 2000 and model 6 och and ney 2003 ,0,1,0
note that the translation direction is inverted from what would be normally expected correspondingly the models built around this equation are often called invertedtranslationmodels brown et al 1990 1993 ,0,1,0
four alternatives are proposed in these special issues 1 brent 1993 2 briscoe and carroll this issue 3 hindle and rooth this issue and 4 weischedel et al ,0,1,0
this is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues biber 1993 brent 1993 hindle and rooth this issue pustejovsky et al ,0,1,0
related work the recent availability of large amounts of bilingual data has attracted interest in several areas including sentence alignment gale and church 1991b brown lai and mercer 1991 simard foster and isabelle 1992 gale and church 1993 chen 1993 word alignment gale and church 1991a brown et al 1993 dagan church and gale 1993 fung and mckeown 1994 fung 1995b alignment of groups of words smadja 1992 kupiec 1993 van der eijk 1993 and statistical translation brown et al 1993 ,0,1,0
notice that most incontext and dictionary translations of source words are bounded within the same category in a typical thesaurus such as the lloce mcarthur 1992 and cilin mei et al 1993 ,0,1,0
parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis brown et al 1990 gale and church 1991 gale church and yarowsky 1992 church 1993 brown et al 1993 dagan church and gale 1993 department of computer science university of science and technology clear water bay hong kong ,0,1,0
the usual chinese nlp architecture first preprocesses input text through a word segmentation module chiang et al 1992 lin chiang and su 1992 1993 chang and chen 1993 wu and tseng 1993 sproat et al 1994 wu and fung 1994 but clearly bilingual parsing will be hampered by any errors arising from segmentation ambiguities that could not be resolved in the isolated monolingual context because even if the chinese segmentation is acceptable monolingually it may not agree with the words present in the english sentence ,0,1,0
p 18 whether this is a useful perspective for machine translation is debatable brown et al 1993 knoblock 1996however it is a deadon description of transliteration ,0,1,0
then they adapted brown et als 1993 statistical translation model 2 to work with this model of cooccurrence ,0,1,0
a limitation of churchs method and therefore also of dagan church and gales method is that orthographic cognates exist only among languages with similar alphabets church et al 1993 ,0,1,0
although the above statement was made about translation problems faced by human translators recent research brown et al 1993 melamed 1996b suggests that it also applies to problems in machine translation ,0,1,0
for example bilingual lexicographers can use bitexts to discover new crosslanguage lexicalization patterns catizone russell and warwick 1993 gale and church 1991b students of foreign languages can use one half of a bitext to practice their reading skills referring to the other half for translation when they get stuck nerbonne et al 1997 ,0,1,0
1 introduction most of the current work in statistical machine translation builds on word replacement models developed at ibm in the early 1990s brown et al 1990 1993 berger et al 1994 1996 ,0,1,0
for more information on these models please refer to brown et al 1993 ,0,1,0
as the first method we learn phrase alignments from a corpus that has been wordaligned by a training toolkit for a wordbased translation model the giza och and ney 2000 toolkit for the ibm models brown et al 1993 ,0,1,0
1 phrasebased unigram model various papers use phrasebased translation systems och et al 1999 marcu and wong 2002 yamada and knight 2002 that have shown to improve translation quality over singleword based translation systems introduced in brown et al 1993 ,0,0,1
by segmenting words into morphemes we can improve the performance of natural language systems including machine translation brown et al 1993 and information retrieval franz m and mccarley s 2002 ,0,0,1
for comparison purposes we consider two different algorithms for our answerextraction module one that does not bridge the lexical chasm based on ngram cooccurrences between the question terms and the answer terms and one that attempts to bridge the lexical chasm using statistical machine translation inspired techniques brown et al 1993 in order to find the best answer for a given question ,0,1,0
the mapping of answer terms to question terms is modeled using black et als 1993 simplest model called ibm model 1 ,0,1,0
we are given a source chinese sentence f fj1 f1fjfj which is to be translated into a target english sentence e ei1 e1eiei among all possible target sentences we will choose the sentence with the highest probability ei1 argmax ei1 prei1fj1 1 as an alternative to the often used sourcechannel approach brown et al 1993 we directly model the posterior probability prei1fj1 och and ney 2002 using a loglinear combination of feature functions ,0,1,0
41 model 1 score we used ibm model 1 brown et al 1993 as one of the feature functions ,0,1,0
1 introduction the statistical machine translation framework smt formulates the problem of translating a sentence from a source language s into a target language t as the maximization problem of the conditional probability tm lm argmaxt psjt pt 1 where psjt is called a translation model tm representing the generation probability from t into s pt is called a language model lm and represents the likelihood of the target language brown et al 1993 ,0,1,0
introduction translation of two languages with highly different morphological structures as exemplified by arabic and english poses a challenge to successful implementation of statistical machine translation models brown et al 1993 ,0,1,0
the orientation model is related to the distortion model in brown et al 1993 but we do not compute a block alignment during training ,0,1,0
 inversem1 directm1 inversemle directmle atreelets s t atreelets t s atreelets atreelets tspatsf stpatsf c catsf c catsf we use word probability tables pt s and ps t estimated by ibm model 1 brown et al 1993 ,0,1,0
by 17 0 10 20 30 40 50 60 70 80 90 100 10000 100000 1e06 1e07 test set items with translations training corpus size num words unigrams bigrams trigrams 4grams figure 1 percent of unique unigrams bigrams trigrams and 4grams from the europarl spanish test sentences for which translations were learned in increasingly large training corpora increasing the size of the basic unit of translation phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation brown et al 1993 ,0,0,1
1 introduction as with many other statistical natural language processing tasks statistical machine translation brown et al 1993 produces high quality results when ample training data is available ,1,0,0
stance the ibm models brown et al 1993 can be improved by adding more context dependencies into the translation model using a me framework rather than using only pf j e i garciavarea et al 2002 ,0,0,1
1 introduction word alignmentdetection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation mt brown et al 1993 och and ney 2003 koehn et al 2003 but also has been shown useful for other applications such as construction of bilingual lexicons wordsense disambiguation projection of resources and crosslanguage information retrieval ,0,1,0
more specifically a statistical word alignment model brown et al 1993 is used to acquire a bilingual lexicon consisting of nl substrings coupled with their translations in the target mrl ,0,1,0
in this work we use the giza implementation och and ney 2003 of ibm model 5 brown et al 1993 ,0,1,0
lexical relationships under the standard ibm models brown et al 1993 do not account for manytomany mappings and phrase extraction relies heavily on the accuracy of the ibm wordtoword alignment ,0,0,1
alignment quality can be further improved when the chunking procedure is based on translation lexicons from ibm model1 alignment model brown et al 1993 ,0,1,0
atthefinestlevel thisinvolvesthealignment of words and phrases within two sentences that are known to be translations brown et al 1993 och and ney 2003 vogel et al 1996 deng and byrne 2005 ,0,1,0
12 statistical modeling for translation earlier work in statistical machine translation brown et al 1993 is based on the noisychannel formulation where t arg max t ptjs argmax t ptpsjt 1 where the target language model pt is further decomposed as pt productdisplay i ptijti1 tik1 where k is the order of the language model and the translation model psjt has been modeled by a sequence of five models with increasing complexity brown et al 1993 ,0,1,0
the translation model is estimated via the em algorithm or approximations that are bootstrapped from the previous model in the sequence as introduced in brown et al 1993 ,0,1,0
3 a categorization of block styles in brown et al 1993 multiword cepts which are realized in our block concept are discussed and the authors state that when a target sequence is sufficiently different from a word by word translation only then should the target sequence should be promoted to a cept ,0,1,0
following the perspective of brown et al 1993 a minimal set of phrase blocks with lengths m n where either m or n must be greater than zero results in the following types of blocks 1 ,0,1,0
compared to earlier wordbased methods such as ibm models brown et al 1993 phrasebased methods such as pharaoh are much more effective in producing idiomatic translations and are currently the best performing methods in smt koehn and monz 2006 ,0,0,1
these rules are learned using a word alignment model which finds an optimal mapping from words to mr predicates given a set of training sentences and their correct mrs word alignment models have been widely used for lexical acquisition in smt brown et al 1993 koehn et al 2003 ,0,1,0
413 letter lexical transliteration similar to ibm model1 brown et al 1993 we use a bagofletter generative model within a block to approximate the lexical transliteration equivalence pfjlj eiki jlproductdisplay jprimej iksummationdisplay iprimei pfjprimeeiprimepeiprimeeiki 10 where peiprimeeiki similarequal 1k1 is approximated by a bagofword unigram ,0,1,0
standard smt alignment models brown et al 1993 are used to align letterpairs within named entity pairs for transliteration ,0,1,0
3 bistream hmms for transliteration standard ibm translation models brown et al 1993 can be used to obtain lettertoletter translations ,0,1,0
1 introduction the rapid and steady progress in corpusbased machine translation nagao 1981 brown et al 1993 has been supported by large parallel corpora such as the arabicenglish and chineseenglish parallel corpora distributed by the linguistic data consortium and the europarl corpus koehn 2005 which consists of 11 european languages ,0,1,0
we attribute the difference in m34 scores to the fact we use a viterbilike training procedure ie we consider a single configuration of the hidden variables in em training while giza uses pegging brown et al 1993 to sum over a set of likely hidden variable configurations in em ,0,1,0
similar to work in image retrieval barnard et al 2003 we cast the problem in terms of machine translation given a paired corpus of words and a set of video event representations to which they refer we make the ibm model 1 assumption and use the expectationmaximization method to estimate the parameters brown et al 1993 m j ajm jvideowordpl cvideowordp 1 1 1 this paired corpus is created from a corpus of raw video by first abstracting each video into the feature streams described above ,0,1,0
standard ci model 1 training initialised with a uniform translation table so that tejf is constant for all sourcetarget word pairs fe was run on untagged data for 10 iterations in each direction brown et al 1993 deng and byrne 2005b ,0,1,0
1993 introduce ibm models 15 for alignment modelling vogel et al ,0,1,0
then pei1jfj1 summationtextai 1 pei1ai1jfj1 brown et al 1993 ,0,1,0
the triplet lexicon model presented in this work can also be interpreted as an extension of the standard ibm model 1 brown et al 1993 with an additional trigger ,0,1,0
in this paper sentence pairs are extracted by a simple model that is based on the socalled ibm model1 brown et al 1993 ,0,1,0
the translation problem can be statistically formulated as in brown et al 1993 ,0,1,0
the work reported in this paper is most closely related to work on statistical machine translation particularly the ibmstyle work on candide brown et al 1993 ,0,1,0
examples of such contexts are verbobject relations and nounmodifier relations which were traditionally used in word similarity tasks from nonparallel corpora pereira et al 1993 hatzivassiloglou and mckeown 1993 ,0,1,0
this characteristic of our corpus is similar to problems with noisy and comparable corpora veronis 2000 and it prevents us from using methods developed in the mt community based on clean parallel corpora such as brown et al 1993 ,0,1,0
we also record for each token its derivational root using the celexbaayen et al 1993 database ,0,1,0
p d p l d 4 statistical approaches to language modeling have been used in much nlp research such as machine translation brown et al 1993 and speech recognition bahl et al 1983 ,0,1,0
similar techniques are used in papineni et al 1996 papineni et al 1998 for socalled direct translation models instead of those proposed in brown et al 1993 ,0,1,0
this is exactly the standard lexicon probability a27a28a18a26a4 a20a12 a22 employed in the translation model described in brown et al 1993 and in section 2 ,0,1,0
in this framework the source language lets say english is assumed to be generated by a noisy probabilistic source1 most of the current statistical mt systems treat this source as a sequence of words brown et al 1993 ,0,1,0
first we show how one can use an existing statistical translation model brown et al 1993 in order to automatically derive a statistical tmem ,0,1,0
2 the ibm model 4 for the work described in this paper we used a modified version of the statistical machine translation tool developed in the context of the 1999 johns hopkinssummer workshop alonaizan et al 1999 which implements ibm translation model 4 brown et al 1993 ,0,1,0
a65 the rest of the factors denote distorsion probabilities d which capture the probability that words change their position when translated from one language into another the probability of some french words being generated from an invisible english null element pa6 etc see brown et al 1993 or germann et al 2001 for a detailed discussion of this translation model and a description of its parameters ,0,1,0
mathematical details are fully described in brown et al 1993 ,0,1,0
let a183a49a48a50 a69 a188 a50 a51a181a51a181a51a212a188 a50a7a51a24a52 a48a54a53 a185a56a55 be a substring of a183 from the word a188 a50 with length a57 note this notation is different from brown et al 1993 ,0,1,0
following brown et al 1993 and the other literature in tm this paper only focuses the details of tm ,0,1,0
to make this paper comparable to brown et al 1993 we use englishfrench notation in this section ,0,1,0
1 perform the following maximization ei1 argmax ei1 fprei1prfj1 jei1g 2 this approach is referred to as sourcechannel approach to statistical mt sometimes it is also referred to as the fundamental equation of statistical mt brown et al 1993 ,0,1,0
if the language model prei1 p ei1 depends on parameters and the translation model prfj1 jei1 p fj1 jei1 depends on parameters then the optimal parameter values are obtained by maximizing the likelihood on a parallel training corpus fs1es1 brown et al 1993 argmax sy s1 p fsjes 3 argmax sy s1 p es 4 computational linguistics acl philadelphia july 2002 pp ,0,1,0
for the ibm models defined by a pioneering paper brown et al 1993 a decoding algorithm based on a lefttoright search was described in berger et al 1996 ,1,0,0
proceedings of the 40th annual meeting of the association for brown et al 1990 brown et al 1993 a number of other algorithms have been developed ,0,1,0
being inspired by the success of noisychannelbased approaches in applications as diverse as speech recognition jelinek 1997 part of speech tagging church 1988 machine translation brown et al 1993 information retrieval berger and lafferty 1999 and text summarization knight and marcu 2002 we develop a noisy channel model for qa ,1,0,0
see brown et al 1993 for a detailed mathematical description of the model and the formula for computing the probability of an alignment and target string given a source string ,0,1,0
to help our model learn that it is desirable to copy answer words into the question we add to each corpus a list of identical dictionary word pairs w iw i for each corpus we use giza alonaizan et al 1999 a publicly available smt package that implements the ibm models brown et al 1993 to train a qa noisychannel model that maps flattened answer parse trees obtained using the cut procedure described in section 31 into questions ,0,1,0
these constraints tie words in such a way that the space of alignments cannot be enumerated as in ibm models 1 and 2 brown et al 1993 ,0,1,0
234 word translation probability estimation many methods are used to estimate word translation probabilities from unparallel or parallel bilingual corpora koehn and knight 2000 brown et al 1993 ,0,1,0
the next section briefly reviews the word alignment based statistical machine translation brown et al 1993 ,0,1,0
the former term pe is called a language model representing the likelihood of e the latter term pje is called a translation model representing the generation probability from e into j as an implementation of pje the word alignment based statistical translation brown et al 1993 has been successfully applied to similar language pairs such as frenchenglish and german english but not to drastically dierent ones such as japaneseenglish ,1,0,0
as a baseline we use an ibm model 4 brown et al 1993 system3 with a greedy decoder4 germann et al 2001 ,0,1,0
reordering effects across languages have been modeled in several ways including wordbased brown et al 1993 templatebased och et al 1999 and syntaxbased yamada knight 2001 ,0,1,0
the traditional framework presented in brown et al 1993 assumes a generative process where the source sentence is passed through a noisy stochastic process to produce the target sentence ,0,1,0
within the generative model the bayes reformulation is used to estimate a31 a0a15a14a35a33a1a26a13a37a36 a31 a0a15a14a19a13 a31 a0a2a1a38a33a14a39a13 where a31 a0a15a14a39a13 is considered the language model and a31 a0a2a1a38a33a14a19a13 is the translation model the ibm brown et al 1993 models being the de facto standard ,1,0,0
22 the translation model we adapted model 1 brown et al 1993 to our purposes ,0,1,0
darwish 2002 is not very useful for applications like statistical machine translation brown et al 1993 for which an accurate wordtoword alignment between the source and the target languages is critical for high quality translations ,0,1,0
by segmenting words into morphemes we can improve the performance of natural language systems including machine translation brown et al 1993 and information retrieval franz m and mccarley s 2002 ,0,0,1
4 we see strong parallels between transtype and itu language model enumerating word sequences vs 4 initially statistical mt used a noisychannel approach brown et al 1993 but recently och and ney 2002 have introduced a more general framework based on the maximumentropy principle which shows nice prospects in terms of flexibility and learnability ,0,1,0
he then goes on to adapt the conventional noisy channel mt model of brown et al 1993 to nlu where extracting a semantic representation from an input text corresponds to finding argmaxsem pinputsem psem where psem is a model for generating semantic representations and pinputsem is a model for the relation between semantic representations and corresponding texts ,0,1,0
some studies have been done for acquiring collocation translations using parallel corpora smadja et al 1996 kupiec 1993 echizenya et al 2003 ,0,1,0
most previous research in translation knowledge acquisition is based on parallel corpora brown et al 1993 ,0,1,0
these range from twoword to multiword with or without syntactic structure smadja 1993 lin 1998 pearce 2001 seretan et al 2003 ,0,1,0
1 introduction machine translation systems based on probabilistic translation models brown et al 1993 are generally trained using sentencealigned parallel corpora ,0,1,0
alm does this by using alignment models from the statistical machine translation literature brown et al 1993 ,0,1,0
note that our use of cepts differs slightly from that of brown et al 1993 sec3 inasmuch cepts may not overlap according to our definition ,0,1,0
obtaining a wordaligned corpus usually involves training a wordbased translation models brown et al 1993 in each directions and combining the resulting alignments ,0,1,0
1 introduction ibm model 1 brown et al 1993a is a wordalignment model that is widely used in working with parallel bilingual corpora ,1,0,0
bootstrapping a pmtg from a lowerdimensional pmtg and a wordtoword translation model is similar in spirit to the way that regular grammars can help to estimate cfgs lari young 1990 and the way that simple translation models can help to bootstrap more sophisticated ones brown et al 1993 ,1,0,0
2 21 word alignment adaptation bidirectional word alignment in statistical translation models brown et al 1993 only onetoone and moretoone word alignment links can be found ,0,0,1
1 introduction bilingual word alignment is first introduced as an intermediate result in statistical machine translation smt brown et al 1993 ,0,1,0
for the results in this paper we have used pointwise mutual information pmi instead of ibm model 1 brown et al 1993 since rogati and yang 2004 found it to be as effective on springer but faster to compute ,0,0,1
syntaxlight alignment models such as the five ibm models brown et al 1993 and their relatives have proved to be very successful and robust at producing wordlevel alignments especially for closely related languages with similar word order and mostly local reorderings which can be captured via simple models of relative word distortion ,1,0,0
in machine translation for example sentences are produced using applicationspecific decoders inspired by work on speech recognition brown et al 1993 whereas in summarization summaries are produced as either extracts or using taskspecific strategies barzilay 2003 ,0,1,0
translation including the joint probability phrasebased model marcu and wong 2002 and a variant on the alignment template approach och and ney 2004 and contrast them to the performance of the wordbased ibm model 4 brown et al 1993 ,0,1,0
by increasing the size of the basic unit of translation phrasebased machine translation does away with many of the problems associated with the original wordbased formulation of statistical machine translation brown et al 1993 in particular the brown et al ,0,0,1
statistical approaches which depend on a set of unknown parameters that are learned from training data try to describe the relationship between a bilingual sentence pair brown et al 1993 vogel and ney 1996 ,0,1,0
if e has length l and f has length m there are possible 2lm alignments between e and f brown et al 1993 ,0,1,0
2 statistical word alignment according to the ibm models brown et al 1993 the statistical word alignment model can be generally represented as in equation 1 ,0,1,0
this simplified version does not take word classes into account as described in brown et al 1993 ,0,1,0
1 introduction word alignment was first proposed as an intermediate result of statistical machine translation brown et al 1993 ,0,1,0
these methods go beyond the original ibm machine translation models brown et al 1993 by allowing multiword units phrases in one language to be translated directly into phrases in another language ,0,0,1
1 introduction statistical approaches to machine translation pioneered by brown et al 1993 achieved impressive performance by leveraging large amounts of parallel corpora ,1,0,0
as a unified approach we augment the sdig by adding all the possible word pairs ji fe as a parallel et pair and using the ibm model 1 brown et al 1993 word to word translation probability as the et translation probability ,0,1,0
in comparison we deployed the giza mt modeling tool kit which is an implementation of the ibm models 1 to 4 brown et al 1993 alonaizan et al 1999 och and ney 2003 ,0,1,0
most of the phrasebased translation models have adopted the noisychannel based ibm style models brown et al 1993 cmct c1 bd bp cpd6cvd1cpdc ct c1 bd c8d6b4cu c2 bd cyct c1 bd b5c8d6b4ct c1 bd b5 1 in these model we have two types of knowledge translation model c8d6b4cu c2 bd cyct c1 bd b5 and language model c8d6b4ct c1 bd b5 ,0,1,0
31 learning chunkbased translation we learn chunk alignments from a corpus that has been wordaligned by a training toolkit for wordbased translation models the giza och and ney 2000 toolkit for the ibm models brown et al 1993 ,0,1,0
the first work on smt done at ibm brown et al 1990 brown et al 1992 brown et al 1993 berger et al 1994 used a noisychannel model resulting in what brown et al ,0,1,0
there are basically two kinds of systems working at these segmentation levels the most widespread rely on statistical models in particular the ibm ones brown et al 1993 others combine simpler association measures with different kinds of linguistic information arhenberg et al 2000 barbu 2004 ,1,0,0
2 related work starting with the ibm models brown et al 1993 researchers have developed various statistical word alignment systems based on different models such as hidden markov models hmm vogel et al 1996 loglinear models och and ney 2003 and similaritybased heuristic methods melamed 2000 ,0,1,0
most current smt systems och and ney 2004 koehn et al 2003 use a generative model for word alignment such as the freely available giza och and ney 2003 an implementation of the ibm alignment models brown et al 1993 ,0,1,0
the first one gizalex is obtained by running the giza2 implementation of the ibm word alignment models brown et al 1993 on the initial parallel corpus ,0,1,0
in this paper we show that a noisy channel model instantiated within the paradigm of statistical machine translation smt brown et al 1993 can successfully provide editorial assistance for nonnative writers ,1,0,0
is combined with e jit1 to be aligned with f nmt then attcntattr e k e i fef jinmjinmprp1 1 where k is the degree of ein finally the node translation probability is modeled as tntnlnlnnn eifleiflejfl prprpr and the text translation probability ef ttpr is model using ibm model i brown et al 1993 ,0,1,0
however current sentence alignment models brown et al 1991 gale church 1991 wu 1994 chen 489 1993 zhao and vogel 2002 etc ,0,1,0
ngram language models have also been used in statistical machine translation smt as proposed by brown et al 1990 brown et al 1993 ,0,1,0
distortion models were first proposed by brown et al 1993 in the socalled ibm models ,0,1,0
1 introduction phrasebased translation models marcu and wong 2002 koehn et al 2003 och and ney 2004 which go beyond the original ibm translation models brown et al 1993 1 by modeling translations of phrases rather than individual words have been suggested to be the stateoftheart in statistical machine translation by empirical evaluations ,0,0,1
use of sententially aligned corpora for word alignment has already been recommended in brown et al 1993 ,0,1,0
1 introduction several approaches including statistical techniques gale and church 1991 brown et al 1993 lexical techniques huang and choi 2000 tiedemann 2003 and hybrid techniques ahrenberg et al 2000 have been pursued to design schemes for word alignment which aims at establishing links between words of a source language and a target language in a parallel corpus ,0,1,0
5 discussion and future work the work in this paper substantially differs from previous work in smt based on the noisy channel approach presented in brown et al 1993 ,0,1,0
1 introduction the most widely applied training procedure for statistical machine translation ibm model 4 brown et al 1993 unsupervised training followed by postprocessing with symmetrization heuristics och and ney 2003 yields low quality word alignments ,1,0,0
we rst recast the problem of estimating the ibm models brown et al 1993 in a discriminative framework which leads to an initial increase in wordalignment accuracy ,0,1,0
4 semisupervised training for word alignments intuitively in approximate em training for model 4 brown et al 1993 the estep corresponds to calculating the probability of all alignments according to the current model estimate while the mstep is the creation of a new model estimate given a probability distribution over alignments calculated in the estep ,0,1,0
aligning tokens in parallel sentences using the ibm models brown et al 1993 och and ney 2003 may require less information than fullblown translation since the task is constrained by the source and target tokens present in each sentence pair ,1,0,0
we thus propose to adapt the statistical machine translation model brown et al 1993 zens and ney 2004 for sms text normalization ,0,1,0
a null assuming that one sms word is mapped exactly to one english word in the channel model under an alignment we need to consider only two types of probabilities the alignment probabilities denoted by pm and the lexicon mapping probabilities denoted by brown et al 1993 ,0,1,0
47 fertilitybased transducer in brown et al 1993 three alignment models are described that include fertility models these are ibm models 3 4 and 5 ,0,1,0
in brown et al 1994 the authors proposed a method to integrate the ibm translation model 2 brown et al 1993 with an asr system ,0,1,0
we rescore the asr nbest lists with the standard hmm vogel et al 1996 and ibm brown et al 1993 mt models ,0,1,0
this is similar to model 3 of brown et al 1993 but without nullgenerated elements or reordering ,0,1,0
learned vowels include in order of generation probability e a o u i y learned sonorous consonants include n s r l m learned nonsonorous consonants include d c t l b m p q the model bootstrapping is good for dealing with too many parameters we see a similar approach in brown et als 1993 march from model 1 to model 5 ,0,1,0
machine translation has codelike characteristics and indeed the initial models of brown et al 1993 took a wordsubstitutiontransposition approach trained on a parallel text ,0,1,0
such methods have also been a key driver of progress in statistical machine translation which depends heavily on unsupervised word alignments brown et al 1993 ,1,0,0
wordalignment however isalmost exclusively done using statistics brown et al 1993 hiemstra 1996 vogel et al 1999 toutanova et al 2002 ,0,1,0
22 word alignment aligning below the sentence level is usually done using statistical models for machine translation brown et al 1991 brown et al 1993 hiemstra 1996 vogel et al 1999 where any word of the targetlanguageistakentobeapossibletranslation for each source language word ,0,1,0
32 word order differences another problem that has been noticed as early as 1993 with the first research on word alignment brown et al 1993 concerns the differences in word order between source and target language ,0,1,0
while simple statistical alignment models like ibm1 brown et al 1993 and the symmetric alignment approach by hiemstra 1996 treat sentences as unstructured bags of words the more sophisticated ibmmodels by brown et al ,0,1,0
1 introduction aligning parallel text ie automatically setting the sentences or words in one text into correspondence with their equivalents in a translation is a very useful preprocessing step for a range of applications including but not limited to machine translation brown et al 1993 crosslanguage information retrieval hiemstra 1996 dictionary creation smadja et al 1996 and induction of nlptools kuhn 2004 ,0,1,0
the classical bayes relation is used to introduce a target language model brown et al 1993 e argmaxe pref argmaxe prfepre where prfe is the translation model and pre is the target language model ,0,1,0
a similar intuition holds for the machine translation models generically known as the ibm models brown et al 1993 which assume that certain words in a source language sentence tend to trigger the usage of certain words in a target language translation of that sentence ,0,1,0
the methodology used brown et al 1993 is based on the definition of a function prti1sj1 that returns the probability that ti1 is a 835 source transferir documentos explorados a otro directorio interaction0 move documents scanned to other directory interaction1 move s canned documents to other directory interaction2 move scanned documents to a nother directory interaction3 move scanned documents to another f older acceptance move scanned documents to another folder figure 1 example of cat system interactions to translate the spanish source sentence into english ,0,1,0
models of this kind assume that an input word is generated by only one output word brown et al 1993 ,0,1,0
these alignments can be obtained from singleword models brown et al 1993 using the available public software giza och and ney 2003 ,0,1,0
for this we used two resources celex a linguistically annotated dictionary of english dutch and german baayen et al 1993 and the dutch snowball stemmer implementing a suf x stripping algorithm based on the porter stemmer ,0,1,0
for the word alignment we apply standard techniques derived from statistical machine translation using the wellknown ibm alignment models brown et al 1993 implemented in the opensource tool giza och 2003 ,1,0,0
1 introduction word alignment was first proposed as an intermediate result of statistical machine translation brown et al 1993 ,0,1,0
1 a cept is defined as the set of target words connected to a source word brown et al 1993 ,0,1,0
1 introduction word alignment was first proposed as an intermediate result of statistical machine translation brown et al 1993 ,0,1,0
21 baseline ibm model1 the translation process can be viewed as operations of word substitutions permutations and insertionsdeletions brown et al 1993 in noisychannel modeling scheme at parallel sentencepair level ,0,1,0
1 1we follow the notations in brown et al 1993 for englishfrench ie e f although our models are tested in this paper for englishchinese ,0,1,0
for instance the most relaxed ibm model1 which assumes that any source word can be generated by any target word equally regardless of distance can be improved by demanding a markov process of alignments as in hmmbased models vogel et al 1996 or implementing a distribution of number of target words linked to a source word as in ibm fertilitybased models brown et al 1993 ,0,1,0
for the simple bagofword bilingual lsa as describedinsection221aftersvdonthesparsematrix using the toolkit svdpack berry et al 1993 all source and target words are projected into a lowdimensional r 88 lsaspace ,0,1,0
it can be applied to complicated models such ibm model4 brown et al 1993 ,0,1,0
second it can be applied to control the quality of parallel bilingual sentences mined from the web which are critical sources for a wide range of applications such as statistical machine translation brown et al 1993 and crosslingual information retrieval nie et al 1999 ,0,1,0
by treating a lettercharacter as a word and a group of letterscharacters as a phrase or token unit in smt one can easily apply the traditional smt models such as the ibm generative model brown et al 1993 or the phrasebased translation model crego et al 2005 to transliteration ,0,1,0
43 baseline we use a standard loglinear phrasebased statistical machine translation system as a baseline giza implementation of ibm word alignment model 4 brown et al 1993 och and ney 20038 the refinement and phraseextraction heuristics described in koehn et al 2003 minimumerrorrate training 7more specifically we choose the first english reference from the 7 references and the chinese sentence to construct new sentence pairs ,0,1,0
to quickly and approximately evaluate this phenomenon we trained the statistical ibm wordalignment model 4 brown et al 19931 using the giza software och and ney 2003 for the following language pairs chineseenglish italian english and dutchenglish using the iwslt2006 corpus takezawa et al 2002 paul 2006 for the first two language pairs and the europarl corpus koehn 2005 for the last one ,0,1,0
they can be seen as extensions of the simpler ibm models 1 and 2 brown et al 1993 ,0,1,0
most current statistical models brown et al 1993 vogel et al 1996 deng and byrne 2005 treat the aligned sentences in the corpus as sequences of tokens that are meant to be words the goal of the alignment process is to find links between source and target words ,0,1,0
this situation is very similar to the training process of translation models in statistical machine translation brown et al 1993 where parallel corpus is used to find the mappings between words from different languages by exploiting their cooccurrence patterns ,0,1,0
in pursuit of better translation phrasebased models ochandney2004havesignificantlyimprovedthe quality over classical wordbased models brown et al 1993 ,0,0,1
1 introduction for statistical machine translation smt phrasebased methods koehn et al 2003 och and ney 2004 and syntaxbased methods wu 1997 alshawi et al 2000 yamada and knignt 2001 melamed 2004 chiang 2005 quick et al 2005 mellebeek et al 2006 outperform wordbased methods brown et al 1993 ,0,0,1
similarity measures can be based on any level of linguistic analysis semantic similarity relies on context vectorsrapp 1999 whilesyntacticsimilarityisbased on the alignment of parallel corpora brown et al 1993 ,0,1,0
31 modelbased phrase pair posterior in a statistical generative word alignment model brown et al 1993 it is assumed that i a random variable a specifies how each target word fj is generated by therefore aligned to a source 1 word eaj and ii the likelihood function ffae specifies a generativeprocedurefromthesourcesentencetothe target sentence ,0,1,0
more specifically by using translation probabilities we can rewrite equation 11 and 12 as follow nullnullnullnullnull null nullnull null nullnullnull null null nullnullnullnull null nullnull null null nullnull null nullnull null null null null null null nullnull null nullnull null nullnull null null null null nullnull null nullnull null null null 1nullnull null nullnull null null null nullnullnullnull 13 nullnullnullnullnull null nullnull null nullnullnull null null nullnullnullnull null nullnull null null nullnull null nullnull null null null null null null nullnull null nullnull null nullnull null null null null nullnull null nullnull null null null 1nullnull null nullnull null null null nullnullnullnull 14 where nullnullnullnullnull null null denotes the probability that topic term null is the translation of null null in our experiments to estimate the probability nullnullnullnullnull null null we used the collections of question titles and question descriptions as the parallel corpus and the ibm model 1 brown et al 1993 as the alignment model ,0,1,0
the text was split at the sentence level tokenized and pos tagged in the style of the wall street journal penn treebank marcus et al 1993 ,0,1,0
this probability is computed using ibms model 1 brown et al 1993 pqa productdisplay qq pqa 3 pqa 1pmlqapmlqc 4 pmlqa summationdisplay aa tqapmlaa 5 where the probability that the question term q is generated from answer a pqa is smoothed using the prior probability that the term q is generated from the entire collection of answers c pmlqc ,0,1,0
this model is similar in spirit to ibm model 1 brown et al 1993 ,0,1,0
we use the giza implementation of ibm model 4 brown et al 1993 och and ney 2003 coupled with the phrase extraction heuristics of koehn et al ,0,1,0
1 introduction bilingual data including bilingual sentences and bilingual terms are critical resources for building many applications such as machine translation brown 1993 and cross language information retrieval nie et al 1999 ,0,1,0
this is an important feature from the mt viewpoint since the decomposition into translation model and language model proved to be extremely useful in statistical mt since brown et al 1993 ,1,0,0
1 is based on several realvalued feature functions fi their computation is based on the socalled ibm model1 brown et al 1993 ,0,1,0
widely used alignment models such as ibm model serial brown et al 1993 and hmm all assume onetomany alignments ,1,0,0
11 however modeling word order under translation is notoriously difficult brown et al 1993 and it is unclear how much improvement in accuracy a good model of word order would provide ,0,1,0
since chinese text is not orthographically separated into words the standard methodology is to first preproce input texts through a segmentation module chiang et al 1992 linet al 1992 chang chert 1993 linet al 1993 wu tseng 1993 sproat et al 1994 ,0,1,0
1 introduction parallel corpora have been shown to provide an extremely rich source of constraints for statistical analysis eg brown et al 1990 gale church 1991 gale et al 1992 church 1993 brown et al 1993 dagan et al 1993 dagan church 1994 fung church 1994 wu xia 1994 fung mckeown 1994 ,0,1,0
a simpler related idea of penalizing distortion from some ideal matching pattern can be found in the statistical translation brown et al 1990 brown et al 1993 and word alignment dagan et al 1993 dagan church 1994 models ,0,1,0
this approach addresses the problematic aspects of both pure knowledgebased generation where incomplete knowledge is inevitable and pure statistical bag generation brown et al 1993 where the statistical system has no linguistic guidance ,0,0,1
however compositional approaches to lexical choice have been successful whenever detailed representations of lexical constraints can be collected and entered into the lexicon eg elhadad 1993 kukich et al 1994 ,0,1,0
estimation of the parameters has been described elsewhere brown et al 1993 ,0,1,0
such linguisticpreprocessing techniques could 1various models have been constructed by the ibm team brown et al 1993 ,0,1,0
the node mapping function f for the entire tree thus has a different role from the alignment function in the ibm statistical translation model brown et al 1990 1993 the role of the latter includes the linear ordering of words in the target string ,0,1,0
model 1 is the wordpair translation model used in simple machine translation and understanding models brown et al 1993 epstein et al 1996 ,0,1,0
among all possible target strings we will choose the one with the highest probability which is given by bayes decision rule brown et al 1993 argmaxpelfg argmax pef ,0,1,0
the concept of these alignments is similar to the ones introduced by brown et al 1993 but we will use another type of dependence in the probability distributions ,0,1,0
therefore the probability of alignment aj for position j should have a dependence on the previous alignment position o jl pj j1 a similar approach has been chosen by dagan et al 1993 and vogel et al 1996 ,0,1,0
the ibm model 1 brown et al 1993 is used to find an initial estimate of the translation probabilities ,0,1,0
correspondence points associated with frequent token types church 1993 or by deleting frequent token types from the bitext altogether dagan et al 1993 ,0,1,0
one important application of bitext maps is the construction of translation lexicons dagan et al 1993 and as discussed translation lexicons are an important information source for bitext mapping ,0,1,0
in addition to their use in machine translation sato nagao 1990 brown et al 1993 melamed 1997 translation models can be applied to machineassisted translation sato 1992 foster et al 1996 crosslingual information retrieval sigir 1996 and gisting of world wide web pages resnik 1997 ,0,1,0
bitexts also play a role in less automated applications such as concordancing for bilingual lexicography catizone et al 1993 gale church 1991b computerassisted language learning and tools for translators eg ,0,1,0
5 effectiveness comparison 51 englishchinese atis models both the transfer and transducer systems were trained and evaluated on englishtomandarin chinese translation of transcribed utterances from the atis corpus hirschman et al 1993 ,0,1,0
therefore pg l e is the sum of the probabilities of generating g from e over all possible alignments a in which the position i in the target sentence g is aligned to the position ai in the source sentence e pgle i l m e it tg lejlaa ij lm al0 amm0jl m e 1i tg l eailj t m 3 jl i0 brown et al 1993 also described how to use the em algorithm to estimate the parameters ai i jl m and g i e in the aforementioned model ,0,1,0
although the authors of brown et al 1993 stated that they would discuss the search problem in a followup arti cle so far there have no publications devoted to the decoding issue for statistical machine translation ,0,0,1
we dealt with this by either limiting the translation probability from the null word brown 367 et al 1993 at the hypothetical 0positionbrown et al 1993 over a threshold during the em training or setting sho j to a small probability 7r instead of 0 for the initial null hypothesis h0 ,0,1,0
the cooccurrence relation can also be based on distance in a bitext space which is a more general representations of bitext correspondence dagan et al 1993 resnik melamed 1997 or it can be restricted to words pairs that satisfy some matching predicate which can be extrinsic to the model melamed 1995 melamed 1997 ,0,1,0
it is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values brown et al 1990 dagan et al 1993 chen 1996 ,0,1,0
this approach has also been used by dagan and itai 1994 gale et al 1992 shiitze 1992 gale et al 1993 yarowsky 1995 gale and church 1lunar is not an unknown word in english yeltsin finds its translation in the 4th candidate ,0,1,0
some of the early statistical terminology translation methods are brown et al 1993 wu and xia 1994 dagan and church 1994 gale and church 1991 kupiec 1993 smadja et al 1996 kay and rsscheisen 1993 fung and church 1994 fung 1995b ,0,1,0
in the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translationbrown et al 1993 brown et al 1991 gale and church 1993 church 1993 simard et al 1992 large amount of human effort and time has been invested in collecting parallel corpora of translated texts ,0,1,0
1 introduction early works gale and church 1993 brown et al 1993 and to a certain extent kay and r6scheisen 1993 presented methods to exct biigua ,0,1,0
on the other hand dagan et al 1993 proposed an algorithm borrowed to the field of dynamic programming and based on the output of their previous work to find the best alignment subject to certain constraints between words in parallel sentences ,0,1,0
960 12 alignment with mixture distribution several papers have discussed the first issue especially the problem of word alignments for bilingual corpora brown et al 1993 dagan et al 1993 kay and rsscheisen 1993 fung and church 1994 vogel et al 1996 ,0,1,0
in our search procedure we use a mixturebased alignment model that slightly differs from the model introduced as model 2 in brown et al 1993 ,0,1,0
it assumes that the distance of the positions relative to the diagonal of the j i plane is the dominating factor ri j i pilj j i 7 eil ri j as described in brown et al 1993 the em algorithm can be used to estimate the parameters of the model ,0,1,0
the simple model 1 brown et al 1993 for the translation of a sl sentence d dldt in a tl sentence e el em assumes that every tl word is generated independently as a mixture of the sl words m l ped h tejdi 2 jl io in the equation above tejdi stands for the probability that ej is generated by di ,0,1,0
in the refined model 2 brown et al 1993 alignment probabilities ailj l m are included to model the effect that the position of a word influences the position of its translation ,0,1,0
the application of this algorithm to the basic problem using a parallel bilingual corpus aligned on the sentence level is described in brown et al 1993 ,0,1,0
1 introduction most if not all statistical machine translation systems employ a wordbased alignment model brown et al 1993 vogel ney and tillman 1996 wang and waibel 1997 which treats words in a sentence as independent entities and ignores the structural relationship among them ,0,1,0
the subset was the neighboring alignments brown et al 1993 of the viterbi alignments discovered by model 1 and model 2 ,0,1,0
estimation of the parameters has been described elsewhere brown et al 1993 ,0,1,0
i various models have been constructed by the ibm team brown et al 1993 ,0,1,0
this model is trained on approximately 5 million sentence pairs of hansard canadian parliamentary and un proceedings which have been aligned on a sentencebysentence basis by the methods of brown et al 1991 and then further aligned on a wordbyword basis by methods similar to brown et al 1993 ,0,1,0
221 the evaluator the evaluator is a function ptt s which assigns to each targettext unit t an estimate of its probability given a source text s and the tokens t which precede t in the current translation of s our approach to modeling this distribution is based to a large extent on that of the ibm group brown et al 1993 but it diflhrs in one significant aspect whereas the ibm model involves a noisy channel decomposition we use a linear combination of separate predictions from a language model ptt and a translation model pts ,0,1,0
this formula follows the convention of brown et al 1993 in letting so designate the null state ,0,1,0
the statistical machine translation approach is based on the noisy channel paradigm and the maximumaposteriori decoding algorithm brown et al 1993 ,0,1,0
the sequence ws is thought as a noisy version of wt and the best guess id is then computed as w argmax pwtws wt argmax pwslwtpwt 1 wt in brown et al 1993 they propose a method for maximizing pwtiws by estimating pwt and pwsiwt and solving the problem in equation 1 ,0,1,0
our approach to statistical machine translation differs from the model proposed in brown et al 1993 in that we compute the joint model pws wt from the bilanguage corpus to account for the direct mapping of the source sentence ws into the target sentence ivt that is ordered according to the source language word order ,0,1,0
the model consists of a set of wordpair parameters pts and position parameters pji in model 1 ibm1 the latter are fixed at 11 1 as each position including the empty position 0 is considered equally likely to contain a translation for w maximum likelihood estimates for these parameters can be obtained with the em algorithm over a bilingual training corpus as described in brown et al 1993 ,0,1,0
pd p l d 4 statistical approaches to language modeling have been used in much nlp research such as machine translation brown et al 1993 and speech recognition bahl et al 1983 ,0,1,0
in order to minimize the number of decision errors at the sentence level we have to choose the sequence of target words ei1 according to the equation brown et al 1993 ei1 argmax ei1 n prei1jfj1 o argmax ei1 n prei1prfj1 jei1 o here the posterior probability prei1jfj1 is decomposed into the language model probability prej1 and the string translation probability prfj1 jei1 ,0,1,0
models describing these types of dependencies are referred to as alignment mappings brown et al 1993 alignment mapping j i aj which assigns a source word fj in position j to a target word ei in position i aj ,0,1,0
as a result the string translation probability can be decomposed into a lexicon probability and an alignment probability brown et al 1993 ,0,1,0
3 experimental results whereas stochastic modelling is widely used in speech recognition there are so far only a few research groups that apply stochastic modelling to language translation berger et al 1994 brown et al 1993 knight 1999 ,1,0,0
if we assign a probability a13a15a14a17a16 a10a12a11a5a19a18a2 a3a5a21a20 to each pair of strings a16 a10 a11a5a12a22 a2a4a3a5 a20 then according to bayes decision rule we have to choose the english string that maximizes the product of the english language model a13a23a14a24a16 a10 a11a5 a20 and the string translation model a13a15a14a17a16a25a2 a3a5a26a18a10a27a11a5a28a20 many existing systems for statistical machine translation wang and waibel 1997 nieen et al 1998 och and weber 1998 make use of a special way of structuring the string translation model like proposed by brown et al 1993 the correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position ,0,1,0
table 2 summarizes the characteristics of the training corpus used for training the parameters of model 4 proposed in brown et al 1993 ,0,1,0
in this paper we use the socalled model 4 from brown et al 1993 ,0,1,0
for a detailed description for model 4 the reader is referred to brown et al 1993 ,0,1,0
they developed a simple heuristic function for model 2 from brown et al 1993 which was non admissible ,0,1,0
many statistical translation models brown et al 1993 vogel et al 1996 och and ney 2000b try to model wordtoword correspondences between source and target words ,0,1,0
we trained ibm translation model 4 brown et al 1993 both on our corpus alone and on the augmented corpus using the egypt toolkit knight et al 1999 alonaizan et al 1999 and then translated a number of texts using different translation models and different transfer methods namely glossing replacing each tamil word by the most likely candidate from the translation tables created with the egypt toolkit and model 4 decoding brown et al 1995 germann et al 2001 ,0,1,0
6 concluding remarks our work presents a set of improvements on previous state of the art of grammar association first by providing better language models to the original system described in vidal et al 1993 second by setting the technique into a rigorous statistical framework clarifying which kind of probabilities have to be estimated by association models third by developing a novel and especially adequate association model loco c on the other hand though experimental results are quite good we find them particularly relevant for pointing out directions to follow for further improvement of the grammar association technique ,0,1,0
however in the grammar association context when developing using bayes decomposition the basic equations of the system presented in vidal et al 1993 it is said that the reverse model for a28 a13a37a3a38a5a39a32a21a0a35a7 does not seem to admit a simple factorization which is also correct and convenient so crude heuristics were adopted in the mathematical development of the expression to be maximized ,0,1,0
we based our design on the ibm models 1 and 2 brown et al 1993 but taking into account that our model must generate correct derivations in a given grammar not any sebegin some endanimals eat animals a some a88 animalsa89 eat a88 animalsa89 begin some endanimals eat are animals dangerous b some a88 animalsa89 are dangerous begin animals some end eat are animals dangerous c a88 animalsa89 are dangerous begin snakes rats people some end eat are snakes rats people dangerous d expansion of a88 animalsa89 figure 3 using a category a86 animalsa87 for snakes rats and people in the example of figure 1 ,0,1,0
one interesting approach to extending the current system is to introduce a statistical translation model brown et al 1993 to filter out irrelevant translation candidates and to extract the most appropriate subpart from a long english sequence as the translation by locally aligning the japanese and english sequences ,0,1,0
1993 and the hmm alignment model of vogel et al 1996 ,0,1,0
we refer to a3a16a5a7 as the source language string and a10 a11a7 as the target language string in accordance with the noisy channel terminology used in the ibm models of brown et al 1993 ,0,1,0
intuitively if we allow any source words to be aligned to any target words the best alignment that we can come up with is the one in figure 1c sentence pair s2 t2 offers strong evidence that b c in language s means the same thing as x in language t on the basis of this evidence we expect the system to also learn from sentence pair s1 t1 that a in language s means the same thing as y in language t unfortunately if one works with translation models that do not allow target words to be aligned to more than one source word as it is the case in the ibm models brown et al 1993 it is impossible to learn that the phrase b c in language s means the same thing as word x in language t the ibm model 4 brown et al 1993 for example converges to the word alignments shown in figure 1b and learns the translation probabilities shown in figure 1a2 since in the ibm model one cannot link a target word to more than a source word the training procedure 2to train the ibm4 model we used giza alonaizan et al 1999 ,0,1,0
1 motivation most of the noisychannelbased models used in statistical machine translation mt brown et al 1993 are conditional probability models ,0,1,0
2 wordtoword bitext alignment we will study the problem of aligning an english sentence to a french sentence and we will use the word alignment of the ibm statistical translation models brown et al 1993 ,0,1,0
54 ibm3 word alignment models since the true distribution over alignments is not known we used the ibm3 statistical translation model brown et al 1993 to approximate this model is specified through four components fertility probabilities for words fertility probabilities for null word translation probabilities and distortion probabilities ,0,1,0
the translation component is an analog of the ibm model 2 brown et al 1993 with parameters that are optimized for use with the trigram ,0,1,0
using alignment for grammar and lexicon induction has been an active area of research both in monolingual settings van zaanen 2000 and in machine translation mt brown et al 1993 melamed 2000 och and ney 2000 interestingly statistical mt techniques have been used to derive lexicosemantic mappings in the reverse direction of language understanding rather than generation papineni et al 1997 macherey et al 2001 ,0,1,0
when an s alignment exists there will always also exist a p alignment such that p a65 s the english sentences were parsed using a stateoftheart statistical parser charniak 2000 trained on the university of pennsylvania treebank marcus et al 1993 ,0,1,0
2 our statistical engine 21 the statistical models in this study we built an smt engine designed to translate from french to english following the noisychannel paradigm flrst described by brown et al 1993b ,0,1,0
four teams had approaches that relied to varying degrees on an ibm model of statistical machine translation brown et al 1993 ,0,1,0
proalign models paef directly using a different decomposition of terms than the model used by ibm brown et al 1993 ,0,1,0
to avoid this problem we sample from a space of probable alignments as is done in ibm models 3 and above brown et al 1993 and weight counts based on the likelihood of each alignment sampled under the current probability model ,0,1,0
however instead of estimating the probabilities for the production rules via em as described in wu 1997 we assign the probabilities to the rules using the model1 statistical translation lexicon brown et al 1993 ,0,1,0
yet the very nature of these alignments as defined in the ibm modeling approach brown et al 1993 lead to descriptions of the correspondences between sourcelanguage sl and targetlanguage tl words of a translation that are often unsatisfactory at least from a human perspective ,0,1,0
brown et al 1993 introduced five statistical translation models ibm models 1 5 ,0,1,0
the duluth word alignment system is a perl implementation of ibm model 2 brown et al 1993 ,0,1,0
this cost can often be substantial as with the penn treebank marcus et al 1993 ,0,1,0
these methods are based on ibm statistical translation model 2 brown et al 1993 but take advantage of certain characteristics of the segments of text that can typically be extracted from translation memories ,0,1,0
given training data consisting of parallel sentences 1 sief ii our model1 training for tfe is as follows s s ss e efefceft 1 1 where 1 e is a normalization factor such that 01 j j eft ss efefc denotes the expected number of times that word e connects to word f l i i m j jl k k ss eeff eft eft efefc 11 1 with the conditional probability tfe the probability for an alignment of foreign string f given english string e is in 1 m j n i ijm eft l efp 1 0 1 1 1 the probability of alignment f given e efp is shown to achieve the global maximum under this em framework as stated in brown et al 1993 ,0,1,0
in our approach equation 1 is further normalized so that the probability for different lengths of f is comparable at the word level m m j n i ijm eft l efp 1 10 1 1 2 the alignment models described in brown et al 1993 are all based on the notion that an alignment aligns each source word to exactly one target word ,0,1,0
the first model referred to as maxent1 below is a loglinear combination of a trigram language model with a maximum entropy translation component that is an analog of the ibm translation model 2 brown et al 1993 ,0,1,0
brown et al 1990 brown et al 1993 are best known and studied ,1,0,0
probabilistic translation models generally seek to find the translation string e that maximizes the probability pra5 ea6fa7 given the source string f where f referred to french and e to english in the original work brown et al 1993 ,0,1,0
fortunately there is a straightforward parallel between our object recognition formulation and the statistical machine translation problem of building a lexicon from an aligned bitext brown et al 1993 alonaizan et al 1999 ,0,1,0
1 introduction various papers use phrasebased translation systems och et al 1999 marcu and wong 2002 yamada and knight 2002 that have shown to improve translation quality over singleword based translation systems introduced in brown et al 1993 ,0,1,0
2 prior work statistical machine translation as pioneered by ibm eg brown et al 1993 is grounded in the noisy channel model ,1,0,0
pos tagging and phrase chunking in english were done using the trained systems provided with the fntbl toolkit ngai and florian 2001 both were trained from the annotated penn treebank corpus marcus et al 1993 ,0,1,0
specifically stochastic translation lexicons estimated using the ibm method brown et al 1993 from a fairly large sentencealigned chineseenglish parallel corpus are used in their approach a considerable demand for a resourcedeficient language ,0,1,0
the ibm sourcechannel model for statistical machine translation p brown et al 1993 plays a central role in our system ,1,0,0
to solve this problem we will adapt the idea of null generated words from machine translation brown et al 1993 ,0,1,0
the relationship between the translation model and the alignment model is given by prfj1 jei1 x aj1 prfj1 aj1jei1 3 in this paper we use the models ibm1 ibm4 from brown et al 1993 and the hiddenmarkovalignmentmodelhmmfromvogelet al 1996 ,0,1,0
further we can learn the channel probabilities in an unsupervised manner using a variant of the em algorithm similar to machine translation brown et al 1993 and statistical language understanding epstein 1996 ,0,1,0
we follow ibm model 1 brown et al 1993 and assume that each word in an utterance is generated by exactly one role in the parallel frame using standard em to learn the role to word mapping is only sufficient if one knows to which level in the tree the utterance should be mapped ,0,1,0
a word based approach depends upon traditional statistical machine translation techniques such as ibm model1 brown et al 1993 and may not always yield satisfactory results due to its inability to handle difficult manytomany phrase translations ,0,0,1
previous work from wang et al 1996 showed improvements in perplexityoriented measures using mixturebased translation lexicon brown et al 1993 ,0,1,0
several teams had approaches that relied to varying degrees on an ibm model of statistical machine translation brown et al 1993 with different improvements brought by different teams consisting of new submodels improvements in the hmm model model combination for optimal alignment etc several teams used symmetrization metrics as introduced in och and ney 2003 union intersection refined most of the times applied on the alignments produced for the two directions sourcetarget and targetsource but also as a way to combine different word alignment systems ,0,0,1
first we considered single sentences as documents and tokens as sentences we define a token as a sequence of characters delimited by 1in our case the score we seek to globally maximize by dynamic programming is not only taking into account the length criteria described in gale and church 1993 but also a cognatebased one similar to simard et al 1992 ,0,1,0
when efficient techniques have been proposed brown et al 1993 och and ney 2003 they have been mostly evaluated on safe pairs of languages where the notion of word is rather clear ,1,0,0
ibm model 4 parameters are then estimated over this partial search space as an approximation to em brown et al 1993 och and ney 2003 ,0,1,0
1 introduction the most widely used alignment model is ibm model 4 brown et al 1993 ,1,0,0
turning off the extensions to giza and training p0 as in brown et al 1993 produces a substantial increase in aer ,1,0,0
we solve this using the local search defined in brown et al 1993 ,0,1,0
the system used for baseline experiments is two runs of ibm model 4 brown et al 1993 in the giza och and ney 2003 implementation which includes smoothing extensions to model 4 ,0,1,0
the idea is that the translation of a sentence x into a sentence y can be performed in the following steps1 a if x is small enough ibms model 1 brown et al 1993 is employed for the translation ,0,1,0
word correspondence was further developed in ibm model1 brown et al 1993 for statistical machine translation ,0,1,0
the first one is a hypotheses testing approach gale and church 1991 melamed 2001 tufi 2002 while the second one is closer to a model estimating approach brown et al 1993 och and ney 2000 ,0,1,0
a quite different approach from our hypotheses testing implemented in the treqal aligner is taken by the modelestimating aligners most of them relying on the ibm models 1 to 5 described in the brown et al 1993 seminal paper ,1,0,0
finally the fourth and fifth feature functions corresponded to two lexicon models based on ibm model 1 lexical parameters pts brown et al 1993 ,0,1,0
3 length model dynamic programming given the word fertility de nitions in ibm models brown et al 1993 we can compute a probability to predict phrase length given the candidate target phrase english ei1 and a source phrase french of length j the model gives the estimation of pjei1 via a dynamic programming algorithm using the source word fertilities ,0,1,0
far from full syntactic complexity we suggest to go back to the simpler alignment methods first described by brown et al 1993 ,0,1,0
different models have been presented in the literature see for instance brown et al 1993 och and ney 2004 vidal et al 1993 vogel et al 1996 ,0,1,0
this concept of alignment has been also used for tasks like authomatic vocabulary derivation and corpus alignment dagan et al 1993 ,0,1,0
the elements of this set are pairs x y where y is a possible translation for x 4 ibms model 1 ibms model 1 is the simplest of a hierarchy of five statistical models introduced in brown et al 1993 ,0,1,0
stateofart systems for doing word alignment use generative models like giza och and ney 2003 brown et al 1993 ,1,0,0
to derive the joint counts cst from which pst and pts are estimated we use the phrase induction algorithm described in koehn et al 2003 with symmetrized word alignments generated using ibm model 2 brown et al 1993 ,0,1,0
this feature which is based on the lexical parameters of the ibm model 1 brown et al 1993 provides a complementary probability for each tuple in the translation table ,0,1,0
giza consists of a set of statistical translation models of different complexity namely the ibm ones brown et al 1993 ,0,1,0
1 introduction the availability of large amounts of socalled parallel texts has motivated the application of statistical techniques to the problem of machine translation starting with the seminal work at ibm in the early 90s brown et al 1992 brown et al 1993 ,1,0,0
initial estimates of lexical translation probabilities came from the ibm model 4 translation tables produced by giza brown et al 1993 och and ney 2003 ,0,1,0
12 from synchronous to quasisynchronous grammars because our approach will let anything align to anything it is reminiscent of ibm models 15 brown et al 1993 ,0,1,0
specifically in the task of word alignment heuristic approaches such as the dice coefficient consistently underperform their reestimated counterparts such as the ibm word alignment models brown et al 1993 ,1,0,0
statistical models for machine translation heavily depend on the concept of alignment specifically the well known ibm word based models brown et al 1993 ,1,0,0
alignment models to structure the translation model are introduced in brown et al 1993 ,0,1,0
nevertheless in the problem described in this article the source and the target sentences are given and we are focusing on the optimization of the aligment a the translation probability prfae can be rewritten as follows prfae jproductdisplay j1 prfjajfj11aj11ei1 jproductdisplay j1 prajfj11aj11ei1 prfjfj11aj1ei1 2 the probability prfae can be estimated by using the wordbased ibm statistical alignment models brown et al 1993 ,0,1,0
the most widely used singlewordbased statistical alignment models sams have been proposed in brown et al 1993 ney et al 2000 ,1,0,0
the original ibm models brown et al 1993 learn wordtoword alignment probabilities which makes it computationally feasible to estimate model parameters from large amounts of training data ,0,1,0
based on ibm model 1 lexical parametersbrown et al 1993 providing a complementary probability for each tuple in the translation table ,0,1,0
4 it constitutes a bijection between source and target sentence positions since the intersecting alignments are functions according to their definition in brown et al 1993 3 ,0,1,0
appendix a derivation of the probability of rwe we take a noisy channel approach which is a common technique in nlp for example brown et al 1993 including spellchecking kernighan et al 1990 ,0,1,0
this alignment system is powered by the ibm translation models brown et al 1993 in which one sentence generates the other ,0,1,0
1 introduction recent works in statistical machine translation smt shows how phrasebased modeling och and ney 2000a koehn et al 2003 significantly outperform the historical wordbased modeling brown et al 1993 ,0,0,1
systems based on wordtoword lexicons such as the ibm systems brown et al 1990 brown et al 1993 incorporate further devices that allow reordering of words a distortion model and ranking of alternatives a monolingual language model ,0,1,0
to generate phrase pairs from a parallel corpus we use the diagand phrase induction algorithm described in koehn et al 2003 with symmetrized word alignments generated using ibm model 2 brown et al 1993 ,0,1,0
in the original work brown et al 1993 the posterior probability pei1fj1 is decomposed following a noisychannel approach but current stateoftheart systems model the translation probability directly using a loglinear modeloch and ney 2002 pei1fj1 exp parenleftbigsummationtextm m1 mhme i1fj1 parenrightbig summationdisplay ei1 exp parenleftbigsummationtextm m1 mhmei1fj1 parenrightbig 2 with hm different models m scaling factors and the denominator a normalization factor that can be ignored in the maximization process ,0,1,0
a monotonic segmentation copes with monotonic alignments that is j k aj ak following the notation of brown et al 1993 ,0,1,0
2006 tried a different generative phrase translation model analogous to ibm wordtranslation model 3 brown et al 1993 and again found that the standard model outperformed their generative model ,0,0,1
the lexical scores are computed as the unnormalized log probability of the viterbi alignment for a phrase pair under ibm wordtranslation model 1 brown et al 1993 ,0,1,0
to derive the joint counts cst from which pst and pts are estimated we use the phrase induction algorithm described in koehn et al 2003 with symmetrized word alignments generated using ibm model 2 brown et al 1993 ,0,1,0
this feature which is based on the lexical parameters of the ibm model 1 brown et al 1993 provides a complementary probability for each tuple in the translation table ,0,1,0
these lists are rescored with the different models described above a character penalty and three different features based on ibm models 1 and 2 brown et al 1993 calculated in both translation directions ,0,1,0
training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text brown et al 1993 for identifying corresponding word blocks assuming no further linguistic analysis of the source or target language ,0,1,0
giza och and ney 2003 an implementation of the ibm brown et al 1993 and hmm ,0,1,0
the simple idea that words in a source chunk are typically aligned to words in a single possible target chunk is used to discard alignments which link words from 2we use ibm1 to ibm5 models brown et al 1993 implemented with giza och and ney 2003 ,0,1,0
in the wellknown socalled ibm word alignment models brown et al 1993 reestimating the model parameters depends on the empirical probability pekfk for each sentence pair ekfk ,1,0,0
the empirical probability for each sentence pair is estimated by maximum likelihood estimation over the training data brown et al 1993 ,0,1,0
assuming that the parameters petkfsk are known the most likely alignment is computed by a simple dynamicprogramming algorithm1 instead of using an expectationmaximization algorithm to estimate these parameters as commonly done when performing word alignment brown et al 1993 och and ney 2003 we directly compute these parameters by relying on the information contained within the chunks ,0,1,0
the ibm models together with a hidden markov model hmm form a class of generative models that are based on a lexical translation model pfjei where each word fj in the foreign sentence fm1 is generated by precisely one word ei in the sentence el1 independently of the other translation decisions brown et al 1993 vogel et al 1996 och and ney 2000 ,0,1,0
203 estimating the parameters for these models is more difficult and more computationally expensive than with the models considered in the previous section rather than simply being able to count the word pairs and alignment relationships and estimate the models directly we must use an existing model to compute the expected counts for all possible alignments and then use these counts to update the new model7 this training strategy is referred to as expectationmaximization em and is guaranteed to always improve the quality of the prior model at each iteration brown et al 1993 dempster et al 1977 ,0,1,0
the notation will assume chineseenglish word alignment and chineseenglish mt here we adopt a notation similar to brown et al 1993 ,0,1,0
the word alignment models implemented in giza the socalled ibm brown et al 1993 and hmm alignment models vogel et al 1996 are typical implementation of the em algorithm dempster et al 1977 ,0,1,0
22 implementation of giza giza is an implementation of ml estimators for several statistical alignment models including ibm model 1 through 5 brown et al 1993 hmm vogel et al 1996 and model 6 och and ney 2003 ,0,1,0
for example brown et al 1993 suggested two different methods using only the alignment with the maximum probability the socalled viterbi alignment or generating a set of alignments by starting from the viterbi alignment and making changes which keep the alignment probability high ,0,1,0
we use the ibm model 1 brown et al 1993 and the hidden markov model hmm vogel et al 1996 to estimate the alignment model ,0,1,0
we then built separate englishtospanish and spanishtoenglish directed word alignments using ibm model 4 brown et al 1993 combined them using the intersectgrow heuristic och and ney 2003 and extracted phraselevel translation pairs of maximum length 7 using the alignment template approach och and ney 2004 ,0,1,0
several automatic sentence alignment approaches have been proposed based on sentence length brown et al 1991 and lexical information kay and roscheisen 1993 ,0,1,0
to address this drawback we proposed a new method3 to compute a more reliable and smoothed score in the undefined case based on the ibm model 1 brown et al 1993 ,0,1,0
the lexical acquisition phase uses the giza wordalignment tool an implementation och and ney 2003 of ibm model 5 brown et al 1993 to construct an alignment of mrs with nl strings ,0,1,0
probabilistic generative models like ibm 15 brown et al 1993 hmm vogel et al 1996 itg wu 1997 and leaf fraser and marcu 2007 define formulas for pf e or pe f with okvoon ororok sprok atvoon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat okdrubel okvoon anok plok sprok atdrubel atvoon pippat rrat dat okvoon anok drok brok jok atvoon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok okyurp totat nnat quat oloat atyurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat figure 1 word alignment exercise knight 1997 ,0,1,0
practical model 4 systems therefore make substantial search approximations brown et al 1993 ,0,1,0
for now we consider it to be one where every foreign word is aligned exactly once brown et al 1993 ,0,1,0
we use these tuples to calculate a balanced fscore against the gold alignment tuples4 method dict size fscore gold 28 1000 monotone 39 689 ibm1 brown et al 1993 30 803 ibm4 brown et al 1993 29 869 ip 28 959 the last line shows an average fscore over the 8 tied ip solutions ,0,1,0
4 conclusions compared with other word alignment algorithms brown et al 1993 gale and church 1991a wordalign does not require sentence alignment as input and was shown to produce useful alignments for small and noisy corpora ,0,0,1
the program takes the output of charalign church 1993 a robust alternative to sentencebased alignment programs and applies wordlevel constraints using a version of brown el als model 2 brown et al 1993 modified and extended to deal with robustness issues ,0,0,1
the method was intended as a replacement for sentencebased methods eg brown et al 1991a gale and church 1991b kay and rosenschein 1993 which are very sensitive to noise ,0,0,1
2 the alignment algorithm 21 estimation of translation probabilities the translation probabilities are estimated using a method based on brown et als model 2 1993 which is summarized in the following subsection 211 ,0,1,0
1 introduction aligning parallel texts has recently received considerable attention warwick et al 1990 brown et al 1991a gale and church 1991b gale and church 1991a kay and rosenschein 1993 simard et al 1992 church 1993 kupiec 1993 matsumoto et al 1993 ,0,1,0
these methods have been used in machine translation brown et al 1990 sadler 1989 terminology research and translation aids isabelle 1992 ogden and gonzales 1993 bilingual lexicography klavans and tzoukermann 1990 collocation studies smadja 1992 wordsense disambiguation brown et al 1991b gale et al 1992 and information retrieval in a multilingual environment landauer and littman 1990 ,0,1,0
numerous experiments have shown parallel bilingual corpora to provide a rich source of constraints for statistical analysis eg brown et al 1990 gale church 1991 gale et al 1992 church 1993 brown et al 1993 dagan et al 1993 fung church 1994 wu xia 1994 fung mckeown 1994 ,0,1,0
1 introduction a number of empirical studies have found bracketing to be a useful type of corpus annotation eg pereira schabes 1992 black et al 1993 ,0,1,0
it is interesting to constrast this method with the parseparsematch approaches that have been reported recently for producing parallel bracketed corpora sadler vendelmans 1990 kaji et al 1992 matsumoto et al 1993 cranias et al 1994 gfishman 1994 ,0,1,0
1 introduction despite a surge in research using parallel corpora for various machine translation tasks brown et al 1993brown et al 1991 gale church 1993 church 1993 dagan church 1994 simard et al 1992 chen 1993 melamed 1995 wu xia 1994 wu 1994 smadja et ai ,0,0,1
brown et al 1993 the heuristics in section 6 are designed specifically to find the interesting features in that featureless desert ,0,1,0
several authors have used mutual information and similar statistics as an objective function for word clustering dagan et al 1993 brown et al 1992 pereira et al 1993 wang et al 1996 for automatic determination of phonemic baseforms lucassen mercer 1984 and for language modeling for speech recognition ries ct al 1996 ,0,1,0
2 translation models a translation model can be constructed automatically from texts that exist in two languages bitexts brown et al 1993 melamed 1997 ,0,1,0
pure statistical machine translation brown et al 1993 mltst in principle recover the most probable alignment out of all possible alignments between the input and a translation ,0,1,0
bilingual alignments have so far shown that they can play multiple roles in a wide range of linguistic applications such as computer assisted translation isabelle et al 1993 brown et al 1990 terminology dagan and church 1994 lexicography langlois 1996 klavans and tzoukermann 1995 melamed 1996 and crosslanguage information retrieval nie et al this research was funded by the canadian department of foreign affairs and international trade httpdfaitmaecigcca via the agence de la francophonie http ,0,1,0
however in the experiments described here we focus on alignment at the level of sentences this for a number of reasons first sentence alignments have so far proven their usefulness in a number of applications eg bilingual lexicography langlois 1996 klavans and tzoukermann 1995 dagan and church 1994 automatic translation verification macklovitch 1995 macklovitch 1996 and the automatic acquisition of knowledge about translation brown et al 1993 ,0,1,0
many statistical translation models vogel et al 1996 tillmann et al 1997 niessen et al 1998 brown et al 1993 try to model wordtoword correspondences between source and target words ,0,1,0
this alignment representation is a generalization of the baseline alignments described in brown et al 1993 and allows for manytomany alignments ,0,1,0
the approach is able to achieve 94 precision and recall for base nps derived from the penn treebank wall street journal marcus et al 1993 ,0,1,0
recent comparisons of approaches that can be trained on corpora van halteren et al 1998 volk and schneider 1998 have shown that in most cases statistical aproaches cutting et al 1992 schmid 1995 ratnaparkhi 1996 yield better results than finitestate rulebased or memorybased taggers brill 1993 daelemans et al 1996 ,0,1,0
the annotation consists of four parts 1 a contextfree structure augmented with traces to mark movement and discontinuous constituents 2 phrasal categories that are annotated as node labels 3 a small set of grammatical functions that are annotated as extensions to the node labels and 4 partofspeech tags marcus et al 1993 ,0,1,0
as two examples rabiner 1989 and charniak et al 1993 give good overviews of the techniques and equations used for markov models and partofspeech tagging but they are not very explicit in the details that are needed for their application ,0,1,0
additionally we present results of the tagger on the negra corpus brants et al 1999 and the penn treebank marcus et al 1993 ,0,1,0
23 experiment the training set for these experiments was sections 0121 of the penn treebank marcus et al 1993 ,0,1,0
the main data set consist of four sections 1518 of the wall street journal wsj part of the penn treebank marcus et al 1993 as training material and one section 20 as test material 1 ,0,1,0
we evaluate this method over the part of speech tagged portion of the penn treebank corpus marcus et al 1993 ,0,1,0
so they conform to the penn treebank corpus marcus et al 1993 annotation style and then do experiments using models built with treebank data ,0,1,0
however because these estimates are too sparse to be relied upon we use interpolated estimates consisting of mixtures of successively lowerorder estimates as in placeway et al 1993 ,0,1,0
we were already using a generative statistical model for partofspeech tagging weischedel et al 1993 and more recently had begun using a generative statistical model for name finding bikel et al 1997 ,0,1,0
word features are introduced primarily to help with unknown words as in weischedel et al 1993 ,0,1,0
the pt grammar 2 was extracted from the penn treebank marcus et al 1993 ,0,1,0
222 english training data for training in the english experiments we used wsj marcus et al 1993 ,0,1,0
the part of the 1release 2 of this data set can be obtained trmn the linguistic data consortium with catalogue number ldc94t4b httpwwwldcupenneduldcnofranmhtml 2there are 48 labels defined in marcus et al 1993 however three of ttmm do not appear in the corpus ,0,1,0
we compared this nonprobabilistic dop model against tile probabilistic dop model which estimales the most probable parse for each sentence on three different domains tbe penn atis treebank marcus et al 1993 the dutch ovis treebank bonnema el al 1997 and tile penn wall street journal wsj treebank marcus el al 1993 ,0,1,0
while this technique has been sttccessfully applied to parsing lhe atis portion in the penn treebank marcus et al 1993 it is extremely time consuming ,0,1,0
experimental comparison 41 experiments on the atis corpus for our first comparison we used i0 splits from the penn atis corpus marcus et al 1993 into training sets of 675 sentences and test sets of 75 sentences ,0,1,0
levelopment of cor1ora with morlhosyntati and syntacti mmotation marcus et al 1993 sampson 1995 ,0,1,0
2he wsj corpus marcus et al 1993 ,0,1,0
51 the prague dependency tree bank pdt in the sequel which has been inspired by the buildup of the penn treebank marcus santorini marcinkiewicz 1993 marcus kim marcinkiewicz et al 1994 is aimed at a complex annotation of a part of the czech national corpus cnc in the sequel the creation of which is under progress at the department of czech national corpus at the faculty of philosophy charles university the corpus currently comprises about 100 million tokens of word forms ,0,1,0
carpenter 1992 copestake 1999 dsrre and dorna 1993 d6ire et al 1996 emele and zajac 1990 h6htld and smolka 1988 and to pick those ingredients which are known to be coniutationally tractable in some sense ,0,1,0
1 introduction syntactically annotated corpora like the penn treebank marcus et al 1993 the negra corpus skut et al 1998 or the statistically dismnbiguated parses in bell et al 1999 provide a wealth of intbrmation which can only be exploited with an adequate query language ,1,0,0
without removing them extracted rules cannot be triggered until when completely the same strings appear in a text4 6 performance evaluation we measured the performance of our robust parsing algorithm by measuring coverage and degree of overgeneration for the wall street journal in the penn treebank marcus et al 1993 ,0,1,0
2 background default unification has been investigated by many researchers bouma 1990 russell et al 1991 copestake 1993 carpenter 1993 lascarides and copestake 1999 in the context of developing lexical semantics ,0,1,0
as other researchers pursued efficient default unification bouma 1990 russell et al 1991 copestake 1993 we also propose another definition of default unification which we call lenient default unification ,0,1,0
for penn treebank ii style annotation marcus et al 1993 in which a nonterminal symbol is a category together with zero or more functional tags we adopt the following scheme the atomic pattern a matches any label with category a or functional tag a moreover we define boolean operators and ,0,1,0
the data set consisting of 249994 tfss was generated by parsing the figure 3 the size of dpi for the size of the data set 800 bracketed sentences in the wall street journal corpus the first 800 sentences in wall street journal 00 in the penn treebank marcus et al 1993 with the xhpsg grammar tateisi et al 1998 ,0,1,0
in recent years hmms have enjoyed great success in many tagging applications most notably partofspeech pos tagging church 1988 weischedel et al 1993 merialdo 1994 and named entity recognition bikel et al 1999 zhou et al 2002 ,0,1,0
experimentation the corpus used in shallow parsing is extracted from the penn treebank marcus et al 1993 of 1 million words 25 sections by a program provided by sabine buchholz from tilburg university ,0,1,0
the learning algorithm used is the ib1 algorithm aha et al 1991 with k 5 ie classification based on 5 nearest neighbors4 distances are measured using the modified value difference metric mvdm stanfill and waltz 1986 cost and salzberg 1993 for instances with a frequency of at least 3 and the simple overlap metric otherwise and classification is based on distance weighted class voting with inverse distance weighting dudani 1976 ,0,1,0
dependency analyzer ppattachment resolver rootnode finder base np chunker pos tagger svm preference learning figure 2 module layers in the system that is we use penn treebanks wall street journal data marcus et al 1993 ,0,1,0
the trips structure generally has more levels of structure roughly corresponding to levels in xbar theory than the penn treebank analyses marcus et al 1993 in particular for base noun phrases ,0,1,0
the tagger described in this paper is based on the standard hidden markov model architecture charniak et al 1993 brants 2000 ,0,1,0
31 experiments the model described in section 2 has been tested on the brown corpus francis and kucera 1982 tagged with the 45 tags of the penn treebank tagset marcus et al 1993 which constitute the initial tagset t0 ,0,1,0
the brill tagger comes with an english default version also trained on generalpurpose language corpora like the penn treebank marcus et al 1993 ,0,1,0
this is most prominently evidenced by the penn treebank marcus et al 1993 ,0,1,0
the training set is extracted from treebank marcus et al 1993 section 1518 the development set used in tuning parameters of the system from section 20 and the test set from section 21 ,0,1,0
2 data sets for the experiments 21 coordination annotation in the penn treebank for our experiments we used the wsj part of the penn treebank marcus et al 1993 ,0,1,0
for instance about 38 of verbs in the training sections of the penn treebank ptb marcus et al 1993 occur only once the lexical properties of these verbs such as their most common subcategorization frames cannot be represented accurately in a model trained exclusively on the penn treebank ,0,0,1
for example in the wsj corpus part of the penn treebank 3 release marcus et al 1993 the string in 1 is a variation 12gram since off is a variation nucleus that is tagged preposition in in one corpus occurrence and particle rp in another1 dickinson 2005 shows that examining those cases with identical local contextin this case lookingat ward off aresultsinanestimated error detection precision of 925 ,0,1,0
1999 openccg white 2004 and xle crouch et al 2007 or created semiautomatically belz 2007 or fully automatically extracted from annotated corpora like the hpsg nakanishi et al 2005 lfg cahill and van genabith 2006 hogan et al 2007 and ccg white et al 2007 resources derived from the pennii treebank ptb marcus et al 1993 ,0,1,0
statistical dependency parsers of english must therefore rely on dependency structures automatically converted from a constituent corpus such as the penn treebank marcus et al 1993 ,0,1,0
as mentioned in section 22 there are words which have two or more candidate pos tags in the ptb corpus marcus et al 1993 ,0,1,0
marcus et al 1993 marcus m santorini b and malvinkiewicz ma ,0,1,0
to estimale a model clustering words and measured the il distancd between lhe k distance relative cltlopy which is widely used in information theory and sta tist ics is a nleasur2 of dista nc lwcen two distributions 52 experiment 2 qualitative evaluation we extracted roughly 180000 case flanles from the bracketed wsj wall street journal corpus of the penn tree bank marcus et al 1993 as cooccurrence data ,0,1,0
in particular we used this method with wordnet miller et al 1993 and using the same training data ,0,1,0
have been proposed hindle 1990 brown et al 1992 pereira et al 1993 tokunaga et al 1995 ,0,1,0
treebanks have been used within the field of natural language processing as a source of training data for statistical part og speech taggers black et al 1992 brill 1994 merialdo 1994 weischedel et al 1993 and for statistical parsers black et al 1993 brill 1993 aelinek et al 1994 magerman 1995 magerman and marcus 1991 ,0,1,0
all of the features of the atrlancaster treebank that are described below represent a radical departure from extant largescale eyes and leech 1993 garside and mcenery 1993 marcus et al 1993 treebanks ,0,1,0
4 experiments the penn treebank marcus et al 1993 is used as the testing corpus ,0,1,0
marcus et al 1993 316 ,0,1,0
the tagger used is thus one that does not need tagged and disambiguated material to be trained on namely the xpost originally constructed at xerox parc cutting et al 1992 cutting and pedersen 1993 ,0,1,0
4 information base 41 text corpus text corpora are essential to statistical modeling in developing formal theories of the grammars investigating prosodic phenomena in speech and evaluating or comparing the adequacy of parsing models marcus et al 1993 ,0,1,0
successflfl examples of reuse of data resources include the wordnet thesaurus miller el al 1993 the penn tree bank marcus et al 1993 the longmans dictionary of contemporary english summers 1995 ,1,0,0
this sort of problem can be solved in principle by conditional variants of the expectationmaximization algorithm baum et al 1970 dempster et al 1977 meng and rubin 1993 jebara and pentland 1999 ,0,1,0
similarly murdock and croft 2005 adopted a simple translation model from ibm model 1 brown et al 1990 brown et al 1993 and applied it to qa ,0,1,0
the tree is produced by a stateoftheart dependency parser mcdonald et al 2005 trained on the wall street journal penn treebank marcus et al 1993 ,0,1,0
we use as our english corpus the wall street journal wsj portion of the penn treebank marcus et al 1993 ,0,1,0
when tested on fstructures for all sentences from section 23 of the penn wall street journal wsj treebank mar267 cus et al 1993 the techniques described in this paper improve bleu score from 6652 to 6882 ,0,1,0
2 evaluation all of the experiments described below have the same basic structure an estimator is used to infer a bitag hmm from the unsupervised training corpus the words of penn treebank ptb wall street journal corpus marcus et al 1993 and then the resulting model is used to label each word of that corpus with one of the hmms hidden states ,0,1,0
for a second set of parsing experiments we used the wsj portion of the penn tree bank marcus et al 1993 and helmut schmids enrichment program tmod schmid 2006 ,0,1,0
1 introduction syntaxbased translation models eisner 2003 galley et al 2006 marcu et al 2006 are usually built directly from penn treebank ptb marcus et al 1993 style parse trees by composing treebank grammar rules ,0,1,0
6 evaluation 61 data the data used for our comparison experiments were developed as part of the ontonotes project hovy et al 2006 which uses the wsj part of the penn treebank marcus et al 1993 ,0,1,0
918 english for english we used the wall street journal section of the penn treebank marcus et al 1993 ,0,1,0
32 domain adaptation track as mentioned previously the source data is drawn from a corpus of news specifically the wall street journal section of the penn treebank marcus et al 1993 ,0,1,0
1 introduction in the multilingual track of the conll 2007 shared task on dependency parsing a single parser must be trained to handle data from ten different languages arabic hajic et al 2004 basque aduriz et al 2003 catalan mart et al 2007 chinese chen et al 2003 czech bohmova et al 2003 english marcus et al 1993 johansson and nugues 2007 greek prokopidis et al 2005 hungarian csendes et al 2005 italian montemagni et al 2003 and turkish oflazer et al 20031 our contribution is a study in multilingual parser optimization using the freely available maltparser system which performs 1for more information about the task and the data sets see nivre et al ,0,1,0
4 experiments we evaluated the isbn parser on all the languages considered in the shared task hajic et al 2004 aduriz et al 2003 mart et al 2007 chen et al 2003 bohmova et al 2003 marcus et al 1993 johansson and nugues 2007 prokopidis et al 2005 csendes et al 2005 montemagni et al 2003 oflazer et al 2003 ,0,1,0
we participated in the multilingual track of the conll 2007 shared task nivre et al 2007 and evaluated the system on data sets of 10 languages hajic et al 2004 aduriz et al 2003 mart et al 2007 chen et al 2003 bohmova et al 2003 marcus et al 1993 johansson and nugues 2007 prokopidis et al 2005 csendes et al 2005 montemagni et al 2003 oflazer et al 2003 ,0,1,0
in the multilingual parsing track participants train dependency parsers using treebanks provided for ten languages arabic hajic et al 2004 basque aduriz et al 2003 catalan mart et al 2007 chinese chen et al 2003 czech bhmova et al 2003 english marcus et al 1993 johansson and nugues 2007 greek prokopidis et al 2005 hungarian czendes et al 2005 italian montemagni et al 2003 and turkish oflazer et al 2003 ,0,1,0
in the domain adaptation track participants were provided with english training data from the wall street journal portion of the penn treebank marcus et al 1993 converted to dependencies johansson and nugues 2007 to train parsers to be evaluated on material in the biological development set and chemical test set domains kulick et al 2004 and optionally on text from the childes database macwhinney 2000 brown 1973 ,0,1,0
the annotation guidelines for the penn treebank flattened noun phrases to simplify annotation marcus et al 1993 so there is no complex structure to nps ,0,1,0
the following treebanks were used for training the parser aduriz et al 2003 bhmov et al 2003 chen et al 2003 haji et al 2004 marcus et al 1993 mart et al 2002 montemagni et al 2003 oflazer et al 2003 prokopidis et al 2005 csendes et al 2005 ,0,1,0
1993 johansson and nugues 2007 prokopidis et al ,0,1,0
we took part the multilingual track of all ten languages provided by the conll2007 shared task organizershajic et al 2004 aduriz et al 2003 mart et al 2007 chen et al 2003 bohmova et al 2003 marcus et al 1993 johansson and nugues 2007 prokopidis et al 2005 csendes et al 2005 montemagni et al 2003 oflazer et al 2003 ,0,1,0
5 results and discussion the system with online learning and nivres parsing algorithm was trained on the data released by conll shared task organizers for all the ten languages hajic et al 2004 aduriz et al 2003 mart et al 2007 chen et al 2003 bohmova et al 2003 marcus et al 1993 johansson and nugues 2007 prokopidis et al 2005 csendes et al 2005 montemagni et al 2003 oflazer et al 2003 ,0,1,0
building heavily on the ideas of historybased parsing black et al 1993 nivre 2006 training the parser means essentially running the parsing algorithms in a learning mode on the data in order to gather training instances for the memorybased learner ,0,1,0
the pchemtbclosed shared task marcus et al 1993 johansson and nugues 2007 kulick et al 2004 is used to illustrate our models ,0,1,0
3 experimental results and discussion we test our parsing models on the conll2007 hajic et al 2004 aduriz et al 2003 mart et al 2007 chen et al 2003 bohmova et al 2003 marcus et al 1993 johansson and nugues 2007 prokopidis et al 2005 csendes et al 2005 montemagni et al 2003 oflazer et al 2003 data set on various languages including arabic basque catalan chinese english italian hungarian and turkish ,0,1,0
3 experiments and results all experiments were conducted on the treebanks provided in the shared task hajic et al 2004 aduriz et al 2003 mart et al 2007 chen et al 2003 bhmov et al 2003 marcus et al 1993 johansson and nugues 2007 prokopidis et al 2005 csendes et al 2005 montemagni et al 2003 oflazer et al 2003 ,0,1,0
we have achieved average results in the conll domain adaptation track open submission marcus et al 1993 johansson and nugues 2007 kulick et al 2004 macwhinney 2000 brown 1973 ,0,1,0
we use a handwritten competence grammar combined with performancedriven disambiguation obtained from the penn treebank marcus et al 1993 ,0,1,0
4 experiments our experiments were conducted on conll2007 shared task domain adaptation track nivre et al 2007 using treebanks marcus et al 1993 johansson and nugues 2007 kulick et al 2004 ,0,1,0
in this year conll2007 shared task nivre et al 2007 focuses on multilingual dependency parsing based on ten different languages hajic et al 2004 aduriz et al 2003 mart et al 2007 chen et al 2003 bhmova et al 2003 marcus et al 1993 johansson and nugues 2007 prokopidis et al 2005 czendes et al 2005 montemagni et al 2003 oflazer et al 2003 and domain adaptation for english marcus et al 1993 johansson and nugues 2007 kulick et al 2004 macwhinney 2000 brown 1973 without taking the languagespecific knowledge into consideration ,0,1,0
most systems for automatic rolesemantic analysis have used constituent syntax as in the penn treebank marcus et al 1993 although there has also been much research on the use of shallow syntax carreras and mrquez 2004 in srl ,0,1,0
1 introduction most stateoftheart widecoverage parsers are based on the penn treebank marcus et al 1993 making such parsers highly tuned to newspaper text ,0,1,0
these categories were automatically generated using the labeled parses in penn treebank marcus et al 1993 and the labeled semantic roles of propbank kingsbury et al 2002 ,0,1,0
the model was trained on sections 221 from the english penn treebank marcus et al 1993 ,0,1,0
41 data sets our results are based on syntactic data drawn from the penn treebank marcus et al 1993 specifically the portion used by conll 2000 shared task tjong kim sang and buchholz 2000 ,0,1,0
we use 3500 sentences from conll tjong kim sang and de meulder 2003 as the ner data and section 2023 of the wsj marcus et al 1993 ramshaw and marcus 1995 as the poschunk data 8936 sentences ,0,1,0
the other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks such as wsj marcus et al 1993 and assume that the accuracy measure will carry over to the domains of interest ,0,1,0
building on the annotations from the wall street journal wsj portion of the penn treebank marcus et al 1993 the project added several new layers of semantic annotations such as coreference information word senses etc in its first release ldc2007t21 through the linguistic data consortium ldc the project manually sensetagged more than 40000 examples belonging to hundreds of noun and verb types with an ita of 90 based on a coarsegrained sense inventory where each word has an average of only 32 senses ,0,1,0
we removed all but the first two characters of each pos tag resulting in a set of 57 tags which more closely resembles that of the penn treebank marcus et al 1993 ,0,1,0
the propbank corpus adds a semantic layer to parse trees from the wall street journal section of the penn treebank ii corpus marcus et al 1993 ,0,1,0
for english we used the penn treebank marcus et al 1993 in our experiments and the tool penn2malt7 to convert the data into dependency structures using a standard set of head rules yamada and matsumoto 2003 ,0,1,0
though this model uses trees in the formal sense it does not create penn treebank marcus et al 1993 style linguistic trees but uses only one nonterminal label x to create those trees using six simple rule structures ,0,1,0
hockenmaierandsteedman2007showedthat a ccg corpus could be created by adapting the penn treebank marcus et al 1993 ,0,1,0
averaged perceptron algorithm 5 experiments we evaluate our method on both chinese and english syntactic parsing task with the standard division on chinese penn treebank version 50 and wsj english treebank 30 marcus et al 1993 as shown in table 1 ,0,1,0
41 experimental setup we used two different corpora propbank wwwcisupenneduace along with penntree bank 2 marcus et al 1993 and framenet ,0,1,0
for example in the wsj corpus part of the penn treebank 3 release marcus et al 1993 the string in 1 is a variation 12gram since off is a variation nucleus that in one corpus occurrence is tagged as a preposition in while in another it is tagged as a particle rp ,0,1,0
287 system train base test base 1 baseline 8789 8789 2 contrastive 8870 082 8845 056 5 trialsfold 3 contrastive 8882 093 8855 066 greedy selection table 1 average f1 of 7way crossvalidation to generate the alignments we used model 4 brown et al 1993 as implemented in giza och and ney 2003 ,0,1,0
our test set is 3718 sentences from the english penn treebank marcus et al 1993 which were translated into german ,0,1,0
corpora in various languages such as the english penn treebank corpus marcus et al 1993 the swedish stockholmume corpus ejerhed et al 1992 and the icelandic frequency dictionary ifd corpus pind et al 1991 have been used to train in the case of datadriven methods and develop in the case of linguistic rulebased methods different taggers and to evaluate their accuracy eg ,0,1,0
purely syntactic categories lead to a smaller number of tags which also improves the accuracy of manual tagging 2 marcus et al 1993 ,0,1,0
our model uses an exemplar memory that consists of 133566 verbrolenoun triples extracted from the wall street journal and brown parts of the penn treebank marcus et al 1993 ,0,1,0
41 the test environment for our experiments we used a manually corrected version of the air travel information system atis spoken language corpus hemphill et al 1990 annotated in the pennsylvania treebank marcus et al 1993 ,0,1,0
note in passing that the ratio 104108997 compares very favourably with other systems cf 30993 by post weischedel et al 1993 and 104976 or 109986 by de marcken 1990 ,0,1,0
a more optimistic view can be found in leech and eyes 1993 p 39 marcus et al 1993 p 328 they argue that a near100 interjudge agreement is possible provided the partofspeech annotation is done carefully by experts ,0,1,0
our experiments created translation modules for two evaluation corpora written news stories from the penn treebank corpus marcus et al 1993 and spoken taskoriented dialogues from the trains93 corpus heeman and allen 1995 ,0,1,0
the tags sets we shall examine are the set used in the penn tree bank ptb marcus et al 1993 and the c5 tagset used by the claws partofspeech tagger garside 1996 ,0,1,0
in fact the largest source of english dependency trees is automatically generated from the penn treebank marcus et al 1993 and is by convention exclusively projective ,0,1,0
table 2 shows the results for english projective dependency trees extracted from the penn treebank marcus et al 1993 using the rules of yamada and matsumoto 2003 ,0,1,0
examples are the penn treebank marcus et al 1993 for american english annotated at the university of pennsylvania the french treebank abeille and clement 1999 developed in paris the tiger corpus brants et al 2002 for german annotated at the universities of saarbrcurrency1ucken and this research was funded by a german science foundation grant dfg sfb4416 ,0,1,0
this is possible because of the availability of statistical parsers which can be trained on humanannotated treebanks marcus et al 1993 xia et al 2000 maamouri and bies 2004 for multiple languages 2 the binding theory is used as a guideline and syntactic structures are encoded as features in a maximum entropy coreference system 3 the syntactic features are evaluated on three languages arabic chinese and english one goal is to see if features motivated by the english language can help coreference resolution in other languages ,0,1,0
li and roth demonstrated that their shallow parser trained to label shallow constituents along the lines of the wellknown conll2000 task sang and buchholz 2000 outperformed the collins parser in correctly identifying these constituents in the penn wall street journal wsj treebank marcus et al 1993 ,0,1,0
in a test set of 756 utterances containing 26 repairs dowding et al 1993 they obtained a detection recall rate of 42 and a precision of 846 for correction they obtained a recall rate of 30 and a precision rate of 62 ,0,1,0
good partofspeech results can be obtained using only the preceding category weischedel et al 1993 which is what we will be using ,0,1,0
ebonsai first performs syntactic analysis of a sentence using a parser based on glr algorithm mslr parser tanaka et al 1993 and provides candidates of its syntactic structure ,0,1,0
the mslr parser tanaka et al 1993 performs syntactic analysis of the sentence ,0,1,0
however most parsers still tend to show low performance on the long sentences li et al 1990 doi et al 1993 kim et al 2000 ,0,1,0
this kind of corpus has served as an extremely valuable resource for computational linguistics applications such as machine translation and question answering lee et al 1997 choi 2001 and has also proved useful in theoretical linguistics research marcus et al 1993 ,1,0,0
ng and low 2004 toutanova et al 2003 brants 2000 ratnaparkhi 1996 samuelsson 1993 ,0,1,0
the implementation of the algorithm is one that has a core of code that can run on either the penn treebank marcus et al 1993 or on the chinese treebank ,0,1,0
for many languages largescale syntactically annotated corpora have been built eg the penn treebank marcus et al 1993 and many parsing algorithms using cfgs have been proposed ,0,1,0
since the texts in the rst treebank are taken from the syntactically annotated penn treebank marcus et al 1993 it is natural to ask what the relation is between the discourse structures in the rst treebank and the syntactic structures of the penn treebank ,0,1,0
evaluations are typically carried out on newspaper texts ie on section 23 of the penn treebank ptb marcus et al 1993 ,0,1,0
other works based on this scheme like bharati et al 1993 bharati et al 2002 pedersen et al 2004 have shown promising results ,0,1,0
it has been shown repeatedlyeg briscoe and carroll 1993 charniak 1997 collins 1997 inui et al ,0,1,0
next we use the conclusions from two psycholinguistic experiments on ranking the cflist the salience of discourse entities in prepended phrases gordon grosz and gilliom 1993 and the ordering of possessor and possessed in complex nps gordon et al 1999 to try to improve the performance of lrc ,0,1,0
5 the semcor collection miller et al 1993 is a subset of the brown corpus and consists of 352 news articles distributed into three sets in which the nouns verbs adverbs and adjectives have been manually tagged with their corresponding wordnet senses and partofspeech tags using brills tagger 1995 ,0,1,0
one of the largest and earliest such efforts is the penn treebank marcus santorini and marcinkiewicz 1993 marcus et al 1994 which contains a onemillion word institute for research in cognitive science university of pennsylvania 3401 walnut street suite 400a philadelphia pa 191046228 usa ,1,0,0
dependency treebank hajic et al 2001 bohmova et al 2003 and in figure 2 for an english sentence taken from the penn treebank marcus santorini and marcinkiewicz 1993 marcus et al 1994 ,0,1,0
lexical collocation functions especially those determined statistically have recently attracted considerable attention in computational linguistics calzolari and bindi 1990 church and hanks 1990 sekine et al 1992 hindle and rooth 1993 mainly though not exclusively for use in disambiguation ,0,1,0
more specifically the work on optimizing preference factors and semantic collocations was done as part of a project on spoken language translation in which the cle was used for analysis and generation of both english and swedish agnss et al 1993 ,0,1,0
rulebased taggers brill 1992 elenius 1990 jacobs and zernik 1988 karlsson 1990 karlsson et al 1991 voutilainen heikkila and antitila 1992 voutilainen and tapanainen 1993 use posdependent constraints defined by experienced linguists ,0,1,0
 1995 association for computational linguistics computational linguistics volume 21 number 2 and mancini 1991 meteer schwartz and weischedel 1991 merialdo 1991 pelillo moro and refice 1992 weischedel et al 1993 wothke et al 1993 ,0,1,0
recently several solutions to the problem of tagging unknown words have been presented charniak et al 1993 meteer schwartz and weischedel 1991 ,0,1,0
hypotheses for unknown words both stochastic dermatas and kokkinakis 1993 1994 maltese and mancini 1991 weischedel et al 1993 and connectionist eineborg and gamback 1993 elenius 1990 have been applied to unlimited vocabulary taggers ,0,1,0
when the training text is adequate to estimate the tagger parameters more efficient stochastic taggers dermatas and kokkinakis 1994 maltese and mancini 1991 weischedel et al 1993 and training methods can be implemented merialdo 1994 ,0,1,0
partofspeech tagging is an active area of research a great deal of work has been done in this area over the past few years eg jelinek 1985 church 1988 derose 1988 hindle 1989 demarcken 1990 merialdo 1994 brill 1992 black et al 1992 cutting et al 1992 kupiec 1992 charniak et al 1993 weischedel et al 1993 schutze and singer 1994 ,0,1,0
almost all recent work in developing automatically trained partofspeech taggers has been on further exploring markovmodel based tagging jelinek 1985 church 1988 derose 1988 demarcken 1990 merialdo 1994 cutting et al 1992 kupiec 1992 charniak et al 1993 weischedel et al 1993 schutze and singer 1994 ,0,1,0
such methods can achieve better performance reaching tagging accuracy of up to 85 on unknown words for english brill 1994 weischedel et al 1993 ,0,1,0
in the past two or three years this kind of verification has been attempted for other aspects of semantic interpretation by passonneau and litman 1993 for segmentation and by kowtko isard and doherty 1992 and carletta et al ,0,1,0
the second approach sekine et al 1992 chang luo and su 1992 resnik 1993a grishman and sterling 1994 alshawi and carter 1994 takes triples verb prep noun2 and nounl prep noun2 like those in table 10 as training data for acquiring semantic knowledge and performs ppattachment disambiguation on quadruples ,0,1,0
22 statistical parsers pioneered by the ibm natural language group fujisaki et al 1989 and later pursued by for example schabes roth and osborne 1993 jelinek et al ,0,1,0
supertags partofspeech disambiguation techniques pos taggers church 1988 weischedel et al 1993 brill 1993 are often used prior to parsing to eliminate or substantially reduce the partofspeech ambiguity ,0,1,0
in a test set containing 26 repairs dowding et al 1993 they obtained a detection recall rate of 42 with a precision of 85 and a correction recall rate of 31 with a precision of 62 ,0,1,0
264285rawstring citation citation validtrue authors authort fukushimaauthor authorm okumuraauthor authors titletext summarization challenge text summarization in japantitle date2001date booktitlein proceedings of naacl 2001 workshop automatic summarizationbooktitle pages5159pages contexts contextconferences muc chinchor et al 1993 tipster summac text summarization evaluation mani et al 1998 document understanding conference duc duc 2004 and text summarization challenge tsc fukushima and okumura 2001 have attested the importance of this topic ,0,1,0
statistics in linguistics oxford basil blackwellrawstring citation citation validtrue authors authorn chinchorauthor authors titleevaluating message understanding systems an analysis of the third message understanding conference muc3title date1993date journalcomputational linguisticsjournal volume19volume pages409449pages markerchinchor 1993marker rawstringchinchor n et al 1993 ,0,1,0
in acknowledgment of this fact a series of conferences like text retrieval conferences trec voorhees and harman 1999 message understanding conferences muc chinchor et al 1993 tipster summac text summarization evaluation mani et al 1998 document understanding conference duc duc 2004 and text summarcontext contexts markervoorhees harman 1999marker rawstringvoorhees e m and harman d k 1999 ,0,1,0
training data our source for syntactically annotated training data was the penn treebank marcus et al 1993 ,0,1,0
6 experiment 61 setup the experiments we report were done on the penn treebank wsj corpus marcus et al 1993 ,0,1,0
3 building the catvar the catvar database was developed using a combination of resources and algorithms including the lexical conceptual structure lcs verb and preposition databases dorr 2001 the brown corpus section of the penn treebank marcus et al 1993 an english morphological analysis lexicon developed for pckimmo englex antworth 1990 nomlex macleod et al 1998 longman dictionary of contemporary english 2for a deeper discussion and classification of porter stemmers errors see krovetz 1993 ,0,1,0
resources specifying the relations among lexical items such as wordnet fellbaum 1998 and hownet dong 2000 among others have inspired the work of many researchers in nlp carpuat et al 2002 dorr et al 2000 resnik 1999 hearst 1998 voorhees 1993 ,0,1,0
6 the experimental results we used the penn treebank marcus et al 1993 to perform empirical experiments on this parsing model ,0,1,0
1 introduction current stateoftheart statistical parsers collins 1999 charniak 2000 are trained on large annotated corpora such as the penn treebank marcus et al 1993 ,0,1,0
secondly while all taggers use lexical information and indeed it is wellknown that lexical probabilities are much more revealing than tag sequence probabilities charniak et al 1993 most taggers make quite limited use of lexical probabilities compared with for example the bilexical probabilities commonly used in current statistical parsers ,0,1,0
the parser has been trained developed and tested on a large collection of syntactically analyzed sentences the penn treebank marcus et al 1993 ,0,1,0
table 6 shows 3an exception is golding 1995 who uses the entire brown corpus for training 1m words and 34 of the wall street journal corpus marcus et al 1993 for testing ,0,1,0
33 corpora our labeled data comes from the penn treebank marcus et al 1993 and consists of about 40000 sentences from wall street journal wsj articles 153 annotated with syntactic information ,0,1,0
for english we used the penn treebank version 30 marcus et al 1993 and extracted dependency relations by applying the headfinding rules of yamada and matsumoto 2003 ,0,1,0
the yield of this tree gives the target translation the gunman was killed by police the penn english treebank ptb marcus et al 1993 is our source of syntactic information largely due to the availability of reliable parsers ,0,1,0
finally section 4 reports the results of parsing experiments using our exhaustive kbest cyk parser with the concise pcfgs induced from the penn wsj treebank marcus et al 1993 ,0,1,0
we use data from the conll2004 shared taskthe propbank palmer et al 2005 annotations of the penn treebank marcus et al 1993 with sections 1518 as the training set and section 20 as the development set ,0,1,0
2 treebanking the penn treebank marcus et al 1993 is annotated with information to make predicateargument structure easy to decode including function tags and markers of empty categories that represent displaced constituents ,1,0,0
for our outofdomain training condition the parser was trained on sections 221 of the wall street journal wsj corpus marcus et al 1993 ,0,1,0
propbank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the penn treebank marcus et al 1993 ,0,1,0
much of this work has been fueled by the availability of large corpora annotated with syntactic structures especially the penn treebank marcus et al 1993 ,0,1,0
english german chinese marcus et al 1993 skut et al 1997 xue et al 2002 trainset section 221 sentences 118602 articles 26270 devset section 22 1860319602 articles 125 testset section 23 1960320602 articles 271300 table 3 experimental setup ,0,1,0
21 training the model as with minnen et al 2000 we train the language model on the penn treebank marcus et al 1993 ,0,1,0
we also test our language model using leaveoneout crossvalidation on the penn treebank marcus et al 1993 wsj giving us 8674 accuracy see table 1 ,0,1,0
4 experiments our experiments involve data from two treebanks the wall street journal penn treebank marcus et al 1993 and the chinese treebank xue et al 2004 ,0,1,0
standard ci model 1 training initialised with a uniform translation table so that tejf is constant for all sourcetarget word pairs fe was run on untagged data for 10 iterations in each direction brown et al 1993 deng and byrne 2005b ,0,1,0
21 em parameter estimation we train using expectation maximisation em optimising the log probability of the training setfesfsgss1 brown et al 1993 ,0,1,0
then pei1jfj1 summationtextai 1 pei1ai1jfj1 brown et al 1993 ,0,1,0
we used the berkeley parser 2 to train such grammars on sections 221 of the penn treebank marcus et al 1993 ,0,1,0
for this paper we use an exact inference exhaustive search cyk parser using a simple probabilistic contextfree grammar pcfg induced from the penn wsj treebank marcus et al 1993 ,0,1,0
173 the standard features for genre classification models include words partofspeech pos tags and punctuation kessler et al 1997 stamatatos et al 2000 lee and myaeng 2002 biber 1993 but constituentbased syntactic categories have also been explored karlgren and cutting 1994 ,0,1,0
a similar approach is used here including a collapsed version of the treebank pos tag set marcus et al 1993 with additions for specific words eg personal pronouns and filled pause markers compound punctuation eg multiple exclamation marks and a general emoticon tag resulting in a total of 41 tags ,0,1,0
2 previous work we briefly outline the most important existing methods and cite error rates on a standard english data set sections 0306 of the wall street journal wsj corpus marcus et al 1993 containing nearly 27000 examples ,0,1,0
4 experimental setup for the experiments we use the wsj portion of the penn tree bank marcus et al 1993 using the standard traindevelopmenttest splits viz 39832 sentences from 221 sections 2416 sentences from section 23 for testing and 1700 sentences from section 22 for development ,0,1,0
ohara and wiebe 2003 also make use of high level features in their case the penn treebank marcus et al 1993 and framenet baker et al 1998 to classify prepositions ,0,1,0
ohara and wiebe 2003 make use of penn treebank marcus et al 1993 and framenet baker et al 1998 to classify prepositions ,0,1,0
in all of the cited approaches the penn wall street journal treebank marcus et al 1993 is used the availability of whichobviates the standard eort required for treebank traininghandannotating large corpora of specic domains of specic languages with specic parse types ,0,1,0
we used treebank grammars induced directly from the local trees of the entire wsj section of the penn treebank marcus et al 1993 release 3 ,0,1,0
evaluating the algorithm on the output of charniaks parser charniak 2000 and the penn treebank marcus et al 1993 shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity ,0,1,0
penn treebank corpus marcus et al 1993 sections 020 were used for training sections 2124 for testing ,0,1,0
task church 1988 brill 1993 ratnaparkhi 1996 daelemans et al 1996 and reported errors in the range of 26 are common ,0,1,0
our chunks and functions are based on the annotations in the third release of the penn treebank marcus et al 1993 ,0,1,0
the annotation scheme skut et al 1997 is modeled to a certain extent on that of the penn treebank marcus et al 1993 with crucial differences ,0,1,0
however most of the existing models have been developed for english and trained on the penn treebank marcus et al 1993 which raises the question whether these models generalize to other languages and to annotation schemes that differ from the penn treebank markup ,0,1,0
we used the wall street journal wsj part of the penn treebank marcus et al 1993 where extraction is represented by coindexing an empty terminal element henceforth ee to its antecedent ,0,1,0
it achieves 901 average precisionrecall for sentences with maximum length 40 and 895 for sentences with maximum length 100 when trained and tested on the standard sections of the wall street journal treebank marcus et al 1993 ,0,1,0
we performed experiments with two statistical classifiers the decision tree induction system c45 quinlan 1993 and the tilburg memorybased learner timbl daelemans et al 2002 ,0,1,0
the corpus was automatically derived from the penn treebank ii corpus marcus et al 1993 by means of the script chunklinkpl buchholz 2002 that we modified to fit our purposes ,0,1,0
we performed a comparison between the existing cfg filtering techniques for ltag poller and becker 1998 and hpsg torisawa et al 2000 using strongly equivalent grammars obtained by converting ltags extracted from the penn treebank marcus et al 1993 into hpsgstyle ,0,1,0
the first stage parser is a bestfirst pcfg parser trained on sections 2 through 22 and 24 of the penn wsj treebank marcus et al 1993 ,0,1,0
41 corpora setup the above kernels were experimented over two corpora propbank wwwcisupennedu ace along with penn treebank5 2 marcus et al 1993 and framenet ,0,1,0
4 evaluation as our algorithm works in open domains we were able to perform a corpusbased evaluation using the penn wsj treebank marcus et al 1993 ,0,1,0
also attribute classi cation is a hard problem and there is no existing classi cation scheme that can be used for open domains like newswire for example wordnet miller et al 1993 organises adjectives as concepts that are related by the nonhierarchical relations of synonymy and antonymy unlike nouns that are related through hierarchical links such as hyponymy hypernymy and metonymy ,0,1,0
using linguistic principles to recover empty categories richard campbell microsoft research one microsoft way redmond wa 98052 usa richcampmicrosoftcom abstract this paper describes an algorithm for detecting empty nodes in the penn treebank marcus et al 1993 finding their antecedents and assigning them function tags without access to lexical information such as valency ,0,1,0
in the penn treebank marcus et al 1993 null elements or empty categories are used to indicate nonlocal dependencies discontinuous constituents and certain missing elements ,0,1,0
3 experiments we tested our methods experimentally on the english penn treebank marcus et al 1993 and on the czech prague dependency treebank hajic 1998 ,0,1,0
5 the experimental results we used the penn treebank wsj corpus marcus et al 1993 to perform empirical experiments on the proposed parsing models ,0,1,0
the most sophisticated of these techniques such as support vector machines are unfortunately too computationally expensive to be used on large datasets like the penn treebank marcus et al 1993 ,0,1,0
the km model creates a packed parse forest of all possible compressions that are grammatical with respect to the penn treebank marcus et al 1993 ,0,1,0
compared to the penn treebank ptb marcus et al 1993 the pos tagset of the french treebank is smaller 13 tags vs 36 tags all punctuation marks are represented as the single ponct tag there are no separate tags for modal verbs whwords and possessives ,0,1,0
it is available in several formats and in this paper we use the penn treebank marcus et al 1993 format of negra ,0,1,0
2001 compare taggers trained and tested on the wall street journal wsj marcus et al 1993 and the lancasteroslobergen lob johansson 1986 corpora and find that the results for the wsj perform significantly worse ,0,1,0
given the estimated 3 error rate of the wsj tagging marcus et al 1993 they argue that the difference in performance is not sufficient to establish which of the two taggers is actually better ,0,1,0
in the february 2004 version of the propbank corpus annotations are done on top of the penn treebank ii parse trees marcus et al 1993 ,0,1,0
4 analysis of experimental data most of the existing research in computational linguistics that uses human annotators is within the framework of classification where an annotator decides for every test item on an appropriate tag out of the prespecified set of tags poesio and vieira 1998 webber and byron 2004 hearst 1997 marcus et al 1993 ,0,1,0
a timeconsuming process litman and pan 2002 marcus et al 1993 xia et al 2000 wiebe 2002 ,0,0,1
for instance the penn treebank policy marcus et al 1993 marcus et al 1994 is to annotate the lowest node that is unfinished with an unf tag as in figure 4a ,0,1,0
in an evaluation on the penn treebank marcus et al 1993 the parser outperformed other unlexicalized pcfg parsers in terms of labeled bracketing fscore ,0,1,0
1 introduction empty categories also called null elements are used in the annotation of the penn treebank marcus et al 1993 in order to represent syntactic phenomena like constituent movement eg whextraction discontinuous constituents and missing elements pro elements empty complementizers and relative pronouns ,0,1,0
but the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain the wall street journal wsj section of the penn treebank marcus et al 1993 ,0,1,0
there are cases though where the labels consist of several related but not entirely correlated properties examples include mention detectionthe task we are interested in syntactic parsing with functional tag assignment besides identifying the syntactic parse also label the constituent nodes with their functional category as defined in the penn treebank marcus et al 1993 and to a lesser extent partofspeech tagging in highly inflected languages4 the particular type of mention detection that we are examining in this paper follows the ace general definition each mention in the text a reference to a realworld entity is assigned three types of information5 an entity type describing the type of the entity it points to eg person location organization etc an entity subtype further detailing the type eg organizations can be commercial governmental and nonprofit while locations can be a nation population center or an international region a mention type specifying the way the entity is realized a mention can be named eg john smith nominal eg professor or pronominal eg she ,0,1,0
large treebanks are available for major languages however these are often based on a speci c text type or genre eg nancial newspaper text the pennii treebank marcus et al 1993 ,0,1,0
1 introduction a number of widecoverage tag ccg lfg and hpsg grammars xia 1999 chen et al 2005 hockenmaier and steedman 2002a odonovan et al 2005 miyao et al 2004 have been extracted from the penn treebank marcus et al 1993 and have enabled the creation of widecoverage parsers for english which recover local and nonlocal dependencies that approximate the underlying predicateargument structure hockenmaier and steedman 2002b clark and curran 2004 miyao and tsujii 2005 shen and joshi 2005 ,0,1,0
33 methods we parsed the english side of each bilingual bitext and both sides of each englishenglish bitext using an offtheshelf syntactic parser bikel 2004 which was trained on sections 0221 of the penn english treebank marcus et al 1993 ,0,1,0
22 generalization pseudocode in order to identify the portions in common between the patterns and to generalize them we apply the following pseudocode ruizcasado et al in press 1all the pos examples in this paper are done with penn treebank labels marcus et al 1993 ,0,1,0
with the exception of hindle and rooth 1993 most unsupervised work on pp attachment is based on superficial analysis of the unlabeled corpus without the use of partial parsing volk 2001 calvo et al 2005 ,0,1,0
the labeled corpus is the penn wall street journal treebank marcus et al 1993 ,0,1,0
1 introduction the best performing systems for many tasks in natural language processing are based on supervised training on annotated corpora such as the penn treebank marcus et al 1993 and the prepositional phrase data set first described in ratnaparkhi et al 1994 ,0,1,0
policy shift left right start over 156545 26351 27918 stay 117819 26351 27918 step back 43374 26351 27918 table 1 the number of actions required to build all the trees for the sentences in section 23 of penn treebank marcus et al 1993 as a function of the focus point placement policy ,0,1,0
the data consist of sections of the wall street journal wsj part of the penn treebank marcus et al 1993 with information on predicateargument structures extracted from the propbank corpus palmer et al 2005 ,0,1,0
typically the local context around the 215 word to be sensetagged is used to disambiguate the sense yarowsky 1993 and it is common for linguistic resources such as wordnet li et al 1995 mihalcea and moldovan 1998 ramakrishnan and prithviraj 2004 or bilingual data li and li 2002 to be employed as well as more longrange context ,0,1,0
1 introduction robust statistical syntactic parsers made possible by new statistical techniques collins 1999 charniak 2000 bikel 2004 and by the availability of large handannotated training corpora such as wsj marcus et al 1993 and switchboard godefrey et al 1992 have had a major impact on the field of natural language processing ,0,1,0
the experiment used all 578 sentences in the atis corpus with a parse tree in the penn treebank marcus et al 1993 ,0,1,0
however evaluations on the widely used wsj corpus of the penn treebank marcus et al 1993 show that the accuracy of these parsers still lags behind the stateoftheart ,1,0,0
4 experiments we evaluated our classifierbased bestfirst parser on the wall street journal corpus of the penn treebank marcus et al 1993 using the standard split sections 221 were used for training section 22 was used for development and tuning of parameters and features and section 23 was used for testing ,0,1,0
1 introduction robust statistical syntactic parsers made possible by new statistical techniques collins 1999 charniak 2000 bikel 2004 and by the availability of large handannotated training corpora such as wsj marcus et al 1993 and switchboard godefrey et al 1992 have had a major impact on the field of natural language processing ,1,0,0
the data consists of sections of the wall street journal part of the penn treebank marcus et al 1993 with information on predicateargument structures extracted from the propbank corpus palmer et al 2005 ,0,1,0
1 introduction the penn treebank marcus et al 1993 is perhaps the most in uential resource in natural language processing nlp ,1,0,0
recall fscore brackets 8917 8750 8833 dependencies 9640 9640 9640 brackets revised 9756 9803 9779 dependencies revised 9927 9927 9927 table 1 agreement between annotators few weeks and increased to about 1000 words per hour after gaining more experience marcus et al 1993 ,0,1,0
the current version of the dataset gives semantic tags for the same sentencesas inthe penntreebank marcuset al 1993 whichareexcerptsfromthewallstreetjournal ,0,1,0
we created a dependency training corpus based on the penn treebank marcus et al 1993 or more specifically on the hpsg treebank generated from the penn treebank see section 22 ,0,1,0
4 experiments we evaluate the accuracy of hpsg parsing with dependencyconstraintsonthehpsgtreebankmiyao et al 2003 which is extracted from the wall street journal portion of the penn treebank marcus et al 19931 ,0,1,0
we used the penn treebank wsj corpus marcus et al 1993 to perform the empirical evaluation of the considered approaches ,0,1,0
first we trained a finitestate shallow parser on base phrases extracted from the penn wall st journal wsj treebank marcus et al 1993 ,0,1,0
this is because their training data the penn treebank marcus et al 1993 does not fully annotate np structure ,0,0,1
treebank marcus et al 1993 six of which are errors ,0,1,0
5 experiments we compare the performance of our forest reranker against nbest reranking on the penn english treebank marcus et al 1993 ,0,1,0
we show that our semisupervised approach yields improvements for fixed datasets by performing parsing experiments on the penn treebank marcus et al 1993 and prague dependency treebank hajic 1998 hajic et al 2001 see sections 41 and 43 ,0,1,0
the english experiments were performed on the penn treebank marcus et al 1993 using a standard set of headselection rules yamada and matsumoto 2003 to convert the phrase structure syntax of the treebank to a dependency tree representation6 we split the treebank into a training set sections 221 a development set section 22 and several test sets sections 07 1 23 and 24 ,0,1,0
the text was split at the sentence level tokenized and pos tagged in the style of the wall street journal penn treebank marcus et al 1993 ,0,1,0
this probability is computed using ibms model 1 brown et al 1993 pqa productdisplay qq pqa 3 pqa 1pmlqapmlqc 4 pmlqa summationdisplay aa tqapmlaa 5 where the probability that the question term q is generated from answer a pqa is smoothed using the prior probability that the term q is generated from the entire collection of answers c pmlqc ,0,1,0
the wsj corpus is based on the wsj part of the penn treebank marcus et al 1993 we used the first 10000 sentences of section 221 as the pool set and section 00 as evaluation set 1921 sentences ,0,1,0
in the general language upenn annotation efforts for the wsj sections of the penn treebank marcus et al 1993 sentences are annotated with pos tags parse trees as well as discourse annotation from the penn discourse treebank miltsakaki et al 2008 while verbs and verb arguments are annotated with propbank rolesets palmer et al 2005 ,0,1,0
in bayraktar et al 1998 the wsj penntreebank corpus marcus et al 1993 is analyzed and a very detailed list of syntactic patterns that correspond to different roles of commas is created ,0,1,0
4 corpus annotation for our corpus we selected 1000 sentences containing at least one comma from the penn treebank marcus et al 1993 wsj section 00 and manually annotated them with comma information3 ,0,1,0
one possible use for this technique is for parser adaptation initially training the parser on one type of data for which handlabeled trees are available eg wall street journal m marcus et al 1993 and then selftraining on a second type of data in order to adapt the parser to the second domain ,0,1,0
1 introduction the last few decades have seen the emergence of multiple treebanks annotated with different grammar formalisms motivated by the diversity of languages and linguistic theories which is crucial to the success of statistical parsing abeille et al 2000 brants et al 1999 bohmova et al 2003 han et al 2002 kurohashi and nagao 1998 marcus et al 1993 moreno et al 2003 xue et al 2005 ,0,1,0
21 data and semantic role annotation proposition bank palmer et al 2005 adds levins style predicateargument annotation and indication of verbs alternations to the syntactic structures of the penn treebank marcus et al 289 1993 ,0,1,0
the wall street journal wsj sections of the penn treebank marcus et al 1993 as training set tests on brown sections typically result in a 68 drop in labeled attachment scores although the average sentence length is much shorter in brown than that in wsj ,0,1,0
the unlabeled data for english we use is the union of the penn treebank tagged wsj data marcus et al 1993 and the bllip corpus5 for the rest of the languages we use only the text of george orwells novel 1984 which is provided in morphologically disambiguated form as part of multexteast but we dont use the annotations ,0,1,0
1 introduction much of statistical nlp research relies on some sort of manually annotated corpora to train their models but these resources are extremely expensive to build especially at a large scale for example in treebanking marcus et al 1993 ,0,1,0
we used the berkeley parser4 to learn such grammars from sections 221 of the penn treebank marcus et al 1993 ,0,1,0
31 data the english data set consists of the wall street journal sections 224 of the penn treebank marcus et al 1993 converted to dependency format ,0,1,0
1 introduction research in language processing has benefited greatly from the collection of large annotated corpora such as penn propbank kingsbury and palmer 2002 and penn treebank marcus et al 1993 ,1,0,0
one major resource for corpusbased research is the treebanks available in many research organizations marcus et al1993 which carry skeletal syntactic structures or brackets that have been manually verified ,1,0,0
several frameworks for finding translation equivalents or translation units in machine translation such as chang and su 1993 isabelle et al1993 and other examplebased mt approaches might be used to select the preferred mapping ,0,1,0
in addition corpusbased stochastic modelling of lexical patterns see weischedel et al 1993 may provide information about word sense frequency of the kind advocated since ford et al 1982 ,0,1,0
a similar approach was taken in weischedel et al 1993 where an unknown word was guessed given the probabilities for an unknown word to be of a particular pos its capitalisation feature and its ending ,0,1,0
these texts were not seen at the training phase which means that neither the 6since brills tagger was trained on the penn tagset marcus et al 1993 we provided an additional mapping ,0,1,0
35 adding context to the model next we added of a stochastic pos tagger charniak et al 1993 to provide a model of context ,0,1,0
33 accuracy results weischedel et al 1993 describe a model for unknown words that uses four features but treats the features ms independent ,0,1,0
with parse action sequences for 40000 wall street journal sentences derived from the penn treebank marcus et al 1993 ,0,1,0
the bracketed portions of figure 1 for example show the base nps in one sentence from the penn treebank wall street journal wsj corpus marcus et al 1993 ,0,1,0
penn treebankmarcus et al 1993 was also used to induce partofspeech pos taggers because the corpus contains very precise and detailed pos markers as well as bracket annotations ,1,0,0
the simplest periodspacecapitalletter approach works well for simple texts but is rather unreliable for texts with many proper names and abbreviations at the end of sentence as for instance the wall street journal wsj corpus marcus et al 1993 ,0,1,0
to identify conjunctions lists and appositives we first parsed the corpus using an efficient statistical parser charniak et al 1998 trmned on the penn wall street journal treebank marcus et al 1993 ,0,1,0
1 introduction the probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers charniak 1995 collins 19961997 joshi and srinivas 1994 kim srinivas and trueswell 1997 stolcke et al 1997 and in psychological theories of language processing clifton et al 1984 ferfeira mcclure 1997 gamsey et al 1997 jurafsky 1996 macdonald 1994 mitchell holmes 1985 tanenhaus et al 1990 trueswell et al 1993 ,0,1,0
this can be done automatically with unparsed corpora briscoe and carroll 1997 manning 1993 ushioda et al 1993 from parsed corpora such as marcus et als 1993 treebank merlo 1994 framis 1994 or manually as was done for comlex macleod and grishman 1994 ,0,1,0
1984 written discourse brown and wsj from penn treebank marcus et al 1993 and conversational data switchboard godfrey et al 1992 ,0,1,0
21418 examples of structures of the kind vb n1 prep n2 were extracted from the penntreebank wall street journal marcus et al 1993 ,0,1,0
6 experiments 61 data preparation our experiments were conducted with data made available through the penn treebank annotation effort marcus et al 1993 ,0,1,0
by core phrases we mean the kind of nonrecursive simplifications of the np and vp that in the literature go by names such as nounverb groups appelt et al 1993 or chunks and base nps ramshaw and marcus 1995 ,0,1,0
charniak charniak et al 1993 gives a thorough explanation of the equations for an hmm model and kupiec kupiec 1992 describes an hmm tagging system in detail ,0,1,0
weischedels group weischedel et al 1993 examines unknown words in the context of partofspeech tagging ,0,1,0
an example set of tags can be found in the penn treebank project marcus et al 1993 ,0,1,0
1 to train their system rm used a 200kword chunk of the penn treebank parsed wall street journal marcus et al 1993 tagged using a transformationbased tagger brill 1995 and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics like treating the possessive marker as the first word of a new base noun phrase to flatten the recursive structure of the parse ,0,1,0
the study is conducted on both a simple air travel information system atis corpus hemphill et al 1990 and the more complex wall street journal wsj corpus marcus et al 1993 ,0,1,0
some of the data comes from the parsed files 221 of the wall street journal penn treebank corpus marcus et al 1993 and additional parsed text was obtained by parsing the 1987 wall street journal text using the parser described in charniak et al ,0,1,0
4 the corpus we used two corpora for our analysis hospital discharge summaries from 1991 to 1997 from the columbiapresbyterian medical center and the january 1996 part of the wall street journal corpus from the penn treebank marcus et al 1993 ,0,1,0
both taggers used the penn treebank tagset and were trained on the wall street journal corpus marcus et al 1993 ,0,1,0
much research has been done to improve tagging accuracy using several different models and methods including hidden markov models hmms kupiec 1992 charniak et al 1993 rulebased systems brill 1994 brill 1995 memorybased systems daelemans et al 1996 maximumentropy systems ratnaparkhi 1996 path voting constraint systems tiir and oflazer 1998 linear separator systems roth and zelenko 1998 and majority voting systems van halteren et al 1998 ,0,1,0
the tagger was tested on two corporathe brown corpus from the treebank ii cdrom marcus et al 1993 and the wall street journal corpus from the same source ,0,1,0
the mbt daelemans et al 1996 180 tagger type standard trigram weischedel et al 1993 mbt daelemans et al 1996 rulebased brill 1994 maximumentropy ratnaparkhi 1996 full secondorder hmm snow roth and zelenko 1998 voting constraints tiir and oflazer 1998 full secondorder hmm known unknown overall openclosed lexicon ,0,1,0
most work in the area of unknown words and tagging deals with predicting partofspeech information based on word endings and affixation information as shown by work in mikheev 1996 mikheev 1997 weischedel et al 1993 and thede 1998 ,0,1,0
the penn treebank documentation marcus et al 1993 defines a commonly used set of tags ,0,1,0
the grammars were induced from sections 221 of the penn wall st journal treebank marcus et al 1993 and tested on section 23 ,0,1,0
3 evaluation of algorithms all four algorithms were run on a 3900 utterance subset of the penn treebank annotated corpus marcus et al 1993 provided by charniak and ge 1998 ,0,1,0
4 experiments the experiments described here were conducted using the wall street journal penn treebank corpus marcus et al 1993 ,0,1,0
the syntactic and partofspeech informations were obtained from the part of the corpus processed in the penn treebank project marcus et al 1993 ,0,1,0
the data sets used are the standard data sets for this problem ramshaw and maxcus 1995 argamon et al 1999 mufioz et al 1999 tjong kim sang and veenstra 1999 taken from the wall street journal corpus in the penn treebank marcus et al 1993 ,0,1,0
we have chosen to work with a corpus with parse information the wall street journal wsj part of the penn treebank ii corpus marcus et al 1993 and to extract chunk information from the parse trees in this corpus ,0,1,0
while the tag features containing wsj paxtofspeech tags marcus et al 1993 have about 45 values the word features have more than 10000 values ,0,1,0
1 data data for 64 verbs shown in table 1 was collected from three corpora the british national corpus bnc httpjinfooxacukbncindexhtml the penn treehank parsed version of the brown corpus brown and the penn treebank wall street journal corpas wsj marcus et al 1993 ,0,1,0
introduction verb subcategorizafion probabilities play an important role in both computational linguistic applications eg carroll minnen and briscoe 1998 charniak 1997 collins 19961997 joshi and srinivas 1994 kim srinivas and tmeswell 1997 stolcke et al 1997 and psycholinguisfic models of language processing eg boland 1997 clifton et al 1984 ferreira mcclure 1997 fodor 1978 garnsey et al 1997 jurafsky 1996 macdonald 1994 mitchell holmes 1985 tanenhaus et al 1990 trueswell et al 1993 ,0,1,0
the success of statistical methods in particular has been quite evident in the area of syntactic parsing most recently with the outstanding results of charniak 2000 and colhns 2000 on the nowstandard english test set of the penn treebank marcus et al 1993 ,0,1,0
by comparing derivation trees for parallel sentences in two languages instances of structural divergences dorr 1993 dorr 1994 palmer et al 1998 can be automatically detected ,0,1,0
22 three treebanks the treebanks that we used in this paper are the english penn treebank ii marcus et al 1993 the chinese penn treebank xia et al 2000b and the korean penn treebank chunghye han 2000 ,0,1,0
the corpus consists of sections 1518 and section 20 of the penn treebank marcus et al 1993 and is predivided into a 8936sentence 211727 tokens training set and a 2012sentence 47377 tokens test set ,0,1,0
this paper presents an empirical study measuring the effectiveness of our evaluation functions at selecting training sentences from the wall street journal wsj corpus marcuset al 1993 for inducing grammars ,0,1,0
35 the experiments we have ran lextract on the onemillionword english penn treebank marcus et al 1993 and got two treebank grammars ,0,1,0
the data used for all our experiments is extracted from the penn wsj treebank marcus et al 1993 by the program provided by sabine buchholz from tilbug university ,0,1,0
32 probability structure of the original model we use p to denote the unlexicalized nonterminal corresponding to p and similarly for li ri and h we now present the toplevel generation probabilities along with examples from 4the inclusion of the word feature in the bbn model was due to the work described in weischedel et al 1993 where word features helped reduce part of speech ambiguity for unknown words ,0,1,0
the analyserand therefore the generatorincludes exception lists derived from wordnet version 15 miller et al 1993 ,0,1,0
corpus garside et al 1987 the penn treebank marcus et al 1993 the susanne corpus sampson 1995 the spoken english corpus taylor and knowles 1988 the oxford psycholinguistic database quinlan 1992 and the computerusable version of the oxford advanced learners dictionary of current english oaldce mitton 1992 ,0,1,0
parsers precisiona4 recalla4 a5a7a6 a4 a8km00 a9 9345 9351 9348 a8hal00 a9 9313 9351 9332 a8cscl a9 9341 9264 9302 a8tks00 a9 9404 9100 9250 a8zst00 a9 9199 9225 9212 a8dej00 a9 9187 9131 9209 a8koe00 a9 9208 9186 9197 a8osb00 a9 9165 9223 9194 a8vb00 a9 9105 9203 9154 a8pmp00 a9 9063 8965 9014 a8joh00 a9 8624 8825 8723 a8vd00 a9 8882 8291 8576 baseline 7258 8214 7707 22 data training was done on the penn treebank marcus et al 1993 wall street journal data sections 0221 ,0,1,0
we have used three different algorithms the nearest neighbour algorithm ib1ig which is part of the timbl software package daelemans et al 1999 the decision tree learner igtree also from timbl and c50 a commercial version of the decision tree learner c45 quinlan 1993 ,0,1,0
it consists of sections 1518 of the wall street journal part of the penn treebank ii marcus et al 1993 as training data 211727 tokens and section 20 as test data 47377 tokens ,0,1,0
here we present experiments performed using two complex corpora c1 and c2 extracted from the penn treebank marcus et al 1993 marcus et al 1994 ,0,1,0
cll has then been applied to a corpus of declarative sentences from the penn treebank marcus et al 1993 marcus et al 1994 on which it has been shown to perform comparatively well with respect to much less psychologically plausible systems which are significantly more supervised and are applied to somewhat simpler problems ,0,1,0
for example the penn treebank marcus et al 1993 marcus et al 1994 bies et al 1994 provides a large corpus of syntactically annotated examples mostly from the wall street journal ,0,1,0
31 the corpus the systems are applied to examples from the penn treebank marcus et al 1993 marcus et al 1994 bies et al 1994 a corpus of over 45 million words of american english annotated with both partofspeech and syntactic tree information ,0,1,0
firstly there is also hrb aadvp declined hvbd hvp the dollar adt hnn cnpsbj hvp hs figure 2 a tree with constituents marked the topdown method which is a version of the algorithm described by hockenmaier et al hockenmaier et al 2000 but used for translating into simple ab cg rather than the steedmans combinatory categorial grammar ccg steedman 1993 ,0,1,0
41 data we used penntreebank marcus et al 1993 data presented in table 1 ,0,1,0
the resulting corpus contains 385 documents of american english selected from the penn treebank marcus et al 1993 annotated in the framework of rhetorical structure theory ,0,1,0
 previous research has shown that rst trees can play a crucial role in building natural language generation systems hovy 1993 moore and paris 1993 moore 1995 and text summarization systems marcu 2000 can be used to increase the naturalness of machine translation outputs marcu et al 2000 and can be used to build essayscoring systems that provide students with discoursebased feedback burstein et al 2001 ,0,1,0
one judge annotated allarticles in four datasets of the wall street journal treebank corpus marcus et al 1993 w94 w910 w922 and w933 each approximately 160k words as well as thecorpusofwall street journal articles used in wiebe et al 1999 called wsjse below ,0,1,0
3 previous work on subjectivity tagging in previous work wiebe et al 1999 bruce and wiebe 1999 a corpus of sentences from the wall street journal treebank corpus marcus et al 1993 was manually annotated with subjectivity classi cations bymultiplejudges ,0,1,0
one of the first large scale hand tagging efforts is reported in miller et al 1993 where a subset of the brown corpus was tagged with wordnet july 2002 pp ,0,1,0
31 data the starting corpus we use is formed by a mix of three different sources of data namely the penn treebank corpus marcus et al 1993 the los angeles times collection as provided during trec conferences1 and open mind common sense2 a collection of about 400000 commonsense assertions in english as contributed by volunteers over the web ,0,1,0
each dataset consisted of a collection of flat rules such as sputnp put np pp extracted from the penn treebank marcus et al 1993 ,0,1,0
even for relatively general texts such as the wall street journal marcus et al 1993 or terrorism articles muc4 proceedings 1992 roark and charniak roark and charniak 1998 reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in wordnet ,0,1,0
when an s alignment exists there will always also exist a p alignment such that p a65 s the english sentences were parsed using a stateoftheart statistical parser charniak 2000 trained on the university of pennsylvania treebank marcus et al 1993 ,0,1,0
the first work in smt done at ibm brown et al 1993 developed a noisychannel model factoring the translation process into two portions the translation model and the language model ,0,1,0
in this paper we give an overview of nlpwin a multiapplication natural language analysis and generation system under development at microsoft research jensen et al 1993 gamon et al 1997 heidorn 2000 incorporating analysis systems for 7 languages chinese english french german japanese korean and spanish ,0,1,0
consistency among raters who may have different levels of fluency in the source language raters are not shown the original french or spanish sentence for similar methodologies see ringger et al 2001 white et al 1993 ,0,1,0
the most common answer is component testing where the component is compared against a standard of goodness usually the penn treebank for english marcus et al 1993 allowing a numerical score of precision and recall eg collins 1997 ,0,1,0
with the availability of large natural language corpora annotated for syntactic structure the treebanks eg marcus et al 1993 automatic grammar extraction became possible chen and vijayshanker 2000 xia 1999 ,0,1,0
any linguistic annotation required during the extraction process therefore is produced through automatic means and it is only for reasons of accessibility and comparability with other research that we choose to work over the wall street journal section of the penn treebank marcus et al 1993 ,0,1,0
22 corpus occurrence in order to get a feel for the relative frequency of vpcs in the corpus targeted for extraction namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 vpc types corpus frequency figure 1 frequency distribution of vpcs in the wsj tagger correctextracted prec rec ffl1 brill 135135 1000 0177 0301 penn 667800 0834 0565 0673 table 1 posbased extraction results the wsj section of the penn treebank we took a random sample of 200 vpcs from the alvey natural language tools grammar grover et al 1993 and did a manual corpus search for each ,0,1,0
this cost can often be substantial as with the penn treebank marcus et al 1993 ,0,0,1
table 3 compares precision recall and f scores for our system with conll2001 results training on sections 1518 of the penn treebank and testing on section 21 marcus et al 1993 ,0,1,0
for example 10 million words of the american national corpus ide et al 2002 will have manually corrected pos tags a tenfold increase over the penn treebank marcus et al 1993 currently used for training pos taggers ,0,0,1
machine learning methods should be interchangeable transformationbased learning tbl brill 1993 and memorybased learning mbl daelemans et al 2002 have been applied to many different problems so a single interchangeable component should be used to represent each method ,0,1,0
our work so far has focused on data in the penn treebank marcus et al 1993 particularly the brown corpus and some examples from the wall street journal corpus ,0,1,0
2 prior work statistical machine translation as pioneered by ibm eg brown et al 1993 is grounded in the noisy channel model ,0,1,0
the propbank superimposes an annotation of semantic predicateargument structures on top of the penn treebank ptb marcus et al 1993 marcus et al 1994 ,0,1,0
41 experimental setup we use the whole penn treebank corpus marcus et al 1993 as our data set ,0,1,0
the creation of the penn english treebank marcus et al 1993 a syntactically interpreted corpus played a crucial role in the advances in natural language parsing technology collins 1997 collins 2000 charniak 2000 for english ,1,0,0
although few corpora annotated with semantic knowledge are available now there are some valuable lexical databases describing the lexical semantics in dictionary form for example english wordnet miller et al 1993 and chinese hownet dong and dong 2001 ,0,1,0
ontologies are formal specifications of a conceptualization gruber 1993 so that it seems straightforward to formalize annotation schemes as ontologies and make use of semantic annotation tools such as ontomat handschuh et al 2001 for the purpose of linguistic annotation ,0,1,0
partofspeech pos annotation for example can be seen as the task of choosing the appropriate tag for a word from an ontology of word categories compare for example the penn treebank pos tagset as described in marcus et al 1993 ,0,1,0
1 introduction large scale annotated corpora such as the penn treebank marcus et al 1993 have played a central role in speech and natural language research ,1,0,0
however developing the pdtb may help facilitate the production of more such corpora through an initial pass of automatic annotation followed by manual correction much as was done in developing the ptb marcus et al 1993 ,0,1,0
since parsing is just an initial stage of natural language understanding the project was focused not just on obtaining syntactic trees alone as is done in many other parsed corpora for example penn treebank marcus et al 1993 or tiger brants and plaehn 2000 ,0,1,0
the elementary trees were extracted from the parse trees in sections 0221 of the wall street journal in penn treebank marcus et al 1993 which is transformed by using parentchild annotation and left factoring roark and johnson 1999 ,0,1,0
6 the experiments to investigate the e ects of lookahead on our family of deterministic parsers we ran empirical experiments on the standard the penn treebank marcus et al 1993 datasets ,0,1,0
although ldd annotation is actually provided in treebanks such as the penn treebank marcus et al 1993 over which they are typically trained most probabilistic parsers largely or fully ignore this information ,0,1,0
2 detecting discoursenew definite descriptions 21 vieira and poesio poesio and vieira 1998 carried out corpus studies indicating that in corpora like the wall street journal portion of the penn treebank marcus et al 1993 around 52 of dds are discoursenew prince 1992 and another 15 or so are bridging references for a total of about 6667 firstmention ,0,1,0
word association norms mutual information and lexicography computational linguistics 161 2229 marcus m et al 1993 ,0,1,0
collocation dictionary of modern chinese lexical words business publisher china yuan liu et al 1993 ,0,1,0
the segmentation is based on the guidelines given in the chinese national standard gb13715 liu et al 1993 and the pos tagging specification was developed according to the grammatical knowledgebase of contemporary chinese ,0,1,0
also in the penn treebank marcus et al 1993 marcus et al 1994 a limited set of relations is placed over the constituencybased annotation in order to make explicit the morphosyntactic or semantic roles that the constituents play ,0,1,0
annotated reference corpora such as the brown corpus kucera francis 1967 the penn treebank marcus et al 1993 and the bnc leech et al 2001 have helped both the development of english computational linguistics tools and english corpus linguistics ,1,0,0
a quick search in the penn treebank marcus et al 1993 shows that about 17 of all sentences contain parentheticals or other sentence fragments interjections or unbracketable constituents ,0,1,0
although grammatical function and empty nodes annotation expressing longdistance dependencies are provided in treebanks such as the penn treebank marcus et al 1993 most statistical treebank trained parsers fully or largely ignore them 1 which entails two problems first the training cannot profit from valuable annotation data ,0,1,0
on the other hand highquality treebanks such as the penn treebank marcus et al 1993 and the kyoto university text corpus kurohashi and nagao 1997 have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structure analysis ,1,0,0
the definitions of partofspeech pos categories and syntactic labels follow those of the treebank i style marcus et al 1993 ,0,1,0
4 the experiments for the experiments we used propbank wwwcisupenneduace along with penntreebank5 2 wwwcisupennedutreebank marcus et al 1993 ,0,1,0
thus the penn treebank of american english marcus et al 1993 has been used to train and evaluate the best available parsers of unrestricted english text collins 1999 charniak 2000 ,0,1,0
3 data the data consists of six sections of the wall street journal part of the penn treebank marcus et al 1993 and follows the setting of past editions of the conll shared task training set sections 1518 development set section 20 and test set section 21 ,0,1,0
the penn treebank ptb is an example of such a resource with worldwide impact on natural language processing marcus et al 1993 ,1,0,0
since czech is a language with relatively high degree of wordorder freedom and its sentences contain certain syntactic phenomena such as discontinuous constituents nonprojective constructions which cannot be straightforwardly handled using the annotation scheme of penn treebank marcus et al 1993 linguistic data consortium 1999 based on phrasestructure trees we decided to adopt for the pcedt the dependencybased annotation scheme of the prague dependency treebank pdt linguistic data consortium 2001 ,0,0,1
a model was trained using maximum likelihood from the upenn treebank marcus et al 1993 ,0,1,0
introduction the creation of the penn treebank marcus et al 1993 and the word senseannotated semcor fellbaum 1997 have shown how even limited amounts of annotated data can result in major improvements in complex natural language understanding systems ,1,0,0
1 introduction the overall goal of the penn discourse treebank pdtb is to annotate the million word wsj corpus in the penn treebank marcus et al 1993 with a layer of discourse annotations ,0,1,0
1 introduction there is a pressing need for a consensus on a taskoriented level of semantic representation that can enable the development of powerful new semantic analyzers in the same way that the penn treebank marcus et al 1993 enabled the development of statistical syntactic parsers collins 1999 charniak 2001 ,1,0,0
6 discussion lack of interannotator agreement presents a significant problem in annotation efforts see eg marcus et al 1993 ,0,1,0
postediting of automatic annotation has been pursued in various projects eg brants 2000 and marcus et al 1993 ,0,1,0
the latter group did an experiment early on in which they found that manual tagging took about twice as long as correcting automated tagging with about twice the interannotator disagreement rate and an error rate that was about 50 higher marcus et al 1993 ,0,1,0
the list is obtained by first extracting the phrases with tmp function tags from the penntree bank and taking the words in these phrases marcus et al 1993 ,0,1,0
in our framework we employ a simple hmmbased tagger where the most probable tag sequence a29a30 given the words a31 is output weischedel et al 1993 a29 a30 a20a22a32a34a33a36a35a38a37a39a32a41a40 a42 a43a45a44 a30a47a46 a31a49a48a17a20a22a32a34a33a50a35a38a37a39a32a41a40 a42 a43a45a44 a31 a46a30 a48 a43a51a44 a30 a48 since we do not have enough data which is manually tagged with partofspeech tags for our applications we used penn treebank marcus et al 1994 as our training set ,0,1,0
as referring dataset we used the propbank corpora available at wwwcisupenneduace along with the penn treebank 2 wwwcisupennedutreebank marcus et al 1993 ,0,1,0
22 closed challenge setting the organization provided training development and test sets derived from the standard sections of the penn treebank marcus et al 1993 and propbank palmer et al 2005 corpora ,0,1,0
3 data the data consists of sections of the wall street journal part of the penn treebank marcus et al 1993 with information on predicateargument structures extracted from the propbank corpus palmer et al 2005 ,0,1,0
44 corpora we ran the three syntactic preprocessors over a total of three corpora of varying size the brown corpus 460k tokens and wall street journal corpus 12m tokens both derived from the penn treebank marcus et al 1993 and the written component of the british national corpus 98m tokens burnard 2000 ,0,1,0
for this experiment we used sections 02 21 of the penn treebank ptb marcus et al 1993 as the training data and section 23 2416 sentences for evaluation as is now standard ,0,1,0
3 formulation following klein and manning 2001 we use weighted directed hypergraphs gallo et al 1993 as an abstraction of the probabilistic parsing problem ,0,1,0
stateoftheart statistical parsers trained on the penn treebank ptb marcus et al 1993 pros a8a8 a8a8a8 a72a72 a72a72a72 npsbj a16a16a16 a80a80a80the authority vp a16a16a16 a16a16a16a16 a0 a0a0 a64 a64a64 a80a80a80 a80a80a80a80 vbd dropped pptmp a8a8 a72a72in at np nn midnight nptmp nnp tuesday ppdir a8a8 a72a72to to np qp a16a16a16 a80a80a80 280 trillion figure 1 a sample syntactic structure with function labels ,0,1,0
we evaluated the generator on the penn treebank marcus et al 1993 which is highly reliable corpus consisting of realworld texts ,1,0,0
most of them were developed for exhaustive parsing ie producing all parse results that are given by the grammar matsumoto et al 1983 maxwell and kaplan 1993 van noord 1997 kiefer et al 1999 malouf et al 2000 torisawa et al 2000 oepen et al 2002 penn and munteanu 2003 ,0,1,0
data and parameters to facilitate comparison with previous work we trained our models on sections 221 of the wsj section of the penn treebank marcus et al 1993 ,0,1,0
we trained and tested the parser on the wall street journal corpus of the penn treebank marcus et al 1993 using the standard split sections 221 were used for training section 22 was used for development and tuning of parameters and features and section 23 was used for testing ,0,1,0
for instance the halogen statistical realizer langkildegeary 2002 underwent the most comprehensive evaluation of any surface realizer which was conducted by measuring sentences extracted from the penn treebank marcus et al 1993 converting them into its input formalism and then producing output strings ,0,1,0
this corpus contains annotations of semantic pass superimposed on the penn treebank ptb marcus et al 1993 marcus et al 1994 ,0,1,0
marcus et al 1993 santorini 1990 the syntactic annotation task consists of marking constituent boundaries inserting empty categories traces of movement pro pro showing the relationships between constituents argumentadjunct structures and specifying a particular subset of adverbial roles ,0,1,0
section 4 concludes the paper with a critical assessment of the proposed approach and a discussion of the prospects for application in the construction of corpora comparable in size and quality to existing treebanks such as for example the penn treebank for english marcus et al 1993 or the tiger treebank for german brants et al 2002 ,0,1,0
42 word alignment we have used ibm models proposed by brown brown et al 1993 for word aligning the parallel corpus ,0,1,0
5 datasets and evaluation we train our models with verb instances extracted from three parsed corpora 1 the wall street journal section of the penn treebank ptb which was parsed by human annotators marcus et al 1993 2 the brown laboratory for linguistic information processing corpus of wall street journal text bllip which was parsed automatically by the charniak parser charniak 2000 and 3 the gigaword corpus of raw newswire text gw which we parsed ourselves with the stanford parser ,0,1,0
the parser is trained on dependencies extracted from the english penn treebank version 30 marcus et al 1993 by using the headpercolation rules of yamada and matsumoto 2003 ,0,1,0
a third of the corpus is syntactically parsed as part of the penn treebank marcus et al 1993 2this type corresponds to princes 1981 1992 inferrables ,0,1,0
5 data sets and supervised tagger 51 source domain wsj we used sections 0221 of the penn treebank marcus et al 1993 for training ,0,1,0
there are many choices for modeling cooccurrence data brown et al 1992 pereira et al 1993 blei et al 2003 ,0,1,0
321 jensenshannon divergence is defined as dqr 12 parenleftbigg d parenleftbigg q q r2 parenrightbigg d parenleftbigg r q r2 parenrightbiggparenrightbigg these experiments are a kind of poor mans version of the deterministic annealing clustering algorithm pereira et al 1993 rose 1998 which gradually increases the number of clusters during the clustering process ,0,1,0
we used sections 220 of the penn treebank 2 wall street journal corpus marcus et al 1993 for training section 22 as development set and section 23 for testing ,0,1,0
5 experimental evaluation to perform empirical evaluations of the proposed methods we considered the task of parsing the penn treebank wall street journal corpus marcus et al 1993 ,0,1,0
4 data collection we evaluated out method by running rasp over brown corpus and wall street journal as contained in the penn treebank marcus et al 1993 ,0,1,0
neither hindle and rooth 1993 with 67 nor ratnaparkhi et al 1994 with 59 noun attachment were anywhere close to this figure ,0,1,0
but it makes obvious that ratnaparkhi et al 1994 were tackling a problem different from hindle and rooth 1993 given the fact that their baseline was at 59 guessing noun attachment rather than 67 in the hindle and rooth experiments3 of course the baseline is not a direct indicator of the difficulty of the disambiguation task ,0,1,0
31 results for english we used sections 0 to 12 of the wsj part of the penn treebank marcus et al 1993 with a total of 24618 sentences for our experiments ,0,1,0
propbank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the penn treebank marcus et al 1993 ,0,1,0
we use the penn treebank wall street journal corpus as the large corpus and individual sections of the brown corpus as the target corpora marcus et al 1993 ,0,1,0
this research has focused mostly on the development of statistical parsers trained on large annotated corpora in particular the penn treebank wsj corpus marcus et al 1993 ,0,1,0
we retrained the parser on lowercased penn treebank ii marcus et al 1993 to match the lowercased output of the mt decoder ,0,1,0
we measured the accuracy of the pos tagger trained in three settings original the tagger is trained with the union of wall street journal wsj section of penn treebank marcus et al 1993 genia and penn bioie ,0,1,0
as the third test set we selected all tokens of the brown corpus part of the penn treebank marcus et al 1993 a selected portion of the original onemillion word brown corpus kucera and francis 1967 a collection of samples of american english in many different genres from sources printed in 1961 we refer to this test set as brown ,0,1,0
tag sets for english are derived from the penn treebank marcus et al 1993 ,0,1,0
the cdr morris 1993 is assigned with access to clinical and cognitive test information independent of performance on the battery of neuropsychological tests used for this research study and has been shown to have high expert interannotator reliability morris et al 1997 ,0,1,0
narrative retellings provide a natural conversational speech sample that can be analyzed for many of the characteristics of speech and language that have been shown to discriminate between healthy and impaired subjects including syntactic complexity kemper et al 1993 lyons et al 1994 and mean pause duration singh et al 2001 ,0,1,0
empirical evaluation has been done with the erg on a small set of texts from the wall street journal section 22 of the penn treebank marcus et al 1993 ,0,1,0
after the success in syntactic penn treebank marcus et al 1993 and propositional encodings penn propbank palmer et al 2005 more sophisticated semantic data such as temporal pustejovsky et al 2003 or opinion annotations wiebe et al 2005 and discourse data eg for anaphora resolution van deemter and kibble 2000 and rhetorical parsing carlson et al 2003 are being generated ,1,0,0
while significant time savings have already been reported on the basis of automatic pretagging eg for pos and parse tree taggings in the penn treebank marcus et al 1993 or named entity taggings for the genia corpus ohta et al 2002 this kind of preprocessing does not reduce the number of text tokens actually to be considered ,1,0,0
with respect to already available pos tagsets the scheme allows corresponding extensions of the supertype postag to eg pennpostag for the penn tag set marcus et al 1993 or geniapostag for the genia tag set ohta et al 2002 ,0,1,0
currently the scheme supports phrasechunks with subtypes such as np vp pp or adjp marcus et al 1993 ,0,1,0
the dublin core metadata initiative3 established a de facto standard for the semantic web4 for computational linguistics proper syntactic annotation schemes such as the one from the penn treebank marcus et al 1993 or semantic annotations such as the one underlying ace doddington et al 2004 are increasingly being used in a quasi standard way ,0,1,0
the penn treebank annotation marcus et al 1993 was chosen to be the first among equals it is the starting point for the merger and data from other annotations are attached at tree nodes ,0,1,0
6 penn discourse treebank bonnie webber edinburgh the penn discourse treebank miltsakaki et al 2004 prasad et al 2004 webber 2005 annotates discourse relations over the wall street journal corpus marcus et al 1993 in terms of discourse connectives and their arguments ,0,1,0
it has been difficult to identify all and only those cases where a token functions as a discourse connective and in many cases the syntactic analysis in the penn treebank marcus et al 1993 provides no help ,0,0,1
for this reason each preposition and verb was assigned a weight based on the proportion of occurrences of that word in the penn treebank marcus et al 1993 which are labelled with a spatial meaning ,0,1,0
we trained the parser on the penn treebank marcus et al 1993 ,0,1,0
we parsed the timeeval data using mstparser v02 mcdonald and pereira 2006 which is trained with all penn treebank marcus et al 1993 without dependency label ,0,1,0
the sentences included in the gold standard were chosen at random from the bnc subject to the condition that they contain a verb which does not occur in the training sections of the wsj section of the ptb marcus et al 1993 ,0,1,0
examples of this work include a system by liu et al 1990 and experiments by hindle and rooth 1993 and resnik and hearst 19932 these efforts had mixed success suggesting that while multilevel preference scores are problematic integrating some corpus data does not solve the problems ,0,1,0
figure 1 gives an example dependency graph for the sentence mr tomash will remain as a director emeritus whichhasbeenextractedfromthe penn treebank marcus et al 1993 ,0,1,0
5 parsing experiments 51 data and setup we used the standard partitions of the wall street journal penn treebank marcus et al 1993 ie sections 221 for training section 22 for development and section 23 for evaluation ,0,1,0
1 introduction large scale annotated corpora eg the penn treebank ptb project marcus et al 1993 have played an important role in textmining ,1,0,0
the current release of pdtb20 contains the annotations of 1808 wall street journal articles 1 million words from the penn treebank marcus et al 1993 ii distribution and a total of 40600 discourse connective tokens prasad et al 2008b ,0,1,0
other languagesfor which this is the case include english with the penn treebank marcus et al 1993 the susanne corpus sampson 1993 and the british section of the ice corpus wallis and nelson 2006 and italian with isst montegmagni et al 2000 and tut bosco et al 2000 ,0,1,0
first we noted how frequently wordnet fellbaum 1998 gets used compared to other resources such as framenet fillmore et al 2003 or the penn treebank marcus et al 1993 ,0,1,0
2 the data our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the penn treebank ptb with propbank prbk marcus et al 1993 palmer et al 2005 as shown in figure 1 ,0,1,0
32 conversion to dependencies 321 syntactic dependencies there exists no largescale dependency treebank for english and we thus had to construct a dependencyannotated corpus automatically from the penn treebank marcus et al 1993 ,0,1,0
in this vein the conll 2008 shared task sets the challenge of learning jointly both syntactic dependencies extracted from the penn treebank marcus et al 1993 and semantic dependencies extracted both from propbank palmer et al 2005 c2008 ,0,1,0
my guess is that the features used in eg the collins 2003 or charniak 2000 parsers are probably close to optimal for english penn treebank parsing marcus et al 1993 but that other features might improve parsing of other languages or even other english genres ,0,1,0
i have made a preliminary analysis of the inventory of syntactic categories used in the tagging for labelling trees in the 18 penn treebank marcus et al 1993 comparing them to the categories used in cgel ,0,1,0
however with their system trained on the medical corpus and then tested on the wall street journal corpus marcus et al 1993 they achieve an overall prediction accuracy of only 54 ,0,1,0
for testing purposes we used the wall street journal part of the penn treebank corpus marcus et al 1993 ,0,1,0
the spanish corpus was parsed using the mst dependency parser mcdonald et al 2005 trained using dependency trees generated from the the english penn treebank marcus et al 1993 and spanish conllx data buchholz and marsi 2006 ,0,1,0
the parser expresses distinctions that are especially important for a predicateargument based deep syntactic representation as far as they are expressed in the training data generated from the penn treebank marcus et al 1993 ,0,1,0
a description of the flat featurized dependencystyle syntactic representation we use is available in langkildegeary and betteridge 2006 which describes how the entire penn treebank marcus et al 1993 was converted to this representation ,0,1,0
53 experimental setup we used the stanford parser klein and manning 2003 for both languages penn english treebank marcus et al 1993 and penn arabic treebank set kulick et al 2006 ,0,1,0
3 network evaluation we present an evaluation which has been carried out on an initial set of annotations of english articles from the wall street journal covering those annotated at the syntactic level in the penn treebank marcus et al 1993 ,0,1,0
the penn treebank marcus et al 1993 has until recently been the only such corpus covering 45m words in a single genre of financial reporting ,1,0,0
transformationbased errordriven learning has been applied to a number of natural language problems including part of speech tagging prepositional phrase attachment disambiguation speech generation and syntactic parsing brill 1992 brill 1994 ramshaw and marcus 1994 roche and schabes 1995 brill and resnik 1994 huang et al 1994 brill 1993a brill 1993b ,0,1,0
below is an example of the initialstate tagging of a sentence from the penn treebank marcus et al 1993 where an underscore is to be read as or ,0,1,0
the performance figures given below are based on training each method on the 1millionword brown corpus kuera and francis 1967 and testing it on a 34millionword corpus of wall street journal text marcus et al 1993 ,0,1,0
1993 found that direct annotation takes twice as long as automatic tagging plus correction for partofspeech annotation and the output quality reflects the difficulty of the task interannotator disagreement is on the order of 10 as contrasted with the approximately 3 error rate reported for partofspeech annotation by marcus et al ,0,1,0
the traditional method of evaluating similarity in a semantic network by measuring the path length between two nodes lee et al 1993 rada et al 1989 also captures this albeit indirectly when the semantic network is just an isa hierarchy if the minimal path of isa links between two nodes is long that means it is necessary to go high in the taxonomy to more abstract concepts in order to find their least upper bound ,0,1,0
bensch and savitch 1992 brill 1991 brown et al 1992 grefenstette 1994 mckcown and hatzivassiloglou 1993 pereira et al 1993 schtltze 1993 ,0,1,0
furthermore training corpora for information extraction are typically annotated with domainspecific tags in contrast to generalpurpose annotations such as partofspeech tags or nounphrase bracketing eg the brown corpus francis and kucera 1982 and the penn treebank marcus et al 1993 ,0,1,0
general purpose text annotations such as partofspeech tags and nounphrase bracketing are costly to obtain but have wide applicability and have been used successfully to develop statistical nlp systems eg church 1989 weischedel et al 1993 ,0,1,0
in previous work we tested the dop method on a cleanedup set of analyzed partofspeech strings from the penn treebank marcus et al 1993 achieving excellent test results bod 1993a b ,0,1,0
the latter approach has become increasingly popular eg schabes et al 1993 weischedel et al 1993 briscoe 1994 magerman 1995 collins 1996 ,0,1,0
to deal with this question we use atis pos trees as found in the penn treebank marcus et al 1993 ,0,1,0
1993 chang et al 1992 collins and brooks 1995 fujisaki 1989 hindle and rooth 1991 hindle and rooth 1993 jelinek et al 1990 magerman and marcus 1991 magerman 1995 ratnaparkhi et al 1994 resnik 1993 su and chang 1988 ,0,1,0
we extracted 181250 case frames from the wsj wall street journal bracketed corpus of the penn tree bank marcus et al 1993 ,0,1,0
clusters are created by means of distributional techniques in ratnaparkhi et al 1994 while in resnik and hearst 1993 low level synonim sets in wordnet are used ,0,1,0
this method is described hereafter while the subsequent steps that use deeper rulebased levels of knowledge are implemented into the ariostolex lexical learning system described in basili et al 1993b 1933c and 1996 ,0,1,0
the class based disambiguation operator is the mutual conditioned plausibility mcpi basili et al 1993a ,0,1,0
in general the training set is the parsed wall street journal marcus et al 1993 with few exceptions and the size of the training samples is around 1020000 test cases ,0,1,0
this incremental process can be iterated to the point that the system 1 it is not just a matter of time but also of required linguistic skills see for example marcus et al 1993 ,0,1,0
these later inductive phases may rely on some level of a priori knowledge like for example the naive case relations used in the ariostolex system basili et al 1993c 1996 ,0,1,0
to simplify the plausibility of a detected esl is roughly inversely proportional to the number of mutually excluding syntactic structures in the text segment that generated the esl see basili et al 1993a for details ,0,1,0
learning to disambiguate word senses several recent research projects have taken a corpusbased approach to lexical disambiguation brown dellapietra dellapietra mercer 1991 gale church yarowsky 1992b leacock et al 1993b lehman 1994 ,0,1,0
 gtpdl allmwilqlf idwtlio r i1 ii mlmulm ip illllb l i i i i i i i i i 0 200 400 600 800 1000 1200 1400 1600 1800 article 2000 figure 1 distribution of tags for the word about vs article training sizewrdsi test571190 sizewrds i baseline44478 9704 specialized 19713 table 10 performance of baseline specialized model when tested on consistent subset of development set 139 pos tag 35 30 25 2o 15 10 5 0 1 i o oho m i i i b m m i i i 2 3 4 annotator figure 2 distribution of tags for the word about vs annotator weischedel et al 1993 provide the results from a battery of tritag markov model experiments in which the probability pwt of observing a word sequence w wlw2wn together with a tag sequence t tlt2tn is given by ptiwpw ptlpt21tl h ptiltilti2 pwilti i3 furthermore pwilti for unknown words is computed by the following heuristic which uses a set of 35 predetermined endings pwilti punknownwordlti x pcapitalfeatureti x pendings hypenationlti this approximation works as well as the maxent model giving 85 unknown word accuracyweischedel et al 1993 on the wall st journal but cannot be generalized to handle more diverse information sources ,0,1,0
previous uses of this model include language modelinglau et al 1993 machine translationberger et al 1996 prepositional phrase attachmentratnaparkhi et al 1994 and word morphologydella pietra et al 1995 ,0,1,0
comparison with previous work most of the recent corpusbased pos taggers in the literature are either statistically based and use markov modelweischedel et al 1993 merialdo 1994 or statistical decision treejelinek et al 1994 magerman 1995sdt techniques or are primarily rule based such as drills transformation based learnerdrill 1994tbl ,0,1,0
in all other respects our work departs from previous research on broadcoverage 16 i t i i i i i i i i i i i i i i i i i i i i i 1 i i i i i i i 1 i i i i probabilistic parsing which either attempts to learn to predict grrarntical structure of test data directly from a training treebank brill 1993 collins 1996 eisner 1996 jelinek et al 1994 magerman 1995 skine and orishman 1995 sharman et al 1990 or employs a grammar and sometimes a dictionary to capture linguistic expertise directly black et al 1993a grinberg et al 1995 schabes 1992 but arguably at a less detailed and informative level than in the research reported here ,0,1,0
clearly the present research task is quite considerably harder than the parsing and tagging tasks undertaken in jelinek et al 1994 magerman 1995 black et al 1993b which would seem to be the closest work to ours and any comparison between this work and ours must be approached with extreme caution ,0,1,0
table 3 shows the differences between the treebank utilized in jelinek et al 1994 on the one hand and in the work reported here on the other is table 4 shows relevant lsfigures for average sentence length lraluing corpus and training set size for the ibm manuals corpus are approximate and cze fzom black et al 1993a ,0,1,0
by labelling treebn nodes with grramar rule names and not with phrasal and clausal nraes as in other nongrrarnarbased treebanks eyes and leech 1993 garside and mcenery 1993 marcus et al 1993 we gain access to all information provided by the grammar regarding each reebank node ,0,1,0
for example the feature 1 on the atr english grammar see below for a detailed description of a precursor to the grrraar see black et al 1993a ,0,1,0
we collected training samples from the brown corpus distributed with the penn treebank marcus et al1993 ,0,1,0
3 the effect of training corpus size a number of past research work on wsd such as leacock et al 1993 bruce and wiebe 1994 mooney 1996 were tested on a small number of words like line and interest ,0,1,0
edu abstract this paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12925 sentences of the wall street journal treebank corpus marcus et al 1993 ,0,1,0
1 introduction this paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12925 sentences of the wall street journal treebank corpus marcus et al 1993 ,0,1,0
test and training materials were derived from the brown corpus of american english all of which has been parsed and manually verified by the penn teebank project marcus et al 1993 and parts of which have been manually sensetagged by the wordnet group miller et al 1993 ,0,1,0
the approach combines statistical and knowledgebased methods but unlike many recent corpusbased approaches to sense disambiguation arowsky 1993 bruce and wiebe 1994 miller et al 1994 it takes as its starting point the assumption that senseannotated training text is not available ,0,1,0
3 probability model this paper takes a historybased approach black et al 1993 where each treebuilding procedure uses a probability model palb derived from pa b to weight any action a based on the available context or history b first we present a few simple categories of contextual predicates that capture any information in b that is useful for predicting a next the predicates are used to extract a set of features from a corpus of manually parsed sentences ,0,1,0
we have processed the susanne corpus sampson 1995 and penn treebank marcus et al 1993 to provide tables of word and subtree alignments ,0,1,0
statistical and information theoretic approaches hindle and rooth 1993 ratnaparkhi et al 1994collins and brooks 1995 franz 1996 using lexical collocations to determine ppa with statistical techniques was first proposed by hindle and rooth 1993 ,0,1,0
many mainstream systems and formalisms would satisfy these criteria including ones such as the university of pennsylvania treebank marcus et al 1993 which are purely syntactic though of course only syntactic properties could then be extracted ,1,0,0
because our algorithm does not consider the context given by the preceding sentences we have conducted the following experiment to see to what extent the discourse context could improve the performance of the wordsense disambiguation using the semantic concordance files miller et al 1993 we have counted the occurrences of content words which previously appear in the same discourse file ,0,1,0
both for the training and for the testing of our algorithm we used the syntactically analysed sentences of the brown corpus marcus 1993 which have been manually semantically tagged miller et al 1993 into semantic concordance files semcor ,0,1,0
1994 from the penn treebank marcus et al 1993 wsj corpus ,0,1,0
systems which are able to acquire a small number of verbal subcategorisation classes automatically from corpus text have been described by brent 1991 1993 and ushioda et al ,0,1,0
4 the experiment for our experiment we used a treebank grammar induced from sections 221 of the penn wall street journal text marcus et al 1993 with section 22 reserved for testing ,0,1,0
this program differs from earlier work in its almost complete lack of handcrafting relying instead on a very small corpus of penn wall street journal treebank text marcus et al 1993 that has been marked with coreference information ,0,1,0
dialogs speakers turns words fragments distinct words distinct wordspos singleton words singleton wordspos intonational phrases speech repairs 98 34 6163 58298 756 859 1101 252 350 10947 2396 table 1 size of the trains corpus 21 pos annotations our pos tagset is based on the penn treebank tagset marcus et al 1993 but modified to include tags for discourse markers and endofturns and to provide richer syntactic information heeman 1997 ,0,1,0
the data consists of 2544 main clauses from the wall street journal treebank corpus marcus et al 1993 ,0,1,0
we use the finitestate parses of fastu appelt et al 1993 for recognizing these entities but the method extends to any basic phrasal parser 4 ,0,1,0
this knowledge is represented in axiomatic form using the notation proposed in hobbs et al 1993 and previously implemented in tacitus ,0,1,0
the first one makes use of the advances in the parsing technology or on the availability of large parsed corpora eg trcebank marcus et al1993 to produce algorithms inspired by hobbs baseline method hobbs 1978 ,0,1,0
html provided by lynette hirschman syntactic structures in the style of the penn treebank marcus et al 1993 provided by ann taylor and an alternative annotation for the f0 aspects of prosody known as tilt taylor 1998 and provided by its inventor paul taylor ,0,1,0
it us widely acknowledged that word sense dsamblguatmn wsd us a central problem m natural language processing in order for computers to be able to understand and process natural language beyond simple keyword matching the problem of dsamblguatmg word sense or dlscermng the meamng of a word m context must be effectively dealt with advances in wsd v ill have slgmficant impact on apphcatlons hke information retrieval and machine translation for natural language subtasks hke partofspeech tagging or sntactm parsing there are relatlvely well defined and agreedupon cnterm of what it means to have the correct part of speech or syntactic structure assigned to a word or sentence for instance the penn treebank corpus marcus et al 1993 proidet large repotory of texts annotated wth partofspeech and sntactm structure mformatlon tvo independent human annotators can achieve a high rate of agreement on assigning partofspeech tags to words m a gven sentence unfortunately ths us not the case for word sense assignment frstly it is rarely the case that any two dictionaries will have the same set of sense defimtmns for a gven word different dctlonanes tend to carve up the semantic space m a different way so to speak secondly the hst of senses for a word m a typical dmtmnar tend to be rather refined and comprehensive this is especmlly so for the commonly used words which have a large number of senses the sense dustmctmn between the different senses for a commonly used word m a dctmnary hke wordnet miller 1990 tend to be rather fine hence two human annotators may genuinely dusagree m their sense assignment to a word m context the agreement rate between human annotators on word sense assignment us an important concern for the evaluatmn of wsd algorithms one would prefer to define a dusamblguatlon task for which there us reasonably hlgh agreement between human annotators the agreement rate between human annotators will then form the upper ceiling against whmh to compare the performance of wsd algorithms for instance the senseval exerclse has performed a detaded study to find out the raterannotator agreement among ts lexicographers taggrog the word senses kllgamff 1998c kllgarnff 1998a kflgarrlff 1998b 2 a case study in thispaper we examine the ssue of raterannotator agreement by comparing the agreement rate of human annotators on a large sensetagged corpus of more than 30000 instances of the most frequently occurring nouns and verbs of enghsh this corpus is the intersection of the wordnet semcor corpus miller et al 1993 and the dso corpus ng and lee 1996 ng 1997 which has been independently tagged wlth the refined senses of wordnet by two separate groups of human annotators the semcor corpus us a subset of the brown corpus tagged with vordnet senses and consists of more than 670000 words from 352 text files sense taggmg was done on the content words nouns erbs adjectives and adverbs m this subset the dso corpus consists of sentences drawn from the brown corpus and the wall street journal for each word w from a hst of 191 frequently occurring words of enghsh 121 nouns and 70 verbs sentences containing w m singular or plural form and m its various reflectional verb form are selected and each word occurrence w s tagged wth a sense from wordnet there s a total of about 192800 sentences in the dso corpus m which one word occurrence has been sensetagged m each sentence the intersection of the semcor corpus and the dso corpus thus consists of brown corpus sentences m which a word occurrence w is sensetagged m each sentence where w is one ofthe 191 frequently occurrmg english nouns or verbs since this common pomon has been sensetagged by two independent groups of human annotators t serves as our data set for investigating interannotator agreement in this paper 3 sentence matching to determine the extent of interannotator agreement the first step s to match each sentence m semcor to its corresponding counterpart in the dso corpus this step s comphcated by the following factors 1 although the intersected portion of both corpora came from brown corpus they adopted different tokemzatmn convention and segmentartan into sentences differed sometimes 2 the latest versmn of semcor makes use of the senses from wordnet 1 6 whereas the senses used m the dso corpus were from wordnet 15 1 to match the sentences we first converted the senses m the dso corpus to those of wordnet 1 6 we ignored all sentences m the dso corpus m which a word is tagged with sense 0 or 1 a word is tagged with sense 0 or 1 ff none of the given senses m wordnft applies 4 sentence from semcor is considered to match one from the dso corpus ff both sentences are exactl ldentcal or ff the differ only m the preence or absence of the characters permd or hyphen for each remaining semcor sentence taking into account word ordering ff 75 or more of the words m the sentence match those in a dso corpus sentence then a potential match s recorded these i kctualy the wordqet senses used m the dso corpus were from a shght variant of the official wordnei 1 5 release ths ssas brought to our attention after the pubhc release of the dso corpus potential matches are then manually verffied to ensure that they are true matches and to eed out any false matches using this method of matching a total of 13188 sentencepalrs contasnmg nouns and 17127 sentencepars containing verbs are found to match from both corpora ymldmg 30315 sentences which form the intersected corpus used m our present study 4 the kappa statistic suppose there are n sentences m our corpus where each sentence contains the word w assume that w has m senses let 4 be the number of sentences which are assigned identical sense b two human annotators then a simple measure to quantify the agreement rate between two human annotators is pc where pc an the drawback of this simple measure is that it does not take into account chance agreement between two annotators the kappa statistic a cohen 1960 is a better measure of raterannotator agreement which takes into account the effect of chance agreement it has been used recently wthm computatmnal hngustlcs to measure raterannotator agreement bruce and wmbe 1998 carletta 1996 veroms 1998 let cj be the sum of the number of sentences which have been assigned sense 3 by annotator 1 and the number of sentences whmh have been assigned sense 3 by annotator 2 then pp 1p where m jl and pe measures the chance agreement between two annotators a kappa alue of 0 indicates that the agreement is purely due to chance agreement whereas a kappa alue of 1 indicates perfect agreement a kappa alue of 0 8 and above is considered as mdmatmg good agreement carletta 1996 table 1 summarizes the interannotator agreement on the mtersected corpus the first becond row denotes agreement on the nouns xerbs whle the lass row denotes agreement on all words combined the aerage reported m the table is a smpie average of the individual value of each word the agreement rate on the 30315 sentences as measured by p is 57 this tallies with the figure reported n our earlier paper ng and lee 1996 where we performed a quick test on a subset of 5317 sentencesn the intersection of both the semcor corpus and the dso corpus 10 mm m m m m m mm m m m m mm m m m type num of v ords a n p avg nouns 121 7676 13188 i 0 582 0 300 verbs 70 9520 17127 i 0 555 0 347 all i 191 i 17196 30315 i 056t 0317 table 1 raw interannotator agreement 5 algorithm since the raterannotator agreement on the intersected corpus is not high we would like to find out how the agreement rate would be affected if different sense classes were in use in this section we present a greedy search algorithm that can automatmalb derive coarser sense classes based on the sense tags assigned by two human annotators the resulting derived coarse sense classes achmve a higher agreement rate but we still maintain as many of the original sense classes as possible the algorithm is given m figure 1 the algorithm operates on a set of sentences where each sentence contains an occurrence of the word w whmh has been sensetagged by two human annotators at each iteration of the algorithm tt finds the pair of sense classes ct and cj such that merging these two sense classes results in the highest t value for the resulting merged group of sense classes it then proceeds to merge cz and c thin process is repeated until the value reaches a satisfactory value t which we set as 0 8 note that this algorithm is also applicable to deriving any coarser set of classes from a refined set for any nlp tasks in which prior human agreement rate may not be high enough such nlp tasks could be discourse tagging speechact categorization etc 6 results for each word w from the list of 121 nouns and 70 verbs e applied the greedy search algorithm to each set of sentences in the intersected corpus contaming w for a subset of 95 words 53 nouns and 42 verbs the algorithm was able to derive a coarser set of 2 or more senses for each of these 95 words such that the resulting kappa alue reaches 0 8 or higher for the other 96 words m order for the kappa value to reach 0 8 or higher the algorithm collapses all senses of the ord to a single trivial class table 2 and 3 summarizes the results for the set of 53 nouns and 42 erbs respectively table 2 mdcates that before the collapse of sense classes these 53 nouns have an average of 7 6 senses per noun there is a total of 5339 sentences in the intersected corpus containing these nouns of which 3387 sentences were assigned the same sense by the two groups of human annotators the average kappa statistic computed as a simple average of the kappa statistic of he mdlwdual nouns is 0 463 after the collapse of sense classes by the greedy search algorithm the average number of senses per noun for these 53 nouns drops to 40 howeer the number of sentences which have been asmgned the same coarse sense by the annotators increases to 5033 that is about 94 3 of the sentences have been assigned the same coarse sense and that the average kappa statistic has improved to 0 862 mgmfymg high raterannotator agreement on the derived coarse senses table3 gles the analogous figures for the 42 verbs agmn mdmatmg that high agreement is achieved on the coarse sense classes dened for verbs 7 discussion our findings on raterannotator agreement for word sense tagging indicate that for average language users it is quite dlcult to achieve high agreement when they are asked to assign refned sense tags such as those found in wordnet given only the scanty definition entries m the wordnet dlctionary and a few or no example sentences for the usage of each word sense thin observation agrees wlth that obtmned m a recent study done by veroms 1998 where the agreement on sensetagging by naive users was also not hlgh thus it appears that an average language user is able to process language wlthout needing to perform the task of dlsamblguatmg word sense to a very finegrained resolutmn as formulated m a tradltlonal dmtlonary in contrast expert lexicographers tagged the ord sense in the sentences used m the senseval exerclse where high raterannotator agreement was reported there are also fuller dlctlonary entries m the hector dlctlonary used and more eamples showing the usage of each word sense m hector these factors are likely to have contributed to the difference in raterannotator agreement observed m the three studies conducted we also examined the coarse sense classes derived by the greedy search algorithm vve found some interesting groupings of coarse senses for nouns which e hst in table 4 from table 4 it is apparent that the greedy search algorithm can derive interesting groupings of word senses that correspond to human mtmtwe judgment of sense grazulanty it is clear that some of the disagreement between the two groups of human annotators can be attributed solely to the overly refined senses of wordnet as an example there is a total ii loop let ct c m denote the current m sense classes oo for all z3 such that 1 3 m let ccw 1 denote the resulting m 1 sense classes by mergmg c and c 3 compute c ct ff c c4x then cct z end for merge the sense class c ,1,0,0
3 tagging 31 corpus to facilitate comparison with previous results we used the upenn treebank corpus marcus et al 1993 ,0,1,0
1998 present a probabilistic model for pronoun resolution trained on a small subset of the penn treebank wall street journal corpus marcus et al 1993 ,0,1,0
as an example consider the fiat np structures that are in the penn treebank marcus et al 1993 ,0,1,0
some works woods et al 1972 boguraev 1979 marcus et al 1993 suggested several strategies that based their 231 decisionmaking on the relationships existing between predicates and argumentswhat katz and fodor 1963 called selectional restrictions ,0,1,0
a very impor232 author best hindle and rooth 1993 800 resnik and hearst 1993 839 wn resnik and hearst 1993 750 ratnaparkhi et al ,0,1,0
in this data set the 4tuples of the test and training sets were extracted from penn treebank wall street journal marcus et al 1993 ,0,1,0
the data for all our experiments was extracted from the penn treebank ii wall street journal wsj corpus marcus et al 1993 ,0,1,0
experiments we have conducted a series of lexical acquisition experiments with the above algorithm on largescale english corpora eg the brown corpus francis and kucera 1982 and the ptb wsj corpus marcus et al 1993 ,0,1,0
the wsjnpvp set consists of partof speech tagged wall street journal material marcus santorini marcinkiewicz 1993 supplemented with syntactic tags indicating noun phrase and verb phrase boundaries daelemans et al 1999iii ,0,1,0
the figures given above were the original 1998 results for the system in argamon et al 1998 which came from training and testing on data derived from the penn treebank corpus marcus et al 1993 in which the added null elements like null subjects were left in ,0,1,0
our training and test corpora for instance are lessthangargantuan compared to such collections as the penn treebank marcus et al 1993 ,1,0,0
many systems eg the kernel system palmer et al 1993 use these relationships as an intermediate form when determining the semantics of syntactically parsed text ,0,1,0
training data our source for syntactically annotated training data was the penn treebank marcus et al 1993 ,0,1,0
techniques for weakening the independence assumptions made by the ibm models 1 and 2 have been proposed in recent work brown et al 1993 berger et al 1996 och and weber 98 wang and waibel 98 wu and wong 98 ,0,1,0
our approach differs from the corpusbased surface generation approaches of langkilde and knight 1998 and berger et al 1996 ,0,1,0
there are more sophisticated surface generation packages such as fufsurge elhadad and robin 1996 kpml bateman 1996 mumble meteer et al 1987 and realpro lavoie and rambow 1997 which produce natural language text from an abstract semantic representation ,0,1,0
the only trainable approaches known to the author to surface generation are the purely statistical machine translation mt systems such as berger et al 1996 and the corpusbased generation system described in langkilde and knight 1998 ,0,1,0
the form of the maximum entropy probability model is identical to the one used in berger et al 1996 ratnaparkhi 1998 k fwiwi1wi2atri yiji otj pwilwil wi2attri zwil wi2 attri k to t jl where wi ranges over v t3 stop ,0,1,0
the features used in nlg2 are described in the next section and the feature weights aj obtained from the improved iterative scaling algorithm berger et al 1996 are set to maximize the likelihood of the training data ,0,1,0
this is concordant with the usage in the maximum entropy literature berger et al 1996 ,0,1,0
however the naive bayes classifier has been found to perform well for wordsense disambiguation both here and in a variety of other works eg bruce and wiebe 1994a gale et al 1992 leacock et al 1993 and mooney 1996 ,0,1,0
because their joint distributions have such closedform expressions the parameters can be estimated directly from the training data without the need for an iterative fitting procedure as is required for example to estimate the parameters of maximum entropy models berger et al 1996 ,0,1,0
the significance of g 2 based on the exact conditional distribution does not rely on an asymptotic approximation and is accurate for sparse and skewed data samples pedersen et al 1996 42 information criteria the family of model evaluation criteria known as information criteria have the following expression ic g 2 x dof 3 where g and dof are defined above ,0,1,0
5 experimental data the sensetagged text and feature set used in these experiments are the same as in bruce et al 1996 ,0,1,0
1 2 show the examples of wrious transliterations in ktset 20park et al 1996 ,0,1,0
4 maximum entropy to explain our method we lriefly desribe the conept of maximum entroly recently many allnoaches lased on the maximum entroiy lnodel have teen applied to natural language processing berger el al 994 berger et al 1996 pietra et al 1997 ,0,1,0
wu 1996 adopted chammls that eliminate syntactically unlikely alignments and wang et al ,0,1,0
a2 maximumentropy method the maximumentropy method is useful with sparse data conditions and has been used by many researchers berger et al 1996 ratnaparkhi 1996 ratnaparkhi 1997 borthwick el al 1998 uchimoto et al 1999 ,0,1,0
for every class the weights of the active features are combined and the best scoring class is chosen berger et al 1996 ,0,1,0
we implemented these models within an maximum entropy framework berger et al 1996 ristad 1997 ristad 1998 ,0,1,0
we used a maximummatching algorithm and a dictionary compiled from the ctb sproat et al 1996 xue 2001 to do segmentation and trained a maximum entropy partofspeech tagger ratnaparkhi 1998 and tagbased parser bikel and chiang 2000 on the ctb to do tagging and parsing4 then the same feature extraction and modeltraining was done for the pdn corpus as for the ctb ,0,1,0
under the maximum entropy framework berger et al 1996 evidence from different features can be combined with no assumptions of feature independence ,0,1,0
one is to find unknown words from corpora and put them into a dictionary eg mori and nagao 1996 and the other is to estimate a model that can identify unknown words correctly eg kashioka et al 1997 nagata 1999 ,0,1,0
in previous research on splitting sentences many methods have been based on wordsequence characteristics like ngram lavie et al 1996 berger et al 1996 nakajima and yamamoto 2001 gupta et al 2002 ,0,1,0
the candidates of unknown words can be generated by heuristic rulesmatsumoto et al 2001 or statistical word models which predict the probabilities for any strings to be unknown words sproat et al 1996 nagata 1999 ,0,1,0
in the above equation pti and pwit are estimated by the maximumlikelihood method and the probability of a poc tag ti given a character wi ptijwiti 2 tpoc is estimated using me models berger et al 1996 ,0,1,0
the statistical classifier used in the experiments reported in this paper is a maximum entropy classifier berger et al 1996 ratnaparkhi 1997b ,0,1,0
furthermore good results have been produced in other areas of nlp research using maximum entropy techniques berger et al 1996 koeling 2001 ratnaparkhi 1997a ,1,0,0
3 maximum entropy me models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible berger et al 1996 ,0,1,0
following recent research about disambiguation models on linguistic grammars abney 1997 johnson et al 1999 riezler et al 2002 clark and curran 2003 miyao et al 2003 malouf and van noord 2004 we apply a loglinear model or maximum entropy model berger et al 1996 on hpsg derivations ,0,1,0
the maximum entropy approach berger et al 1996 is known to be well suited to solve the classification problem ,1,0,0
3 implementation 31 pronoun resolution model we built a machine learning based pronoun resolution engine using a maximum entropy ranker model berger et al 1996 similar with denis and baldridges model denis and baldridge 2007 ,0,1,0
preparing an aligned abbreviation corpus we obtain the optimal combination of the features by using the maximum entropy framework berger et al 1996 ,0,1,0
we directly model the conditional probability of the alignment a given x and y using the maximum entropy framework berger et al 1996 paxy expfaxysummationdisplay acxy expfaxy ,0,1,0
we utilize maximum entropy maxent model berger et al 1996 to design the basic classifier used in active learning for wsd and tc tasks ,0,1,0
62 experimental settings we utilize a maximum entropy me model berger et al 1996 to design the basic classifier for wsd and tc tasks ,0,1,0
when we have a junction tree for each document we can efficiently perform belief propagation in order to compute argmax in equation 1 or the marginal probabilities of cliques and labels necessary for the parameter estimation of machine learning classifiers including perceptrons collins 2002 and maximum entropy models berger et al 1996 ,0,1,0
in the following experiments we run two machine learning classifiers bayes point machines bpm herbrich et al 2001 and the maximum entropy model me berger et al 1996 ,0,1,0
one is how to learn a statistical model to estimate the conditional probability and the other is how to generate confusion set c of a given query q 41 maximum entropy model for query spelling correction we take a featurebased approach to model the posterior probability specifically we use the maximum entropy model berger et al 1996 for this task exp 1 exp 1 2 where exp 1 is the normalization factor is a feature function defined over query q and correction candidate c while is the corresponding feature weight ,0,1,0
optimization approaches which aim at selecting those examples that optimize some algorithmdependent objective function such as prediction variance cohn et al 1996 and heuristic methods with uncertainty sampling lewis and catlett 1994 and querybycommittee qbc seung et al 1992 just to name the most prominent ones ,0,1,0
42 classifier and features for our al framework we decided to employ a maximum entropy me classifier berger et al 1996 ,0,1,0
2 related work a number of researchers brown et al 1992 berger et al 1996 niessen and ney 2004 xia and mccord 2004 collins et al 2005 have described approaches that preprocess the source language input in smt systems ,0,1,0
we utilize a maximum entropy me model berger et al 1996 to design the basic classifier used in active learning for wsd ,0,1,0
the first lr model for each language uses maximum entropy classification berger et al 1996 to determine possible parser actions and their probabilities4 ,0,1,0
1 here the candidate generator gens enumerates candidates of destination correct strings and the scorer pts denotes the conditional probability of the string t for the given s the scorer was modeled by a noisychannel model shannon 1948 brill and moore 2000 ahmad and kondrak 2005 and maximum entropy framework berger et al 1996 li et al 2006 chen et al 2007 ,0,1,0
the classification is performed with a statistical approach built around the maximum entropy maxent principle berger et al 1996 that has the advantage of combining arbitrary types of information in making a classification decision ,1,0,0
the ijj1m weights are estimated during the training phase to maximize the likelihood of the data berger et al 1996 ,0,1,0
to estimate the parameters of the memmpred model we turn to the successful maximum entropy berger et al 1996 parameter estimation method ,1,0,0
2 maximum entropy models maximum entropy me models berger et al 1996 manning and klein 2003 also known as loglinear and exponential learning models provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging named entity recognition etc maximum entropy models can integrate features from many heterogeneous information sources for classification ,1,0,0
1113 berger et al 1996 ,0,1,0
14 where i is the parameter to be estimated and f i a b is a feature function corresponding to i berger et al 1996 ratnaparkhi 1997 pe p e g productdisplay i pep i ep i1 ik eg ik ik 11 pc p e g e p 12 productdisplay i pcp i cp i1 ik eg ep ik ik pc g e g e p c p 13 productdisplay i pcg i cg i1 ik eg ep cp ik ik pba exp summationtext i i f i a b summationtext b prime exp summationtext i i f i a b prime 14 f i a b is a binary function returning true or false based on context a and output b if f i a b1 its corresponding model parameter i contributes toward conditional probability pba berger et al 1996 ratnaparkhi 1997 ,0,1,0
dtm2 introduced in ittycheriah and roukos 2007 expresses the phrasebased translation task in a unified loglinear probabilistic framework consisting of three components i a prior conditional distribution p0s ii a number of feature functions i that capture the translation and language model effects and iii the weights of the features i that are estimated under maxent berger et al 1996 as in 1 pts p0tjsz expsummationdisplay i iitjs 1 here j is the skip reordering factor for the phrase pair captured by i and represents the jump from the previous source word and z is the per source sentence normalization term ,0,1,0
as described in section 4 we define the problem of term variation identifica1484 tion as a binary classification task and build two types of classifiers according to the maximum entropy model berger et al 1996 and the mart algorithm friedman 2001 where all term similarity metrics are incorporated as features and are jointly optimized ,0,1,0
21 loglinear models the loglinear model llm or also known as maximumentropy model berger et al 1996 is a linear classifier widely used in the nlp literature ,0,1,0
by introducing the hidden word alignment variable a the following approximate optimization criterion can be applied for that purpose e argmaxe pre f argmaxe summationdisplay a prea f argmaxea prea f exploiting the maximum entropy berger et al 1996 framework the conditional distribution prea f can be determined through suitable real valued functions called features hrefar 1r and takes the parametric form pea f exp rsummationdisplay r1 rhrefa the itcirst system chen et al 2005 is based on a loglinear model which extends the original ibm model 4 brown et al 1993 to phrases koehn et al 2003 federico and bertoldi 2005 ,0,1,0
hence either the best translation hypothesis is directly extracted from the word graph and output or an nbest list of translations is computed tran et al 1996 ,0,1,0
in order to estimate the conditional distributions shown in table 1 we use the general technique of choosing the maxent distribution that properly estimates the average of each feature over the training data berger et al 1996 ,1,0,0
these feature vectors and the associated parser actions are used to train maximum entropy models berger et al 1996 ,0,1,0
we chose to train maximum entropy models berger et al 1996 ,0,1,0
so far most previous work on domain adaptation for parsing has focused on datadriven systems gildea 2001 roark and bacchiani 2003 mcclosky et al 2006 shimizu and nakagawa 2007 ie systems employing constituent or dependency based treebank grammars charniak 1996 ,0,1,0
berger et al 1996 1we are overloading the word state to mean arabic word position ,0,1,0
we use a simple single parameter distribution with 80 throughout pkme pkml k wordtophrase alignment alignment is a markov process that specifies the lengths of phrases and their alignment with source words pak1hk1k1 kme kproductdisplay k1 pakhkkak1k1e kproductdisplay k1 pakak1hkldhknkeak the actual wordtophrase alignment ak is a firstorder markov process as in hmmbased wordtoword alignment vogel et al 1996 ,0,1,0
the bigram translation probability t2ffe specifies the likelihood that target word f is to follow f in a phrase generated by source word e 170 21 properties of the model and prior work the formulation of the wtop alignment model was motivated by both the hmm word alignment model vogel et al 1996 and ibm model4 with the goal of building on the strengths of each ,0,1,0
in fact the wtop model is a segmental hidden markov model ostendorf et al 1996 in which states emit observation sequences ,0,1,0
the bigram translation probability relies on word context known to be helpful in translation berger et al 1996 to improve the identification of target phrases ,1,0,0
the mbt pos tagger daelemans et al 1996 is used to provide pos information ,0,1,0
as the taskisanimportantprecursortomanynaturallanguage processing systems it receives a lot of attentions in the literature for the past decade wu and tseng 1993 sproat et al 1996 ,0,1,0
3 maxent model and features 31 maxent model for nor the principle of maximum entropy maxent model is that given a collection of facts choose a model consistent with all the facts but otherwise as uniform as possible berger et al 1996 ,0,1,0
there are other types of variations for phrases for example insertion deletion or substitution of words and permutation of words such as view point and point of view are such variations daille et al 1996 ,0,1,0
then we build a classier learned by training data using a maximum entropy model berger et al 1996 and the features related to spelling variations in table 3 ,0,1,0
uses maximum entropy berger et al 1996 classification trained on jnlpba kim et al 2004 ner ,0,1,0
1996 warnke et al ,0,1,0
the idea caught on very quickly suhm and waibel 1994 mast et al 1996 warnke et al ,0,1,0
suhm and waibel 1994 and eckert gallwitz and niemann 1996 each condition a recognizer lm on lefttoright da predictions and are able to 366 stolcke et al dialogue act modeling show reductions in word error rate of 1 on taskoriented corpora ,0,1,0
automatic segmentation of spontaneous speech is an open research problem in its own right mast et al 1996 stolcke and shriberg 1996 ,0,1,0
feature selection methods have been proposed in the maximumentropy literature by several authors ratnaparkhi roukos and ward 1994 berger della pietra and della pietra 1996 della pietra della pietra and lafferty 1997 papineni roukos and ward 1997 1998 mccallum 2003 zhou et al 2003 riezler and vasserman 2004 ,0,1,0
64 feature selection methods a number of previous papers berger della pietra and della pietra 1996 ratnaparkhi 1998 della pietra della pietra and lafferty 1997 mccallum 2003 zhou et al 2003 riezler and vasserman 2004 describe feature selection approaches for loglinear models applied to nlp problems ,0,1,0
more recent work mccallum 2003 zhou et al 2003 riezler and vasserman 2004 has considered methods for speeding up the feature selection methods described in berger della pietra and della pietra 1996 ratnaparkhi 1998 and della pietra della pietra and lafferty 1997 ,0,0,1
among the most widely studied is the gibbs distribution mark miller and grenander 1996 mark et al 1996 mark 1997 abney 1997 ,0,1,0
these distributions are modeled using a maximum entropy formulation berger et al 1996 using training data which consists of human judgments of question answer pairs ,0,1,0
berger et al 1996 ,0,1,0
for mention detection we use approaches based on maximum entropy maxent henceforth berger et al 1996 and robust risk minimization rrm henceforth 1for a description of the ace program see httpwwwnistgovspeechtestsace ,0,1,0
maximum entropy modeling as previously indicated the weightbased scheme of ll suggests maxent modeling berger et al 1996 as a particularly natural choice for a machine learning approach ,0,1,0
1 introduction conditional maximum entropy maxent models have been widely used for a variety of tasks including language modeling rosenfeld 1994 partofspeech tagging prepositional phrase attachment and parsing ratnaparkhi 1998 word selection for machine translation berger et al 1996 and finding sentence boundaries reynar and ratnaparkhi 1997 ,0,1,0
a major issue in maxent training is how to select proper features and determine the feature targets berger et al 1996 jebara and jaakkola 2000 ,0,1,0
3 feature selection berger et al 1996 proposed an iterative procedure of adding news features to feature set driven by data ,0,1,0
given a collection of facts me chooses a model consistent with all the facts but otherwise as uniform as possible berger et al 1996 ,0,1,0
maximum entropy me models have been used in bilingual sense disambiguation word reordering and sentence segmentation berger et al 1996 parsing pos tagging and pp attachment ratnaparkhi 1998 machine translation och and ney 2002 and framenet classification fleischman et al 2003 ,0,1,0
32 learning algorithm for learning coreference decisions we used a maximum entropy berger et al 1996 model ,0,1,0
maximum entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible berger et al 1996 ,0,1,0
the algorithm employs the opennlp maxent implementation of the maximum entropy classification algorithm berger et al 1996 to develop word sense recognition signatures for each lemma which predicts the most likely sense for the lemma according to the context in which the lemma occurs ,0,1,0
the best prosodic label sequence is then l argmax l nproductdisplay i pli 6 to estimate the conditional distribution pli we use the general technique of choosing the maximum entropy maxent distribution that estimates the average of each feature over the training data berger et al 1996 ,0,1,0
we report results on the boston university bu radio speech corpus ostendorf et al 1995 and boston directions corpus bdc hirschberg and nakatani 1996 two publicly available speech corpora with manual tobi annotations intended for experiments in automatic prosody labeling ,0,1,0
once the set of features functions are selected algorithm such as improved iterative scaling berger et al 1996 or sequential conditional generalized iterative scaling goodman 2002 can be used to find the optimal parameter values of fkg and fig ,0,1,0
model parameters are estimated using maximum entropy berger et al 1996 ,0,1,0
with handlabeled data m can be learnt via generalized iterative scaling algorithm gis darroch and ratcliff 1972 or improved iterative scaling iis berger 367 et al 1996 ,0,1,0
this sequential property is well suited to hmms vogel et al 1996 in which the jumps from the current aligned position can only be forward ,0,1,0
for the classifier we used the opennlp maxent implementation maxentsourceforgenet of the maximum entropy classification algorithm berger et al 1996 ,0,1,0
35 maximum entropy model in order to build a unified probabilistic query alteration model we used the maximum entropy approach of beger et al 1996 which li et al ,0,1,0
we decided to use the class of maximum entropy models which are probabilistically sound can make use of possibly many overlapping features and can be trained efficiently berger et al 1996 ,1,0,0
21 the standard machine learning approach we use maximum entropy maxent classification berger et al 1996 in conjunction with the 33 features described in ng 2007 to acquire a model pc for determining the probability that two mentions mi and mj are coreferent ,0,1,0
research in the first category aims to identify specific types of nonanaphoric phrases with some identifying pleonastic it using heuristics eg paice and husk 1987 lappin and leass 1994 kennedy and boguraev 1996 supervised approaches eg evans 2001 muller 2006 versley et al ,0,1,0
our approach is to use maximum entropy models berger et al 1996 to learn a suitable mapping from features derived from the words in the asr output to semantic frames ,0,1,0
in teevan et al 1996 it was observed that a significant percent of the queries made by a user in a search engine are associated to a repeated search ,0,1,0
23 classifier training we chose maximum entropy berger et al 1996 as our primary classifier since it had been successfully applied by the highest performing systems in both the semeval2007 preposition sense disambiguation task ye and baldwin 2007 and the general word sense disambiguation task tratz et al 2007 ,1,0,0
23 classifier training we chose maximum entropy berger 1996 as our primary classifier because the highest performing systems in both the semeval2007 preposition sense disambiguation task ye and baldwin 2007 and the general word sense disambiguation task tratz et al 2007 used it ,1,0,0
using the me principle we can combine information from a variety of sources into the same language model berger et al 1996 rosenfeld 1996 ,0,1,0
berger et al 1996 applies this approach to the socalled ibm candide system to build context dependent models compute automatic sentence splitting and to improve word reordering in translation ,0,1,0
similar techniques are used in papineni et al 1996 papineni et al 1998 for socalled direct translation models instead of those proposed in brown et al 1993 ,0,1,0
other authors have applied this approach to language modeling rosenfeld 1996 martin et al 1999 peters and klakow 1999 ,0,1,0
the resulting model has an exponential form with free parameters a102 a91 a24a94a93 a8 a87 a24 a10a11a10a11a10 a24a46a95 the parameter values which maximize the likelihood for a given training corpus can be computed with the socalled gis algorithm general iterative scaling or its improved version iis pietra et al 1997 berger et al 1996 ,0,1,0
in this work we use the following contextual information a3 target context as in berger et al 1996 we consider a window of 3 words to the left and to the right of the target word considered ,0,1,0
in statistical computational linguistics maximum conditional likelihood estimators have mostly been used with general exponential or maximum entropy models because standard maximum likelihood estimation is usually computationally intractable berger et al 1996 della pietra et al 1997 jelinek 1997 ,0,1,0
1 introduction conditional maximum entropy models have been used for a variety of natural language tasks including language modeling rosenfeld 1994 partofspeech tagging prepositional phrase attachment and parsing ratnaparkhi 1998 word selection for machine translation berger et al 1996 and finding sentence boundaries reynar and ratnaparkhi 1997 ,0,1,0
one solution would be to apply the maximum entropy estimation technique maxent berger et al 1996 to all of the three components of the slm or at least to the constructor ,0,1,0
an especially wellfounded framework for doing this is maximum entropy berger et al 1996 ,0,1,0
various learning models have been studied such as hidden markov models hmms rabiner and juang 1993 decision trees breiman et al 1984 and maximum entropy models berger et al 1996 ,0,1,0
maximum entropy can be used to improve ibmstyle translation probabilities by using features such as improvements to pfe in berger et al 1996 ,1,0,0
it has been observed that words close to each other in the source language tend to remain close to each other in the translation vogel et al 1996 ker and change 1997 ,0,1,0
we used the maximum entropy approach5 berger et al 1996 as a machine learner for this task ,0,1,0
the other main difference is the apparently nonlocal nature of the problem which motivates our choice of a maximum entropy me model for the tagging task berger et al 1996 ,0,1,0
we implemented this model within an me modeling framework jaynes 1957 jaynes 1979 berger et al 1996 ,0,1,0
they use a conditional model based on collins 1996 which as the authors acknowledge has a number of theoretical deficiencies thus the results of clark et al provide a useful baseline for the new models presented here ,0,1,0
setting the gradient to zero yields the usual maximum entropy constraints berger et al 1996 except that in this case the empirical values are themselves expectations over all derivations leading to each gold standard dependency structure ,0,1,0
we use maximum entropy model berger et al 1996 for both the mentionpair model 9 and the entitymention model 8 a83a84a1a86a85a88a87 a43 a44 a71 a43 a16 a5a13a7 a55a35a34a23a36 a6a35a37 a6a39a38a40a6a42a41 a31a44a43a3a45a31 a6 a45a46a48a47a24a49 a50 a1 a43 a44 a71 a43 a16 a5 a71 10 a83a84a1a4a85 a87 a55 a81 a71 a43 a16 a5a13a7 a55a35a34 a36 a6 a37 a6a39a38a40a6a42a41 a11a7a32 a45a31 a6 a45a46a48a47 a49 a50 a1 a55a39a81 a71 a43 a16 a5 a71 11 wherea57 a16 a1a51a8 a71a52a8 a71a90a85a73a5 is a feature and a53 a16 is its weight a50 a1a33a8 a71a54a8a5 is a normalizing factor to ensure that 10 or 11 is a probability ,0,1,0
speaker ranking accuracy table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets the performance is 8939 when using all feature sets and reaches 902 after applying gaussian smoothing and using incremental feature selection as described in berger et al 1996 and implemented in the yasmetfs package6 note that restricting ourselves to only backward looking features decreases the performance significantly as we can see in table 2 ,0,1,0
we consider three learning algorithms namely the c45 decision tree induction system quinlan 1993 the ripper rule learning algorithm cohen 1995 and maximum entropy classification berger et al 1996 ,0,1,0
216 the maximum entropy principle berger et al 1996 is to nd a model p argmax pc hp which means a probability model pyx that maximizes entropy hp ,0,1,0
 maxent zhang les c implementation8 of maximum entropy modelling berger et al 1996 ,0,1,0
54 maximum entropy maximum entropy has been proven to be an effective method in various natural language processing applications berger et al 1996 ,1,0,0
statistical approaches which depend on a set of unknown parameters that are learned from training data try to describe the relationship between a bilingual sentence pair brown et al 1993 vogel and ney 1996 ,0,1,0
we use a standard maximum entropy classifier berger et al 1996 implemented as part of mallet mccallum 2002 ,0,1,0
for this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems eg see alshawi 1996 wu 1997 yamada and knight 2001 gildea 2003 melamed 2004 graehl and knight 2004 och et al 2004 xia and mccord 2004 ,0,1,0
212 research on syntaxbased smt a number of researchers alshawi 1996 wu 1997 yamada and knight 2001 gildea 2003 melamed 2004 graehl and knight 2004 galley et al 2004 have proposed models where the translation process involves syntactic representations of the source andor target languages ,0,1,0
a number of other re532 searchers berger et al 1996 niessen and ney 2004 xia and mccord 2004 have described previous work on preprocessing methods ,0,1,0
we employ loglinear models berger et al 1996 for the disambiguation ,0,1,0
however in order to cope with the prediction errors of the classi er we approximate a74a51a18a77a76 a28 with an a80 gram language model on sequences of the re ned tag labels a38a58a39 a41 a81 a43a82a44a47a46a83a48a47a50a75a44a15a52 a53a9a54a49a84 a53a9a54a83a84a49a85a9a86a13a87a89a88a91a90 a55a57a56 a38a40a39 a81 a59a60a42a61 2 a92 a44a47a46a83a48a47a50a75a44a15a52 a53a9a54 a84 a53a9a54a83a84a49a85a9a86a13a87a89a88a91a90 a93 a94a96a95 a55a57a56a98a97a66a99 a95 a59a100a27a61 3 in order to estimate the conditional distribution a101 a18a20a19a15a21 a1 a68 a72 a28 we use the general technique of choosing the maximum entropy maxent distribution that properly estimates the average of each feature over the training data berger et al 1996 ,0,1,0
7however the algorithms shares many common points with iterative algorithm that are known to converge and that have been proposed to find maximum entropy probability distributions under a set of constraints berger et al 1996 ,0,1,0
21 conditional maximum entropy model the goal of cme is to find the most uniform conditional distribution of y given observation x xyp subject to constraints specified by a set of features yxf i where features typically take the value of either 0 or 1 berger et al 1996 ,0,1,0
this leads to a good amount of work in this area ratnaparkhi et al 1994 berger et al 1996 pietra et al 1997 zhou et al 2003 riezler and vasserman 2004 in the most basic approach such as ratnaparkhi et al ,0,1,0
579 the maxent algorithm associates a set of weights iji1nj1m with the features which are estimated during the training phase to maximize the likelihood of the data berger et al 1996 ,0,1,0
our appoach is based on maximum entropy maxent henceforth technique berger et al 1996 ,0,1,0
there have been many studies on pos guessing of unknown words mori and nagao 1996 mikheev 1997 chen et al 1997 nagata 1999 orphanos and christodoulakis 1999 ,0,1,0
p0tw is calculated by me models as follows berger et al 1996 p0tw 1yw exp braceleftbigg hsummationdisplay h1 hghwt bracerightbigg 20 709 language features english prefixes of 0 up to four characters suffixes of 0 up to four characters 0 contains arabic numerals 0 contains uppercase characters 0 contains hyphens ,0,1,0
the features we use are shown in table 2 which are based on the features used by ratnaparkhi 1996 and uchimoto et al ,0,1,0
the maximum entropy model berger et al 1996 provides us with a wellfounded framework for this purpose which has been extensively used in natural lan guage processing tasks ranging from partofspeech tagging to machine translation ,1,0,0
42 cast3lb function tagging for the task of cast3lb function tag assignment we experimented with three generic machine learning algorithms a memorybased learner daelemans and van den bosch 2005 a maximum entropy classifier berger et al 1996 and a support vector machine classifier vapnik 1998 ,0,1,0
maximum entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible berger et al 1996 ,0,1,0
one such approach is maximum entropy classification berger et al 1996 which we use in the form of a library implemented by tsuruoka1 and used in his classifierbased parser tsuruoka and tsujii 2005 ,0,1,0
several algorithms have been proposed in the literature that try to find the best splits see for instance berger et al 1996 ,0,1,0
22 maximum entropy model the maximum entropy model berger et al 1996 estimates a probability distribution from training data ,0,1,0
this logistic regression is also called maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data berger et al 1996 ,0,1,0
the disambiguation model of this parser is based on a maximum entropy model berger et al 1996 ,0,1,0
1 introduction several efficient accurate and robust approaches to datadriven dependency parsing have been proposed recently nivre and scholz 2004 mcdonald et al 2005 buchholz and marsi 2006 for syntactic analysis of natural language using bilexical dependency relations eisner 1996 ,0,1,0
following ratnaparkhi 1996 collins 2002 toutanova et al 2003 tsuruoka and tsujii 2005 765 feature sets templates error a ratnaparkhis 305 b a t0t1t0t1t1t0t1t2 292 c b t0t2t0t2t0t2w0t0t1w0t0t1w0 t0t2w0 t0t2t1w0t0t1t1w0t0t1t2w0 284 d c t0w1w0t0w1w0 278 e d t0x prefix or suffix of w04 x 9 272 table 2 experiments on the development data with beam width of 3 we cut the ptb into the training development and test sets as shown in table 1 ,0,1,0
for classi cation we use a maximum entropy model berger et al 1996 from the logistic regression package in weka witten and frank 2005 with all default parameter settings ,0,1,0
formally the approach we take can be thought of as a noisier channel where an observed signal o gives rise to a set of sourcelanguage strings fprime fo and we seek e arg maxe max fprimefo prefprimeo 2 arg maxe max fprimefo preprfprimeeo 3 arg maxe max fprimefo preprfprimeeprofprime4 following och and ney 2002 we use the maximum entropy framework berger et al 1996 to directly model the posterior prefprimeo with parameters tuned to minimize a loss function representing 1012 the quality only of the resulting translations ,0,1,0
themodeling approachhere describedis discriminative and is based on maximum entropy me models firstly applied to natural language problems in berger et al 1996 ,0,1,0
for the identification and labeling steps we train a maximum entropy classifier berger et al 1996 over sections 0221 of a version of the ccgbank corpus hockenmaier and steedman 2007 that has been augmented by projecting the propbank semantic annotations boxwell and white 2008 ,0,1,0
41 evaluation of different features and models in pilot experiments on a subset of the features we provide a comparison of hmsvm with other two learning models maximum entropy maxent model berger et al 1996 and svm model kudo 2001 to test the effectiveness of hmsvm on function labeling task as well as the generality of our hypothesis on different learning 58 table 3 features used in each experiment round ,0,1,0
we adopted the stop condition suggested in berger et al 1996 the maximization of the likelihood on a crossvalidation set of samples which is unseen at the parameter estimation ,0,1,0
to 848 make feature ranking computationally tractable in della pietra et al 1995 and berger et al 1996 a simplified process proposed at the feature ranking stage when adding a new feature to the model all previously computed parameters are kept fixed and thus we have to fit only one new constraint imposed by the candidate feature ,0,1,0
we also do not require a newly added feature to be either atomic or a collocation of an atomic feature with a feature already included into the model as it was proposed in della pietra et al 1995 berger et al 1996 ,0,1,0
therefore estimating a natural language model based on the maximum entropy me method pietra et al 1995 berger et al 1996 has been highlighted recently ,0,1,0
then to solve p e c in equation 8 is equivalent to solve h that maximize the loglikelihood xlog zjz x i 10 h argmax kvh such h can be solved by one of the numerical algorithm called the improved iteratire scaling algorithm berger et al 1996 ,0,1,0
we build a subset s c incrementally by iterating to adjoin a feature f e which maximizes loglikelihood of the model to s this algorithm is called the basic feature selection berger et al 1996 ,0,1,0
a word is considered to be known when it has an ambiguous tag henceforth ambitag attributed to it in the lexicon which is compiled in the same way as for the mbttagger daelemans et al 1996 ,0,1,0
in previous work foster 2000 i described a maximum entropyminimum divergence memd model berger et al 1996 for pwhi s which incorporates a trigram language model and a translation component which is an analog of the wellknown ibm translation model 1 brown et al 1993 ,0,1,0
for a given choice of q and f the iis algorithm berger et al 1996 can be used to find maximum likelihood values for the parameters ,0,1,0
we have used the improved iterative scaling algorithm iis berger et al 1996 ,0,1,0
in the last few years there has been an increasing interest in applying maxent models for nlp applications ratnaparkhi 1998 berger et al 1996 rosenfeld 1994 ristad 1998 ,0,1,0
for every class the weights of the active features are combined and the best scoring class is chosen berger et al 1996 ,0,1,0
we use the maximum entropy tagging method described in kazama et al 2001 for the experiments which is a variant of ratnaparkhi 1996 modified to use hmm state features ,0,1,0
support vector machines svms vapnik 1995 and maximum entropy me method berger et al 1996 are powerful learning methods that satisfy such requirements and are applied successfully to other nlp tasks kudo and matsumoto 2000 nakagawa et al 2001 ratnaparkhi 1996 ,1,0,0
unconstrained cl corresponds exactly to a conditional maximum entropy model berger et al 1996 lafferty et al 2001 ,0,1,0
however featureclass functions are traditionally deflned as binary berger et al 1996 hence explicitly incorporating frequencies would require difierent functions for each count or count bin making training impractical ,0,1,0
52 maximum entropy maximum entropy classiflcation maxent or me for short is an alternative technique which has proven efiective in a number of natural language processing applications berger et al 1996 ,1,0,0
a conditional maximum entropy model qxjw for p has the parametric form berger et al 1996 chi 1998 johnson et al 1999 qxjw exp t f x y2yw expt f y 1 where is a ddimensional parameter vector and t f x is the inner product of the parameter vector and a feature vector ,0,1,0
in natural language processing recent years have seen me techniques used for sentence boundary detection part of speech tagging parse selection and ambiguity resolution and stochastic attributevalue grammars to name just a few applications abney 1997 berger et al 1996 ratnaparkhi 1998 johnson et al 1999 ,0,1,0
finally it should be noted that in the current implementation we have not applied any of the possible optimizations that appear in the literature lafferty and suhm 1996 wu and khudanpur 2000 lafferty et al 2001 to speed up normalization of the probability distribution q these improvements take advantage of a models structure to simplify the evaluation of the denominator in 1 ,0,1,0
maximum entropy models jaynes 1957 berger et al 1996 della pietra et al 1997 are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources ,1,0,0
a possible solution to this problem is to directly estimate paw by applying a maximum entropy model berger et al 1996 ,0,1,0
the parsing algorithm was ckystyle parsing with beam thresholding which was similar to ones used in collins 1996 clark et al 2002 ,0,1,0
recently used machine learning methods including maximum entropy models berger et al 1996 and support vector machines vapnik 1995 provide grounds for this type of modeling because it allows various dependent features to be incorporated into the model without the independence assumption ,0,1,0
stateoftheart machine learning techniques including support vector machines vapnik 1995 adaboost schapire and singer 2000 and maximum entropy models ratnaparkhi 1998 berger et al 1996 provide high performance classifiers if one has abundant correctly labeled examples ,1,0,0
thus we obtain the following secondorder model a36a39a38a41a40 a17 a5a7 a42a4 a5a7 a44 a8 a5a57 a15a27a58 a7 a36a39a38a41a40 a17a20a15a59a42a17 a15a41a49 a7 a7 a60 a4 a5a7 a44 a8 ma61a63a62a65a64a33a66 a5a57 a15a27a58 a7a68a67 a40 a17 a15 a42a17 a15a50a49 a7 a15a50a49a51a48 a60 a4 a15a27a47a55a48 a15a50a49a54a48 a44 a11 a wellfounded framework for directly modeling the posterior probability a67 a40 a17 a15 a42a17 a15a50a49 a7 a15a50a49a54a48 a60 a4 a15a12a47a55a48 a15a50a49a54a48 a44 is maximum entropy berger et al 1996 ,0,1,0
1 introduction in this paper we present an approach for extracting the named entities ne of natural language inputs which uses the maximum entropy me framework berger et al 1996 ,0,1,0
the model weights are trained using the improved iterative scaling algorithm berger et al 1996 ,0,1,0
1999 a robust risk minimization classifier based on a regularized winnow method zhang et al 2002 henceforth rrm and a maximum entropy classifier darroch and ratcliff 1972 berger et al 1996 borthwick 1999 henceforth maxent ,0,1,0
the first two phases are approached as straightforward classification in a maximum entropy framework berger et al 1996 ,0,1,0
we have implemented a parallel version of our gis code using the mpich library gropp et al 1996 an opensource implementation of the message passing interface mpi standard ,0,1,0
1 introduction the maximum entropy model berger et al 1996 pietra et al 1997 has attained great popularity in the nlp field due to its power robustness and successful performance in various nlp tasks ratnaparkhi 1996 nigam et al 1999 borthwick 1999 ,1,0,0
a more refined algorithm the incremental feature selection algorithm by berger et al 1996 allows one feature being added at each selection and at the same time keeps estimated parameter values for the features selected in the previous stages ,1,0,0
in contrast to what is shown in berger et al 1996s paper here is how the different values in this variant of the ifs algorithm are computed ,0,1,0
the goal of each selection stage is to select the feature f that maximizes the gain of the log likelihood where the a and gain of f are derived through following steps let the log likelihood of the model be yx xzysump pl log and the empirical expectation of feature f be e p f p xyfxy xy with the approximation assumption in berger et al 1996s paper the unnormalized component and the normalization factor of the model have the following recursive forms aa exysumxysum sfs z f the approximate gain of the log likelihood is computed by g sf alp sf a lp s p xlogz sfa x x z s x ae p f 1 the maximum approximate gain and its corresponding a are represented as max a fs gfsl d maxarg f 3 a fast feature selection algorithm the inefficiency of the ifs algorithm is due to the following reasons ,0,1,0
1 introduction maximum entropy me modeling has received a lot of attention in language modeling and natural language processing for the past few years eg rosenfeld 1994 berger et al 1996 ratnaparkhi 1998 koeling 2000 ,0,1,0
chinese word segmentation is a wellknown problem that has been studied extensively wu and fung 1994 sproat et al 1996 luo and roukos 1996 and it is known that human agreement is relatively low ,0,1,0
the training algorithm we used is the improved iterative scaling iis described in berger et al 19963 ,0,1,0
models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence but otherwise is as uniform as possible berger et al 1996 ,0,1,0
figures 1 and 2 present best results in the learning experiments for the complete set of patterns used in the collocation approach over two of our evaluation corpora11 type positions tagswords features accuracy precision recall gis 1 w 1254 097 096 098 iis 1 t 136 095 096 094 nb 1 t 136 088 097 084 9 see rish 2001 ratnaparkhi 1997 and berger et al 1996 for a formal description of these algorithms ,0,1,0
it can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy is unique and has the following expone ntial form berger et al 1996 1 k j cajf jczcap 1 1 a where zc is a normalization factor fjac are the values of k features of the pair ac and correspond to the linguistic cues of c that are relevant to predict the outcome a features are extracted from the training data and define the constraints that the probabilistic model p must satisfy ,0,1,0
when labeled training data is available we can use the maximum entropy principle berger et al 1996 to optimize the weights ,0,1,0
in our srl system we select maximum entropy berger et al 1996 as a classi er to implement the semantic role labeling system ,0,1,0
the principle of maximum entropy states that when one searches among probability distributions that model the observed data evidence the preferred one is the one that maximizes the entropy a measure of the uncertainty of the model berger et al 1996 ,0,1,0
where mk is one mention in entity e and the basic model building block pll 1je mk m is an exponential or maximum entropy model berger et al 1996 ,0,1,0
both systems are built around from the maximumentropy technique berger et al 1996 ,0,1,0
in this paper we adopt a maximum entropy model berger et al 1996 to estimate the local probabilities a28 a14 a1 a25 a19a1 a25a30a29 a2 a9a22a21 since it can incorporate diverse types of features with reasonable computational cost ,0,1,0
for a more detailed introduction to maximum entropy estimation see berger et al 1996 ,0,1,0
the forest representation was obtained by adopting chart generation kay 1996 car93 roll et al 1999 where ambiguous candidates are packed into an equivalence class and mapping a chart into a forest in the same way as parsing ,0,1,0
23 probabilistic models for generation with hpsg some existing studies on probabilistic models for hpsg parsing malouf and van noord 2004 miyao and tsujii 2005 adopted loglinear models berger et al 1996 ,0,1,0
previous studies abney 1997 johnson et al 1999 riezler et al 2000 miyao et al 2003 malouf and van noord 2004 kaplan et al 2004 miyao and tsujii 2005 defined a probabilistic model of unificationbased grammars as a loglinear model or maximum entropy model berger et al 1996 ,0,1,0
22 maximum entropy our next approach is the maximum entropy berger et al 1996 classification approach ,0,1,0
as a learning algorithm for our classification model we used maximum entropy berger et al 1996 ,0,1,0
96 research on da classification initially focused on twoparty conversational speech mast et al 1996 stolcke et al 1998 shriberg et al 1998 and more recently has extended to multiparty audio recordings like the icsi corpus shriberg et al 2004 ,0,1,0
since its introduction to the natural language processing nlp community berger et al 1996 mebased classifiers have been shown to be effective in various nlp tasks ,1,0,0
previous studies abney 1997 johnson et al 1999 riezler et al 2000 malouf and van noord 2004 kaplan et al 2004 miyao and tsujii 2005 defined a probabilistic model of unificationbased grammars including hpsg as a loglinear model or maximum entropy model berger et al 1996 ,0,1,0
based on the data seen a maximum entropy model berger et al 1996 offers an expression 1 for the probability that there exists coreference c between a mention mi and a mention mj ,0,1,0
we performed feature selection by incrementally growing a loglinear model with order0 features fxyt using a forward feature selection procedure similar to berger et al 1996 ,0,1,0
despite me theory and its related training algorithm darroch and ratcliff 1972 do not set restrictions on the range of feature functions1 popular nlp text books manning and schutze 1999 and research papers berger et al 1996 seem to limit them to binary features ,0,0,1
6 parameter estimation from the duality of me and maximum likelihood berger et al 1996 optimal parameters for model 3 can be found by maximizing the loglikelihood function over a training sample xtyt t 1n ie argmax nsummationdisplay t1 logpytxt ,0,1,0
4 the dependency labeler 41 classifier we used a maximum entropy classifier berger et al 1996 to assign labels to the unlabeled dependencies produced by the bayes point machine ,0,1,0
many reordering constraints have been used for word reorderings such as itg constraints wu 1996 ibm constraints berger et al 1996 and local constraints kanthak et al 2005 ,0,1,0
the probability distributions of these binary classifiers are learnt using maximum entropy model berger et al 1996 haffner 2006 ,0,1,0
2006 but we use a maximum entropy classifier berger et al 1996 to determine parser actions which makes parsing extremely fast ,1,0,0
maximum entropy modeling maxent berger et al 1996 and support vector machine svm vapnik 1995 were used to build the classifiers in our solution ,0,1,0
is the previous bio tag s is the target sentence and fj and lj are feature functions and parameters of a loglinear model berger et al 1996 ,0,1,0
52 maximum entropy model we use the maximum entropy me model berger et al 1996 for our classification task ,0,1,0
dahl et al 1987 hull and gomez 1996 use handcoded slotfilling rules to determine the semantic roles of the arguments of a nominalization ,0,1,0
we utilize the opennlp maxent implementation2 of the maximum entropy classification algorithm berger et al 1996 to train classification models for each lemma and partofspeech combination in the training corpus ,0,1,0
exponential family models are a mainstay of modern statistical modeling brown 1986 and they are widely and successfully used for example in text classification berger et al 1996 ,1,0,0
the disambiguation model of enju is based on a feature forest model miyao and tsujii 2002 which is a loglinear model berger et al 1996 on packed forest structure ,0,1,0
this was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a loglinear model or maximum entropy model berger et al 1996 with many features for parse trees abney 1997 johnson et al 1999 riezler et al 2000 malouf and van noord 2004 kaplan et al 2004 miyao and tsujii 2005 ,0,1,0
previous studies abney 1997 johnson et al 1999 riezler et al 2000 malouf and van noord 2004 kaplan et al 2004 miyao and tsujii 2005 defined a probabilistic model of unificationbased grammars including hpsg as a loglinear model or maximum entropy model berger et al 1996 ,0,1,0
maximum entropy estimation for translation of individual words dates back to berger et al 1996 and the idea of using multiclass classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroducedundertherubricofwordsensedisambiguation and generalized to substrings chan et al 2007 carpuat and wu 2007a carpuat and wu 2007b ,0,1,0
2006 but we use a maximum entropy classifier berger et al 1996 to determine parser actions which makes parsing considerably faster ,1,0,0
these feature functions fi were used to train a maximum entropy classifier berger et al 1996 le 2004thatassignsaprobabilitytoareregiven context cx as follows pre cx zcxexp nsummationdisplay i1 ificxre where zcx is a normalizing sum and the i are the parameters feature weights learned ,0,1,0
in this paper a discriminative parser is proposed to implement maximum entropy me models berger et al 1996 to address the learning task ,0,1,0
the maximum entropy classier berger et al 1996 used is le zhangs maximum entropy modeling toolkit and the lbfgs parameter estimation algorithm with gaussian prior smoothing chen and rosenfeld 1999 ,0,1,0
wu 1996 and berger et al ,0,1,0
2007 the committee consists of k 3 maximum entropy me classifiers berger et al 1996 ,0,1,0
during the src stage a maximum entropy berger et al 1996 classifier is used to predict the probabilities of a word in the sentence language noduplicatedroles catalan arg0agt arg0cau arg1pat arg2atr arg2loc chinese a0 a1 a2 a3 a4 a5 czech act addr crit loc pat dir3 cond english a0 a1 a2 a3 a4 a5 german a0 a1 a2 a3 a4 a5 japanese de ga tmp wo spanish arg0agt arg0cau arg1pat arg1tem arg2atr arg2loc arg2null arg4des arglnull argmcau argmext argmfin table 1 noduplicatedroles for different languages to be each semantic role ,0,1,0
ibm constraints berger et al 1996 the lexical word reordering model tillmann 2004 and inversion transduction grammar itg constraints wu 1995 wu 1997 belong to this type of approach ,0,1,0
previous uses of this model include language modelinglau et al 1993 machine translationberger et al 1996 prepositional phrase attachmentratnaparkhi et al 1994 and word morphologydella pietra et al 1995 ,0,1,0
to make feature ranking computationally tractable in della pietra et al 1995 and berger et al 1996 a simplified process proposed at the feature ranking stage when adding a new feature to the model all previously computed parameters are kept fixed and thus we have to fit only one new constraint imposed by a candidate feature ,0,1,0
first as the configuration space we can use only the reference nodes w from the lattice which makes it similar to the method of berger et al 1996 described in section 21 ,0,1,0
we adopted the stop condition suggested in berger et al 1996 the maximization of the likelihood on a crossvalidation set of samples which is unseen at the parameter estition ,0,1,0
6 comparison with previous work the two parsers which have previously reported the best accuracies on the penn treebank wall st journal are the bigram parser described in collins 1996 and the spatter parser described in jelinek et al 1994 magerman 1995 ,0,1,0
164 and itai 1990 dagan et al 1995 kennedy and boguraev 1996a kennedy and boguraev 1996b ,0,1,0
1996 show that this model is a member of an exponential family with one parameter for each constraint specifically a model of the form 1 i x pyl e in which zx ez y the parameters a1 an are lagrange multipliers that impose the constraints corresponding to the chosen features fl fnthe term zx normalizes the probabilities by summing over all possible outcomes y berger et al ,0,1,0
figure 1 exhibits this scenario with a typical ie system such as sris fastus system hobbs et al 1996 ,0,1,0
the approach made use of a maximum entropy model berger et al 1996 formulated from frequency information for various combinations of the observed features ,0,1,0
 ie ll lj mazzij u ii where xijue qi and maxxiju is the highest score in the line of the matrix qi which corresponds to the head word sense j n is the number of modifiers of the head word h at the current tree level and k i lj jl lj where k is the number of senses of the head word h the reason why gj i0 is calculated as a sum of the best scores ll rather than by using the traditional maximum likelihood estimate berger et al 1996gah eta ,0,1,0
to determine the tree headword we used a set of rules similar to that described by magerman 1995jelinek et al 1994 and also used by collins 1996 which we modified in the following way the head of a prepositional phrase ppin np was substituted by a function the name of which corresponds to the preposition and its sole argument corresponds to the head of the noun phrase np ,0,1,0
its applications range from sentence boundary disambiguation reynar and ratnaparkhi 1997 to partofspeech tagging ratnaparkhi 1996 parsing ratnaparkhi 1997 and machine translation berger et al 1996 ,0,1,0
clearly a more sophisticated feature selection routine such as the ones in berger et al 1996 or berger and printz 1998 would be required in this case ,1,0,0
other recent work has applied me to language modeling rosenfeld 1994 machine translation berger et al 1996 and reference resolution kehler 1997 ,0,1,0
this allows us to compute the conditional probability as follows berger et al 1996 pflh i hi 2 zh zh ii h a ff i the maximum entropy estimation technique guarantees that for every feature gi the expected value of gi according to the me model will equal the empirical expectation of gi in the training corpus ,0,1,0
more complete discussions of me as applied to computational linguistics including a description of the me estimation procedure can be found in berger et al 1996 and della pietra et al 1995 ,1,0,0
1 introduction on measures for interrater reliability carletta 1996 on frameworks for evaluating spoken dialogue agents walker et al 1998 and on the use of different corpora in the development of a particular system the carnegiemellon communicator eskenazi et al ,0,1,0
carletta 1996 argues that the kappa statistic a should be adopted to judge annotator consistency for classification tasks in the area of discourse and dialogue analysis ,0,1,0
it has been claimed that content analysis researchers usually regard a 8 to demonstrate good reliability and 67 8 alf16 lows tentative conclusions to be drawn see carletta 1996 ,0,1,0
carletta mentions this problem asking what the difference would be if the kappa statistic were computed across clause boundaries transcribed word boundaries and transcribed phoneme boundaries carletta 1996 p 252 rather than the sentence boundaries she suggested ,0,1,0
45 consistency of annotations in order to assess the consistency of annotation we follow carletta 1996 in using cohens a chancecorrected measure of interrater agreement ,0,1,0
in order to determine interannotator agreement for the database of annotated texts we computed kappa statistics carletta 1996 ,0,1,0
the reliability for the two annotation tasks statistics carletta 1996 was of 094 and 090 respectively ,0,1,0
kd1 2371 23 reliability to evaluate the reliability of the annotation we use the kappa coe cient k carletta 1996 which measures pairwise agreement between a set of coders making category judgements correcting for expected chance agreement ,0,1,0
the kappa statistic carletta 1996 for identifying question segments is 068 and for linking question and answer segments given a question segment is 081 ,0,1,0
carletta 1996 says that 067 a10a14a11a15a10 08 allows just tentative conclusions to be drawn ,0,1,0
while the need for annotation by multiple raters has been well established in nlp tasks carletta 1996 most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors or to check the systems output ,0,1,0
to support a more rigorous analysis however wc have followed carlettas suggestion 1996 of using the k coettmcnt siegel and castellan 1988 as a measure of coder agreement ,0,1,0
31 agreement for emotion classes the kappa coefficient of agreement is a statistic adopted by the computational linguistics community as a standard measure for this purpose carletta 1996 ,0,1,0
we then examined the interannotator reliability of the annotation by calculating the score carletta 1996 ,0,1,0
after each step the annotations were compared using the statistic as reliability measure for all classification tasks carletta 1996 ,0,1,0
kappa is a better measurement of agreement than raw percentage agreement carletta 1996 because it factors out the level of agreement which would be reached by random annotators using the same distribution of categories as the real coders ,0,1,0
once an acceptable rate of interjudge agreement was verified on the first nine clusters kappa carletta 1996 of 068 the remaining 11 clusters were annotated by one judge each ,0,1,0
as argued in carletta 1996 kappa values of 08 or higher are desirable for detecting associations between several coded variables we were thus satisfied with the level of agreement achieved ,0,1,0
carletta 1996 cites the convention from the domain of content analysis indicating that 67 k k 8 indicates marginal agreement while k 8 is an indication of good agreement ,0,1,0
although the kappa coefficient has a number of advantages over percentage agreement eg it takes into account the expected chance interrater agreement see carletta 1996 for details we also report percentage agreement as it allows us to compare straightforwardly the human performance and the automatic methods described below whose performance will also be reported in terms of percentage agreement ,1,0,0
other commonly used measures include kappa carletta 1996 and relative utility radev jing and budzikowska 2000 both of which take into account the performance of a summarizer that randomly picks passages from the original document to produce an extract ,0,1,0
carletta 1996 deserves the credit for bringing to the attention of computational linguists ,0,1,0
the agreement on identifying the boundaries of units using the kappa statistic discussed in carletta 1996 was 9 for two annotators and 500 units the agreement on features two annotators and at least 200 units was as follows utype 76 verbed 9 nite 81 ,0,1,0
in order to determine interannotator agreement for step 2 of the coding procedure for the database of annotated texts we calculated kappa statistics carletta 1996 ,0,1,0
for example the coding manual for the switchboard damsl dialogue act annotation scheme jurafsky shriberg and biasca 1997 page 2 states that kappa is used to assess labelling accuracy and di eugenio and glass 2004 relate reliability to the objectivity of decisions whereas carletta 1996 regards reliability as the degree to which we understand the judgments that annotators are asked to make ,0,1,0
this is an unsuitable measure for inferring reliability and it was the use of this measure that prompted carletta 1996 to recommend chancecorrected measures ,0,1,0
since jean carletta 1996 exposed computational linguists to the desirability of using chancecorrected agreement statistics to infer the reliability of data generated by applying coding schemes there has been a general acceptance of their use within the field ,1,0,0
the prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies carletta 1996 di eugenio and glass 2004 krippendorff 2004a is probably due to a desire for a simple system that can be easily applied to a scheme ,0,1,0
he uses a specic reliability statistic for his measurements but carletta 1996 implicitly assumes kappalike metrics are similar enough in practice for the rule of thumb to apply to them as wella detailed discussion on the differences and similarities of these and other measures is provided by krippendorff 2004 in this article we will use cohens 1960 to investigate the value of the 08 reliability cutoff for computational linguistics ,0,1,0
42 interpreting reliability results it has been argued elsewhere carletta 1996 that since the amount of agreement one would expect by chance depends on the number and relative frequencies of the categories under test reliability for category classifications should be measured using the kappa coefficient ,0,1,0
61 reader judgments there is a growing concern surrounding issues of intercoder reliability when using human judgments to evaluate discourseprocessing algorithms carletta 1996 condon and cech 1995 ,0,1,0
proposals have recently been made for protocols for the collection of human discourse segmentation data nakatani et al 1995 and for how to evaluate the validity of judgments so obtained carletta 1996 isard and carletta 1995 ros6 1995 passonneau and litman 1993 litman and passonneau 1995 ,0,1,0
carletta 1996 and ros6 1995 point out the importance of taking into account the expected chance agreement among judges when computing whether or not judges agree significantly ,0,1,0
according to carletta 1996 k measures pairwise agreement among a set of coders making category judgments correcting for expected chance agreement as follows kpa pe 1 pe where pa is the proportion of times that the coders agree and pe is the proportion of times that they would be expected to agree by chance ,0,1,0
carletta 1996 also states that in the behavioral sciences k 8 signals good replicability and 67 k 8 allows tentative conclusions to be drawn ,0,1,0
reliability metrics krippendorff 1980 carletta 1996 are designed to give a robust measure of how well distinct sets of data agree with or replicate one another ,0,1,0
in hirschberg and nakatani 1996 average reliability measured using the kappa coefficient discussed in carletta 1996 of segmentinitial labels among 3 coders on 9 monologues produced by the same speaker labeled using text and speech is8 or above for both read and spontaneous speech values of at least 8 are typically viewed as representing high reliability see section 32 ,0,1,0
idiom 0 0 1 1 0 2 v doubt 3 0 4 0 0 7 total a 294 160 546 39 1 1040 in order to measure the agreement in a more precise way we used the kappa statistic siegel and castellan 1988 recently proposed by carletta as a measure of agreement for discourse analysis carletta 1996 ,0,1,0
and indeed the agreement figures went up from k 063 to k 068 ignoring doubts when we did so ie within the tentative margins of agreement according to carletta 1996 068 x 08 ,0,1,0
the resulting kappa statistics carletta 1996 over the annotated data yields a0a2a1 a3a5a4a7a6 which seems to indicate that human annotators can reliably distinguish between coherent samples as in example 1a and incoherent ones as in example 1b ,0,1,0
the two annotators agreed on the annotations of 385453 turns achieving 8499 agreement with kappa 0682 this interannotator agreement exceeds that of prior studies of emotion annotation in naturally occurring speech 2a3a5a4a7a6a8a6a9a4a11a10a13a12a15a14a17a16a19a18a21a20a22a12a23a14a25a24a26a18 a27 a20a22a12a23a14a25a24a26a18 carletta 1996 ,0,1,0
the kappa carletta 1996 obtained on this feature was 093 ,0,1,0
the metric we used is the kappa statistic carletta 1996 which factors out the agreement that is expected by chance 1 ep epap where pa is the observed agreement among the raters and pe is the expected agreement ie the probability that the raters agree by chance ,0,1,0
one of our goals was to use for our study only information that could be annotated reliably passonneau and litman 1993 carletta 1996 as we believe this will make our results easier to replicate ,0,1,0
the agreement on identifying the boundaries of units using the ak statistic discussed in carletta 1996 was ak bp bmbl for two annotators and 500 units the agreement on features2 annotators and at least 200 units was follows attribute ak value utype 76 verbed 9 finite 81 subject 86 nps our instructions for identifying np markables derive from those proposed in the mate project scheme for annotating anaphoric relations poesio et al 1999 ,0,1,0
on the one hand even the higher of the kappa coefficients mentioned above is significantly lower than the standard suggested for good reliability a124a126a125a128a127a130a129 or even the level where tentative conclusions may be drawn a127a130a131a133a132a135a134a72a124 a134 a127a130a129 carletta 1996 krippendorff 1980 ,0,1,0
this information can be annotated reliably a1a3a2a5a4a7a6a9a8 a10a12a11a14a13a16a15 and a1a17a2a5a4a19a18a20a8 a10a12a11a14a13a16a21 4 4following carletta 1996 we use the a22 statistic to estimate reliability of annotation ,0,1,0
analyze resulting findings to determine a progression of competence in michaud et al 2001 we discuss the initial steps we took in this process including the development of a list of error codes documented by a coding manual the verification of our manual and coding scheme by testing intercoder reliability in a subset of the corpus where we achieved a kappa agreement score carletta 1996 of a0 a1a3a2a5a4a7a6 2 and the subsequent tagging of the entire corpus ,0,1,0
the reliability of the annotations was checked using the kappa statistic carletta 1996 ,0,1,0
coselection measures include precision and recall of coselected sentences relative utility radev et al 2000 and kappa siegel and castellan 1988 carletta 1996 ,0,1,0
312 kappa kappa siegel and castellan 1988 is an evaluation measure which is increasingly used in nlp annotation work krippendorff 1980 carletta 1996 ,0,1,0
interannotator agreement was determined for six pairs of two annotators each resulting in kappa values carletta 1996 ranging from 062 to 082 for the whole database carlson et al ,0,1,0
to support this claim first we used the coefficient krippendorff 1980 carletta 1996 to assess the agreement between the classification made by flsa and the classification from the corpora see table 8 ,0,1,0
we use the by now standard a0 statistic di eugenio and glass 2004 carletta 1996 marcu et al 1999 webber and byron 2004 to quantify the degree of abovechance agreement between multiple annotators and the a1 statistic for analysis of sources of unreliability krippendorff 1980 ,0,1,0
labelling was carried out by three computational linguistics graduate students with 89 agreement resulting in a kappa statistic of 087 which is a satisfactory indication that our corpus can be labelled with high reliability using our tag set carletta 1996 ,0,1,0
therefore the results are more informative than a simple agreement average cohen 1960 carletta 1996 ,0,1,0
kappa is defined as k pape1pe carletta 1996 where pa is the proportion of times that the labels agree and pe is the proportion of times that they may agree by chance ,0,1,0
to measure interannotator agreement we compute cohens kappa carletta 1996 from the two sets of annotations obtaining a kappa value of only 043 ,0,1,0
we then used cohens kappa to determine the level of agreement carletta 1996 ,0,1,0
we then used the kappa statistic siegel and castellan 1988 carletta 1996 to assess the level of agreement between the three coders with respect to the 2 an agent holds the task initiative during a turn as long as some utterance during the turn directly proposes how the agents should accomplish their goal as in utterance 3c ,0,1,0
carletta suggests that content analysis researchers consider k 8 as good reliability with67 8 allowing tentative conclusions to be drawn carletta 1996 ,0,1,0
we perform a statistical analysis that provides information that complements the information provided by cohens kappa cohen 1960 carletta 1996 ,0,1,0
the table also shows cohens to an agreement measure that corrects for chance agreement carletta 1996 the most important t value in the table is the value of 07 for the two human judges which can be interpreted as sufficiently high to indicate that the task is reasonably well defined ,0,1,0
we measured stability the degree to which the same annotator will produce an annotation after 6 weeks and reproducibility the degree to which two unrelated annotators will produce the same annotation using the kappa coefficient k siegel and castellan 1988 carletta 1996 which controls agreement pa for chance agreement pe k pape 1pz kappa is 0 for if agreement is only as would be expected by chance annotation following the same distribution as the observed distribution and 1 for perfect agreement ,0,1,0
the agreement was statistically significant kappa 0650 001 for japanese and kappa 07480 001 for english carletta 1996 siegeland castellan 1988 ,0,1,0
in other words 4b can be used in substitution of 4a whereas 5b cannot so easily 41n carletta 1996 a value of k between 8 and i indicates good agreement a value between 6 and 8 indicates some agreement ,0,1,0
to test the reliability of group segmentation within gdmis we calculate the kappa coefficient c3 8 carletta 1996 carletta et al 1997 flammia 1998 to measure pairwise agreement between the subject and the expert ,0,1,0
 from carletta 1996 9 combined metric by bp b4ac be b7bdb5c8cabpb4ac be c8 b7 cab5 from jurafsky and martin 2000 p578 ac bpbd ,0,1,0
a detailed discussion on the use of kappa in natural language processing is presented in carletta 1996 ,0,1,0
we chose nouns that occur a minimum of 10 times in the corpus have no undetermined translations and at least five different translations in the six nonenglish languages and have the log likelihood score of at least 18 that is llt t t s 2 1 ij n j ji ij n log 18 where n ij stands for the number of times t t and t s have been seen together in aligned sentences n i and n j stand for the number occurrences of t t and t s respectively and n represents the total 4 we computed raw percentages only common measures of annotator agreement such as the kappa statistic carletta 1996 proved to be inappropriate for our twocategory yesno classification scheme ,0,1,0
the kappa value carletta 1996 was used to evaluate the agreement among the judges and to estimate how difficult the evaluation task was ,0,1,0
though interrater reliability using the kappa statistic carletta 1996 may be calculated for each group the distribution of categories in the contribution group was highly skewed and warrants further discussion ,0,1,0
overall agreement among judges for 250 propositions 601 a commonly used metric for evaluating interrater reliability in categorization of data is the kappa statistic carletta 1996 ,0,1,0
the statistic carletta 1996 is recast as fswsyssys agrfswsyssys p agrfssyssys n p agrfssyssys n in this modified form fsw represents the divergence in relative agreement wrt f s for target noun w relative to the mean relative agreement wrt f s over all words ,0,1,0
in fact it has been shown that the agreement of subjects annotating bridging poesio and vieira 1998 or discourse cimiano 2003 relations can be too low for tentative conclusion to be drawn carletta 1996 ,0,1,0
in this sense instead of measuring only the categorial agreement between annotators with the kappa statistic carletta 1996 or the performance of a system in terms of precisionrecall we could take into account the hierarchical organization of the categories or concepts by making use of measures considering the hierarchical distance between two concepts such as proposed by hahn and schnattinger 1998 or madche et al 2002 ,0,1,0
the agreement on identifying the boundaries of units using the statistic discussed in carletta 1996 was 9 for two annotators and 500 units the agreement on features 2 annotators and at least 200 units was as follows utype 76 verbed 9 finite 81 ,0,1,0
6 coding reliability the reliability of the annotation was evaluated using the kappa statistic carletta 1996 ,0,1,0
the reliability for the two annotation tasks statistics carletta 1996 was of 094 and 090 respectively ,0,1,0
kappa coefficient is given in 1 carletta 1996 1 1 ep epap kappa where pa is the proportion of times the annotators actually agree and pe is the proportion of times the annotators are expected to agree due to chance 3 ,0,1,0
an acceptable agreement for most nlp classification tasks lies between 07 and 08 carletta 1996 poessio and vieira 1988 ,0,1,0
the classbased kappa statistic of cohen 1960 carletta 1996 cannot be applied here as the classes vary depending on the number of ambiguities per entry in the lexicon ,0,0,1
the a0 coefficient is computed as follows a0 a47 a1a32a2 a9 a1 a30 a68 a9 a1a32a30 carletta 1996 reports that content analysis researchers generally think of a0a34a33 a49a36a35a37 as good reliability with a49a36a35a38a40a39a37a41 a0 a41a25a49a36a35a37 allowing tentative conclusions to be drawn all that remains is to define the chance agreement probability a1 a30 let a1a32a41 a1 a30 a7 and a1a32a42 a1 a30 a7 be the fraction of utterances that begin or end one or more segments in segmentation a30 respectively ,0,1,0
4 data analysis to test the reliability of the annotation we first considered the kappa statistic siegel and castellan 1988 which is used extensively in empirical studies of discourse carletta 1996 ,0,1,0
much like kappa statistics proposed by carletta 1996 existing employments of majority class baselines assume an equal set of identical potential markups ie attributes and their values for all markables ,0,1,0
we evaluated annotation reliability by using the kappa statistic carletta 1996 ,0,1,0
in the summac experiments the kappa score carletta 1996 eugenio and glass 2004 for interannotator agreement was reported to be 038 mani et al 2002 ,0,1,0
the judges had an acceptable 074 mean agreement carletta 1996 for the assignment of the primary class but a meaningless 021 for the secondary class they did not even agree on which lemmata were polysemous ,0,1,0
the table also shows the score which is another commonly used measure for interannotator agreement carletta 1996 ,0,1,0
in the literature on the kappa statistic most authors address only category data some can handle more general data such as data in interval scales or ratio scales krippendorff 1980 carletta 1996 ,0,1,0
4this was a straightforward task two annotators annotated independently with very high agreementkappa score of over 095 carletta 1996 ,0,1,0
7following carletta 1996 we measure agreement in kappa which follows the formula k pape1pe where pa is observed and pe expected agreement ,0,1,0
interannotator agreement is typically measured by the kappa statistic carletta 1996 dekappa frequency 00 02 04 06 08 10 0 2 4 6 8 figure 2 distribution of interannotator agreement across the 54 icsi meetings tagged by two annotators ,0,1,0
agreement is sometimes measured as percentage of the cases on which the annotators agree but more often expected agreement is taken into account in using the kappa statistic cohen 1960 carletta 1996 which is given by po pe1 p e 1 where po is the observed proportion of agreement and pe is the proportion of agreement expected by chance ,0,1,0
ever since its introduction in general cohen 1960 and in computational linguistics carletta 1996 many researchers have pointed out that there are quite some problems in using eg ,0,0,1
following the suggestions in carletta 1996 core et al consider kappa scores above 067 to indicate significant agreement and scores above 08 reliable agreement ,0,1,0
interannotator agreement was assessed mainly using fscore and percentage agreement as well as 11 table 1 annotation examples of superlative adjectives example sup span det num car mod comp set the thirdlargest thrift institution in puerto rico also 22 def sg no ord 37 the agriculture department reported that feedlots in the 13 biggest ranch states held 910 def pl yes no 1112 the failed takeover would have given ual employees 75 voting control of the nation s secondlargest airline 1717 pos sg no ord 1418 the kappa statistics k where applicable carletta 1996 ,0,1,0
interannotator agreement was measured using the kappa k statistics cohen 1960 carletta 1996 on 1502 instances three switchboard dialogues marked by two annotators who followed specific written guidelines ,0,1,0
7 for the most frequent 184 expressions on the average the agreement rate between two human annotators is 093 and the kappa value is 073 which means allowing tentative conclusions to be drawn carletta 1996 ng et al 1999 ,0,1,0
51 agreement between translators in an attempt to quantify the agreement between the two groups of translators we computed the kappa coefficient for annotation tasks as defined by carletta 1996 ,0,1,0
for these classications we calculated a kappa statistic of 0528 carletta 1996 ,0,1,0
obtained percent agreement of 0988 and coefficient carletta 1996 of 0975 suggest high convergence of both annotations ,0,1,0
annotation was highly reliable with a kappa carletta 1996 of 3httpswwwciagovciapublications factbookindexhtml 4given that the task is not about standard named entity recognition we assume that the general semantic class of the name is already known ,0,1,0
the resulting intercoder reliability measured with the kappa statisticcarletta1996 is considered excellent 080 ,0,1,0
the percentage agreement for each of the features is shown in the following table feature percent agreement form 100 intentionality 749 awareness 935 safety 907 as advocated by carletta 1996 we have used the kappa coefficient siegel and castellan 1988 as a measure of coder agreement ,0,1,0
we will do this by examining how humans perform on summary extraction and evaluating the reliability of their performance using the kappa statistic a metric standardly used in the behavioral sciences jean carletta 1996 sidney siegel and n john castellan jr 1988 ,0,1,0
measurement of beliability the kappa statistic following jean carletta 1996 we use the kappa statistic sidney siegel and n john castellan jr 1988 to measure degree of agreement among subjects ,0,1,0
as aptly pointed out in jean carletta 1996 agreement measures proposed so far in the computational linguistics literature has failed to ask an important question of whether results obtained using agreement data are in any way different from random data ,1,0,0
its roots are the same as computational linguistics cl but it has been largely ignored in cl until recently dunning 1993 carletta 1996 kilgarriff 1996 ,0,1,0
intercoder reliability was assessed using cohens kappa statistic siegel castellan 1988 carletta 1996 ,0,1,0
a value of 08 or greater indicates a high level of reliability among raters with values between 067 and 08 indicating only moderate agreement hirschberg nakatani 1996 carletta 1996 ,0,1,0
cohens kappa bakeman and gottman 1986 carletta 1996 ,0,1,0
the labeling agreement was 84 n 80 carletta 1996 ,0,1,0
however check moves are almost always about some information which the speaker has been told carletta et al1996 a description that models the backward looking functionality of a dialogue act ,0,1,0
it has been argued that the reliability of a coding schema can be assessed only on the basis of judgments made by naive coders carletta 1996 ,0,1,0
k pa pe 3 1pe carletta 1996 suggests that the units over which the kappa statistic is computed affects the outcome ,0,1,0
the rationale for using kappa is explained in carletta 1996 ,0,1,0
it us widely acknowledged that word sense dsamblguatmn wsd us a central problem m natural language processing in order for computers to be able to understand and process natural language beyond simple keyword matching the problem of dsamblguatmg word sense or dlscermng the meamng of a word m context must be effectively dealt with advances in wsd v ill have slgmficant impact on apphcatlons hke information retrieval and machine translation for natural language subtasks hke partofspeech tagging or sntactm parsing there are relatlvely well defined and agreedupon cnterm of what it means to have the correct part of speech or syntactic structure assigned to a word or sentence for instance the penn treebank corpus marcus et al 1993 proidet large repotory of texts annotated wth partofspeech and sntactm structure mformatlon tvo independent human annotators can achieve a high rate of agreement on assigning partofspeech tags to words m a gven sentence unfortunately ths us not the case for word sense assignment frstly it is rarely the case that any two dictionaries will have the same set of sense defimtmns for a gven word different dctlonanes tend to carve up the semantic space m a different way so to speak secondly the hst of senses for a word m a typical dmtmnar tend to be rather refined and comprehensive this is especmlly so for the commonly used words which have a large number of senses the sense dustmctmn between the different senses for a commonly used word m a dctmnary hke wordnet miller 1990 tend to be rather fine hence two human annotators may genuinely dusagree m their sense assignment to a word m context the agreement rate between human annotators on word sense assignment us an important concern for the evaluatmn of wsd algorithms one would prefer to define a dusamblguatlon task for which there us reasonably hlgh agreement between human annotators the agreement rate between human annotators will then form the upper ceiling against whmh to compare the performance of wsd algorithms for instance the senseval exerclse has performed a detaded study to find out the raterannotator agreement among ts lexicographers taggrog the word senses kllgamff 1998c kllgarnff 1998a kflgarrlff 1998b 2 a case study in thispaper we examine the ssue of raterannotator agreement by comparing the agreement rate of human annotators on a large sensetagged corpus of more than 30000 instances of the most frequently occurring nouns and verbs of enghsh this corpus is the intersection of the wordnet semcor corpus miller et al 1993 and the dso corpus ng and lee 1996 ng 1997 which has been independently tagged wlth the refined senses of wordnet by two separate groups of human annotators the semcor corpus us a subset of the brown corpus tagged with vordnet senses and consists of more than 670000 words from 352 text files sense taggmg was done on the content words nouns erbs adjectives and adverbs m this subset the dso corpus consists of sentences drawn from the brown corpus and the wall street journal for each word w from a hst of 191 frequently occurring words of enghsh 121 nouns and 70 verbs sentences containing w m singular or plural form and m its various reflectional verb form are selected and each word occurrence w s tagged wth a sense from wordnet there s a total of about 192800 sentences in the dso corpus m which one word occurrence has been sensetagged m each sentence the intersection of the semcor corpus and the dso corpus thus consists of brown corpus sentences m which a word occurrence w is sensetagged m each sentence where w is one ofthe 191 frequently occurrmg english nouns or verbs since this common pomon has been sensetagged by two independent groups of human annotators t serves as our data set for investigating interannotator agreement in this paper 3 sentence matching to determine the extent of interannotator agreement the first step s to match each sentence m semcor to its corresponding counterpart in the dso corpus this step s comphcated by the following factors 1 although the intersected portion of both corpora came from brown corpus they adopted different tokemzatmn convention and segmentartan into sentences differed sometimes 2 the latest versmn of semcor makes use of the senses from wordnet 1 6 whereas the senses used m the dso corpus were from wordnet 15 1 to match the sentences we first converted the senses m the dso corpus to those of wordnet 1 6 we ignored all sentences m the dso corpus m which a word is tagged with sense 0 or 1 a word is tagged with sense 0 or 1 ff none of the given senses m wordnft applies 4 sentence from semcor is considered to match one from the dso corpus ff both sentences are exactl ldentcal or ff the differ only m the preence or absence of the characters permd or hyphen for each remaining semcor sentence taking into account word ordering ff 75 or more of the words m the sentence match those in a dso corpus sentence then a potential match s recorded these i kctualy the wordqet senses used m the dso corpus were from a shght variant of the official wordnei 1 5 release ths ssas brought to our attention after the pubhc release of the dso corpus potential matches are then manually verffied to ensure that they are true matches and to eed out any false matches using this method of matching a total of 13188 sentencepalrs contasnmg nouns and 17127 sentencepars containing verbs are found to match from both corpora ymldmg 30315 sentences which form the intersected corpus used m our present study 4 the kappa statistic suppose there are n sentences m our corpus where each sentence contains the word w assume that w has m senses let 4 be the number of sentences which are assigned identical sense b two human annotators then a simple measure to quantify the agreement rate between two human annotators is pc where pc an the drawback of this simple measure is that it does not take into account chance agreement between two annotators the kappa statistic a cohen 1960 is a better measure of raterannotator agreement which takes into account the effect of chance agreement it has been used recently wthm computatmnal hngustlcs to measure raterannotator agreement bruce and wmbe 1998 carletta 1996 veroms 1998 let cj be the sum of the number of sentences which have been assigned sense 3 by annotator 1 and the number of sentences whmh have been assigned sense 3 by annotator 2 then pp 1p where m jl and pe measures the chance agreement between two annotators a kappa alue of 0 indicates that the agreement is purely due to chance agreement whereas a kappa alue of 1 indicates perfect agreement a kappa alue of 0 8 and above is considered as mdmatmg good agreement carletta 1996 table 1 summarizes the interannotator agreement on the mtersected corpus the first becond row denotes agreement on the nouns xerbs whle the lass row denotes agreement on all words combined the aerage reported m the table is a smpie average of the individual value of each word the agreement rate on the 30315 sentences as measured by p is 57 this tallies with the figure reported n our earlier paper ng and lee 1996 where we performed a quick test on a subset of 5317 sentencesn the intersection of both the semcor corpus and the dso corpus 10 mm m m m m m mm m m m m mm m m m type num of v ords a n p avg nouns 121 7676 13188 i 0 582 0 300 verbs 70 9520 17127 i 0 555 0 347 all i 191 i 17196 30315 i 056t 0317 table 1 raw interannotator agreement 5 algorithm since the raterannotator agreement on the intersected corpus is not high we would like to find out how the agreement rate would be affected if different sense classes were in use in this section we present a greedy search algorithm that can automatmalb derive coarser sense classes based on the sense tags assigned by two human annotators the resulting derived coarse sense classes achmve a higher agreement rate but we still maintain as many of the original sense classes as possible the algorithm is given m figure 1 the algorithm operates on a set of sentences where each sentence contains an occurrence of the word w whmh has been sensetagged by two human annotators at each iteration of the algorithm tt finds the pair of sense classes ct and cj such that merging these two sense classes results in the highest t value for the resulting merged group of sense classes it then proceeds to merge cz and c thin process is repeated until the value reaches a satisfactory value t which we set as 0 8 note that this algorithm is also applicable to deriving any coarser set of classes from a refined set for any nlp tasks in which prior human agreement rate may not be high enough such nlp tasks could be discourse tagging speechact categorization etc 6 results for each word w from the list of 121 nouns and 70 verbs e applied the greedy search algorithm to each set of sentences in the intersected corpus contaming w for a subset of 95 words 53 nouns and 42 verbs the algorithm was able to derive a coarser set of 2 or more senses for each of these 95 words such that the resulting kappa alue reaches 0 8 or higher for the other 96 words m order for the kappa value to reach 0 8 or higher the algorithm collapses all senses of the ord to a single trivial class table 2 and 3 summarizes the results for the set of 53 nouns and 42 erbs respectively table 2 mdcates that before the collapse of sense classes these 53 nouns have an average of 7 6 senses per noun there is a total of 5339 sentences in the intersected corpus containing these nouns of which 3387 sentences were assigned the same sense by the two groups of human annotators the average kappa statistic computed as a simple average of the kappa statistic of he mdlwdual nouns is 0 463 after the collapse of sense classes by the greedy search algorithm the average number of senses per noun for these 53 nouns drops to 40 howeer the number of sentences which have been asmgned the same coarse sense by the annotators increases to 5033 that is about 94 3 of the sentences have been assigned the same coarse sense and that the average kappa statistic has improved to 0 862 mgmfymg high raterannotator agreement on the derived coarse senses table3 gles the analogous figures for the 42 verbs agmn mdmatmg that high agreement is achieved on the coarse sense classes dened for verbs 7 discussion our findings on raterannotator agreement for word sense tagging indicate that for average language users it is quite dlcult to achieve high agreement when they are asked to assign refned sense tags such as those found in wordnet given only the scanty definition entries m the wordnet dlctionary and a few or no example sentences for the usage of each word sense thin observation agrees wlth that obtmned m a recent study done by veroms 1998 where the agreement on sensetagging by naive users was also not hlgh thus it appears that an average language user is able to process language wlthout needing to perform the task of dlsamblguatmg word sense to a very finegrained resolutmn as formulated m a tradltlonal dmtlonary in contrast expert lexicographers tagged the ord sense in the sentences used m the senseval exerclse where high raterannotator agreement was reported there are also fuller dlctlonary entries m the hector dlctlonary used and more eamples showing the usage of each word sense m hector these factors are likely to have contributed to the difference in raterannotator agreement observed m the three studies conducted we also examined the coarse sense classes derived by the greedy search algorithm vve found some interesting groupings of coarse senses for nouns which e hst in table 4 from table 4 it is apparent that the greedy search algorithm can derive interesting groupings of word senses that correspond to human mtmtwe judgment of sense grazulanty it is clear that some of the disagreement between the two groups of human annotators can be attributed solely to the overly refined senses of wordnet as an example there is a total ii loop let ct c m denote the current m sense classes oo for all z3 such that 1 3 m let ccw 1 denote the resulting m 1 sense classes by mergmg c and c 3 compute c ct ff c c4x then cct z end for merge the sense class c ,0,1,0
eqmvalent ot duty in a parallel french text the correct sense of the enghsh word is identified these studies exploit ths lnformatmn m order to gather cooccurrence data for the different senses which ts then used to dtsambguate new texts in related work dywk 1998 used patterns of translational relatmns in an enghshnorwegian paralle corpus enpc oslo umverslty to define semantic propemes such as synonymy ambtgmty vagueness and semantic helds and suggested a derivation otsemantic representations for signs eg lexemes captunng semantm relatmnshlps such as hyponymy etc fiom such translatmnal relatmns recently resnlk and yarowsky 1997 suggested that fol the purposes ot wsd the different senses of a wod could be detelmlned by considering only sense dstmctmns that are lextcahzed crosshngmstlcally in particular they propose that some set of target languages be dentfied and that the sense dstmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages this idea would seem to povtde an answer at least m part to the problem of determining different senses of a word mtumvely one assumes that ff another language lexlcahzes a word m two or more ways there must be a conceptual monvatmn if we look at enough languages we would be likely to fred the sgmficant lexlcal differences that dehmtt different senses of a word however ths suggestmn raises several questions fo instance t s well known that many ambgumes are preserved across languages for example the french tntdryt and the enghsh interest especmlly languages that are relatively closely related assuming this problem can be overcome should differences found m closely related languages be given lesser or greater weight than those found m more distantly related languages 9 more generally which languages should be considered for this exermse 9 all languages 9 closely related languages9 languages from different language famlhes a mixture of the two 9 how many languages and of which types would be enough to provide adequate lnfotmanon tot this purpose there ts also the questmn ot the crlterm that would be used to estabhsh that a sense distinction is lexlcahzed crosshngustmally how consistent must the dstlnctlon be 9 does it mean that two concepts are expressed by mutually nonlntetchangeable lexmal items in some slgmficant number ot other languages or need tt only be the case that the option ot a different lexlcahzatlon exists m a certain percentage of cases 9 another conslderatmn ts where the crosshngual mformatlon to answer these questmns would come from using bdmgual dictionaries would be extremely tedmus and errorprone gven the substantial dvergence among dctlonanes in terms of the kinds and degree of sense dlstmctmns they make resmk and yalowsky 1997 suggest eutowordnet vossen 1998 as a possible somce of mformatmn but given that eurowordnet ts pttmatdy a lexmon and not a corpus t is subject to many of the same objections as for blhngual dictionaries an alternative would be to gather the reformation from parallel ahgned corpma unlike bilingual and muttthngual dictionaries translatmn eqmvalents xn parallel texts ae determined by experienced translatols who evaluate each instance ot a words use m context rather than as a part of the metahngmstc actlvlty of classifying senses for mclusmn in a dictionary however at present very few parallel ahgned corpora exist the vast majority ot these are bltexts mvolwng only two languages one of which is very often english ideally a serious 53 evaluation of resnik and yarowskys proposal would include parallel texts m languages from several different language families and to maximally ensure that the word m question is used in the exact same sense across languages t would be preferable that the same text were used over all languages in the study the only currently avadable parallel corpora for more than two languages are olwells nmeteen eightyfour erjavec and ide 1998 platos repubhc erjavec et al 1998 the multext journal o the commtston corpus ide and v6roms 1994 and the bible resnlk et al m press it is likely that these corpora do not provide enough appropriate data to reliably determine sense distinctions also t is not clear how the lexlcahzatlon of sense distractions across languages is affected by genre domain style etc thls paper attempts to provide some prehmlnary answers to the questions outhned above in order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions and ff so the ways in which ths reformation might be used given the lack of lalge parallel texts across multiple languages the study is necessarily hmlted however close exammanon of a small sample of parallel data can as a first step provide the basis and dlrectmn for more extensive studies 1 methodology i have conducted a small study using parallel aligned versmns ot george orwells nineteen etghtvfolr euavec and ide 1998m five languages enghsh slovene estonian romanlan and czech i the study therefole involves languages from four language families the owell parallel corpus also includes versons o ntneteenegho four m hungarian bulgarmn latwan llthuaman sebmn and russmn germanic slavic fmnougrec and romance two languages from the same family czech and slovene as well as one nonindoeuropean language estoman nmeteen eightyfour is a text of about 100000 words translated directly from the original english m each of the other languages the parallel versions of the text are sentencealigned to the english and tagged for part of speech although nineteen eightyfour is a work of fiction orwells prose is not highly stylized and as such it provides a reasonable sample ot modern ordinary language that s not tied to a given topic or subdomain such as newspapers technical reports etc furthermore the translations of the text seem to be relatively faithful to the original for instance over 95 ot the sentence alignments in the full pmallel corpus of seven languages are onetoone prlestdorman et al 1997 nine ambiguous english words were considered hard head country hne promise shght seize scrap float the first four were chosen because they have been used in other dlsambguatlon studies the latter five were chosen from among the words used m the senseval dlsamblguatlon exercise kllgamff and palmer forthcoming in all cases the study was necessarily hmlted to words that occurred frequently enough in the orwell text to warrant consideration fve hundred fortytwo sentences contanmg an occurrence or occurrences including morphological variants of each of the nine words were extracted from the enghsh text together wth the parallel sentences m which they occur m the texts ot the four comparison languages czech estonian romantan slovene as walks and stevenson 1998 have pointed out patofspeech tagging accomplishes a good portion of the work ot semantic dlsambguatmn therefore occmrences of wolds that appemed in the data in more than 54 one part of speech were grouped separately 2 the enghsh occurrences were then grouped usmg the sense distinctions m wordnet version 1 6 miller et al 1990 fellbaum 1998 the sense categonzatmn was performed by the author and two student assistants results from the three were compared and a final mutually agreeable set of sense assignments was estabhshed for each of the four comparison languages the corpus of sensegrouped parallel sentences were sent to a llngmst and natlve speaker of the comparison language the hngmsts were asked to provide the lexlcal item m each parallel sentence that corresponds to the ambiguous enghsh word if inflected they were asked to provide both the inflected form and the root form in addttmn the lmgmsts were asked to indicate the type of translatmn according to the dtstmctmns given m table 1 for over 85 of the enghsh word occurrences corresponding to types 1 and 2 m table 1 a specific lexlcal item or items could be identified as the translation equivalent for the corresponding enghsh word for comparison purposes each translanon equivalent was represented by ts lemma or the lemma of the toot form in the case of derivatives and associated wth the wordnet sense to which it corresponds in order to determine the degree to which the assigned sense dlstlncttons correspond to translation eqmvalents a coherence index cl was computed that measures how often each pmr of senses is translated usmg the same word as well as the consistency with which a gven selsz s translated with the same word note that the z the adjective and adverb senses of hard are consadeied together because the distinction is not consistent across the translations used m the study note that the ci s similar to semanuc entropy melamed 1997 however melamed computes cis do not determine whether or not a sense dtstmctton can be lextcahzed in the target language but only the degree to whmh they are lexicahzed differently m the translated text however tt can be assumed that the cis provide a measure of the tendency to lexcahze different wordnet senses differently which can m turn be seen as an mdtcatmn of the degree to which the distraction ts vahd for each ambiguous word the ci is computed for each pair of senses as follows sq t clsqs 1 m rnrt where n s the number of comparison languages under consideration nlq and m are the ntmber of occurrences olsense sqand sense s m the enghsh corpus respectively including occurrences that have no idenufiable translation s m ts the number of times that senses q and r are translated by the same lexcal item m language t i e xy t tjan q roan r the ci ts a value between 0 and 1 computed by examining clusters of occurrences translated by the same word in the othel languages if sense and sense are consistently translated wth the same wod in each comparison language then cls s 1 if they are translated with a different word m every occurrence cls 0 in general the ci for pans of different senses provides an index of thmr relatedness t e the greater the value of cls sj the more frequently occurrences ofsense t and sense j are translated with the same lextcal item when t j we entropy tol wold types lather than word senses 55 obtain a measure of the coherence of a lven sense type meaning 1 a slngle lexlcal item is used to translate the enizsh equivalent possibly a 2 the english word is translated by a phrase of two or more words or a compound meaning as the slngle english word 3 the enizsh word is not lexzcalized in the translation 4 a pronoun is substituted for the english word in the translation an english phrase contalnmng the ambiguous word is translated by a single language which has a broader or more specific meanlng or by a phrase in whl corresponding to the english word is not explicltl lexlcallzed table 1 translation types and their trequencles dizen whlh h 6 6 6 of s p same word description hard 1 1 difficult 2 head i i i 1 table 2 1 2 metahorlcally hard 3 not yielding to pressure 1 4 very strong or lgorous ar 2 i wlth force or vigor adv 3 earnestly intently adv i art of the body 3 intellect 4 rler chef 7 front front part woldnet senses ot hard and head cis were also computed for each language individually as well as for different language groupings romaman czech and estonian three different language families czech and slovene same family romaman czech slovene indoeuropean and estonian nonindoeuropean to better visualize the relationship between senses a hierarchical clustering algorithm was applied to the ci data to generate trees reflecting sense proximity 4 finally in order to determine the degree to which the linguistic relauon between languages may affect coherence a correlation was run among cis for all pairs of the four target languages fol example table 2 gives the senses of hard and head that occurred in the data s the ci data s sobs hard and head are given in tables 3 and 4 uous cis measuring the aff mty of a sense with itselfthat is the tendency for all occurrences of that sense to be translated wlth the same wordshow that all of the sx senses of had have greatel internal consistency tfian athmty with other senses with senses 1 1 dlffcult ci 56 and 13 not soft ci 63 registenng the hghest internal consistency 6 the same holds true for three of the four senses of head while the ci for senses 1 3 intellect and 1 1 part of the body is higher than the ci for 1 31 3 wordnet sense 2 1 2 3 1 4 1 3 1 1 1 2 21 23 1 4 13 0 50 o 13 i ool 0 o0 0 25 i o0 0 04 0 50 0 17 0 56 0 19 0 00 0 00 0 00 0 00 0 00 0 25 0 21 table 3 cis for hard i i 12 063 0 00 0 50 2 results although the data sample is small it gives some insight into ways m which a larger sample might contribute to sense discrimination 4 developed by andleas stolcke results tor all words m the study are avadable at httpwww cs vassar edudewsdcrosshng html 6 senses 2 3 and 1 4 have cis ot 1 because each ot these senses exists m a single occurrence m the corpus and have theretote been dlscarded horn consideration ot cis to individual senses we ae currently mvesugatmg the use oi the kappa staustc carletta 1996 to normahze these sparse data 56 wordnet sense 1 1 1 3 1 4 1 7 1 1 0 69 1 3 0 53 0 45 1 4 0 12 0 07 0 50 1 7 0 40 0 001 0 00 1 00 table 4 cis for head figure 2 shows the sense clusters for hard generated from the ci data 7 the senses fall into two mare clusters wth the two most internally consistent senses 1 1 and 1 3 at the deepest level of each ot the respecuve groups the two adverbml forms 8 are placed in separate groups leflectmg thmr semantic proximity to the different adjecuval meanings of hard the clusters for head figure 2 stmdarly show two dlstmct groupings each anchored in the two senses with the hghest internal consistency and the lowest mutual ci part of the body 1 1 and ruler chief 1 4 the herarchtes apparent m the cluster graphs make intuitive sense structured hke dictmnary enmes the clusters for hard and head might appeal as m fgure 1 this ts not dissimilar to actual dlctlonary entries for hard and head for example the enmes for hard in four differently constructed dlctmnanes colhns enghsh ced longmans ldoce oxjotd advanced learners oald and cobuild all hst the dfficult and not soft senses first and second whmh since most dictionaries hst the most common ol frequently used senses hrst reflects the gross dlwslon apparent m the clusters beyond this t s difficult to assess the 7 foi the purposes ot the cluster analyss cis of l 00 resulting from a single occurrrence were normahzed to 5 8 because oot to ms were used m the analysis no dzstlncuon m uanslauon eqmvalents was made tor part ot speech correspondence between the senses in the dictionary entries and the clusters the remamlng wordnet senses are scattered at various places within the entries or m some cases split across various senses the herarchlcal relatmns apparent m the clusters are not reflected m the dcttonary enmes smce the senses are for the most part presented in flat hnear hsts however it is interesting to note that the first five senses of hard in the cobuild dcuonary which is the only dcttonary in the group constructed on the bass of colpus examples 9 and presents senses m ruder of frequency correspond to hve of the six wordnet senses in thls study wordnets metaphorically hard is spread over multiple senses in the cob uild as itis in the other dctlonarles hard head i 1 dlfflcult 2 vlgorously ii 1 a not soft b strong 2 a earnestly b metaphorlcally hard i 1 a part of the body b zntellect 2 front front part ii ruler chlef flgme 1 clusteis tol hard and head suuctured as dlcuonary entt es the results tor dlftment language groupings show that the tendency to lextcahze senses differently is not aftected by language dstance table 5 in fact the mean ci fol estonian the only nonindoeuropean language m the study s lower than that for any other group mdmatmg that wordnet sense dtstmctmns are slightly less hkely to be lexlcahzed differently m estonian 9 edmons ot the ldoce 1987 vexsmn and oald 1985 version dictlonalles consulted m this study pledate edmons ol those same dctlonanes based on colpus evidence 57 correlations of cis for each language pair table 5 also show no relationship between the degree to which sense dstmcuons are lexlcahzed differently and language distance this is contrary to results obtained by resmk and yarowsky submtted who using a memc slmdar to the one used in this study found that that nonindoeuropean languages tended to lexlcallze english sense dstmctlons more than indoeuropean languages especially at finergrained levels however their translation data was generated by native speakers presented with isolated sentences in english who were asked to provide the translation for a given word in the sentence it is not clear how this data compares to translations generated by trained translators working with full context lanquaqe qroup averaqe ci all 0 27 roessl 0 28 slcs 0 28 roslcs 0 27 es 0 26 table 5 average ci values lanqs hard country llne head ave escs 0 86 0 72 0 68 0 69 0 74 rosl 0 73 0 78 0 68 1 00 0 80 rocs 0 83 0 66 0 67 0 72 0 72 slcs 0 88 0 51 0 72 0 71 0 71 roes 0 97 0 26 0 70 0 98 0 73 essl 0 73 0 59 0 90 0 99 0 80 table 6 ci correlauon tor the tour target languages i i i i i mnlmum dlstance 0 249399 mnlmum dstance 0 434856 mlnlmum dlstance 0 555158 mlnlmum dlstance 0 602972 mnlmum dlstance 0 761327 i 21 i ii i 23 l 13 l 14 i 12 13 23 12 14 ii 21 1412 2313 2 3 1 3 1 4 1 2 2 111 figure 2 cluster tree and distance measures tor the sm senses of hard i 14 i i i i i 1 j i 3 i 17 mlnlmum dlstance 0 441022 mlnlmum dlstance 0 619052 mlnmum dlstance 0 723157 13 ll 17 1113 111317 14 fgure 3 cluster tree and dmtance measures tot the tout senses ot head 58 conclusion the small sample m this study suggests that crosshngual lexlcahzaton can be used to define and structure sense dstmctons the cluster graphs above provide mformaton about relations among wordnet senses that could be used for example to determine the granularity of sense differences whtch m turn could be used in tasks such as machine translatton mtormauon retrieval etc for example it is hkely that as sense dtstmcttons become finer the degree of error s less severe resmk and yarowsky 1997 suggest that confusing freergrained sense dtstmctlons should be penahzed less severely than confusing grosser dstmctons when evaluatmg the performance of sense dtsambtguatt0n systems the clusters also provide insight into the lexlcallzatlon of sense dtstmcttons related by various semantic relations metonymy meronymy etc across languages for instance the part of the body and intellect senses of head are lexcahzed with the same tem a sgnlficant portion of the tme across all languages reformation that could be used m machine translatton in addtton cluster data such as that presented here could be used m lexicography to determine a mole detaded hierarchy of relations among senses in dtctonary entries it is less clear how crosshngual reformation can be used to determine sense dstnctlons independent of a predeflned set such as the wordnet senses used here in an effort to explore how thts mlght be done i have used the small sample from thts study to create word groupmgs from back translations l e additional translations m the original language ot the translations m the target language and developed a metric that uses ths mformatton to determine relatedness between occurrences whtch s m turn used to cluster occurrences into sense groups i have also compared sets of back translations for words representing the various wordnet senses which provtde word groups smdar to wordnet synsets interestingly there ts virtually no overlap between the wordnet synsets and word groups generated from back translations the results show however that sense dlstmctlons useful for natural language processing tasks such as machme translanon could potentsally be determined ot at least influenced by constdehng this mformatton the automatically generated synsets themselves may also be useful m the same apphcatlons where wordnet synsets and ontologtes have been used tn the past more work needs to be done on the topic of crosshngual sense determination utthzmg substantially larger parallel corpora that include a variety ot language types as well as texts fiom several genres this small study explores a possible methodology to apply when such resources become avatlable acknowledgements the author would hke to gratefully acknowledge the contrtbuton of those who provided the translatton mfotmaton tomaz euaec slovene kadrt muxschnek estonian vladtmlr petkevtc czech and dan tubs romanlan as well as dana fleut and darnel khne who helped to transcrtbe and evaluate the data special thanks to dan melamed and hlnrtch schutze for their helpful comments 59 in in i i hg nn i an i am references caletta jean 1996 assessing agreement on classthcatton tasks the kappa statstt computational lmgulstlcs 222 249254 dagan ido and ita alon 1994 wod sense dlsambxguaton using a second language monohngual corpus computattonal ltngmsttcs 204 563596 dagan ido ital alon and schwall ulnke 1991 two languages ae more mformattve than one proceedings of the 29th annual meettng of the assoctatton for computattonal ltngutsttcs 1821 june 1991 berkeley cahfornm 130137 dyvtk helge 1998 translations as semantic mirrors proceedmgs of workshop w13 multzlmguahty in the lextcon ii the 13th biennial european conference on arttftctal lntelhgence eca198 brighton uk 2444 eqavec tomaz and ide nancy 1998 the multexteast corpus proceedlng of the fltst international conference on language resources and evaluatton 2730 may 1998 granada 97174 erjavec tomaz lawson ann and romary laurent 1998 east meets west producing multflmgual resources m a european context pioceedtngs of the ftrst internattonal conference on language resources and evaluation 2730 may 1998 gtanada 98186 fellbaum chttstmne ed 1998 wordnet an electrontc lexlcal database mit press cambridge massachusetts gale wdham a church kenneth w and yatowsky davtd 1993 a method tor dlsamblguatmg word senses m a large cmpus computers and the humamtles 26 415439 hearst mattl a 1991 noun homograph dlsamblguatlon using local0ntext m large corpora proceedtngs of the 7th annual conference of the umverlt of waterloo centre for the new oed and text reaeajch oxford umted kingdom 119 ide nancy and v61oms jean 1998 word sense dsambguaton the state of the alt computational lmgutttc 24 1 140 kdgarttt adam and palmer matha eds forthcoming proceedmgs ot the senseval word sense dsambguatlon workshop specml double ssue otcomputer and the humamttes 33 45 leacock claudia towell geoffrey and voorhees ellen 1993 corpusbased stattstlcal sense resolution proceedtng of the arpa human language technology worslshop san francisco morgan kautman melamed i dan 1997 measuring semantic entropy aclsiglex workshop taggmg tert wtth lextcal semanttcs why what and how april 45 1997 washington d c 4146 mtllet george a beckwlth richard t fellbaum ,0,1,0
1 a bilingual language model itg wu 1997 has proposed a bilingual language model called inversion transduction grammar itg which can be used to parse bilingual sentence pairs simultaneously ,0,1,0
for details please refer to wu 1995 wu 1997 ,0,1,0
s bnp vp pp vp mrg1820g10995 wug2568 playsg6183 basketballg12738g10711 one sundayg7155g7411g3837 s g452 figure 1 inversion transduction grammar parsing any itg can be converted to a normal form where all productions are either lexical productions or binaryfanout nonterminal productionswu 1997 ,0,1,0
because the expressiveness characteristics of itg naturally constrain the space of possible matching in a highly appropriate fashion btg achieves encouraging results for bilingual bracketing using a wordtranslation lexicon alone wu 1997 ,1,0,0
the optimal bilingual parsing tree for a given sentencepair can be computed using dynamic programming dp algorithmwu 1997 ,0,1,0
415458 wu dekai 1997 stochastic inversion transduction grammars and bilingual parsing of parallel corpora ,0,1,0
to deal with the difficulties in parsetoparse matching wu 1997 utilizes inversion transduction grammar itg for bilingual parsing ,0,1,0
22 the crossing constraint according to wu 1997 crossing constraint can be defined in the following ,0,1,0
in addition wu 1997 used a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments ,0,1,0
bilingual bracketing methods were used to produce a word alignment in wu 1997 ,0,1,0
32 itg constraints in this section we describe the itg constraints wu 1995 wu 1997 ,0,1,0
bilingual bracketing methods were used to produce a word alignment in wu 1997 ,0,1,0
this gives the translation model more information about the structure of the source language and further constrains the reorderings to match not just a possible bracketing as in wu 1997 but the specific bracketing of the parse tree provided ,0,1,0
2 the inversion transduction grammar the inversion transduction grammar of wu 1997 can be thought as a a generative process which simultaneously produces strings in both languages through a series of synchronous contextfree grammar productions ,0,1,0
in our experiments we use a grammar with a start symbol s a single preterminal c and two nonterminals a and b used to ensure that only one parse can generate any given wordlevel alignment ignoring insertions and deletions wu 1997 zens and ney 2003 ,0,1,0
the trees may be learned directly from parallel corpora wu 1997 or provided by a parser trained on handannotated treebanks yamada and knight 2001 ,0,1,0
inversion transduction grammar itg is the model of wu 1997 treetostring is the model of yamada and knight 2001 and treetostring clone allows the node cloning operation described above ,0,1,0
the straightforward way is to first generate the best btg tree for each sentence pair using the way of wu 1997 then annotate each btg node with linguistic elements by projecting sourceside syntax tree to btg tree and finally extract rules from these annotated btg trees ,0,1,0
1 introduction formal grammar used in statistical machine translation smt such as bracketing transduction grammar btg proposed by wu 1997 and the synchronous cfg presented by chiang 2005 provides a natural platform for integrating linguistic knowledge into smt because hierarchical structures produced by the formal grammar resemble linguistic structures ,0,1,0
many grammars such as finitestate grammars fsg bracketinversion transduction grammars btgitg wu 1997 contextfree grammar cfg tree substitution grammar tsg comon et al 2007 and their synchronous versions have been explored in smt ,0,1,0
coling 2008 companion volume posters and demonstrations pages 103106 manchester august 2008 range concatenation grammars for translation anders sgaard university of potsdam soegaardlingunipotsdamde abstract positive and bottomup nonerasing binary range concatenation grammars boullier 1998 with at most binary predicates 22brcgs is a ogn6 time strict extension of inversion transduction grammars wu 1997 itgs ,0,1,0
it is shown that 22brcgs induce insideout alignments wu 1997 and crossserial discontinuous translation units cdtus both phenomena can be shown to occur frequently in many handaligned parallel corpora ,0,1,0
itgs translate into simple 22brcgs in the following way see wu 1997 for a definition of itgs ,0,1,0
insideout alignments wu 1997 such as the one in example 13 cannot be induced by any of these theories in fact there seems to be no useful synchronous grammar formalisms available that handle insideout alignments with the possible exceptions of synchronous treeadjoining grammars shieber and schabes 1990 bertsch and nederhof 2001 and generalized multitext grammars melamed et al 2004 which are all way more complex than itg stsg and 22brcg ,0,0,1
in designing leaf we were also inspired by dependencybased alignment models wu 1997 alshawi et al 2000 yamada and knight 2001 cherry and lin 2003 zhang and gildea 2004 ,0,1,0
smt has evolved from the original wordbased approach brown et al 1993 into phrasebased approaches koehn et al 2003 och and ney 2004 and syntaxbased approaches wu 1997 alshawi et al 2000 yamada and knignt 2001 chiang 2005 ,0,1,0
construct a parse chart with a cky parser simultaneously constrained on the foreign string and english tree similar to the bilingual parsing of wu 1997 1 ,0,1,0
the goal of integrating syntactic information into the translation model has prompted many researchers to pursue treebased transfer models wu 1997 alshawi et al 1998 yamada and knight 2001 melamed 2004 menezes and quirk 2005 galley et al 2006 with increasingly encouraging results ,0,1,0
108 to follow related work and to focus on the effects of the language model we present translation resultsunderaninversiontransductiongrammaritg translation model wu 1997 trained on the europarl corpus koehn 2005 described in detail in section 3 and using a trigram language model ,0,1,0
3 inversion transduction grammars while our approach applies in principle to a variety of machine translation systems phrasebased or syntactic we will use the inversion transduction grammar itg approach of wu 1997 to facilitate comparison with previous work zens and ney 2003zhangandgildea2008aswellastofocuson language model complexity ,0,1,0
string alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like itg wu 1997but duchi et al ,0,0,1
2007 are appealing as they have rather simple structure modeling only np vp and lcp via onelevel subtree structure with two children in the source parsetree a special case of itg wu 1997 ,0,1,0
we use binary synchronous contextfree grammar bscfg based on inversion transduction grammar itg wu 1997 chiang 2005a to define the set of eligible segmentations for an aligned sentence pair ,0,1,0
coming from the other direction such observations about phrase reordering between different languages are precisely thekindsoffactsthatparsingapproachestomachine translation are designed to handle and do successfully handle wu 1997 melamed 2003 chiang 2005 ,1,0,0
to be able identify that adjacent blocks eg the development and and progress can be merged into larger blocks our model infers binary nonlinguistic trees reminiscent of wu 1997 chiang 2005 ,0,1,0
early examples of this work include alshawi 1996 wu 1997 more recent models include yamada and knight 2001 eisner 2003 melamed 2004 zhang and gildea 2005 chiang 2005 quirk et al 2005 marcu et al 2006 zollmann and venugopal 2006 nesson et al 2006 cherry 2008 mi et al 2008 shen et al 2008 ,0,1,0
since many concepts are expressed by idiomatic multiword expressions instead of single words and different languages may realize the same concept using different numbers of words ma et al 2007 wu 1997 word alignment based methods which are highly dependent on the probability information at the lexical level are not well suited for this type of translation ,0,1,0
in this paper we bring forward the first idea by studying the issue of how to utilize structured syntactic features for phrase reordering in a phrasebased smt system with btg bracketing transduction grammar constraints wu 1997 ,0,1,0
s s0n sik sij sjk si1i pii figure 1 a grammar for a large neighborhood of permutations given one permutation pi of length n the sik rules are instantiated for each 0 i j k n and the si1i rules for each 0 in we say that two permutations are neighbors iff they can be aligned by an inversion transduction grammar itg wu 1997 which is a familiar reordering device in machine translation ,0,1,0
joint parsing with a simplest synchronous contextfree grammar wu 1997 is on6 as opposed to the monolingual on3 time ,0,1,0
someworkwithintheframework of synchronous grammars wu 1997 melamed 2003 while others create a generative story that includes a parse tree provided for one of the sentences yamada and knight 2001 ,0,1,0
to enable such techniques we bring the cohesion constraint inside the itg framework wu 1997 ,0,1,0
22 itg space inversion transduction grammars or itgs wu 1997 provide an efficient formalism to synchronously parse bitext ,1,0,0
this results in two forbidden alignment structures shown in figure 1 called insideout transpositions in wu 1997 ,0,1,0
zens and ney 2003 explore the reorderings allowed by itgs and provide a formulation for the number of structures that can be built for a sentence pair of size n itgs explore almost all of permutation space when n is small but their coverage of permutation space falls off quickly for n 5 wu 1997 ,0,1,0
31 a simple solution wu 1997 suggests that in order to have an itg take advantage of a known partial structure one can simply stop the parser from using any spans that would violate the structure ,0,1,0
having a single canonical tree structure for each possible alignment can help when flattening binary trees as it indicates arbitrary binarization decisions wu 1997 ,0,1,0
normally one would eliminate the redundant structures produced by the grammar in 1 by replacing it with the canonical form grammar wu 1997 which has the following form s a b c a ab bb cb ac bc cc b aa baca ac bccc c ef 2 by design this grammar allows only one struc147 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a2 a8 a10 a8 a1 a2 a3 a6 a8 a4 a7 a8 a6 a8 a9 a8 a8 a11 a12 a11 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a0 a1 a2 a0 a3 a4 a2 a5 a1 a6 a7 a8 a6 a8 a9 a8 a13 a11 figure 3 an example of how dependency trees interact with itgs ,0,1,0
wu 1995 1997 investigated the use of concurrent parsing of parallel corpora in a transduction inversion framework helping to resolve attachment ambiguities in one language by the coupled parsing state in the second language ,1,0,0
in this respect it resembles bilingual bracketing wu 1997 but our model has more lexical items in the blocks with manytomany word alignment freedom in both inner and outer parts ,0,1,0
these techniques included unweighted fs morphology conditional random fields lafferty et al 2001 synchronous parsers wu 1997 melamed 2003 lexicalized parsers eisner and satta 199922 partially supervised training a la pereira and schabes 199223 and grammar induction klein and manning 2002 ,0,1,0
since one of these filters restricts the number of nonterminal symbols to two our extracted grammar is equivalent to an inversion transduction grammar wu 1997 ,0,1,0
among the several proposals we mention here the models presented in wu 1997 wu and wong 1998 alshawi et al 2000 yamada and knight 2001 gildea 2003 and melamed 2003 ,0,1,0
this problem has been considered for instance in wu 1997 for his inversion transduction grammars and has applications in the support of several tasks of automatic annotation of parallel corpora as for instance segmentation bracketing phrasal and word alignment ,0,1,0
we present a new implication of wus 1997 inversion transduction grammar itg hypothesis on the problem of retrieving truly parallel sentence translations from large collections of highly nonparallel documents ,0,1,0
however formally syntaxbased methods propose simple but efficient ways to parse and translate sentences wu 1997 chiang 2005 ,0,1,0
concluding remarks formalisms for finitestate and contextfree transduction have a long history eg lewis and stearns 1968 aho and ullman 1972 and such formalisms have been applied to the machine translation problem both in the finitestate case eg vilar et al 1996 and the contextfree case eg wu 1997 ,0,1,0
this also makes our grammar weakly equivalent to an inversion transduction grammar wu 1997 although the conversion would create a very large number of new nonterminal symbols ,0,1,0
because our system uses a synchronous cfg it could be thought of as an example of syntaxbased statistical machine translation mt joining a line of research wu 1997 alshawi bangalore and douglas 2000 yamada and knight 2001 that has been fruitful but has not previously produced systems that can compete with phrasebased systems in largescale translation tasks such as the evaluations held by nist ,0,1,0
at one extreme are those exemplified by that of wu 1997 that have no dependence on syntactic theory beyond the idea that natural language is hierarchical ,0,1,0
another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models yamada and knight 2001 wu 1997 ,0,1,0
methods such as wu 1997 alshawi et al 2000 and lopez et al 2002 employ a synchronous parsing procedure to constrain a statistical alignment ,0,1,0
more recently there have been many proposals to introduce syntactic knowledge into smt models wu 1997 alshawi et al 2000 yamada and knight 2001 lopez et al 2002 ,0,1,0
recently specific probabilistic treebased models have been proposed not only for machine translation wu 1997 alshawi bangalore and douglas 2000 yamada and knight 2001 gildea 2003 eisner 2003 but also for this work was supported by darpa contract f496200010337 and arda contract mda90402c0450 ,0,1,0
wu 1997 introduced constraints on alignments using a probabilistic synchronous contextfree grammar restricted to chomskynormal form ,0,1,0
wu 1997 was an implicit or selforganizing syntax model as it did not use a treebank ,0,1,0
one approach here is that of wu 1997 in which wordmovement is modeled by rotations at unlabeled binarybranching nodes ,0,1,0
some approaches have used syntax at the core wu 1997 alshawi et al 2000 yamada and knight 2001 gildea 2003 eisner 2003 hearne and way 2003 melamed 2004 while others have integrated syntax into existing phrasebased frameworks xia and mccord 2004 chiang 2005 collins et al 2005 quirk et al 2005 ,0,1,0
we use and for straight and inverted combinations respectively following the itg notation wu 1997 ,0,1,0
one way around this dif culty is to stipulate that all rules must be binary from the outset as in inversiontransduction grammar itg wu 1997 and the binary synchronous contextfree grammar scfg employed by the hiero system chiang 2005 to model the hierarchical phrases ,0,1,0
wu 1997 shows that parsing a binary scfg is in ow6 while parsing scfg is nphard in general satta and peserico 2005 ,0,1,0
this problem can be cast as an instance of synchronous itg parsing wu 1997 ,0,1,0
we can however produce a useful surrogate a pair of monolingual wcfgs with structures projected by g and weights that when combined underestimate the costs of g parsing optimally relative to a synchronous grammar using a dynamic program requires time on6 in the length of the sentence wu 1997 ,0,1,0
they are most commonly used for parsing and linguistic analysis charniak and johnson 2005 collins 2003 but are now commonly seen in applications like machine translation wu 1997 and question answering wang et al 2007 ,0,1,0
wu 1997 and alshawi et al ,0,1,0
other statistical machine translation systems such as wu 1997 and alshawi et al 2000 also produce a tree a15 given a sentence a16 their models are based on mechanisms that generate two languages at the same time so an english tree a15 is obtained as a subproduct of parsing a16 however their use of the lm is not mathematically motivated since their models do not decompose into pa4a5a2a9a8a3a10a6 and a12a14a4a5a3a7a6 unlike the noisy channel model ,0,0,1
wu 1997 showed that restricting wordlevel alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomialtime solution ,1,0,0
methods such as wu 1997 alshawi et al 2000 and lopez et al 2002 employ a synchronous parsing procedure to constrain a statistical alignment ,0,1,0
for this purpose we adopt the view of the itg constraints as a bilingual grammar as eg in wu 1997 ,0,1,0
obviously these productions are not in the normal form of an itg but with the method described in wu 1997 they can be normalized ,0,1,0
the first constraints are based on inversion transduction grammars itg wu 1995 wu 1997 ,0,1,0
the parse trees of the simple grammar in wu 1997 ,0,1,0
with this constraint each of these binary trees is unique and equivalent to a parse tree of the canonicalform grammar in wu 1997 ,0,1,0
in wu 1997 these forbidden subsequences are called insideout transpositions ,0,1,0
3however the binarybranching scfgs used by wu 1997 and alshawi et al ,0,1,0
previous work in statistical synchronous grammars has been limited to forms of synchronous contextfree grammar wu 1997 alshawi et al 2000 yamada and knight 2001 ,0,1,0
wu 1997 also includes a brief discussion of crossing constraints that can be derived from phrase structure correspondences ,0,1,0
this normal form allows simpler algorithm descriptions than the normal forms used by wu 1997 and melamed 2003 ,0,1,0
item form a32 a2 a49a51 a15 a52 a49 a51a16a33 goal a32a35a34 a49 a51 a15 a23a4a3 a12 a0a36a5 a24 a49 a51a37a33 inference rules scan component d a10a38a8 a7 a8 a0 a39a41a40a43a42a44 a44a45 a23a25a24 a49 a5a47a46 a49 a2 a23a25a24 a5a49a48 a49 a51 a50 a23a25a24 a49 a5a47a46 a49 a20a43a5 a3a22 a23a25a24 a5a49a48 a49 a51 a51a14a52 a52 a53 a54a55 a55 a56 a23a25a24 a49 a5a47a46 a49 a2 a23a25a24 a5a49a48 a49 a51 a50 a23a25a24 a49 a5a47a46 a49a23 a19a57a24 a10a13a12 a19 a24 a23a25a24 a5a49a48 a49 a51 a58a59 a59 a60 compose a61a63a62a65a64 a66a68a67a69 a64 a66a71a70 a61a35a72a37a64 a66a68a67a73 a64 a66a71a70a36a74a76a75 a32a78a77 a64 a66a76a67a69 a64 a66a80a79a81a73 a64 a66 a14 a62a82a64 a66 a14 a72a37a64 a66 a33 a10 a77 a64 a66 a67a69 a64 a66a37a83 a73 a64 a66 a18 figure 3 logic c c for cky these constraints are enforced by the dspan operators a84 and a85 parser c is conceptually simpler than the synchronous parsers of wu 1997 alshawi et al ,0,1,0
thus gcnf is a more restrictive normal form than those used by wu 1997 and melamed 2003 ,0,1,0
inversion transduction grammar itg wu 1997 and syntaxdirected translation schema sdts aho and ullman 1969 lack both of these properties ,0,1,0
in previous alignment methods some researchers modeled the alignments with different statistical models wu 1997 och and ney 2000 cherry and lin 2003 ,0,1,0
the simplest wu 1997 uses constitnp35np48 to denote a np spanning positions 35 in the english string that is aligned with an np spanning positions 48 in the chinese string ,0,1,0
in this respect it resembles wus 264 bilingual bracketer wu 1997 but ours uses a different extraction method that allows more than one lexical item in a rule in keeping with the phrasebased philosophy ,0,1,0
in recent years many researchers have employed statistical models wu 1997 och and ney 2003 cherry and lin 2003 or association measures smadja et al 1996 ahrenberg et al 1998 tufis and barbu 2002 to build alignment links ,0,1,0
wu 1997 demonstrated that for pairs of sentences that are less than 16 words the itg alignment space has a good coverage over all possibilities ,0,1,0
the itg we apply in our experiments has more structural labels than the primitive bracketing grammar it has a start symbol s a single preterminal c and two intermediate nonterminals a and b used to ensure that only one parse can generate any given wordlevel alignment as discussed by wu 1997 and zens and ney 2003 ,0,1,0
1 introduction the inversion transduction grammar itg of wu 1997 is a syntactically motivated algorithm for producing wordlevel alignments of pairs of translationally equivalent sentences in two languages ,0,1,0
212 research on syntaxbased smt a number of researchers alshawi 1996 wu 1997 yamada and knight 2001 gildea 2003 melamed 2004 graehl and knight 2004 galley et al 2004 have proposed models where the translation process involves syntactic representations of the source andor target languages ,0,1,0
here under the itg constraint wu 1997 zens et al 2004 we need to consider just two kinds of reorderings straight and inverted between two consecutive blocks ,0,1,0
wu 1997 proposes inversion transduction grammars treating translation as a process of parallel parsing of the source and target language via a synchronized grammar ,0,1,0
in the hierarchical phrasebased model chiang 2005 and an inversion transduction grammar itg wu 1997 the problem is resolved by restricting to a binarized form where at most two nonterminals are allowed in the righthand side ,1,0,0
7 related work similarly to poutsma 2000 wu 1997 yamada and knight 2001 chiang 2005 the rules discussed in this paper are equivalent to productions of synchronous tree substitution grammars ,0,1,0
wu 1997 has been unable to find real examples of cases where hierarchical alignment would fail under these conditions at least in fixedwordorder languages that are lightly inflected such as english and chinese p 385 ,1,0,0
following wu 1997 the prevailing opinion in the research community has been that more complex patterns of word alignment in real bitexts are mostly attributable to alignment errors ,0,1,0
a hierarchical alignment algorithm is a type of synchronous parser where instead of constraining inferences by the production rules of a grammar the constraints come from word alignments and possibly other sources wu 1997 melamed and wang 2005 ,0,1,0
the inversion transduction grammar or itg formalism described in wu 1997 is well suited for our purposes ,1,0,0
wu 1997 provides anecdotal evidence that only incorrect alignments are eliminated by itg constraints ,0,0,1
fortunately wu 1997 provides a method to have an itg respect a known partial structure ,1,0,0
in recent years many researchers build alignment links with bilingual corpora wu 1997 och and ney 2003 cherry and lin 2003 wu et al 2005 zhang and gildea 2005 ,0,1,0
953 2 bilexicalization of inversion transduction grammar the inversion transduction grammar of wu 1997 models word alignment between a translation pair of sentences by assuming a binary synchronous tree on top of both sides ,0,1,0
synchronous grammar formalisms that are capable of modeling such complex relationships while maintaining the contextfree property in each language have been proposed for many years aho and ullman 1972 wu 1997 yamada and knight 2001 melamed 2003 chiang 2005 but have not been scaled to large corpora and long sentences until recently ,0,0,1
in this paper we focus on the second issue constraining the grammar to the binarybranching inversion transduction grammar of wu 1997 ,0,1,0
alternatively order is modelled in terms of movement of automatically induced hierarchical structure of sentences chiang 2005 wu 1997 ,0,1,0
1 introduction syntactic methods are an increasingly promising approach to statistical machine translation being both algorithmically appealing melamed 2004 wu 1997 and empirically successful chiang 2005 galley et al 2006 ,0,1,0
a few exceptions are the hierarchical possibly syntaxbased transduction models wu 1997 alshawi et al 1998 yamada and knight 2001 chiang 2005 and the string transduction models kanthak et al 2005 ,0,1,0
wu 1997 ,0,1,0
we use a bootstrap approach in which we first extract 1ton word alignments using an existing word aligner and then estimate the confidence of those alignments to decide whether or not the n words have to be grouped if so this group is conwould thus be completely driven by the bilingual alignment process see also wu 1997 tiedemann 2003 for related considerations ,0,1,0
instead of using inversion transduction grammar itg wu 1997 directly we will discuss an itg extension to accommodate gapping ,0,1,0
the utility of itg as a reordering constraint for most language pairs is wellknown both empirically zens and ney 2003 and analytically wu 1997 howeveritgsstraight monotoneandinverted reverse rules exhibit strong cohesiveness which is inadequate to express orientations that require gaps ,0,0,1
among the grammar formalisms successfully put into use in syntaxbased smt are synchronous contextfree grammars scfg wu 1997 and synchronous treesubstitutiongrammarsstsgyamadaandknight 2001 ,0,1,0
methods for syntactic smt held to this assumption in its entirety wu 1997 yamada and knight 2001 ,0,1,0
2 phrasal inversion transduction grammar we use a phrasal extension of inversion transduction grammar wu 1997 as the generative framework ,0,1,0
depending on the type of input these efforts can be divided into two broad categories the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar wu 1997 chiang 2005 galley et al 2006 and the treebased systems whose input is already a parse tree to be directly converted into a target tree or string lin 2004 ding and palmer 2005 quirk et al 2005 liu et al 2006 huang et al 2006 ,0,1,0
this is an instance of the itg alignment algorithm wu 1997 ,0,1,0
recently many syntaxbased models have been proposed to address the above deficiencies wu 1997 chiang 2005 eisner 2003 ding and palmer 2005 quirk et al 2005 cowan et al 2006 zhang et al 2007 bod 2007 yamada and knight 2001 liu et al 2006 liu et al 2007 gildea 2003 poutsma 2000 hearne and way 2003 ,1,0,0
an alternative to tercom considered in this paper is to use the inversion transduction grammar itg formalism wu 1997 which allows one to view the problem of alignment as a problem of bilingual parsing ,0,1,0
research in this direction was pioneered by wu 1997 who developed inversion transduction grammars to capture crosslingual grammar variations such as phrase reorderings ,1,0,0
in this paper we implement the sdb model in a stateoftheart phrasebased system which adapts a binary bracketing transduction grammar btg wu 1997 to phrase translation and reordering described in xiong et al 2006 ,0,1,0
most related to our approach wu 2005 used inversion transduction grammarsa synchronous contextfree formalism wu 1997for this task ,0,1,0
2006 develop a bottomup decoder for btg wu 1997 that uses only phrase pairs ,0,1,0
moreover the inference procedure for each sentence pair is nontrivial proving npcomplete for learning phrase based models denero and klein 2008 or a high order polynomial of3e31 for a subclass of weighted synchronous context free grammars wu 1997 ,0,1,0
this source of overcounting is considered and fixed by wu 1997 and zens and ney 2003 which we briefly review here ,1,0,0
null productions are also a source of double counting as there are many possible orders in 926 n i 2 n in n i n in i i i n n n a normal domain rules i squigglerightn 2 i squigglerightni i squigglerightni i squigglerightn n n n i i i b inverted domain rules n 11 fn 11 n 11 n 10 n 10 n 10 e n 10 n 00 n 11 f n 10 n 10 n 00 e n 00 i 11 n ni 11 n ni 00 n 00 i 11 i 00 n 00 n 10 n 10 n 11 n n i 11 i 11 i 00 n 00 n 11 c normal domain with null rules i 11 squiggleright fi 11 i 11 squigglerighti 10 i 11 squiggleright f i 10 i 10 squiggleright i 10 e i 10 squigglerighti 00 i 10 squiggleright i 00 e i 00 squigglerightn 11 n 00 i i n 00 n 11 n 11 i 00 squigglerightn 11 i i squigglerightn 11 i i squigglerightn 00 i 00 i 00 i 10 i 10 i 11 i 11 d inverted domain with null rules figure 2 illustration of two unambiguous forms of itg grammars in a and b we illustrate the normal grammar without nulls presented in wu 1997 and zens and ney 2003 ,0,1,0
22 inversion transduction grammar wu 1997s inversion transduction grammar itg is a synchronous grammar formalism in which derivations of sentence pairs correspond to alignments ,0,1,0
the set of such itg alignmentsaitg are a strict subset of a11 wu 1997 ,0,1,0
1 introduction inversion transduction grammar itg constraints wu 1997 provide coherent structural constraints on the relationship between a sentence and its translation ,0,1,0
1 introduction the use of various synchronous grammar based formalisms has been a trend for statistical machine translation smt wu 1997 eisner 2003 galley et al 2006 chiang 2007 zhang et al 2008 ,0,1,0
there are rules though rare that cannot be binarized synchronously at all wu 1997 but can be incorporated in twostage decoding with asynchronous binarization ,0,1,0
the work reported in wu 1997 which uses an insideoutside type of training algorithm to learn statistical contextfree transduction has a similar motivation to the current work but the models we describe here being fully lexical are more suitable for direct statistical modelling ,0,0,1
unfortunately this is not always the case and the above methodology suffers from the weaknesses pointed out by wu 1997 concerning parseparsematch procedures ,0,1,0
the model employs a stochastic version of an inversion transduction grammar or itg wu 1995c wu 1995d wu 1997 ,0,1,0
if the target cfg is purely binary branching then the previous theoretical and linguistic analyses wu 1997 suggest that much of the requisite constituent and word order transposition may be accommodated without change to the mirrored itg ,0,1,0
there are other approaches to statistical machine translation where translation is achieved through transduction of source language structure to target language structure alshawi et al 1998b wu 1997 ,0,1,0
several studies have reported alignment or translation performance for syntactically augmented translation models wu 1997 wang 1998 alshawi et al 2000 yamada and knight 2001 jones and havrilla 1998 and these results have been promising ,1,0,0
2 bilingual bracketing in wu 1997 the bilingual bracketing pcfg was introduced which can be simplified as the following production rules a aa 1 a aa 2 a fe 3 a fnull 4 a nulle 5 where f and e are words in the target vocabulary vf and source vocabulary ve respectively ,0,1,0
however instead of estimating the probabilities for the production rules via em as described in wu 1997 we assign the probabilities to the rules using the model1 statistical translation lexicon brown et al 1993 ,0,1,0
bilingual bracketing wu 1997 is one of the bilingual shallow parsing approaches studied for chineseenglish word alignment ,0,1,0
more suitable ways could be bilingual chunk parsing and refining the bracketing grammar as described in wu 1997 ,1,0,0
this contrasts with alternative alignment models such as those of melamed 1998 and wu 1997 which impose a onetoone constraint on alignments ,0,1,0
this contrasts with alternative alignment models such as those of melamed 1998 and wu 1997 which impose a onetoone constraint on alignments ,0,1,0
wu 1997 and jones and havrilla 1998 have sought to more closely tie the allowed motion of constituents between languages to those syntactic transductions supported by the independent rotation of parse tree constituents ,0,1,0
related works generally speaking approaches to mwe extraction proposed so far can be divided into three categories a statistical approaches based on frequency and cooccurrence affinity b knowledgebased or symbolic approaches using parsers lexicons and language filters and c hybrid approaches combining different methods smadja 1993 dagan and church 1994 daille 1995 mcenery et al 1997 wu 1997 wermter et al 1997 michiels and dufour 1998 merkel and andersson 2000 piao and mcenery 2001 sag et al 2001a 2001b biber et al 2003 ,0,1,0
for example wu 1997 used an englishchinese bilingual parser based on stochastic transduction grammars to identify terms including multiword expressions ,0,1,0
5 synchronous dig 51 definition wu 1997 introduced synchronous binary trees and shieber 1990 introduced synchronous tree adjoining grammars both of which view the translation process as a synchronous derivation process of parallel trees ,0,1,0
graphically speaking parsing amounts to identifying rectangular crosslinguistic constituents by assembling smaller rectangles that will together cover the full string spans in both dimensions compare wu 1997 melamed 2003 ,0,1,0
2 x1x2 y1r1y2r2 i1 j1 i2 j2 y1y2 j1 k1 j2 k2 x1x2 y1r1y2r2 i1 k1 i2 k2 3 x1x2 y1r1y2r2 i1 j1 j2 k2 y1y2 j1 k1 i2 j2 x1x2 y1r1y2r2 i1 k1 i2 k2 since each inference rule contains six free variables over string positions i1 j1 k1 i2 j2 k2 we get a parsing complexity of order on6 for unlexicalized grammars where n is the number of words in the longer of the two strings from language l1 and l2 wu 1997 melamed 2003 ,0,1,0
this model shares some similarities with the stochastic inversion transduction grammars sitg presented by wu in wu 1997 ,0,1,0
whereas language generation has benefited from syntax wu 1997 alshawi et al 2000 the performance of statistical phrasebased machine translation when relying solely on syntactic phrases has been reported to be poor koehn et al 2003 ,1,0,0
45 itg constraints another type of reordering can be obtained using inversion transduction grammars itg wu 1997 ,0,1,0
this model shares some similarities with the stochastic inversion transduction grammars sitg presented by wu in wu 1997 ,0,1,0
c2005 association for computational linguistics recognizing paraphrases and textual entailment using inversion transduction grammars dekai wu1 human language technology center hkust department of computer science university of science and technology clear water bay hong kong dekaicsusthk abstract we present first results using paraphrase as well as textual entailment data to test the language universal constraint posited by wus 1995 1997 inversion transduction grammar itg hypothesis ,0,1,0
moreover for reasons discussed by wu 1997 itgs possess an interesting intrinsic combinatorial property of permitting roughly up to four arguments of any frame to be transposed freely but not more ,1,0,0
the result in wu 1997 implies that for the special case of bracketing itgs the time complexity of the algorithm is parenleftbigt3v 3parenrightbig where t and v are the lengths of the two sentences ,0,1,0
1 introduction the inversion transduction grammar or itg formalism which historically was developed in the context of translation and alignment hypothesizes strong expressiveness restrictions that constrain paraphrases to vary word order only in certain allowable nested permutations of arguments wu 1997 ,0,1,0
this is the same complexity as the itg alignment algorithm used by wu 1997 and others meaning complete viterbi decoding is possible without pruning for realisticlength sentences ,0,1,0
2 machine translation using inversion transduction grammar the inversion transduction grammar itg of wu 1997 is a type of contextfree grammar cfg for generating two languages synchronously ,0,1,0
a related example would be a version of synchronous cfg that allows only one pair of linked nonterminals and any number of unlinked nonterminals which could be bitextparsed in on5 time whereas inversion transduction grammar wu 1997 takes on6 ,0,1,0
alignment whether for training a translation model using em or for nding the viterbi alignment of test data is on6 wu 1997 while translation decoding is on7 using a bigram language model and on11 with trigrams ,0,1,0
1 introduction the inversion transduction grammar itg of wu 1997 is a syntactically motivated algorithm for producing wordlevel alignments of pairs of translationally equivalent sentences in two languages ,0,1,0
wu 1997 and alshawi 1996 describe early work on formalisms that make use of transductive grammars graehl and knight 2004 describe methods for training tree transducers ,0,1,0
2 related work the issue of mwe processing has attracted much attention from the natural language processing nlp community including smadja 1993 dagan and church 1994 daille 1995 1995 mcenery et al 1997 wu 1997 michiels and dufour 1998 maynard and ananiadou 2000 merkel and andersson 2000 piao and mcenery 2001 sag et al 2001 tanaka and baldwin 2003 dias 2003 baldwin et al 2003 nivre and nilsson 2004 pereira et al ,0,1,0
the approach presented here has some resemblance to the bracketing transduction grammars btg of wu 1997 which have been applied to a phrasebased machine translation system in zens et al 2004 ,0,1,0
an alternative method wu 1997 makes decisions at the end but has a high computational requirement ,0,0,1
in this work we focus on learning bilingual word phrases by using stochastic inversion transduction grammars sitgs wu 1997 ,0,1,0
3 stochastic inversion transduction grammars stochastic inversion transduction grammars sitgs wu 1997 can be viewed as a restricted subset of stochastic syntaxdirected transduction grammars ,0,1,0
1a normal form for sitgs can be defined wu 1997 by analogy to the chomsky normal form for stochastic contextfree grammars ,0,1,0
besides our model as being linguistically motivated is also more expressive than the formally syntaxbased models of chiang 2005 and wu 1997 ,0,0,1
the efficient block alignment algorithm in section 4 is related to the inversion transduction grammar approach to bilingual parsing described in wu 1997 in both cases the number of alignments is drastically reduced by introducing appropriate reordering restrictions ,1,0,0
wu 1997 yamada and knight 2001 gildea 2003 melamed 2004 graehl and knight 2004 galley et al 2006 ,0,1,0
in the meantime synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents but continue to be employed primarily for wordtoword analysis wu 1997 ,0,1,0
inversion transduction grammar wu 1997 or itg is a wellstudied synchronous grammar formalism ,1,0,0
stochastic itgs are parameterized like their pcfg counterparts wu 1997 productions a x are assigned probability prxa ,0,1,0
wu 1997 used a binary bracketing itg to segment a sen19 tence while simultaneously wordaligning it to its translation but the model was trained heuristically with a fixed segmentation ,0,1,0
the similarities become moreapparentwhenweconsiderthecanonicalform binarybracketing itg wu 1997 shown here s a b c a ab bb cb ac bc cc b aa ba ca ac bc cc c e f 3 3 is employed in place of 2 to reduce redundant alignments and clean up em expectations1 more importantly for our purposes it introduces a preterminal c which generates all phrase pairs or cepts ,0,1,0
wu 1997 demonstrates the case of binary scfg parsing where six string boundary variables three for each language as in monolingual cfg parsing interact with each other yielding an on6 dynamic programming algorithm where n is the string length assuming the two paired strings are comparable in length ,0,1,0
machine translation based on a deeper analysis of the syntactic structure of a sentence has long been identified as a desirable objective in principle consider wu 1997 yamada and knight 2001 ,0,1,0
synchronous parsing models have been explored with moderate success wu 1997 quirk et al 2005 ,1,0,0
actually now that smt has reached some maturity we see several attempts to integrate more structure into these systems ranging from simple hierarchical alignment models wu 1997 chiang 2005 to syntaxbased statistical systems yamada and knight 2001 zollmann and venugopal 2006 ,0,1,0
a few exceptions are the hierarchical possibly syntaxbased transduction models wu 1997 alshawi et al 1998 yamada and knight 2001 chiang 2005 and the string transduction models kanthak et al 2005 ,0,1,0
other models wu 1997 xiong et al ,0,1,0
23 itg constraints the inversion transduction grammar itg wu 1997 a derivative of the syntax directed transduction grammars aho and ullman 1972 constrains the possible permutations of the input string by defining rewrite rules that indicate permutations of the string ,0,1,0
the other form of hybridization a statistical mt model that is based on a deeper analysis of the syntactic 33 structure of a sentence has also long been identified as a desirable objective in principle consider wu 1997 yamada and knight 2001 ,0,1,0
the idea of synchronous ssmt can be traced back to wu 1997s stochastic inversion transduction grammars ,0,1,0
ibm constraints berger et al 1996 lexical word reordering model tillmann 2004 and inversion transduction grammar itg constraints wu 1995 wu 1997 belong to this type of approach ,0,1,0
examples include wus wu 1997 itg and chiangs hierarchical models chiang 2007 ,0,1,0
2 related work syntaxbased translation models engaged with scfg have been actively investigated in the literature wu 1997 yamada and knight 2001 gildea 2003 galley et al 2004 satta and peserico 2005 ,0,1,0
recent work on reordering has been on trying to find smart ways to decide word order using syntactic features such as pos tags lee and ge 2005 parse trees zhang etal 2007 wang etal 2007 collins etal 2005 yamada and knight 2001 to name just a few and synchronized cfg wu 1997 chiang 2005 again to name just a few ,0,1,0
deeper syntax eg phrase or dependency structures has been shown useful in generative models wang and zhou 2004 lopez and resnik 2005 heuristicbased models ayan et al 2004 ozdowska 2004 and even for syntactically motivated models such as itg wu 1997 cherry and lin 2006 ,0,1,0
the underlying formalisms used has been quite broad and include simple formalisms such as itgs wu 1997 hierarchicalsynchronousruleschiang 2005 string to tree models by galley et al 2004 and galley et al 2006 synchronous cfg models such xia and mccord 2004 yamada and knight 2001 synchronous lexical functional grammar inspired approaches probst et al 2002 and others ,0,1,0
probabilistic generative models like ibm 15 brown et al 1993 hmm vogel et al 1996 itg wu 1997 and leaf fraser and marcu 2007 define formulas for pf e or pe f with okvoon ororok sprok atvoon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat okdrubel okvoon anok plok sprok atdrubel atvoon pippat rrat dat okvoon anok drok brok jok atvoon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok okyurp totat nnat quat oloat atyurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat figure 1 word alignment exercise knight 1997 ,0,1,0
4 related work zhang et al 2003 and wu 1997 tackle the problem of segmenting chinese while aligning it to english ,0,1,0
while traditional approaches to syntax based mt were dependent on availability of manual grammar more recent approaches operate within the resources of pbsmt and induce hierarchical or linguistic grammars from existing phrasal units to provide better generality and structure for reordering yamada and knight 2001 chiang 2005 wu 1997 ,0,1,0
production rules are typically learned from alignment structures wu 1997 zhang and gildea 2004 chiang 2007 or from alignment structures and derivation trees for the source string yamada and knight 2001 zhang and gildea 2004 ,0,1,0
they are also used for inducing alignments wu 1997 zhang and gildea 2004 ,0,1,0
the production rules in itgs are of the following form wu 1997 with a notation similar to what is typically used for sdtss and scfgs in the right column a bc a b1c2b1c2 a bc a b1c2c2b1 a e f a ef a e a e a f a f it is important to note that rhss of production rules have at most one sourceside and one targetside terminal symbol ,0,1,0
of linguistics university of potsdam kuhnlingunipotsdamde abstract the empirical adequacy of synchronous contextfree grammars of rank two 2scfgs satta and peserico 2005 used in syntaxbased machine translation systems such as wu 1997 zhang et al ,0,1,0
2006 and chiang 2007 in terms of what alignments they induce has been discussed in wu 1997 and wellington et al ,0,1,0
2 insideout alignments wu 1997 identified socalled insideout alignments two alignment configurations that cannot be induced by binary synchronous contextfree grammars these alignment configurations while infrequent in language pairs such as englishfrench cherry and lin 2006 wellington et al 2006 have been argued to be frequent in other language pairs incl ,0,1,0
to overcome these limitations many syntaxbased smt models have been proposed wu 1997 chiang 2007 ding et al 2005 eisner 2003 quirk et al 2005 liu et al 2007 zhang et al 2007 zhang et al 2008a zhang et al 2008b gildea 2003 galley et al 2004 marcu et al 2006 bod 2007 ,0,1,0
in this paper two synchronous grammar formalisms are discussed inversion transduction grammars itgs wu 1997 and twovariable binary bottomup nonerasing range concatenation grammars 22brcgs sgaard 2008 ,0,1,0
it is known that itgs do not induce the class of insideout alignments discussed in wu 1997 ,0,1,0
the complexities of 15 restricted alignment problems in two very different synchronous grammar formalisms of syntaxbased machine translation inversion transduction grammars itgs wu 1997 and a restricted form of range concatenation grammars 22brcgs sgaard 2008 are investigated ,0,1,0
2 inversion transduction grammars inversion transduction grammars itgs wu 1997 are a notational variant of binary syntaxdirected translation schemas aho and ullman 1972 and are usually presented with a normal form a bc a bc a ef a e a f where abc n and ef t the first production rule intuitively says that the subtree bca in the source language translates into 62 a subtree bca whereas the second production rule inverts the order in the target language ie cba the universal recognition problem of itgs can be solved in time on6g by a cykstyle parsing algorithm with two charts ,0,1,0
ibm constraints berger et al 1996 the lexical word reordering model tillmann 2004 and inversion transduction grammar itg constraints wu 1995 wu 1997 belong to this type of approach ,0,1,0
however the only known work which automates part of a customer service center using natural language dialogue is the one by chucarroll and carpenter 1999 ,1,0,0
using a vectorbased topic identification process salton 1971 chucarroll and carpenter 1999 these keywords are used to determine a set of likely values including null for that attribute ,0,1,0
research prototypes exist for applications such as personal email and calendars travel and restaurant information and personal banking baggia et al 1998 walker et al 1998 seneff et al 1995 sanderman et al 1998 chucarroll and carpenter 1999 inter alia ,0,1,0
2 experimental system and data hmihy is a spoken dialogue system based on the notion of call routing gorin et al 1997 chucarroll and carpenter 1999 ,0,1,0
1 specifically mimic uses an ndimensional call router frontend chucarroll 2000 which is a generalization of the vectorbased callrouting paradigm of semantic interpretation chucarrou and carpenter 1999 that is instead of detecting one concept per utterance mimics semantic interpretation engine detects multiple n concepts or classes conveyed by a single utterance by using n call touters in parallel ,0,1,0
their approaches include the use of a vectorbased information retrieval technique lee et al 2000 chucarroll and carpenter 1999 binbash line 1 a command not found our do mains are more varied which may results in more recognition errors ,0,1,0
an alternative would be using a vector space model for classi cation where calltypes and utterances are represented as vectors including word a2 grams chucarroll and carpenter 1999 ,0,1,0
this step can be seen as a multilabel multiclass call classi cation problem for customer care applications gorin et al 1997 chucarroll and carpenter 1999 gupta et al to appear among others ,0,1,0
an alternative is to create an automatic system that uses a set of training questionanswer pairs to learn the appropriate questionanswer matching algorithm chucarroll and carpenter 1999 ,0,1,0
chucarroll and carpenter 1999 describe a method of disambiguation where disambiguation questions are dynamically constructed on the basis of an analysis of the differences among the closest routing destination vectors ,0,1,0
lee barzilay 2003 for example use multisequence alignment msa to build a corpus of paraphrases involving terrorist acts ,0,1,0
mean number of instances of paraphrase phenomena per sentence such as multiple sequence alignment as employed by barzilay lee 2003 ,0,1,0
multiple translations of the same text barzilay and mckeown 2001 corresponding articles from multiple news sources barzilay and lee 2003 quirk et al 2004 dolan et al 2004 and bilingual corpus bannard and callisonburch 2005 have been utilized ,0,1,0
some works focused on learning rules from comparable corpora containing comparable documents such as different news articles from the same date on the same topic barzilay and lee 2003 ibrahim et al 2003 ,0,1,0
this is related to the wellstudied problem of identifying paraphrases barzilay and lee 2003 pang et al 2003 and the more general variant of recognizing textual entailment which explores whether information expressed in a hypothesis can be inferred from a given premise ,0,1,0
2 related work previous studies on entailment inference rules and paraphrase acquisition are roughly classified into those that require comparable corpora shinyama et al 2002 barzilay and lee 2003 ibrahim et al 2003 and those that do not lin and pantel 2001 weeds and weir 2003 geffet and dagan 2005 pekar 2006 bhagat et al 2007 szpektor and dagan 2008 ,0,1,0
barzilay and lee 2003 also used newspaper articles on the same event as comparable corpora to acquire paraphrases ,0,1,0
2006 propose using a statistical word alignment algorithm as a more robust way of aligning monolingual outputs into a confusion network for system com2barzilay and lee 2003 construct lattices over paraphrases using an iterative pairwise multiple sequence alignment msa algorithm ,0,1,0
the wordbased edit distance heuristic yields pairs that are relatively clean but offer relatively minor rewrites in generation especially when compared to the msa model of barzilay lee 2003 ,0,1,0
a growing body of recent research has focused on the problems of identifying and generating paraphrases eg barzilay mckeown 2001 lin pantel 2002 shinyama et al 2002 barzilay lee 2003 and pang et al ,0,1,0
barzilay lee 2003 and quirk et al ,0,1,0
g2 2 motivation the success of statistical machine translation smt has sparked a successful line of investigation that treats paraphrase acquisition and generation essentially as a monolingual machine translation problem eg barzilay lee 2003 pang et al 2003 quirk et al 2004 finch et al 2004 ,1,0,0
some studies exploit topically related articles derived from multiple news sources barzilay and lee 2003 shinyama and sekine 2003 quirk et al 2004 dolan et al 2004 ,0,1,0
generation of paraphrase examples was also investigated barzilay and lee 2003 quirk et al 2004 ,0,1,0
such a method alleviates the problem of creating templates from examples which would be used in an ulterior phase of generation barzilay and lee 2003 ,1,0,0
the other utilizes a sort of parallel texts such as multiple translation of the same text barzilay and mckeown 2001 pang et al 2003 corresponding articles from multiple news sources barzilay and lee 2003 dolan et al 2004 and bilingual corpus wu and zhou 2003 bannard and callisonburch 2005 ,0,1,0
recently some work has been done on corpusbased paraphrase extraction lin and pantel 2001 barzilay and lee 2003 ,0,1,0
its still possible to use msa if for example the input is preclustered to have the same constituent ordering barzilay and lee 2003 ,0,1,0
but because we want the insertion state a1a16a20 to model digressions or unseen topics we take the novel step of forcing its language model to be complementary to those of the other states by setting a2 a3a27a38 a21 a8 a8 a4 a8 a24 a26a11a28a30a29a6 a39a41a40a43a42a45a44a16a46 a1a48a47a1a50a49 a20 a2 a3 a26a17a21 a8a9a8 a4 a8 a24 a51a53a52a55a54a57a56 a21 a39a58a40a43a42a45a44a16a46 a1a59a47a1a50a49 a20 a2 a3a27a26a11a21a50a60 a4 a8 a24a30a24 a17 4following barzilay and lee 2003 proper names numbers and dates are temporarily replaced with generic tokens to help ensure that clusters contain sentences describing the same event type rather than same actual event ,0,1,0
although a large number of studies have been made on learning paraphrases for example barzilay and lee 2003 there are only a few studies which address the connotational difference of paraphrases ,0,1,0
there are several works that try to learn paraphrase pairs from parallel or comparable corpora barzilay and mckeown 2001 shinyama et al 2002 barzilay and lee 2003 pang et al 2003 ,0,1,0
previous attempts have used for instance the similarities between case frames lin and pan57 tel 2001 anchor words barzilay and lee 2003 shinyama et al 2002 szepektor et al 2004 and a webbased methodszepektor et al 2004geffet and dagan 2005 ,0,1,0
2this can explain why previous attempts to use wordnet for generating sentencelevel paraphrases barzilay and lee 2003 quirk et al 2004 were unsuccessful ,0,0,1
the use of profile hmms for multiple sequence alignment also presents applications to the acquisition of mapping dictionaries barzilay and lee 2002 and sentencelevel paraphrasing barzilay and lee 2003 ,0,1,0
past work barzilay and mckeown 2001 barzilay and lee 2003 pang et al 2003 ibrahim et al 2003 has examined the use of monolingual parallel corpora for paraphrase extraction ,0,1,0
5 related work automatically finding sentences with the same meaning has been extensively studied in the field of automatic paraphrasing using parallel corpora and corporawith multiple descriptionsof the same events barzilay and mckeown 2001 barzilay and lee 2003 ,0,1,0
classi er training set precision recall fmeasure linear 10k pairs 0837 0774 0804 maximum entropy 10k pairs 0881 0851 0866 maximum entropy 450k pairs 0902 0944 0922 table 4 performance of alignment classi er 32 paraphrase acquisition much recent work on automatic paraphrasing barzilay and lee 2003 has used relatively simple statistical techniques to identify text passages that contain the same information from parallel corpora ,0,1,0
in order increase the likelihood that 909 only true paraphrases were considered as phraselevel alternations for an example extracted sentences were clustered using completelink clustering using a technique proposed in barzilay and lee 2003 ,0,1,0
many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus collier 1998 barzilay and lee 2003 sudo et al 2003 ,0,1,0
if we consider these probabilities as a vector the similarities of two english words can be obtained by computing the dot product of their corresponding vectors2 the formula is described below similarityei ej nsummationdisplay k1 peifkpejfk 3 paraphrasing methods based on monolingual parallel corpora such as pang et al 2003 barzilay and lee 2003 can also be used to compute the similarity ratio of two words but they dont have as rich training resources as the bilingual methods do ,0,0,1
previous work aligns a group of sentences into a compact word lattice barzilay and lee 2003 a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases ,0,1,0
2 related work our work is closest in spirit to the two papers that inspired us barzilay and lee 2003 and pang et al 2003 ,0,1,0
22 evaluation of acquisition algorithms many methods for automatic acquisition of rules have been suggested in recent years ranging from distributional similarity to finding shared contexts lin and pantel 2001 ravichandran and hovy 2002 shinyama et al 2002 barzilay and lee 2003 szpektor et al 2004 sekine 2005 ,0,1,0
indeed the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules lin and pantel 2001 shinyama et al 2002 barzilay and lee 2003 pang et al 2003 szpektor et al 2004 sekine 2005 ,0,1,0
indeed only few earlier works reported interjudge agreement level and those that did reported rather low kappa values such as 054 barzilay and lee 2003 and 055 063 szpektor et al 2004 ,0,1,0
2004 and barzilay and lee 2003 used comparable news articles to obtain sentence level paraphrases ,0,1,0
some methods only extract paraphrase patternsusingnewsarticlesoncertaintopicsshinyama et al 2002 barzilay and lee 2003 while some others need seeds as initial input ravichandran and hovy 2002 ,0,1,0
in paraphrase generation a text unit that matches a pattern p can be rewritten using the paraphrase patterns of p avarietyofmethodshavebeenproposedonparaphrase patterns extraction lin and pantel 2001 ravichandran and hovy 2002 shinyama et al 2002 barzilay and lee 2003 ibrahim et al 2003 pang et al 2003 szpektor et al 2004 ,0,1,0
the preci781 start palestinian suicide bomberblew himself up in slot1 on slot2 killing slot3 other people and injuring wounding slot4 end detroit the e a s e building buildingin detroit flattened ground levelled to blasted leveled e was reduced razed leveled to down rubble into ashes e to e 1 2 figure 1 examples of paraphrase patterns extracted by barzilay and lee 2003 and pang et al ,0,1,0
barzilay and lee 2003 applied multisequence alignment msa to parallel news sentences and induced paraphrase patterns for generating new sentences figure 1 1 ,0,1,0
3 monolingual comparable corpus similar to the methods in shinyama et al 2002 barzilay and lee 2003 we construct a corpus of comparable documents from a large corpus d of news articles ,0,1,0
different news articles reporting on the same event are commonly used as monolingual comparable corpora from which both paraphrase patterns and phrasal paraphrases can be derived shinyama et al 2002 barzilay and lee 2003 quirk et al 2004 ,0,1,0
for example barzilay and lee 2003 applied multiplesequence alignment msa to parallel news sentences and induced paraphrasing patterns for generating new sentences ,0,1,0
for instance automatic summary can be seen as a particular paraphrasing task barzilay and lee 2003 with the aim of selecting the shortest paraphrase ,0,1,0
in another generation approach barzilay and lee 2002 2003 look for pairs of slotted word lattices that share many common slot fillers the lattices are generated by applying a multiplesequence alignment algorithm to a corpus of multiple news articles about the same events ,0,1,0
for this reason paraphrase poses a great challenge for many natural language processing nlp tasks just as ambiguity does notably in text summarization and nl generation barzilay and lee 2003 pang et al 2003 ,0,1,0
similar to the work of barzilay and lee 2003 who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event we are currently attempting to solve the data sparseness problem by extending our approach to nonparallel corpora ,0,1,0
similarly barzilay and lee 2003 and shinyanma et al 2002 learn sentence level paraphrase templates from a corpus of news articles stemming from different news source ,0,1,0
our experience suggests that disjunctive lfs are an important capability especially as one seeks to make grammars reusable across applications and to employ domainspecific sentencelevel paraphrases barzilay and lee 2003 ,0,1,0
barzilay and lee 2003 proposed to apply multiplesequence alignment msa for traditional sentencelevel pr ,0,1,0
at the sentence level barzilay and lee 2003 employed an unsupervised learning approach to cluster sentences and extract lattice pairs from comparable monolingual corpora ,0,1,0
most previous work on paraphrase has focused on high quality rather than coverage barzilay and lee 2003 quirk et al 2004 but generating artificial references for mt parameter tuning in our setting has two unique properties compared to other paraphrase applications ,0,1,0
automatically learning entailment rules from the web many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years lin and pantel 2001 1httpjakartaapacheorglucenedocsindexhtml 67 ravichandran and hovy 2002 shinyama et al 2002 barzilay and lee 2003 sudo et al 2003 szpektor et al 2004 satoshi 2005 ,0,1,0
most of the reported work on paraphrase generation from arbitrary input sentences uses machine learning techniques trained on sentences that are known or can be inferred to be paraphrases of each other bannard and callisonburch 2005 barzilay and lee 2003 barzilay and mckeown 2001 callisonburch et al 2006 dolan et al 2004 ibrahim et al 2003 lin and pantel 2001 pang et al 2003 quirk et al 2004 shinyama et al 2002 ,0,1,0
the third estimates the equivalence based on word alignment composed using templates or translation probabilities derived from a set of parallel text barzilay and lee 2003 brockett and dolan 2005 ,0,1,0
second we will discuss the work done by barzilay lee 2003 who use clustering of paraphrases to induce rewriting rules ,0,1,0
2 related work two different approaches have been proposed for sentence compression purely statistical methodologies barzilay lee 2003 le nguyen ho 2004 and hybrid linguisticstatistic methodologies knight marcu 2002 shinyama et al 2002 daelemans et al 2004 marsi krahmer 2005 unno et al 2006 ,0,1,0
experiments by using 4 algorithms and through visualization techniques revealed that clustering is a worthless effort for paraphrase corpora construction contrary to the literature claims barzilay lee 2003 ,0,0,1
as our work is based on the first paradigm we will focus on the works proposed by barzilay lee 2003 and le nguyen ho 2004 ,0,1,0
barzilay lee 2003 present a knowledgelean algorithm that uses multiplesequence alignment to 177 learn generate sentencelevel paraphrases essentially from unannotated corpus data alone ,0,1,0
comparatively barzilay lee 2003 propose to use the ngram overlap metric to capture similarities between sentences and automatically create paraphrase corpora ,0,1,0
unlike le nguyen ho 2004 one interesting idea proposed by barzilay lee 2003 is to cluster similar pairs of paraphrases to apply multiplesequence alignment ,0,1,0
second we discuss the work done by barzilay lee 2003 who use clustering of paraphrases to induce rewriting rules ,0,1,0
however these unsupervised methodologies show a major drawback by extracting quasiexact2 or even exact match pairs of sentences as they rely on classical string similarity measures such as the edit distance in the case of dolan brockett 2004 and word ngram overlap for barzilay lee 2003 ,0,1,0
in particular it shows systematically better fmeasure and accuracy measures over all other metrics showing an improvement of 1 at least 286 in terms of fmeasure and 396 in terms of accuracy and 2 at most 661 in terms of fmeasure and 674 in terms of accuracy compared to the second best metric which is also systematically the word ngram overlap similarity measure used by barzilay lee 2003 ,0,1,0
on one hand as barzilay lee 2003 evidence clusters of paraphrases can lead to better learning of texttotext rewriting rules compared to just pairs of paraphrases ,0,1,0
table 2 figures about clustering algorithms algorithm sentences clusters shac 623 chac 217 qt 232 em 416 in fact table 2 shows that most of the clusters have less than 6 sentences which leads to question the results presented by barzilay lee 2003 who only keep the clusters that contain more than 10 sentences ,0,0,1
sentence compression takes an important place for natural language processing nlp tasks where specific constraints must be satisfied such as length in summarization barzilay lee 2002 knight marcu 2002 shinyama et al 2002 barzilay lee 2003 le nguyen ho 2004 unno et al 2006 style in text simplification marsi krahmer 2005 or sentence simplification for subtitling daelemans et al 2004 ,0,1,0
these results confirm the observed figures in the previous subsection and reinforce the sight that clustering is a worthless effort for automatic paraphrase corpora construction contrarily to what barzilay lee 2003 suggest ,0,1,0
in order to be able to compare the edit distance with the other metrics we have used the following formulawen et al 2002whichnormalisesthe minimum edit distance by the length of the longest questionand transformsit into a similaritymetric normalisededitdistance 1 edit distq1q2max q 1 q2 word ngram overlap this metric compares the word ngramsin both questions ngramoverlap 1n nsummationdisplay n1 gnq1 gnq2 min gnq1 gnq2 where gnq is the set of ngrams of length n in question q and n usually equals 4 barzilay and lee 2003cordeiroet al 2007 ,0,1,0
while word and phrasal paraphrases can be assimilated to the wellstudied notion of synonymy sentencelevel paraphrasingis moredifficult to grasp and cannot be equated with wordforword or phrasebyphrase substitution since it might entail changes in the structure of the sentence barzilay and lee 2003 ,0,1,0
barzilay and lee barzilay and lee 2003 learned paraphrasing patterns as pairs of word lattices which are then used to produce sentence level paraphrases ,0,1,0
a few unsupervised metrics have been applied to automatic paraphrase identification and extraction barzilay lee 2003 dolan et al 2004 ,0,1,0
growdiagfinal koehn et al 2003 ,0,1,0
our mt baseline system is based on moses decoder koehn et al 2007 with word alignment obtained from giza och et al 2003 ,0,1,0
1 introduction stateoftheart statistical machine translation smt systems usually adopt a twopass search strategy och 2003 koehn et al 2003 as shown in figure 1 ,1,0,0
however moores law the driving force of change in computing since then has opened the way for recent progress in the field such as statistical machine translation smt koehn et al 2003 ,0,1,0
1 introduction the emergence of phrasebased statistical machine translation psmt koehn et al 2003 has been one of the major developments in statistical approaches to translation ,1,0,0
then the word alignment is refined by performing growdiagfinal method koehn et al 2003 ,0,1,0
sum of logarithms of sourcetotarget lexical weighting koehn et al 2003 ,0,1,0
43 relaxing length restrictions increasing the maximum phrase length in standard phrasebased translation does not improve bleu koehn et al 2003 zens and ney 2007 ,0,1,0
our results are similar to those for conventional phrasebased models koehn et al 2003 zens and ney 2007 ,0,1,0
our baseline uses giza alignments och and ney 2003 symmetrized with the growdiagfinaland heuristic koehn et al 2003 ,0,1,0
with these linguistic annotations we expect the labtg to address two traditional issues of standard phrasebased smt koehn et al 2003 in a more effective manner ,0,0,1
2 related work there have been various efforts to integrate linguistic knowledge into smt systems either from the target side marcu et al 2006 hassan et al 2007 zollmann and venugopal 2006 the source side quirk et al 2005 liu et al 2006 huang et al 2006 or both sides eisner 2003 ding et al 2005 koehn and hoang 2007 just to name a few ,0,1,0
firstly we run giza och and ney 2000 on the training corpus in both directions and then apply the ogrowdiagfinalprefinement rule koehn et al 2003 to obtain manytomany word alignments ,0,1,0
based on these grammars a great number of smt models have been recently proposed including stringtostring model synchronous fsg brown et al 1993 koehn et al 2003 treetostring model tsgstring huang et al 2006 liu et al 2006 liu et al 2007 stringtotree model stringcfgtsg yamada and knight 2001 galley et al 2006 marcu et al 2006 treetotree model synchronous cfgtsg dataoriented translation chiang 2005 cowan et al 2006 eisner 2003 ding and palmer 2005 zhang et al 2007 bod 2007 quirk wt al 2005 poutsma 2000 hearne and way 2003 and so on ,0,1,0
phrase pairs are extracted up to a fixed maximum length since very long phrases rarely have a tangible impact during translation koehn et al 2003 ,0,1,0
1 introduction in phrasebased statistical machine translation koehn et al 2003 phrases extracted from wordaligned parallel data are the fundamental unit of translation ,0,1,0
this paper proposes a method for building a bilingual lexicon through a pivot language by using phrasebased statistical machine translation smt koehn et al 2003 ,0,1,0
let us suppose that we have two bilingual lexicons l f l p and l p l e we obtain word alignments of these lexicons by applying giza och and ney 2003 and growdiagfinal heuristics koehn et al 2007 ,0,1,0
this operation does not change the collection of phrases or rules extracted from a hypothesized alignment see for instance koehn et al 2003 ,0,1,0
for frenchenglish translation we use a state of the art phrasebased mt system similar to och and ney 2004 koehn et al 2003 ,1,0,0
och and ney 2003 invented heuristic symmetriza57 frenchenglish arabicenglish system fmeasure 04 bleu fmeasure 01 bleu giza 735 3063 758 5155 fraser and marcu 2006b 741 3140 791 5289 leaf unsupervised 745 723 leaf semisupervised 763 3186 845 5434 table 3 experimental results tion of the output of a 1ton model and a mto1 model resulting in a mton alignment this was extended in koehn et al 2003 ,0,1,0
our corpora were automatically aligned with giza och et al 1999 in both directions between source and target and symmetrised using the intersection heuristic koehn et al 2003 ,0,1,0
smt has evolved from the original wordbased approach brown et al 1993 into phrasebased approaches koehn et al 2003 och and ney 2004 and syntaxbased approaches wu 1997 alshawi et al 2000 yamada and knignt 2001 chiang 2005 ,0,1,0
31 phrasebased models according to the translation model presented in koehn et al 2003 given a source sentence f the best target translation can be obtained using the following model best e 288 maxarg maxarg e e e eef fee length lm best pp p 1 where the translation model can be decomposed into efp i i i iii i i ii aefpbadef efp 1 1 1 1 w 2 where i i ef is the phrase translation probability ,0,1,0
in training process we use giza 4 toolkit for word alignment in both translation directions and apply growdiagfinal method to refine it koehn et al 2003 ,0,1,0
in phrasebased smt systems koehn et al 2003 koehn 2004 foreign sentences are firstly segmented into phrases which consists of adjacent words ,0,1,0
there have been considerable amount of efforts to improve the reordering model in smt systems ranging from the fundamental distancebased distortion model och and ney 2004 koehn et al 2003 flat reordering model wu 1996 zens et al 2004 kumar et al 2005 to lexicalized reordering model tillmann 2004 kumar et al 2005 koehn et al 2005 hierarchical phrasebased model chiang 2005 and maximum entropybased phrase reordering model xiong et al 2006 ,0,1,0
approaches include word substitution systems brown et al 1993 phrase substitution systems koehn et al 2003 och and ney 2004 and synchronous contextfree grammar systems wu and wong 1998 chiang 2005 all of which train on string pairs and seek to establish connections between source and target strings ,0,1,0
second the word alignment is refined by a growdiagfinal heuristic koehn et al 2003 ,0,1,0
these joint counts are estimated using the phrase induction algorithm described in koehn et al 2003 with symmetrized word alignments generated using ibm model 2 brown et al 1993 ,0,1,0
the features used are the length of t a singleparameter distortion penalty on phrase reordering in a as described in koehn et al 2003 phrase translation model probabilities and 4gram language model probabilities logpt using kneserney smoothing as implemented in the srilm toolkit stolcke 2002 ,0,1,0
so far these techniques have focused on phrasebased models using contiguous phrases koehn et al 2003 och and ney 2004 ,0,1,0
we symmetrized bidirectional alignments using the growdiagfinal heuristic koehn et al 2003 ,0,1,0
for instance word alignment models are often trained using the giza toolkit och and ney 2003 error minimizing training criteria such as the minimum error rate training och 2003 are employed in order to learn feature function weights for loglinear models and translation candidates are produced using phrasebased decoders koehn et al 2003 in combination with ngram language models brants et al 2007 ,0,1,0
they give a probabilistic formation of paraphrasing which naturally falls out of the fact that they use techniques from phrasebased statistical machine translation e2 argmax e2e2negationslashe1 pe2e1 1 where pe2e1 summationdisplay f pfe1pe2fe1 2 summationdisplay f pfe1pe2f 3 phrase translation probabilities pfe1 and pe2f are commonly calculated using maximum likelihood estimation koehn et al 2003 pfe countefsummationtext f countef 4 where the counts are collected by enumerating all bilingual phrase pairs that are consistent with the 197 conseguido opportunitiesequalcreatetofailedhasprojecteuropeanthe oportunidadesdeigualdadlahanoeuropeoproyectoel figure 1 the interaction of the phrase extraction heuristic with unaligned english words means that the spanish phrase la igualdad aligns with equal create equal and to create equal ,0,1,0
51 experimental setup the baseline model was hiero with the following baseline features chiang 2005 chiang 2007 two language models phrase translation probabilities pf e and pe f lexical weighting in both directions koehn et al 2003 word penalty penalties for automatically extracted rules identity rules translating a word into itself two classes of numbername translation rules glue rules the probability features are base100 logprobabilities ,0,1,0
486 one of the most popular instantiations of loglinear models is that including phrasebased pb models zens et al 2002 koehn et al 2003 ,1,0,0
beamsearch has been successful in many nlp tasks koehn et al 2003 562 inputs training examples xiyi initialization set vectorw 0 algorithm r training iterations n examples for t 1r i 1n zi argmaxygenxi y vectorw if zi negationslash yi vectorw vectorw yizi outputs vectorw figure 1 the perceptron learning algorithm collins and roark 2004 and can achieve accuracy that is close to exact inference ,1,0,0
from this aligned training corpus we extract the phrase pairs according to the heuristics in koehn et al 2003 ,0,1,0
koehn et al 2003 och and ney 2004 ,0,1,0
the pervading method for estimating these probabilities is a simple heuristic based on the relative frequency of the phrase pair in the multiset of the phrase pairs extracted from the wordaligned corpus koehn et al 2003 ,0,1,0
the automatic alignments were extracted by appending the manually aligned sentences on to the respective europarl v3 corpora and aligning them using giza och and ney 2003 and the growfinaldiag algorithm koehn et al 2003 ,0,1,0
1 introduction statistical phrasebased systems och and ney 2004 koehn et al 2003 have consistently delivered stateoftheart performance in recent machine translation evaluations yet these systems remain weak at handling word order changes ,0,0,1
1 introduction many stateoftheart machine translation mt systems over the past few years och and ney 2002 koehn et al 2003 chiang 2007 koehn et al 2007 li et al 2009 rely on several models to evaluate the goodness of a given candidate translation in the target language ,1,0,0
the method thereby retains the full set of lexical entries of phrasebased systems eg koehn et al 20031 the model allows a straightforward integration of lexicalized syntactic language modelsfor example the models of charniak 2001in addition to a surface language model ,0,1,0
the future score is based on the sourcelanguage words that are still to be translatedthis can be directly inferred from the items bitstringthis is similar to the use of future scores in pharoah koehn et al 2003 and in fact we use pharoahs future scores in our model ,0,1,0
we used pharoah koehn et al 2003 as a baseline system for comparison the sphrases used in our system include all phrases with the same scores as those used by pharoah allowing a direct comparison ,0,1,0
these estimates are usually heuristic and inconsistent koehn et al 2003 ,0,1,0
1 introduction we have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms often based on grammatical formalisms1 if we view mt as a machine learning problem features and formalisms imply structural independence assumptions which are in turn exploited by efficient inference algorithms including decoders koehn et al 2003 yamada and knight 2001 ,1,0,0
these heuristics are extensions of those developed for phrasebased models koehn et al 2003 and involve symmetrising two directional word alignments followed by a projection step which uses the alignments to find a mapping between source words and nodes in the target parse trees galley et al 2004 ,0,1,0
the rules are then treated as events in a relative frequency estimate4 we used giza model 4 to obtain word alignments och and ney 2003 using the growdiagfinaland heuristic to symmetrise the two directional predictions koehn et al 2003 ,0,1,0
1 introduction phrasebased systems flat and hierarchical alike koehn et al 2003 koehn 2004b koehn et al 2007 chiang 2005 chiang 2007 have achieved a much better translation coverage than wordbased ones brown et al 1993 but untranslated words remain a major problem in smt ,0,0,1
och et al 1999 koehn et al 2003 liang et al 2006 ,0,1,0
computing the phrase translation probability is trivial in the training corpora but lexical weighting koehn et al 2003 needs lexicallevel alignment ,0,1,0
1 introduction phrasebased method koehn et al 2003 och and ney 2004 koehn et al 2007 and syntaxbased method wu 1997 yamada and knight 2001 eisner 2003 chiang 2005 cowan et al 2006 marcu et al 2006 liu et al 2007 zhang et al 2007c 2008a 2008b shen et al 2008 mi and huang 2008 represent the stateoftheart technologies in statistical machine translation smt ,1,0,0
then we used the refinement technique growdiagfinaland koehn et al 2003 to all 50 50 bidirectional alignment pairs ,0,1,0
the methods for calculating relative frequencies och and ney 2004 and lexical weights koehn et al 2003 are also adapted for the weighted matrix case ,0,1,0
while theoretically sound this approach is computationally challenging both in practice denero et al 2008 and in theory denero and klein 2008 may suffer from reference reachability problems denero et al 2006 and in the end may lead to inferior translation quality koehn et al 2003 ,0,1,0
substringbased transliteration with a generative hybrid model is very similar to existing solutions for phrasal smt koehn et al 2003 operating on characters rather than words ,0,1,0
then we apply a growdiagfinal algorithm which is widely used in bilingual phrase extraction koehn et al 2003 to monolingual alignments ,0,1,0
the prior probability p0 is the prior distribution for the phrase probability which is estimated using the phrase normalized counts commonly used in conventional phrasebased smt systems eg koehn et al 2003 ,0,1,0
1313 e2c c2e union heuristic w big 1337 1266 1455 1428 wo big 1320 1262 1453 1421 table 3 bleu4 scores test set of systems based on giza word alignments 5 6 7 8 bleu4 1427 1442 1443 1445 1455 table 4 bleu4 scores test set of the union alignment using tts templates up to a certain size in terms of the number of leaves in their lhss 41 baseline systems ghkm galley et al 2004 is used to generate the baseline tts templates based on the word alignments computed using giza and different combination methods including union and the diagonal growing heuristic koehn et al 2003 ,0,1,0
the word alignment is computed using giza2 for the selected 73597 sentence pairs in the fbis corpus in both directions and then combined using union and heuristic diagonal growing koehn et al 2003 ,0,1,0
by introducing the hidden word alignment variable a the following approximate optimization criterion can be applied for that purpose e argmaxe pre f argmaxe summationdisplay a prea f argmaxea prea f exploiting the maximum entropy berger et al 1996 framework the conditional distribution prea f can be determined through suitable real valued functions called features hrefar 1r and takes the parametric form pea f exp rsummationdisplay r1 rhrefa the itcirst system chen et al 2005 is based on a loglinear model which extends the original ibm model 4 brown et al 1993 to phrases koehn et al 2003 federico and bertoldi 2005 ,0,1,0
it is today common practice to use phrases as translation units koehn et al 2003 och and ney 2003 instead of the original wordbased approach ,0,1,0
the problem is typically presented in logspace which simplifies computations but otherwise does not change the problem due to the monotonicity of the log function hm log hprimem log pts summationdisplay m m hmts 3 phrasebased models koehn et al 2003 are limited to the mapping of small contiguous chunks of text ,0,0,1
2003 or in more recent implementation the moses mt system1 koehn et al 2007 ,0,1,0
53 baseline system we conducted experiments using different segmenters with a standard loglinear pbsmt model giza implementation of ibm word alignment model 4 och and ney 2003 the refinement and phraseextraction heuristics described in koehn et al 2003 minimumerrorrate training och 2003 a 5gram language model with kneserney smoothing trained with srilm stolcke 2002 on the english side of the training data and moses koehn et al 2007 dyer et al 2008 to translate both single best segmentation and word lattices ,0,1,0
we computed precision recall and error rate on the entire set of sentence pairs for each data set5 to evaluate neuralign we used giza in both directions etof and ftoe where f is either chinese c or spanish s as input and a refined alignment approach och and ney 2000 that uses a heuristic combination method called growdiagfinal koehn et al 2003 for comparison ,0,1,0
phrasepairs are then extracted from the word alignments koehn et al 2003 ,0,1,0
5 phrase pair induction a common approach to phrasebased translation is to extract an inventory of phrase pairs ppi from bitext koehn et al 2003 for example in the phraseextract algorithm och 2002 a word alignment am1 is generated over the bitext and all word subsequences ei2i1 and fj2j1 are found that satisfy am1 aj i1i2 iff j j1j2 ,0,1,0
1 introduction todays statistical machine translation systems rely on high quality phrase translation pairs to acquire stateoftheart performance see koehn et al 2003 zens and ney 2004 och and ney 2003 ,1,0,0
we computed precision recall and error rate on the entire set for each data set6 for an initial alignment we used giza in both directions etof and ftoe where f is either chinese c or spanish s and also two different combined alignments intersection of etof and ftoe and ra using a heuristic combination approach called growdiagfinal koehn et al 2003 ,0,1,0
the standard method to overcome this problem to use the model in both directions interchanging the source and target languages and applying heuristicbased combination techniques to produce a refined alignment och and ney 2000 koehn et al 2003henceforth referred to as ra several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models ,0,1,0
the basic model uses the following features analogous to pharaohs default feature set p and p the lexical weights pw and pw koehn et al 20031 a phrase penalty exp1 a word penalty expl where l is the number of terminals in ,0,1,0
the feature weights are learned by maximizing the bleu score papineni et al 2002 on heldout datausingminimumerrorratetrainingoch2003 as implemented by koehn ,0,1,0
the need for some way to model aspects of syntactic behavior such as the tendency of constituents to move together as a unit is widely recognizedthe role of syntactic units is well attested in recent systematic studies of translation fox 2002 hwa et al 2002 koehn and knight 2003 and their absence in phrasebased models is quite evident when looking at mt system output ,0,1,0
the phrasebased machine translation koehn et al 2003 uses the growdiagfinal heuristic to extend the word alignment to phrase alignment by using the intersection result ,0,1,0
however for remedy many of the current word alignment methods combine the results of both alignment directions via intersection or 249 growdiagfinal heuristic to improve the alignment reliability koehn et al 2003 liang et al 2006 ayan et al 2006 denero et al 2007 ,0,1,0
although bialignments are known to exhibit high precision koehn et al 2003 in the face of sparse annotations we use unidirectional alignments as a fallback as has been proposed in the context of phrasebased machine translation koehn et al 2003 tillmann 2003 ,1,0,0
phrases extracted using these heuristics are also shown to perform better than syntactically motivated phrases the joint model and ibm model 4 koehn et al 2003 ,1,0,0
the phrase translation table is learnt in the following manner the parallel corpus is wordaligned bidirectionally and using various heuristics see koehn et al 2003 for details phrase correspondences are established ,0,1,0
we used the preprocessed data to train the phrasebased translation model by using giza och and ney 2003 and the pharaoh tool kit koehn et al 2003 ,0,1,0
322 features we used eight features och and ney 2003 koehn et al 2003 and their weights for the translations ,0,1,0
however reordering models in traditional phrasebased systems are not sufficient to treat such complex cases when we translate long sentences koehn et al 2003 ,0,0,1
nowadays most stateoftheart smt systems are based on bilingual phrases och tillmann and ney 1999 koehn och and marcu 2003 tillmann 2003 bertoldi et al 2004 vogel et al 2004 zens and ney 2004 chiang 2005 ,1,0,0
above the phrase level some models perform no reordering zens and ney 2004 kumar deng and byrne 2006 some have a simple distortion model that reorders phrases independently of their content koehn och and marcu 2003 och and ney 2004 and some for example the alignment template system och et al 2004 thayer et al 2004 hereafter ats and the ibm phrasebased system tillmann 2004 tillmann and zhang 2005 have phrasereordering models that add some lexical sensitivity ,0,1,0
in koehn et al 2003 various aspects of phrasebased systems are compared eg the phrase extraction method the underlying word alignment model or the maximum phrase length ,0,1,0
along this line koehn et al 2003 present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance the ability to translate nonconstituent phrases such as there are note that and according to turns out to be critical and pervasive ,1,0,0
1 introduction in recent years phrasebased systems for statistical machine translation och et al 1999 koehn et al 2003 venugopal et al 2003 have delivered stateoftheart performance on standard translation tasks ,1,0,0
as an additional baseline we compare against a phrasal smt decoder pharaoh koehn et al 2003 ,0,1,0
we used the heuristic combination described in och and ney 2003 and extracted phrasal translation pairs from this combined alignment as described in koehn et al 2003 ,0,1,0
2 the problem of coverage in smt statistical machine translation made considerable advances in translation quality with the introduction of phrasebased translation marcu and wong 2002 koehn et al 2003 och and ney 2004 ,1,0,0
1 introduction defining scms the work presented here was done in the context of phrasebased mt koehn et al 2003 och and ney 2004 ,0,1,0
based on the observations in koehn et al 2003 we also limited the phrase length to 3 for computational reasons ,0,1,0
1 introduction word alignmentdetection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation mt brown et al 1993 och and ney 2003 koehn et al 2003 but also has been shown useful for other applications such as construction of bilingual lexicons wordsense disambiguation projection of resources and crosslanguage information retrieval ,0,1,0
using giza model 4 alignments and pharaoh koehn et al 2003 we achieved a bleu score of 03035 ,0,1,0
1 introduction word alignment is an important component of a complete statistical machine translation pipeline koehn et al 2003 ,0,1,0
we view this as a particularly promising aspect of our work given that phrasebased systems such as pharaoh koehn et al 2003 perform better with higher recall alignments ,1,0,0
1 introduction recent work in statistical machine translation mt has sought to overcome the limitations of phrasebased models marcu and wong 2002 koehn et al 2003 och and ney 2004 by making use of syntactic information ,0,1,0
most stateoftheart smt systems treat grammatical elements in exactly the same way as content words and rely on generalpurpose phrasal translations and target language models to generate these elements eg och and ney 2002 koehn et al 2003 quirk et al 2005 chiang 2005 galley et al 2006 ,1,0,0
in this paper we present results on using a recent phrasebased smt system pharaoh koehn et al 2003 for nlg1 although moderately effec1we also tried ibm model 4rewrite germann 2003 a wordbased smt system but it gave much worse results ,0,1,0
like wasp1 the phrase extraction algorithm of pharaoh is based on the output of a word alignment model such as giza koehn et al 2003 which performs poorly when applied directly to mrls section 32 ,0,0,1
toremedythis situation we can borrow the probabilistic model of pharaoh and define the parsing model as prded productdisplay dd wrd 4 which is the product of the weights of the rules used in a derivation d the rule weight wx is in turn defined as p1p2pw3pw4 exp5 where p and p are the relative frequencies of and and pw and pw are 176 the lexical weights koehn et al 2003 ,0,1,0
following the phrase extraction phase in pharaoh we eliminate word gaps by incorporating unaligned words as part of the extracted nl phrases koehn et al 2003 ,0,1,0
31 generation using pharaoh pharaoh koehn et al 2003 is an smt system that uses phrases as basic translation units ,0,1,0
2 phrasebased smt we use a phrasebased smt system pharaoh koehn et al 2003 koehn 2004 which is based on a loglinear formulation och and ney 2002 ,0,1,0
for details on these feature functions please refer to koehn et al 2003 koehn 2004 koehn et al 2005 ,0,1,0
that is phrases are heuristically extracted from wordlevel alignments produced by doing giza training on the corresponding parallel corpora koehn et al 2003 ,0,1,0
the definitions of the phrase and lexical translation probabilities are as follows koehn et al 2003 ,0,1,0
even a length limit of 3 as proposed by koehn et al 2003 would result in almost optimal translation quality ,0,1,0
we have investigated this and our results are in line with koehn et al 2003 showing that the translation quality does not improve if we utilize phrases beyond a certain length ,0,1,0
grammar rules were induced with the syntaxbased smt system samt described in zollmann and venugopal 2006 which requires initial phrase alignments that we generated with giza koehn et al 2003 and syntactic parse trees of the target training sentences generated by the stanford parser d klein 2003 pretrained on the penn treebank ,0,1,0
they have been employed in word sense disambiguation diab and resnik 2002 automatic construction of bilingual dictionaries mcewan et al 2002 and inducing statistical machine translation models koehn et al 2003 ,0,1,0
2006 modified from koehn et al 2003 which is an average of pairwise word translation probabilities ,0,1,0
they provide pairs of phrases that are used to construct a large set of potential translations for each input sentence along with feature values associated with each phrase pair that are used to select the best translation from this set1 the most widely used method for building phrase translation tables koehn et al 2003 selects from a word alignment of a parallel bilingual training corpus all pairs of phrases up to a given length that are consistent with the alignment ,1,0,0
322 alignment error rate since mt systems are usually built on the union of the two sets of alignments koehn et al 2003 we consider the union of alignments in the two directions as well as those in each direction ,0,1,0
in particular we adopt the approach of phrasebased statistical machine translation koehn et al 2003 koehn and hoang 2007 ,0,1,0
we obtain aligned parallel sentences and the phrase table after the training of moses which includes running giza och and ney 2003 growdiagonalfinal symmetrization and phrase extraction koehn et al 2005 ,0,1,0
for example in phrasebased smt systems koehn et al 2003 koehn 2004 distortion model is used in which reordering probabilities depend on relative positions of target side phrases between adjacent blocks ,0,1,0
word alignment was carried out by running giza implementation of ibm model 4 initialized with 5 iterations of model 1 5 of the hmm aligner and 3 iterations of model 4 och and ney 2003 in both directions and then symmetrizing using the growdiagfinaland heuristic koehn et al 2003 ,0,1,0
unfortunately determining the optimal segmentation is challenging typically requiring extensive experimentation koehn and knight 2003 habash and sadat 2006 chang et al 2008 ,0,1,0
the features used by the decoder were the english language model log probability logfef the lexical translation log probabilities in both directions koehn et al 2003 and a word count feature ,0,1,0
typically a phrasebased smt system includes a feature that scores phrase pairs using lexical weights koehn et al 2003 which are computed for two directions source to target and target to source ,0,1,0
each model can represent an important feature for the translation such as phrasebased language or lexical models koehn et al 2003 ,0,1,0
the transcription probabilities can then be easily learnt from the alignments induced by giza using a scoring function koehn et al 2003 ,0,1,0
we used minimum error rate training och 2003 and the a beam search decoder implemented by koehn koehn et al 2003 ,0,1,0
for each span in the chart we get a weight factor that is multiplied with the parameterbased expectations9 4 experiments we applied giza alonaizan et al 1999 och and ney 2003 to wordalign parts of the europarl corpus koehn 2002 for english and all other 10 languages ,0,1,0
we use the europarl corpus koehn 2002 and the statistical word alignment was performed with the giza toolkit alonaizan et al 1999 och and ney 20031 for the current experiments we assume no preexisting parser for any of the languages contrary to the information projection scenario ,0,1,0
koehn et al 2003 show that exploiting all contiguous word blocks in phrasebased alignment is better than focusing on syntactic constituents only ,0,1,0
it is important because a wordaligned corpus is typically used as a first step in order to identify phrases or templates in phrasebased machine translation och et al 1999 tillmann and xia 2003 koehn et al 2003 sec ,0,1,0
we compared a baseline system the stateoftheart phrasebased system pharaoh koehn et al 2003 koehn 2004a against our system ,1,0,0
51 baseline the baseline system we used for comparison was pharaoh koehn et al 2003 koehn 2004a as publicly distributed ,0,1,0
above the phrase level these models typically have a simple distortion model that reorders phrases independently of their content och and ney 2004 koehn et al 2003 or not at all zens and ney 2004 kumar et al 2005 ,0,1,0
for our experiments we used the following features analogous to pharaohs default feature set p and p the latter of which is not found in the noisychannel model but has been previously found to be a helpful feature och and ney 2002 the lexical weights pw and pw koehn et al 2003 which estimate how well the words in translate the words in 2 a phrase penalty exp1 which allows the model to learn a preference for longer or shorter derivations analogous to koehns phrase penalty koehn 2003 ,0,1,0
to do this we first identify initial phrase pairs using the same criterion as previous systems och and ney 2004 koehn et al 2003 definition 1 ,0,1,0
in experiments with the system of koehn et al 2003 we have found that in practice a large number of complete translations are completely monotonic ie have a0 skips suggesting that the system has difficulty learning exactly what points in the translation should allow reordering ,0,1,0
results using the method show an improvement from 252 bleu score to 268 bleu score a statistically significant improvement using a phrasebased system koehn et al 2003 which has been shown in the past to be a highly competitive smt system ,1,0,0
more recently phrasebased models och et al 1999 marcu and wong 2002 koehn et al 2003 have been proposed as a highly successful alternative to the ibm models ,1,0,0
reranking methods have also been proposed as a method for using syntactic information koehn and knight 2003 och et al 2004 shen et al 2004 ,0,1,0
34 lexical weighting the lexical weight a27 a14a12a91 a29 a92a93a21 of the block a9 a72 a14a12a91 a19a86a92a93a21 is computed similarly to koehn et al 2003 but the lexical translation probability a27 a14a12a94 a29 a97a100a21 is derived from the block set itself rather than from a word alignment resulting in a simplified training ,0,1,0
two block sets are derived for each of the training sets using a phrasepair selection algorithm similar to koehn et al 2003 tillmann and xia 2003 ,0,1,0
2 block orientation bigrams this section describes a phrasebased model for smt similar to the models presented in koehn et al 2003 och et al 1999 tillmann and xia 2003 ,0,1,0
our method for identifying paraphrases is an extension of recent work in phrasebased statistical machine translation koehn et al 2003 ,0,1,0
koehn 2004 tillmann 2003 and vogel et al ,0,1,0
 statistical phrasebased translation koehn et al 2003 here phrasebased means subsequencebased as there is no guarantee that the phrases learned by the model will have any relation to what we would think of as syntactic phrases ,0,1,0
most current smt systems och and ney 2004 koehn et al 2003 use a generative model for word alignment such as the freely available giza och and ney 2003 an implementation of the ibm alignment models brown et al 1993 ,0,1,0
one is distortion model och and ney 2004 koehn et al 2003 which penalizes translations according to their jump distance instead of their content ,0,1,0
however their decoder is outperformed by phrasebased decoders such as koehn 2004 och et al 1999 and tillmann and ney 2003 ,0,1,0
similarly koehn et al 2003 propose a relative distortion model to be used with a phrase decoder ,0,1,0
1 introduction phrasebased translation models marcu and wong 2002 koehn et al 2003 och and ney 2004 which go beyond the original ibm translation models brown et al 1993 1 by modeling translations of phrases rather than individual words have been suggested to be the stateoftheart in statistical machine translation by empirical evaluations ,1,0,0
here ppicker shows the accuracy when phrases are extracted by using the nbest phrase alignment method described in section 41 while growdiagfinal shows the accuracy when phrases are extracted using the standard phrase extraction algorithm described in koehn et al 2003 ,0,1,0
the translation model used in koehn et al 2003 is the product of translation probability a34a35a4 a29 a0 a33 a6 a29 a2 a33 a8 and distortion probability a36a37a4a39a38 a33a41a40a43a42a44a33a46a45 a32 a8 a3a5a4a35a29 a0 a30 a32 a6 a29 a2 a30 a32 a8 a10 a30 a47 a33a49a48 a32 a34a35a4 a29 a0a22a33 a6 a29 a2 a33a50a8 a36a51a4a39a38 a33 a40a52a42 a33a53a45 a32 a8 1 where a38 a33 denotes the start position of the source phrase translated into the a54 th target phrase and a42 a33a53a45 a32 denotes the end position of the source phrase translated into the a4a53a54 a40a56a55 a8 th target phrase ,0,1,0
the block set is generated using a phrasepair selection algorithm similar to koehn et al 2003 alonaizan et al 2004 which includes some heuristic filtering to mal statement here ,0,1,0
wordbased features are used as well eg feature a75 a11a39a99a78a99a18a11 captures wordtoword translation de4on our test set tillmann and zhang 2005 reports a bleu score of a100a63a101a63a102a43a103 and ittycheriah and roukos 2005 reports a bleu score of a104a89a103a63a102 a105 pendencies similar to the use of model a98 probabilities in koehn et al 2003 ,0,1,0
the process of phrase extraction is difficult to optimize in a nondiscriminative setting many heuristics have been proposed koehn et al 2003 but it is not obvious which one should be chosen for a given language pair ,0,0,1
the discrepancy between dev performance and test performance is due to temporal distance from train and high variance in bleu score11 we also compared our model with pharaoh koehn et al 2003 ,0,1,0
at the end we ran our models once on test to get final numbers2 4 models our experiments used phrasebased models koehn et al 2003 which require a translation table and language model for decoding and feature computation ,0,1,0
in the future we plan to explore our discriminative framework on a full distortion model koehn et al 2003 or even a hierarchical model chiang 2005 ,0,1,0
a phrasebased translation model is one of the modern approaches which exploits a phrase a contiguous sequence of words as a unit of translation koehn et al 2003 zens and ney 2003 tillman 2004 ,0,1,0
manytomany word alignments are induced by running a onetomany word alignment model such as giza och and ney 2003 in both directions and by combining the results based on a heuristic koehn et al 2003 ,0,1,0
second phrase translation pairs are extracted from the word alignment corpus koehn et al 2003 ,0,1,0
4 experiments phrasebased smt systems have been shown to outperform wordbased approaches koehn et al 2003 ,1,0,0
41 applications to phrasebased smt aphrasebasedtranslationmodelcanbeestimated in two stages first a parallel corpus is aligned at the wordlevel and then phrase pairs are extracted koehn et al 2003 ,0,1,0
when evaluated against the stateoftheart phrasebased decoder pharaoh koehn 2004 using the same experimental conditions translation table trained on the fbis corpus 72m chinese words and 92m english words of parallel text trigram language model trained on 155m words of english newswire interpolation weights a65 equation 2 trained using discriminative training och 2003 on the 2002 nist mt evaluation set probabilistic beam a90 set to 001 histogram beam a58 set to 10 and bleu papineni et al 2002 as our metric the widlnglmaa86 a129 algorithm produces translations that have a bleu score of 02570 while pharaoh translations have a bleu score of 02635 ,0,1,0
the normalization is visualized as a translation problem where messages in the sms language are to be translated to normal english using a similar phrasebased statistical mt method koehn et al 2003 ,0,1,0
791 and score the alignment template models phrases koehn et al 2003 ,0,1,0
the second one is heuristic and tries to use a wordaligned corpus zens et al 2002 koehn et al 2003 ,0,1,0
to perform translation stateoftheart mt systems use a statistical phrasebased approach marcu and wong 2002 koehn et al 2003 och and ney 2004 by treating phrases as the basic units of translation ,1,0,0
recently cabezas and resnik 2005 experimented with incorporating wsd translations into pharaoh a stateoftheart phrasebased mt system koehn et al 2003 ,1,0,0
4 smtbased query expansion our smtbased query expansion techniques are based on a recent implementation of the phrasebased smt framework koehn et al 2003 och and ney 2004 ,0,1,0
a similar use of the term phrase exists in machine translation where phrases are often pairs of word sequences consistent with wordbased alignments koehn et al 2003 ,0,1,0
we ran giza och and ney 2000 on the training corpus in both directions using its default setting and then applied the refinement rule diagand described in koehn et al 2003 to obtain a single manytomany word alignment for each sentence pair ,0,1,0
the basic phrase reordering model is a simple unlexicalized contextinsensitive distortion penalty model koehn et al 2003 ,0,1,0
however the pb features yields no noticeable improvement unlike in prefect lexical choice scenario this is similar to the findings in koehn et al 2003 ,0,1,0
the translation table is obtained as described in koehn et al 2003 ie the alignment tool giza is run over the training data in both translation directions and the two aligntest setting bleu b1 standard phrasebased smt 2922 b2 b1 clause splitting 2913 table 2 experiment baseline test setting bleu bleu 2ary 23ary 1 rule 2977 3031 2 me phrase label 2993 3049 3 me leftright 3010 3053 4 me 3head 3024 3071 5 me 3phrase label 3012 3030 6 me 4context 3024 3076 table 3 tests on various reordering models the 3rd column comprises the bleu scores obtained by reordering binary nodes only the 4th column the scores by reordering both binary and 3ary nodes ,0,1,0
the implementation is similar to the idea of lexical weight in koehn et al 2003 all points in the alignment matrices of the entire training corpus are collected to calculate the probabilistic distribution pts of some tl word 3some readers may prefer the expression the subtree rooted at node n to node n the latter term is used in this paper for simplicity ,0,1,0
for example the distancebased reordering model koehn et al 2003 allows a decoder to translate in nonmonotonous order under the constraint that the distance between two phrases translated consecutively does not exceed a limit known as distortion limit ,0,1,0
the translation models and lexical scores were estimated on the training corpus whichwasautomaticallyalignedusinggizaoch et al 1999 in both directions between source and target and symmetrised using the growing heuristic koehn et al 2003 ,0,1,0
this represents the translation probability of a phrase when it is decomposed into a series of independent wordforword translation steps koehn et al 2003 and has proven a very effective feature zens and ney 2004 foster et al 2006 ,0,1,0
as with conventional smoothing methods koehn et al 2003 foster et al 2006 triangulation increases the robustness of phrase translation estimates ,0,1,0
1 introduction statistical machine translation brown et al 1993 has seen many improvements in recent years most notably the transition from wordto phrasebased models koehn et al 2003 ,1,0,0
3 phrasebased smt according to the translation model presented in koehn et al 2003 given a source sentence f the best target translation best e can be obtained according to the following model maxarg maxarg e e e eef fee length lm best pp p 1 where the translation model efp can be decomposed into i i i iii i i ii aefpbadef efp 1 1 1 1 w 2 where i i ef and 1 ii bad denote phrase translation probability and distortion probability respectively ,0,1,0
thus equation 3 can be rewritten as i p i iii i i eppfef 4 42 lexical weight given a phrase pair ef and a word alignment a between the source word positions ni1 and the target word positions mj1 the lexical weight can be estimated according to the following method koehn et al 2003 ,0,1,0
1 introduction for statistical machine translation smt phrasebased methods koehn et al 2003 och and ney 2004 and syntaxbased methods wu 1997 alshawi et al 2000 yamada and knignt 2001 melamed 2004 chiang 2005 quick et al 2005 mellebeek et al 2006 outperform wordbased methods brown et al 1993 ,0,1,0
the phrasebased approach developed for statistical machine translation koehn et al 2003 is designed to overcome the restrictions on manytomany mappings in wordbased translation models ,0,1,0
starting from a wordbased alignment for each pair of sentences the training for the algorithm accepts all contiguous bilingual phrase pairs up to a predetermined maximum length whose words are only aligned with each other koehn et al 2003 ,0,1,0
it is an extension of pharaoh koehn et al 2003 and supports factor training and decoding ,0,1,0
phrasebased decoding koehn et al 2003 is a dominant formalism in statistical machine translation ,1,0,0
the most widely used approach derives phrase pairs from word alignment matrix och and ney 2003 koehn et al 2003 ,1,0,0
41 training and translation setup our decoder is a phrasebased multistack implementation of the loglinear model similar to pharaoh koehn et al 2003 ,0,1,0
since most phrases appear only a few times in training data a phrase pair translation is also evaluated by lexical weights koehn et al 2003 or term weighting zhao et al 2004 as additional features to avoid overestimation ,0,1,0
the commonly used phrase extraction approach based on word alignment heuristics referred as viterbiextract algorithm for comparison in this paper as described in och 2002 koehn et al 2003 is a special case of the algorithm where candidate phrase pairs are restricted to those that respect word alignment boundaries ,0,1,0
in most statistical machine translation smt models och et al 2004 koehn et al 2003 chiang 2005 some of measure words can be generated without modification or additional processing ,0,1,0
we ran giza och and ney 2000 on the training corpus in both directions with ibm model 4 and then applied the refinement rule described in koehn et al 2003 to obtain a manytomany word alignment for each sentence pair ,0,1,0
the standard solution is to approximate the maximum probability translation using a single derivation koehn et al 2003 ,0,1,0
koehn et al 2003 ,0,1,0
while the research in statistical machine translation smt has made significant progress most smt systems koehn et al 2003 chiang 2007 galleyetal 2006 relyonparallel corpora toextract translation entries ,0,1,0
however since most of statistical translation models koehn et al 2003 chiang 2007 galley et al 2006 are symmetrical it is relatively easy to train a translation system to translate from english to chinese except that weneed to train achinese language model from the chinese monolingual data ,1,0,0
however most of them fail to utilize nonsyntactic phrases well that are proven useful in the phrasebased methods koehn et al 2003 ,0,1,0
1 introduction phrasebased modeling method koehn et al 2003 och and ney 2004a is a simple but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well ,1,0,0
lw was originally used to validate the quality of a phrase translation pair in mt koehn et al 2003 ,0,1,0
rules have the form x e f where e and f are phrases containing terminal symbols words and possibly coindexed instances of the nonterminal symbol x2 associated with each rule is a set of translation model features i f e for example one intuitively natural feature of a rule is the phrase translation logprobability f e log pe f directly analogous to the corresponding feature in nonhierarchical phrasebased models like pharaoh koehn et al 2003 ,0,1,0
given phrase p1 and its paraphrase p2 we compute score3p1p2 by relative frequency koehn et al 2003 score3p1p2 pp2p1 countp2p1p pprime countpprimep1 7 people may wonder why we do not use the same method on the monolingual parallel and comparable corpora ,0,1,0
1 introduction currently most of the phrasebased statistical machine translation pbsmt models marcu and wong 2002 koehn et al 2003 adopt full matching strategy for phrase translation which means that a phrase pair tildewideftildewidee can be used for translating a source phrase f only if tildewidef f due to lack of generalization ability the full matching strategy has some limitations ,0,0,1
this unfortunately significantly jeopardizes performance koehn et al 2003 xiong et al 2008 because by integrating syntactic constraint into decoding as a hard constraint it simply prohibits any other useful nonsyntactic translations which violate constituent boundaries ,0,1,0
52 translation experiments with a bigram language model in this section we consider two real translation tasks namely translation from english to french trained on europarl koehn et al 2003 and translation from german to spanish training on the newscommentary corpus ,0,1,0
1 introduction phrasebased systems koehn et al 2003 are probably the most widespread class of statistical machine translation systems and arguably one of the most successful ,1,0,0
we obtained word alignments of the training data by first running giza och and ney 2003 and then applying the refinement rule growdiagfinaland koehn et al 2003 ,0,1,0
on the other hand other authors eg och and ney 2004 koehn et al 2003 chiang 2007 do use the expression phrasebased models ,0,1,0
they recover additional latent variables socalled nuisance variablesthat are not of interest to the user1 for example though machine translation mt seeks to output a string typical mt systems koehn et al 2003 chiang 2007 1these nuisance variables may be annotated in training data but it is more common for them to be latent even there ie there is no supervision as to their correct values ,0,1,0
594 23 viterbi approximation to approximate the intractable decoding problem of 2 most mt systems koehn et al 2003 chiang 2007 use a simple viterbi approximation y argmax ytx pviterbiyx 4 argmax ytx max ddxy pydx 5 y parenleftbigg argmax ddx pydx parenrightbigg 6 clearly 5 replaces the sum in 2 with a max ,0,1,0
1 introduction hierarchical approaches to machine translation have proven increasingly successful in recent years chiang 2005 marcu et al 2006 shen et al 2008 and often outperform phrasebased systems och and ney 2004 koehn et al 2003 on targetlanguage fluency and adequacy ,1,0,0
these wordbased models are used to find the latent wordalignments between bilingual sentence pairs from which a weighted string transducer can be induced either finite state koehn et al 2003 or synchronous context free grammar chiang 2007 ,0,1,0
we use the giza implementation of ibm model 4 brown et al 1993 och and ney 2003 coupled with the phrase extraction heuristics of koehn et al ,0,1,0
1 introduction the field of machine translation has seen many advances in recent years most notably the shift from wordbased brown et al 1993 to phrasebased models which use token ngrams as translation units koehn et al 2003 ,1,0,0
actually it is defined similarly to the translation model in smt koehn et al 2003 ,0,1,0
1ldc2002e18 4000 sentences ldc2002t01 ldc2003e07 ldc2003e14 ldc2004t07 ldc2005t10 ldc2004t08 hk hansards 500000 sentences 2httpwwwstatmtorgwmt07sharedtaskhtml for both the tasks the word alignment were trained by giza in two translation directions and refined by growdiagfinal method koehn et al 2003 ,0,1,0
koehn et al 2003 in which translation and language models are trainable separately too ,0,1,0
this is applied to maximize coverage which is similar as the final in koehn et al 2003 ,0,1,0
our decoder is a phrasebased multistack implementation of the loglinear model similar to pharaoh koehn et al 2003 ,0,1,0
the next two methods are heuristic h in och and ney 2003 and growdiagonal gd proposed in koehn et al 2003 ,0,1,0
it is a fundamental and often a necessary step before linguistic knowledge acquisitions such as training a phrase translation table in phrasal machine translation mt system koehn et al 2003 or extracting hierarchial phrase rules or synchronized grammars in syntaxbased translation framework ,0,1,0
1 introduction phrasebased translation koehn et al 2003 and hierarchical phrasebased translation chiang 2005 are the state of the art in statistical machine translation smt techniques ,0,1,0
2004 better languagespecific preprocessing koehn and knight 2003 and restructuring collins et al 2005 additional feature functions such as word class language models and minimum error rate training och 2003 to optimize parameters ,0,1,0
see och and ney 2000 yamada and knight 2001 koehn and knight 2002 koehn et al 2003 schafer and yarowsky 2003 and gildea 2003 ,0,1,0
1 introduction in recent years various phrase translation approaches marcu and wong 2002 och et al 1999 koehn et al 2003 have been shown to outperform wordtoword translation models brown et al 1993 ,1,0,0
whereas language generation has benefited from syntax wu 1997 alshawi et al 2000 the performance of statistical phrasebased machine translation when relying solely on syntactic phrases has been reported to be poor koehn et al 2003 ,0,1,0
the inclusion of phrases longer than three words in translation resources has been avoided as it has been shown not to have a strong impact on translation performance koehn et al 2003 ,0,1,0
accordingly in this section we describe a set of experiments which extends the work of way and gough 2005 by evaluating the markerbased ebmt system of gough way 2004b against a phrasebased smt system built using the following components giza to extract the wordlevel correspondences the giza word alignments are then refined and used to extract phrasal alignments och ney 2003 or koehn et al 2003 for a more recent implementation probabilities of the extracted phrases are calculated from relative frequencies the resulting phrase translation table is passed to the pharaoh phrasebased smt decoder which along with sri language modelling toolkit5 performs translation ,0,1,0
under a phrase based translation model koehn et al 2003 marcu and wong 2002 this distinction is important and will be discussed in more detail ,0,1,0
the first system is the pharaoh decoder provided by koehn et al 2003 for the shared data task ,0,1,0
for further information on these parameter settings confer koehn et al 2003 ,0,1,0
in the area of statistical machine translation smt recently a combination of the bleu evaluation metric papineni et al 2001 and the bootstrap method for statistical significance testing efron and tibshirani 1993 has become popular och 2003 kumar and byrne 2004 koehn 2004b zhang et al 2004 ,0,1,0
traditionally maximumlikelihood estimation from relative frequencies is used to obtain conditional probabilities koehn et al 2003 eg pst cstsummationtexts cst since the estimation problems for pst and pts are symmetrical we will usually refer only to pst for brevity ,0,1,0
the features used in this study are the length of t a singleparameter distortion penalty on phrase reordering in a as described in koehn et al 2003 phrase translation model probabilities and trigram language model probabilities logpt using kneserney smoothing as implemented in the srilm toolkit stolcke 2002 ,0,1,0
this is the traditional approach for glassbox smoothing koehn et al 2003 zens and ney 2004 ,0,1,0
in englishtogerman this result produces results very comparable to a phrasal smt system koehn et al 2003 trained on the same data ,0,1,0
this dependency graph is partitioned into treelets like koehn et al 2003 we assume a uniform probability distribution over all partitions ,0,1,0
1 introduction during the last few years smt systems have evolved from the original wordbased approach brown et al 1993 to phrasebased translation systems koehn et al 2003 ,1,0,0
table 2 the set of tags used to mark explicit morphemes in english tag meaning jjr adjective comparative jjs adjective superlative nns noun plural pos possessive ending rbr adverb comparative rbs adverb superlative vb verb base form vbd verb past tense vbg verb gerund or present participle vbn verb past participle vbp verb non3rd person singular present vbz verb 3rd person singular present figure 2 morpheme alignment between a turkish and an english sentence 4 experiments we proceeded with the following sequence of experiments 1 baseline as a baseline system we used a pure wordbased approach and used pharaoh training tool 2004 to train on the 22500 sentences and decoded using pharaoh koehn et al 2003 to obtain translations for a test set of 50 sentences ,0,1,0
ppmodel wecollectedtheppparametersbysimply reading the alignment matrices resulting from the word alignment in a way similar to the one described in koehn et al 2003 ,0,1,0
this includes the standard notion of phrase popular with phrasedbased smt koehn et al 2003 vogel et al 2003 aswellassequencesofwordsthatcontaingapspossibly of arbitrary size ,0,1,0
it has the advantage of naturally capturing local reorderings and is shown to outperform wordbased machine translation koehn et al 2003 ,0,0,1
on the other hand models that deal with structures or phrases instead of single words have also been proposed the syntax translation models are described in yamada and knight 2001 alignment templates are used in och 2002 and the alignment template approach is reframed into the socalled phrase based translation pbt in marcu and wong 2002 zens et al 2002 koehn et al 2003 tomas and casacuberta 2003 ,0,1,0
word alignment and phrase extraction we used the giza word alignment software 3 to produce initial word alignments for our miniature bilingual corpus consisting of the source french file and the english reference file and the refined word alignment strategy of och and ney 2003 koehn et al 2003 tiedemann 2004 to obtain improved word and phrase alignments ,0,1,0
in a phrasebased statistical translation koehn et al 2003 a bilingual text is decomposed as k phrase translation pairs e1 fa1 e2 fa2 the input foreign sentence is segmented into phrases f k1 122 mapped into corresponding english ek1 then reordered to form the output english sentence according to a phrase alignment index mapping a in a hierarchical phrasebased translation chiang 2005 translation is modeled after a weighted synchronouscfg consisting of production rules whose righthand side is paired aho and ullman 1969 x where x is a nonterminal and are strings of terminals and nonterminals ,0,1,0
second phrase translation pairs are extracted from the word aligned corpus koehn et al 2003 ,0,1,0
the decoding process is very similar to those described in koehn et al 2003 it starts from an initial empty hypothesis ,0,1,0
for each differently tokenized corpus we computed word alignments by a hmm translation model och and ney 2003 and by a word alignment refinement heuristic of growdiagfinal koehn et al 2003 ,0,1,0
one is a phrasebased translation in which a phrasal unit is employed for translation koehn et al 2003 ,0,1,0
138 2 rule generation we start with phrase translations on the parallel training data using the techniques and implementation described in koehn et al 2003a ,0,1,0
we use the following features for our rules sourceand targetconditioned neglog lexical weights as described in koehn et al 2003b neglog relative frequencies lefthandsideconditioned targetphraseconditioned sourcephraseconditioned counters no rule applications no target words flags ispurelylexical ie contains only terminals ispurelyabstract ie contains only nonterminals isxrule ie nonsyntactical span isgluerule 139 penalties rareness penalty exp1 rulefrequency unbalancedness penalty meantargetsourceratio no source words no target words 4 parsing our syncfg rules are equivalent to a probabilistic contextfree grammar and decoding is therefore an application of chart parsing ,0,1,0
5 results we present results that compare our system against the baseline pharaoh implementation koehn et al 2003a and mer training scripts provided for this workshop ,0,1,0
1 introduction recent work in machine translation has evolved from the traditional word brown et al 1993 and phrase based koehn et al 2003a models to include hierarchical phrase models chiang 2005 and bilingual synchronous grammars melamed 2004 ,0,1,0
the huge increase in computational and storage cost of including longer phrases does not provide a signi cant improvement in quality koehn et al 2003 as the probability of reappearance of larger phrases decreases ,0,1,0
we generated for each phrase pair in the translation table 5 features phrase translation probability both directions lexical weighting koehn et al 2003 both directions and phrase penalty constant value ,0,1,0
it generates a vector of 5 numeric values for each phrase pair phrase translation probability fe count f e countee f count f e count f 2httpwwwphramerorg javabased opensource phrase based smt system 3httpwwwisiedulicensedswcarmel 4httpwwwspeechsricomprojectssrilm 5httpwwwiccsinfedacukpkoehntrainingtgz 150 lexical weighting koehn et al 2003 lex fea nproductdisplay i1 1 ji j a summationdisplay ija wfiej lexefa mproductdisplay j1 1 ii j a summationdisplay ija wejfi phrase penalty fe e log fe 1 22 decoding we used the pharaoh decoder for both the minimum error rate training och 2003 and test dataset decoding ,0,1,0
for the future the joint model would benefit from lexical weighting like that used in the standard model koehn et al 2003 ,0,1,0
154 2 translation models 21 standard phrasebased model most phrasebased translation models och 2003 koehn et al 2003 vogel et al 2003 rely on a preexisting set of wordbased alignments from which they induce their parameters ,0,1,0
on smaller data sets koehn et al 2003 the joint model shows performance comparable to the standard model however the joint model does not reach the level of performance of the stan156 enes esen joint 3gram dl4 2051 2664 5gram dl6 2634 2717 lex ,0,1,0
this translation model differs from the well known phrasebased translation approach koehn et al 2003 in two basic issues rst training data is monotonously segmented into bilingual units and second the model considers ngram probabilities instead of relative frequencies ,1,0,0
2 previous work it is helpful to compare this approach with recent efforts in statistical mt phrasebased models koehn et al 2003 och and ney 2004 are good at learning local translations that are pairs of consecutive substrings but often insufficient in modeling the reorderings of phrases themselves especially between language pairs with very different wordorder ,0,0,1
41 translation modeling we can test our models utility for translation by transforming its parameters into a phrase table for the phrasal decoder pharaoh koehn et al 2003 ,0,1,0
2 background 21 phrase table extraction phrasal decoders require a phrase table koehn et al 2003 which contains bilingual phrase pairs and 17 scores indicating their utility ,0,1,0
the growdiagfinal gdf combination heuristic koehn et al 2003 adds links so that each new link connects a previously unlinked token ,0,1,0
however attempts to retrofit syntactic information into the phrasebased paradigm have not met with enormous success koehn et al 2003 och et al 20031 and purely phrasebased machine translation systems continue to outperform these syntaxphrasebased hybrids ,1,0,0
1 introduction recent works in statistical machine translation smt shows how phrasebased modeling och and ney 2000a koehn et al 2003 significantly outperform the historical wordbased modeling brown et al 1993 ,1,0,0
and again we see this insight informing statistical machine translation systems for instance in the phrasebased approaches of och 2003 and koehn et al ,1,0,0
we compared our system to pharaoh a leading phrasal smt decoder koehn et al 2003 and our treelet system ,0,1,0
1 introduction modern phrasal smt systems such as koehn et al 2003 derive much of their power from being able to memorize and use long phrases ,1,0,0
to generate phrase pairs from a parallel corpus we use the diagand phrase induction algorithm described in koehn et al 2003 with symmetrized word alignments generated using ibm model 2 brown et al 1993 ,0,1,0
we employ the phrasebased smt framework koehn et al 2003 and use the moses toolkit koehn et al 2007 and the srilm language modelling toolkit stolcke 2002 and evaluate our decoded translations using the bleu measure papineni et al 2002 using a single reference translation ,0,1,0
51 the baseline system used for comparison was pharaoh koehn et al 2003 koehn 2004 which uses a beam search algorithm for decoding ,0,1,0
1 introduction word alignment is an important step of most modern approaches to statistical machine translation koehn et al 2003 ,0,1,0
2 previous approaches koehn et als 2003 method of estimating phrasetranslation probabilities is very simple ,0,1,0
1 introduction viewed at a very high level statistical machine translationinvolvesfourphases languageandtranslation model training parameter tuning decoding and evaluation lopez 2007 koehn et al 2003 ,0,1,0
we use the following features in our induced englishtoenglish grammar3 3hiero also uses lexical weights koehn et al 2003 in both 122 the joint probability of the two english hierarchical paraphrases conditioned on the nonterminal symbol as defined by this formula pe1 e2x cx e1 e2summationtext e1prime e2prime cx e1prime e2prime ,0,1,0
2 phrasebased statistical mt our baseline is a standard phrasebased smt system koehn et al 2003 ,0,1,0
the features used in this study are the length of t a singleparameter distortion penalty on phrase reordering in a as described in koehn et al 2003 phrase translation model probabilities and 4gram language model probabilities logpt using kneserney smoothing as implemented in the srilm toolkit ,0,1,0
to derive the joint counts cst from which pst and pts are estimated we use the phrase induction algorithm described in koehn et al 2003 with symmetrized word alignments generated using ibm model 2 brown et al 1993 ,0,1,0
159 21 baseline system the baseline system is a phrasebased smt system koehn et al 2003 built almost entirely using freely available components ,0,1,0
1 introduction nowadays statistical machine translation is mainly based on phrases koehn et al 2003 ,0,1,0
they are generated from the training corpus via the diagandmethod koehn et al 2003 and smoothed using kneserney smoothing foster et al 2006 one or several ngram language models trained with the srilm toolkit stolcke 2002 in the baseline experiments reported here we used a trigram model a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase a word penalty ,0,1,0
2 architecture of the system the goal of statistical machine translation smt is to produce a target sentence e from a source sentence f it is today common practice to use phrases as translation units koehn et al 2003 och and ney 2003 and a log linear framework in order to introduce several models explaining the translation process e argmaxpef argmaxe expsummationdisplay i ihief 1 the feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set och and ney 2002 ,0,1,0
as in phrasebased translation model estimation also contains two lexical weights koehn et al 2003 counters for number of target terminals generated ,0,1,0
table 1 shows the impact of increasing reordering window length koehn et al 2003 on translation quality for the dev06data2 increasing the reordering window past 2 has minimal impact on translation quality implying that most of the reordering effects across spanish and english are well modeled at the local or phrase level ,0,1,0
however many of these models are not applicable to parallel treebanks because they assume translation units where either the source text the target text or both are represented as word sequences without any syntactic structure galley et al 2004 marcu et al 2006 koehn et al 2003 ,0,1,0
4 experiments 41 experiment settings a series of experiments were run to compare the performance of the three swd models against the baseline which is the standard phrasebased approach to smt as elaborated in koehn et al 2003 ,0,1,0
the subsequent construction of translation table was done in exactly the same way as explained 4 in koehn et al 2003 ,0,1,0
baseline we use the moses mt system koehn et al 2007 as a baseline and closely follow the example training procedure given for the wmt07 and wmt08 shared tasks4 in particular we perform word alignment in each direction using giza och and ney 2003 apply the growdiagfinaland heuristic for symmetrization and use a maximum phrase length of 7 ,0,1,0
2003 in which we translate a sourcelanguage sentence f into the targetlanguage sentence e that maximizes a linear combination of features and weights1 ea argmax ea scoreeaf 1 argmax ea msummationdisplay m1 mhmeaf 2 where a represents the segmentation of e and f into phrases and a correspondence between phrases and each hm is a rvalued feature with learned weight m the translation is typically found using beam search koehn et al 2003 ,0,1,0
phrasebased mt systems are straightforward to train from parallel corpora koehn et al 2003 and like the original ibm models brown et al 1990 benefit from standard language models built on large monolingual targetlanguage corpora brants et al 2007 ,1,0,0
for the first two tasks all heuristics of the pharaohtoolkit koehn et al 2003 as well as the refined heuristic och and ney 2003 to combine both ibm4alignments were tested and the best ones are shown in the tables ,0,1,0
the defacto answer came during the 1990s from the research community on statistical machine translation who made use of statistical tools based on a noisy channel model originally developed for speech recognition brown et al 1994 och and weber 1998 rzens et al 2002 och and ney 2001 koehn et al 2003 ,0,1,0
giza refined alignments have been used in stateoftheart phrasebased statistical mt systems such as och 2004 variations on the refined heuristic have been used by koehn et al 2003 diag and diagand and by the phrasebased system moses growdiagfinal koehn et al 2007 ,0,1,0
translation systems och and ney 2004 koehn et al 2003 and use moses koehn et al 2007 to search for the best target sentence ,0,1,0
2 architecture of the system the goal of statistical machine translation smt is to produce a target sentence e from a source sentence f it is today common practice to use phrases as translation units koehn et al 2003 och and ney 2003 and a log linear framework in order to introduce several models explaining the translation process e argmaxpef argmaxe expsummationdisplay i ihief 1 the feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set och and ney 2002 ,0,1,0
after unioning the viterbi alignments the stems were replaced with their original words and phrasepairs of up to five foreign words in length were extracted in the usual fashion koehn et al 2003 ,0,1,0
our system is actually designed as a hybrid of the classic phrasebased smt model koehn et al 2003 and the kernel regression model as follows first for each source sentence a small relevant set of sentence pairs are retrieved from the largescale parallel corpus ,0,1,0
4 phrasebased translation in phrasebased translation the translation process is modeled by splitting the source sentence into phrases a contiguous string of words and translating the phrases as a unit och et al 1999 koehn et al 2003 ,0,1,0
the training and decoding system of our smt used the publicly available pharaoh koehn et al 20032 ,0,1,0
22 phrasebased chinesetoenglish mt the mt system used in this paper is moses a stateoftheart phrasebased system koehn et al 2003 ,1,0,0
our baseline model follows chiangs hierarchical model chiang 2007 in conjunction with additional features conditional probabilities in both directions p and p lexical weights koehn et al 2003 in both directions pw and pw 21 word counts e rule counts d target ngram language model plme glue rule penalty to learn preference of nonterminal rewriting over serial combination through eq ,0,1,0
by diagand symmetrization koehn et al 2003 ,0,1,0
consider the lexical model pwryrx defined following koehn et al 2003 with a denoting the most frequent word alignment observed for the rule in the training set ,0,1,0
2 baseline dp decoder the translation model used in this paper is a phrasebased model koehn et al 2003 where the translation units are socalled blocks a block b is a pair consisting of a source phrase s and a target phrase t which are translations of each other ,0,1,0
73 ment and phraseextraction heuristics described in koehn et al 2003 minimumerrorrate training och 2003 a trigram language model with kneserney smoothing trained with srilm stolcke 2002 on the english side of the training data and moses koehn et al 2007 to decode ,0,1,0
1 introduction phrasebased statistical mt pbsmt koehn et al 2003 has become the predominant approach to machine translation in recent years ,1,0,0
to generate the nbest lists a phrase based smt koehn et al 2003 was used ,0,1,0
4 experiments and evaluation we carried out an evaluation on the local rephrasing of french sentences using english as the pivot language2 we extracted phrase alignments of up to 7 word forms using the giza alignment tool och and ney 2003 and the growdiagfinaland heuristics described in koehn et al 2003 on 948507 sentences of the frenchenglish part of the europarl corpus koehn 2005 and obtained some 42 million phrase pairs for which probabilities were estimated using maximum likelihood estimation ,0,1,0
och and ney 2003 and the phrasebased approach to statistical machine translation koehn et al 2003 has led to the development of heuristics for obtaining alignments between phrases of any number of words ,0,1,0
we use the same featuresas koehnet al 2003 ,0,1,0
our method does not suppose a uniform distribution over all possible phrase segmentationsas koehn et al 2003 since each phrase tree has a probability ,0,1,0
this system uses all featuresof conventionalphrasebasedsmt as in koehn et al 2003 ,0,1,0
4 architecture of the smt system the goal of statistical machine translation smt is to produce a target sentence e from a source sentence f it is today common practice to use phrases as translation units koehn et al 2003 och and ney 2003 and a log linear framework in order to introduce several models explaining the translation process e argmaxpef argmaxe expsummationdisplay i ihief 1 the feature functions hi are the system models and the i weights are typically optimized to maximize a scoring function on a development set och and ney 2002 ,0,1,0
in such tasks feature calculation is also very expensive in terms of time required huge sets of extracted rules must be sorted in two directions for relative frequency calculation of such features as the translation probability pfe and reverse translation probability pef koehn et al 2003 ,0,0,1
then the two models and a search module are used to decode the best translation brown et al 1993 koehn et al 2003 ,0,1,0
phrasebased models och and ney 2004 koehn et al 2003 have been a major paradigm in statistical machine translation in the last few years showing stateoftheart performance for many language pairs ,1,0,0
4 machine translation experiments 41 experimental setting for our mt experiments we used a reimplementation of moses koehn et al 2003 a stateoftheart phrasebased system ,1,0,0
the corpus was aligned with giza och and ney 2003 and symmetrized with the growdiagfinaland heuristic koehn et al 2003 ,0,1,0
1 introduction most recent approaches in smt eg koehn et al 2003 chiang 2005 use a loglinear model to combine probabilistic features ,0,1,0
separating the scoring from the source language reordering also has the advantage that the approach in essence is compatible with other approaches such as a traditional psmt system koehn et al 2003b or a hierarchical phrase system chiang 2005 ,0,1,0
in addition to the manual alignment supplied with these data we create an automatic word alignment for them using giza och and ney 2003 and the growdiagfinal gdf symmetrization algorithm koehn et al 2005 ,0,1,0
by using only the bidirectional word alignment links one can implement a very robust such filter as the bidirectional links are generally reliable even though they have low recall for overall translational correspondences koehn et al 2003 ,0,1,0
our technique is based on a novel gibbs sampler that draws samples from the posterior distributionofaphrasebasedtranslationmodelkoehn et al 2003 but operates in linear time with respect to the number of input words section 2 ,0,1,0
we conclude with some challenges that still remain in applying proactive learning for mt 2 syntax based machine translation in recent years corpus based approaches to machine translation have become predominant with phrase based statistical machine translation pbsmt koehn et al 2003 being the most actively progressing area ,1,0,0
1 introduction the dominance of traditional phrasebased statistical machine translation pbsmt models koehn et al 2003 has recently been challenged by the development and improvement of a number of new models that explicity take into account the syntax of the sentences being translated ,0,0,1
1 introduction phrasebased statistical machine translation models marcu and wong 2002 koehn et al 2003 och and ney 2004 koehn 2004 koehn et al 2007 have achieved significant improvements in translation accuracy over the original ibm wordbased model ,1,0,0
our mt experiments use a reimplementation of moses koehn et al 2003 called phrasal which provides an easier api for adding features ,0,1,0
2 discriminative reordering model basic reordering models in phrasebased systems use linear distance as the cost for phrase movements koehn et al 2003 ,0,1,0
the stateoftheart smt system moses implements a distancebased reordering model koehn et al 2003 and a distortion model operating with rewrite patterns extracted from a phrase alignment table tillman 2004 ,1,0,0
42 smoothing gaussian priors since nlp maximum entropy models usually have lots of features and lots of sparseness eg features seen in testing not occurring in training smoothing is essential as a way to optimize the feature weights chen and rosenfeld 2000 klein and manning 2003 ,0,1,0
as machine learners we used svmlight1 joachims 1998 and the maxent decider from the stanford classifier2 manning and klein 2003 ,0,1,0
22 maximum entropy models maximum entropy me models berger et al 1996 manning and klein 2003 also known as 928 loglinear and exponential learning models provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging named entity recognition etc maximum entropy models can integrate features from many heterogeneous information sources for classification ,1,0,0
in this paper we adopt stanford maximum entropy manning and klein 2003 implementation in our experiments ,0,1,0
there are accurate parsers available such as chaniak parser charniak and johnson 2005 stanford parser klein and manning 2003 and berkeley parser petrov and klein 2007 among which we use the berkeley parser 2 to help identify the head word ,0,1,0
55 dependency validity features like cui et al 2004 we extract the dependency path from the question word to the common word existing in both question and sentence and the path from candidate answer such as conll ne and numerical entity to the common word for each pair of question and candidate sentence using stanford dependency parser klein and manning 2003 marneffe et al 2006 ,0,1,0
2 maximum entropy models maximum entropy me models berger et al 1996 manning and klein 2003 also known as loglinear and exponential learning models provideageneralpurposemachinelearningtechnique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging named entity recognition etc maximum entropy models can integrate features from many heterogeneous information sources for classification ,1,0,0
we used the implementation of maxent classifier described in manning and klein 2003 ,0,1,0
52 results we use a maximum entropy me classi er manning and klein 2003 which allows an e cient combination of many overlapping features ,1,0,0
for instance for maximum entropy i picked berger et al 1996 ratnaparkhi 1997 for the basic theory ratnaparkhi 1996 for an application pos tagging in this case and klein and manning 2003 for more advanced topics such as optimization and smoothing ,0,1,0
from wordlevel alignments such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages galley et al 2004 or with the the wordlevel alignments alone without reference to external syntactic analysis chiang 2005 which is the scenario we address here ,0,1,0
it reconfirms that only allowing sibling nodes reordering as done in scfg may be inadequate for translational equivalence modeling galley et al 2004 4 3 all the three models on the fbis corpus show much lower performance than that on the other two corpora ,0,1,0
this implies that the complexity of structure divergence between two languages is higher than suggested in literature fox 2002 galley et al 2004 ,0,0,1
however as discussed in prior arts galley et al 2004 and this paper linguisticallyinformed scfg is an inadequate model for parallel corpora due to its nature that only allowing childnode reorderings ,0,1,0
other recent work has incorporated constituent and dependency subtrees into the translation rules used by phrasebased systems galley et al 2004 quirk et al 2005 ,0,1,0
we trained three arabicenglish syntaxbased statistical mt systems galley et al 2004 galley et al 2006 using maxb training och 2003 one on a newswire development set one on a weblog development set and one on a combined development set containing documents from both genres ,0,1,0
as a result they are being used in a variety of applications such as question answering hermjakob 2001 speech recognition chelba and jelinek 1998 language modeling roark 2001 language generation soricut 2006 and most notably machine translation charniak et al 2003 galley et al 2004 collins et al 2005 marcu et al 2006 huang et al 2006 avramidis and koehn 2008 ,0,1,0
current treebased models that integrate linguistics and statistics such as ghkm galley et al 2004 are not able to generalize well from a single phrase pair ,0,0,1
the treetostring model galley et al 2004 liu et al 2006 views the translation as a structure mapping process which first breaks the source syntax tree into many tree fragments and then maps each tree fragment into its corresponding target translation using translation rules finally combines these target translations into a complete sentence ,0,1,0
1 introduction recently linguisticallymotivated syntaxbased translation method has achieved great success in statistical machine translation smt galley et al 2004 liu et al 2006 2007 zhang et al 2007 2008a mi et al 2008 mi and huang 2008 zhang et al 2009 ,1,0,0
we envision the use of a clever datastructure would reduce the complexity but leave this to future work as the experiments table 8 show that 5our definition implies that we only consider faithful spans to be contiguous galley et al 2004 ,0,1,0
1313 e2c c2e union heuristic w big 1337 1266 1455 1428 wo big 1320 1262 1453 1421 table 3 bleu4 scores test set of systems based on giza word alignments 5 6 7 8 bleu4 1427 1442 1443 1445 1455 table 4 bleu4 scores test set of the union alignment using tts templates up to a certain size in terms of the number of leaves in their lhss 41 baseline systems ghkm galley et al 2004 is used to generate the baseline tts templates based on the word alignments computed using giza and different combination methods including union and the diagonal growing heuristic koehn et al 2003 ,0,1,0
this algorithm is referred to as ghkm galley et al 2004 and is widely used in ssmt systems galley et al 2006 liu et al 2006 huang et al 2006 ,1,0,0
to this end the translational correspondence is described within a translation rule ie galley et al 2004 or a synchronous production rather than a translational phrase pair and the training data will be derivation forests instead of the phrasealigned bilingual corpus ,0,1,0
step 2 involves extracting minimal xrs rules galley et al 2004 from the set of stringtreealignments triplets ,0,1,0
in this work we employ a syntaxbased model that applies a series of treestring xrs rules galley et al 2004 graehl and knight 2004 to a source language string to produce a target language phrase structure tree ,0,1,0
1 introduction several recent syntaxbased models for machine translation chiang 2005 galley et al 2004 can be seen as instances of the general framework of synchronous grammars and tree transducers ,0,1,0
however to be more expressive and flexible it is often easier to start with a general scfg or treetransducer galley et al 2004 ,1,0,0
experiments show that the resulting rule set significantly improves the speed and accuracy over monolingual binarization see table 1 in a stateoftheart syntaxbased machine translation system galley et al 2004 ,1,0,0
these rules can be learned from a parallel corpus using english parsetrees chinese strings and word alignment galley et al 2004 ,0,1,0
from this data we use the the ghkm minimalrule extraction algorithm of galley et al 2004 to yield rules like npcx0npb ppinof x1npbx1 de x0 though this rule can be used in either direction here we use it righttoleft chinese to english ,0,1,0
stituent alignments galley et al 2004 ,0,1,0
212 research on syntaxbased smt a number of researchers alshawi 1996 wu 1997 yamada and knight 2001 gildea 2003 melamed 2004 graehl and knight 2004 galley et al 2004 have proposed models where the translation process involves syntactic representations of the source andor target languages ,0,1,0
2004 describe how to learn hundreds of millions of treetransformation rules from a parsed aligned chineseenglish corpus and galley et al ,0,1,0
similarly to galley et al 2004 the treetostring alignment templates discussed in this paper are actually transformation rules ,0,1,0
we contrast our work with galley et al 2004 highlight some severe limitations of probability estimates computed from single derivations and demonstrate that it is critical to account for many derivations for each sentence pair ,0,1,0
finally we show that our contextually richer rules provide a 363 bleu point increase over those of galley et al 2004 ,0,0,1
8 conclusions in this paper we developed probability models for the multilevel transfer rules presented in galley et al 2004 showed how to acquire larger rules that crucially condition on more syntactic context and how to pack multiple derivations including interpretations of unaligned words into derivation forests ,0,1,0
we presented some theoretical arguments for not limiting extraction to minimal rules validated them on concrete examples and presented experiments showing that contextually richer rules provide a 363 bleu point increase over the minimal rules of galley et al 2004 ,0,0,1
formally transformational rules ri presented in galley et al 2004 are equivalent to 1state xrs transducers mapping a given pattern subtree to match in pi to a right hand side string ,0,1,0
in this paper we take the framework for acquiring multilevel syntactic translation rules of galley et al 2004 from aligned treestring pairs and present two main extensions of their approach first instead of merely computing a single derivation that minimally explains a sentence pair we construct a large number of derivations that include contextually richer rules and account for multiple interpretations of unaligned words ,0,1,0
we would expect the opposite effect with handaligned data galley et al 2004 ,0,1,0
analogous techniques for treestructured translation models involve either allowing each nonterminal to generate both terminals and other nonterminals groves et al 2004 chiang 2005 or given a constraining parse tree to flatten it fox 2002 zens and ney 2003 galley et al 2004 ,0,1,0
depending on the type of input these efforts can be divided into two broad categories the stringbased systems whose input is a string to be simultaneously parsed and translated by a synchronous grammar wu 1997 chiang 2005 galley et al 2006 and the treebased systems whose input is already a parse tree to be directly converted into a target tree or string lin 2004 ding and palmer 2005 quirk et al 2005 liu et al 2006 huang et al 2006 ,0,1,0
from the above discussion we can see that traditional tree sequencebased method uses single tree as translation input while the forestbased model uses single subtree as the basic translation unit that can only learn treetostring galley et al 2004 liu et al 2006 rules ,0,1,0
while galley 2004 describes extracting treetostring rules from 1best trees mi and huang et al ,0,1,0
the synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse galley et al 2004 ,0,1,0
gildea 2003 and galley et al 2004 discuss different ways of generalizing the treelevel crosslinguistic correspondence relation so it is not confined to single tree nodes thereby avoiding a continuity assumption ,0,1,0
besides being linguistically motivated the need for edl is also supported by empirical findings in mt that onelevel rules are often inadequate fox 2002 galley et al 2004 ,0,1,0
however many of these models are not applicable to parallel treebanks because they assume translation units where either the source text the target text or both are represented as word sequences without any syntactic structure galley et al 2004 marcu et al 2006 koehn et al 2003 ,0,0,1
chiang 2005 imamura et al 2004 galley et al 2004 ,0,1,0
the underlying formalisms used has been quite broad and include simple formalisms such as itgs wu 1997 hierarchicalsynchronousruleschiang 2005 string to tree models by galley et al 2004 and galley et al 2006 synchronous cfg models such xia and mccord 2004 yamada and knight 2001 synchronous lexical functional grammar inspired approaches probst et al 2002 and others ,0,1,0
our process of extraction of rules as synchronous trees and then converting them to synchronous cfg rules is most similar to that of galley et al 2004 ,0,1,0
firstly they classify all the ghkm2 rules galley et al 2004 galley et al 2006 into two categories lexical rules and nonlexical rules ,0,1,0
we guess it is an acronym for the authors of galley et al 2004 michel galley mark hopkins kevin knight and daniel marcu ,0,1,0
recently bean and riloff 2004 have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution using techniques adapted from information extraction ,0,1,0
2001 and unsupervised approaches eg cardie and wagstaff 1999 bean and riloff 2004 ,0,1,0
31 selecting coreference systems a learningbased coreference system can be defined by four elements the learning algorithm used to train the coreference classifier the method of creating training instances for the learner the feature set 2examples of such scoring functions include the dempstershafer rule see kehler 1997 and bean and riloff 2004 and its variants see harabagiu et al ,0,1,0
recently bean and riloff 2004 presented an unsupervised approach to coreference resolution which mined the coreferring np pairs with similar predicatearguments from a large corpus using a bootstrapping method ,0,1,0
since no such corpus exists researchers have used coarser features learned from smaller sets through supervised learning soon et al 2001 ng and cardie 2002 manuallyde ned coreference patterns to mine speci c kinds of data bean and riloff 2004 bergsma 2005 or accepted the noise inherent in unsupervised schemes ge et al 1998 cherry and bergsma 2005 ,0,1,0
bean and riloff 2004 used bootstrapping to extend their semantic compatibility model which they called contextualrole knowledge by identifying certain cases of easilyresolved anaphors and antecedents ,0,1,0
bean and riloff 2004 present a system called babar that uses contextual role knowledge to do coreference resolution ,0,1,0
2004 or wikipedia ponzetto and strube 2006 and the contextual role played by an np see bean and riloff 2004 ,0,1,0
35 anaphoricity determination finally several coreference systems have successfully incorporated anaphoricity determination 660 modules eg ng and cardie 2002a and bean and riloff 2004 ,0,1,0
there are also approaches to anaphora resolution using unsupervised methods to extract useful information such as gender and number ge et al 1998 or contextual roleknowledge bean and riloff 2004 ,0,1,0
it has shown promise in improving the performance of many tasks such as name tagging miller et al 2004 semantic class extraction lin et al 2003 chunking ando and zhang 2005 coreference resolution bean and riloff 2004 and text classification blum and mitchell 1998 ,0,1,0
lscript1regularized loglinear models lscript1llms on the other hand provide sparse solutions in which weights of irrelevant features are exactly zero by assumingalaplacianpriorontheweightstibshirani 1996 kazama and tsujii 2003 goodman 2004 gao et al 2007 ,0,1,0
goodman 2004 and lscript22 regularization lau 1994 chen and rosenfeld 2000 lebanon and lafferty 2001 ,0,1,0
our interpretation is more useful than past interpretations involving marginal constraints kneser and ney 1995 chen and goodman 1998 or maximumentropy models goodman 2004 as it can recover the exact formulation of interpolated kneserney and actually produces superior results ,0,0,1
l1 or lasso regularization of linear models introduced by tibshirani 1996 embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework and has generated a large amount of interest in the nlp community recently eg goodman 2003 riezler and vasserman 2004 ,0,1,0
we used the wordnetsimilarity package pedersen et al 2004 to compute baseline scores for several existing measures noting that one word pair was not processed in ws353 because one of the words was missing from wordnet ,0,1,0
we consider the outputs of the top 3 allwords wsd systems that participated in senseval3 gambl decadt et al 2004 senselearner mihalcea and faruque 2004 and koc university yuret nouns verbs adjectives fscore 04228 04319 04727 feature fscore ablation difference topsig 00403 oed 00355 00126 00124 deriv 00351 00977 00352 res 00287 00147 twin 00285 00109 00130 mn 00188 00358 lesk 00183 00541 00250 sensenum 00155 00146 00147 sensecnt 00121 00160 00168 domain 00119 00082 00265 lch 00099 00068 wup 00036 00168 jcn 00025 00190 antonym 00000 00295 00000 maxmn 00013 00179 vec 00024 00371 00062 hso 00073 00112 00246 lin 00086 00742 cousin 00094 verbgrp 00327 verbfrm 00102 pertainym 00029 table 4 feature ablation study fscore difference obtained by removal of the single feature 2004 ,0,1,0
a variety of synset similarity measures based on properties of wordnet itself have been proposed nine such measures are discussed in pedersen et al 2004 including glossbased heuristics lesk 1986 banerjee and pedersen 2003 informationcontent based measures resnik 1995 lin 1998 jiang and conrath 1997 and others ,0,1,0
we use eight similarity measures implemented within the wordnetsimilarity package5 described in pedersen et al 2004 these include three measures derived from the paths between the synsets in wordnet hso hirst and stonge 1998 lch leacock and chodorow 1998 and wup wu and palmer 1994 three measures based on information content res resnik 1995 lin lin 1998 and jcn jiang and conrath 1997 the glossbased extended lesk measure lesk banerjee and pedersen 2003 and finally the gloss vector similarity measure vector patwardan 2003 ,0,1,0
is a wordnet based relatedness measure pedersen et al 2004 ,0,1,0
we could also use the value of semantic similarity and relatedness measures pedersen et al 2004 or the existence of hypernym or hyponym relations as features ,0,1,0
selectional preferences are estimated using grammatical collocation information from the british national corpus bnc obtained with the word sketch engine wse kilgarriff et al 2004 ,0,1,0
relatedness scores are computed for each pair of senses of the grammatically linked pair of words w1 w2 gr using the wordnetsimilarity103 package and the lesk 759 option pedersen et al 2004 ,0,1,0
in addition we use the measure from resnik 1995 which is computed using an intrinsic information content measure relying on the hierarchical structure of the category tree seco et al 2004 ,0,1,0
1 introduction the last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems soon et al 2001 ng cardie 2002 yang et al 2003 luo et al 2004 inter alia ,0,1,0
we enrich the semantic information available to the classifier by using semantic similarity measures based on the wordnet taxonomy pedersen et al 2004 ,0,1,0
1 introduction estimating the degree of semantic relatedness between words in a text is deemed important in numerous applications wordsense disambiguation banerjee and pedersen 2003 story segmentation stokes et al 2004 error correction hirst and budanitsky 2005 summarization barzilay and elhadad 1997 gurevych and strube 2004 ,0,1,0
we use the default configuration of the measure in wordnetsimilarity012 package pedersen et al 2004 and with a single exception the measure performed below gic see bp in table 1 ,0,1,0
8see formula in appendix b we use pedersen et al 2004 implementation with a minor alteration see beigman klebanov 2006 ,0,1,0
sraw finds the sense of each word that is most relatedormostsimilartothoseofitsneighborsinthe sentence according to any of the ten measures available in wordnetsimilarity pedersen et al 2004 ,0,1,0
extensive research concerning the integration of semantic knowledge into nlp for the english language has been arguably fostered by the emergence of wordnetsimilarity package pedersen et al 20041 in its turn the development of the wordnet based semantic similarity software has been facilitated by the availability of tools to easily retrieve 1httpwwwdumnedua0 tpedersesimilarityhtml data from wordnet eg wordnetquerydata2 jwnl3 research integrating semantic knowledge into nlp for languages other than english is scarce ,1,0,0
the wnsimilarity package pedersen et al 2004 to compute the jiangconrath jc distance jiang and conrath 1997 as in corley and mihalcea 2005 ,0,1,0
the measures vary from simple edgecounting to attempt to factor in peculiarities of the network structure by considering link direction relative path and density such as vector lesk hso lch wup path res lin and jcn pedersen et al 2004 ,0,1,0
the wordnetsimilarity package pedersen et al 2004 implements this distance measure and was used by the authors ,0,1,0
the implementation includes pathlength rada et al 1989 wu palmer 1994 leacock chodorow 1998 informationcontent resnik 1995 seco et al 2004 and textoverlap lesk 1986 banerjee pedersen 2003 measures as described in strube ponzetto 2006 ,0,1,0
we believe that the extensive usage of such measures derives also from the availability of robust and freely availablesoftwarethatallowstocomputethempedersen et al 2004 wordnetsimilarity ,1,0,0
2 wordnetbased semantic relatedness measures 21 basic measures two similaritydistance measures from the perl package wordnetsimilarity written by pedersen et al 2004 are used ,0,1,0
the approaches proposed to the ace rdc task such as kernel methods zelenko et al 2002 and maximum entropy methods kambhatla 2004 required the availability of large set of human annotated corpora which are tagged with relation instances ,0,1,0
the system uses wordnetbased 1httpsenserelatesourceforgenet measures of semantic relatedness2 pedersen et al 2004 to measure the relatedness between the different senses of the target word and the words in its context ,0,1,0
2007 observe that their predominant sense method is not performing as well for 3we use the lesk overlap similarity as implemented by the wordnetsimilarity package pedersen et al 2004 ,0,1,0
we measure semantic similarity using the shortest path length in wordnet fellbaum 1998 as implemented in the wordnet similarity package pedersen et al 2004 ,0,1,0
the wordnetsimilarity package provides a flexible implementation of many of these measures pedersen et al 2004 ,1,0,0
it is often argued that the ability to translate discontiguous phrases is important to modeling translation chiang 2007 simard et al 2005 quirk and menezes 2006 and it may be that this explains the results ,0,1,0
this definition is similar to that of minimal translation units as described in quirk and menezes 2006 although they allow null words on either side ,0,1,0
also slightly restating the advantages of phrasepairs identified in quirk and menezes 2006 these blocks are effective at capturing context including the encoding of noncompositional phrase pairs and capturing local reordering but they lack variables eg embedding between ne pas in french have sparsity problems and lack a strategy for global reordering ,1,0,0
however in quirk and menezes 2006 the authors investigate minimum translation units mtu which is a refinement over a similar approach by banchs et al 2005 to eliminate the overlap issue ,1,0,0
however it cannot handle longdistance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features quirk and menezes 2006 ,0,1,0
one of the theoretical problems with phrase based smt models is that they can not effectively model the discontiguous translations and numerous attempts have been made on this issue simard et al 2005 quirk and menezes 2006 wellington et al 2006 bod 2007 zhang et al 2007 ,0,1,0
4 testing the four hypotheses the question of why selftraining helps in some cases mcclosky et al 2006 reichart and rappoport 2007 but not others charniak 1997 steedman et al 2003 has inspired various theories ,0,1,0
the selftraining protocol is the same as in charniak 1997 mcclosky et al 2006 reichart and rappoport 2007 we parse the entire unlabeled corpus in one iteration ,0,1,0
glish previously used for selftraining of parsers mcclosky et al 2006 ,0,1,0
2006 effectively utilized unlabeled data to improve parsing accuracy on the standard wsj training set but they used a twostage parser comprised of charniaks lexicalized probabilistic parser with nbest parsing and a discriminative reranking parser charniak and johnson 2005 and thus it would be better categorized as cotraining mcclosky et al 2008 ,0,1,0
the other is the selftraining mcclosky et al 2006 which first parses and reranks the nanc corpus and then use them as additional training data to retrain the model ,0,1,0
a totally different approach to improving the accuracy of our parser is to use the idea of selftraining described in mcclosky et al 2006 ,0,1,0
the problem itself has started to get attention only recently roark and bacchiani 2003 hara et al 2005 daume iii and marcu 2006 daume iii 2007 blitzer et al 2006 mcclosky et al 2006 dredze et al 2007 ,0,1,0
in contrast semisupervised domain adaptation blitzer et al 2006 mcclosky et al 2006 dredze et al 2007 is the scenario in which in addition to the labeled source data we only have unlabeled and no labeled target domain data ,0,1,0
so far most previous work on domain adaptation for parsing has focused on datadriven systems gildea 2001 roark and bacchiani 2003 mcclosky et al 2006 shimizu and nakagawa 2007 ie systems employing constituent or dependency based treebank grammars charniak 1996 ,0,1,0
parse selection constitutes an important part of many parsing systems johnson et al 1999 hara et al 2005 van noord and malouf 2005 mcclosky et al 2006 ,0,1,0
there are only a few successful studies such as ando and zhang 2005 for chunking and mcclosky et al 2006a mcclosky et al 2006b on constituency parsing ,1,0,0
theyalsoappliedselftraining to domain adaptation of a constituency parser mcclosky et al 2006b ,0,1,0
recently there have been some improvements to the charniak parser use nbest reranking as reported in charniak and johnson 2005 and selftraining and reranking using data from the north american news corpus nanc and adapts much better to the brown corpus mcclosky et al 2006a mcclosky et al 2006b ,0,1,0
the syntactic parser is the version that is selftrained using 2500000 sentences from nanc and where the starting version is trained only on wsj data mcclosky et al 2006b ,0,1,0
as far as we know language modeling always improves with additional training data so we add data from the north american news text corpus nanc graff 1995 automatically parsed with the charniak parser mcclosky et al 2006 to train our language model on up to 20 million additional words ,0,1,0
while mcclosky et al 2006 showed that this technique was effective when testing on wsj the true distribution was closer to wsj so it made sense to emphasize it ,0,1,0
thus the wsjnanc model has better oracle rates than the wsj model mcclosky et al 2006 for both the wsj and brown domains ,0,1,0
to use the data from nanc we use selftraining mcclosky et al 2006 ,0,1,0
furthermore use of the selftraining techniques described in mcclosky et al 2006 raise this to 878 an error reduction of 28 again without any use of labeled brown data ,1,0,0
the trends are the same as in mcclosky et al 2006 adding nanc data improves parsing performance on brown development considerably improving the fscore from 839 to 864 ,0,1,0
mcclosky et al 2006 ,0,1,0
this can either be semisupervised parsing using both annotated and unannotated data mcclosky et al 2006 or unsupervised parsing training entirely on unannotated text ,0,1,0
while most parsing methods are currently supervised or semisupervised mcclosky et al 2006 henderson 2004 steedman et al 2003 they depend on handannotated data which are difficult to come by and which exist only for a few languages ,0,0,1
unknown words were not identified in mcclosky et al 2006a as a useful predictor for the benefit of selftraining ,0,1,0
622 we also identified a length effect similar to that studied by mcclosky et al 2006a for selftraining using a reranker and large seed as detailed in section 2 ,0,1,0
indeed in the ii scenario steedman et al 2003a mcclosky et al 2006a charniak 1997 reported no improvement of the base parser for small 500 sentences in the first paper and large 40k sentences in the last two papers seed datasets respectively ,0,1,0
in the ii oo and oi scenarios mcclosky et al 2006a 2006b succeeded in improving the parser performance only when a reranker was used to reorder the 50best list of the generative parser with a seed size of 40k sentences ,1,0,0
recently mcclosky et al 2006a mcclosky et al 2006b have successfully applied selftraining to various parser adaptation scenarios using the reranking parser of charniak and johnson 2005 ,1,0,0
mcclosky et al 2006a use sections 221 of the wsj penntreebank as seed data and between 50k to 2500k unlabeled nanc corpus sentences as selftraining data ,0,1,0
as a result the good results of mcclosky et al 2006a 2006b with large seed sets do not immediately imply success with small seed sets ,1,0,0
tighter integration of semantics into the parsing models possibly in the form of discriminative reranking models collins and koo 2005 charniak and johnson 2005 mcclosky et al 2006 is a promising way forward in this regard ,1,0,0
type system f1 d collins 2000 897 henderson 2004 901 charniak and johnson 2005 910 updated johnson 2006 914 this work 917 g bod 2003 907petrov and klein 2007 901 s mcclosky et al ,0,1,0
however more recent results have shown that it can indeed improve parser performance bacchiani et al 2006 mcclosky et al 2006a mcclosky et al 2006b ,0,1,0
recently there have been some works on using multiple treebanks for domain adaptation of parsers where these treebanks have the same grammar formalism mcclosky et al 2006b roark and bacchiani 2003 ,0,1,0
our results on chinese data confirm previous findings on english data shown in mcclosky et al 2006a reichart and rappoport 2007 ,0,1,0
uses for kbest lists include minimum bayes risk decoding goodman 1998 kumar and byrne 2004 discriminative reranking collins 2000 charniak and johnson 2005 and discriminative training och 2003 mcclosky et al 2006 ,0,1,0
these parser output trees can by produced by a second parser in a cotraining scenario steedman et al 2003 or by the same parser with a reranking component in a type of selftraining scenariomccloskyetal 2006 ,0,1,0
5 conclusions and future work the paper compares structural correspondence learning blitzer et al 2006 with various instances of selftraining abney 2007 mcclosky et al 2006 for the adaptation of a parse selection model to wikipedia domains ,0,1,0
we examine structural correspondence learning scl blitzer et al 2006 for this task and compare it to several variants of selftraining abney 2007 mcclosky et al 2006 ,0,1,0
studies on selftraining have focused mainly on generative constituent based parsing steedman et al 2003 mcclosky et al 2006 reichart and rappoport 2007 ,0,1,0
improvements are obtained mcclosky et al 2006 mcclosky and charniak 2008 showing that a reranker is necessary for successful selftraining in such a highresource scenario ,0,1,0
the techniques examined are structural correspondence learning scl blitzer et al 2006 and selftraining abney 2007 mcclosky et al 2006 ,0,1,0
1 introduction and motivation parse selection constitutes an important part of many parsing systems hara et al 2005 van noord and malouf 2005 mcclosky et al 2006 ,0,1,0
strube and ponzetto explored the use of wikipedia for measuring semantic relatedness between two concepts 2006 and for coreference resolution 2006 ,0,1,0
in fact many studies that try to exploit wikipedia as a knowledge source have recently emerged bunescu and pasca 2006 toral and munoz 2006 ruizcasado et al 2006 ponzetto and strube 2006 strube and ponzetto 2006 zesch et al 2007 ,0,1,0
2005 ponzetto and strube 2006 and the exploitation of advanced techniques that involve joint learning eg daume iii and marcu 2005 and joint inference eg denis and baldridge 2007 for coreference resolution and a related extraction task ,0,1,0
also on ws353 our hybrid sensefiltered variants and wordcosll obtained a correlation score higher than published results using wordnetbased measures jarmasz and szpakowicz 2003 33 to 35 and wikipediabased methods ponzetto and strube 2006 19 to 48 and very close to the results obtained by thesaurusbased jarmasz and szpakowicz 2003 55 and lsabased methods finkelstein et al 2002 56 ,0,1,0
2004 ponzetto and strube 2006 ,0,1,0
luo et al 2004 ponzetto and strube 2006 for other approaches with an evaluation based on true mentions only ,0,1,0
and semantic knowledge sources for coreference resolution ponzetto strube 2006 and strube ponzetto 2006 aimed at showing that the encyclopedia that anyone can edit can be indeed used as a semantic resource for research in nlp ,0,1,0
accordingly in ponzetto strube 2006 we used a machine learning based coreference resolution system to provide an extrinsic evaluation of the utility of wordnet and wikipedia relatedness measures for nlp applications ,0,1,0
2 baseline coreference resolution system our baseline coreference system implements the standard machine learning approach to coreference resolution see ng and cardie 2002b ponzetto and strube 2006 yang and su 2007 for instance which consists of probabilistic classification and clustering as described below ,0,1,0
recently ponzetto and strube 2006 suggest to mine semantic relatedness from wikipedia which can deal with the data sparseness problem suffered by using wordnet ,1,0,0
following ponzetto and strube 2006 we consider an anaphoric reference npi correctly resolved if npi and its closest antecedent are in the same coreference chain in the resulting partition ,0,1,0
2001 and ponzetto and strube 2006 we generate training instances as follows a positive instance is created for each anaphoric np npj and its closest antecedent npi and a negative instance is created for npj paired with each of the intervening nps npi1 npi2 npj1 ,0,1,0
we accordingly introduce approaches which attempt to include semantic information into the coreference models from a variety of knowledge sources eg wordnet harabagiu et al 2001 wikipedia ponzetto strube 2006 and automatically harvested patterns poesio et al 2002 markert nissim 2005 yang su 2007 ,0,1,0
riezler and maxwell 2006 combine transferbased and statistical mt they back off to the smt translation when the grammar is inadequate analysing the grammar to determine this ,0,1,0
it is an important and growing field of natural language processing with applications in areas such as transferbased machine translation riezler and maxwell 2006 and sentence condensation riezler et al 2003 ,0,1,0
riezler and maxwell 2006 do not achieve higher bleu scores but do score better according to human grammaticality judgments for incoverage cases ,1,0,0
riezler and maxwell 2006 describe a method for learning a probabilistic model that maps lfg parse structures in german into lfg parse structures in english ,0,1,0
riezler and iii 2006 report an improvement in mt grammaticality on a very restricted test set short sentences parsable by an lfg grammar without backoff rules ,1,0,0
we used a bottomup ckystyle decoder that works with binary xrs rules obtained via a synchronous binarization procedure zhang et al 2006 ,0,1,0
2 related research several researchers melamed et al 2004 zhang et al 2006 have already proposed methods for binarizing synchronous grammars in the context of machine translation ,0,1,0
translation rules can look like phrase pairs with syntax decoration npbnnpprime nnpminister nnpkeizo nnpobuchi bufdfkeubwaz carry extra contextual constraints vpvbdsaid x0sbarc dkx0 according to this rule dk can translate to said only if some chinese sequence to the right ofdk is translated into an sbarc be nonconstituent phrases vpvbdsaid sbarcinthat x0sc dkx0 vpvbdpointed prtrpout x0sbarc dxgpx0 contain noncontiguous phrases effectively phrases with holes ppinon npcnpbdtthe x0nnp nnissue grx0 evabg6 ppinon npcnpbdtthe nnissue x0pp grx0 evevababg6 be purely structural no words sx0npc x1vpx0 x1 reorder their children npcnpbdtthe x0nn ppinof x1npc x1 dfx0 decoding with this model produces a tree in the target language bottomup by parsing the foreign string using a cyk parser and a binarized rule set zhang et al 2006 ,0,1,0
a cykstyle decoder has to rely on binarization to preprocess the grammar as did in zhang et al 2006 to handle multinonterminal rules ,0,1,0
this is in line with earlier work on consistent estimation for similar models zollmann and simaan 2006 and agrees with the most uptodate work that employs bayesian priors over the estimates zhang et al 2008 ,0,1,0
our work expands on the general approach taken by denero et al 2006 moore and quirk 2007 but arrives at insights similar to those of the most recent work zhang et al 2006 albeit in a completely different manner ,0,1,0
31 binarizable segmentations a following zhang et al 2006 huang et al 2008 every sequence of phrase alignments can be viewed 1for example if the cutoff on phrase pairs is ten words all sentence pairs smaller than ten words in the training data will be included as phrase pairs as well ,0,1,0
zhang et al 2006 huang et al 2008 a binarizable segmentationpermutation can be recognized by a binarized synchronous contextfree grammar scfg ie an scfg in which the right hand sides of all nonlexical rules constitute binarizable permutations ,0,1,0
while this heuristic estimator gives good empirical results it does not seem to optimize any intuitively reasonable objective function of the wordaligned parallel corpus see eg denero et al 2006 the mounting number of efforts attacking this problem over the last few years denero et al 2006 marcu and wong 2002 birch et al 2006 moore and quirk 2007 zhang et al 2008 exhibits its difficulty ,0,1,0
binarizing the grammars zhang et al 2006 further increases the size of these sets due to the introduction of virtual nonterminals ,0,1,0
7our decoder lacks certain features shown to be beneficial to synchronous grammar decoding in particular rule binarisation zhang et al 2006 ,0,1,0
the baseline system is based on the synchronous binarization zhang et al 2006 ,0,1,0
42 binarization schemes besides the baseline zhang et al 2006 and iterative cost reduction binarization methods we also perform rightheavy and random synchronous binarizations for comparison ,0,1,0
given the following scfg rule vp vb np jjr vb np will be jjr we can obtain a set of equivalent binary rules using the synchronous binarization method zhang et al 2006 as follows vp v1 jjr v1 jjr v1 vb v2 vb v2 v2 np np will be this binarization is shown with the solid lines as binarization a in figure 1 ,0,1,0
generally two edges can be recombined if they satisfy the following two constraints 1 the lhs lefthand side nonterminals are identical and the subalignments are the same zhang et al 2006 and 2 the boundary words 1 on both sides of the partial translations are equal between the two edges chiang 2007 ,0,1,0
a synchronous 363 binarization method is proposed in zhang et al 2006 whose basic idea is to build a leftheavy binary synchronous tree shapiro and stephens 1991 with a lefttoright shiftreduce algorithm ,0,1,0
3 synchronous binarization optimization by cost reduction as discussed in section 1 binarizing an scfg in a fixed leftheavy way zhang et al 2006 may lead to a large number of competing edges and consequently high risk of making search errors ,0,1,0
the time complexity of the ckybased binarization algorithm is n3 which is higher than that of the linear binarization such as the synchronous binarization zhang et al 2006 ,0,1,0
iterative cost reduction algorithm input an scfg output an equivalent binary scfg of 1 function iterativecostreduction 2 0 3 for each 0do 4 0 5 while does not converge do 6 for each do 7 8 for each do 9 for each do 10 1 11 ckybinarization 12 13 for each do 14 for each do 15 1 16 return in the iterative cost reduction algorithm we first obtain an initial binary scfg 0 using the synchronous binarization method proposed in zhang et al 2006 ,0,1,0
we develop this intuition into a technique called synchronous binarization zhang et al 2006 which binarizes a synchronous production or treetranduction rule on both source and target sides simultaneously ,0,1,0
rulesize and lexicalization affect parsing complexity whether the grammar is binarized explicitly zhang et al 2006 or implicitly binarized using earlystyle intermediate symbols zollmann et al 2006 ,0,1,0
extensions to hiero several authors describe extensions to hiero to incorporate additional syntactic information zollmann and venugopal 2006 zhang and gildea 2006 shen et al 2008 marton and resnik 2008 or to combine it with discriminative latent models blunsom et al 2008 ,0,1,0
not only is this beneficial in terms of parsing complexity but smaller rules can also improve a translation models ability to generalize to new data zhang et al 2006 ,0,1,0
its rule binarization is described in zhang et al 2006 ,0,1,0
so unlike some other studies zens and ney 2003 zhang et al 2006 we used manually annotated alignments instead of automatically generated ones ,0,1,0
decomposing the translational equivalence relations in the training data into smaller units of knowledge can improve a models ability to generalize zhang et al 2006 ,0,1,0
compared with their stringbased counterparts treebased systems offer some attractive features they are much faster in decoding linear time vs cubic time see huang et al 2006 do not require a binarybranching grammar as in stringbased models zhang et al 2006 and can have separate grammars for parsing and translation say a contextfree grammar for the former and a tree substitution grammar for the latter huang et al 2006 ,0,1,0
and gildea 2007 zhang et al 2006 gildea satta and zhang 2006 ,0,1,0
past work has synchronously binarized such rules for efficiency zhang et al 2006 huang et al 2008 ,0,1,0
the decoder uses a binarized representation of the rules which is obtained via a syncronous binarization procedure zhang et al 2006 ,0,1,0
we can use a lineartime algorithm zhang et al 2006 to detect nonitg movement in our highconfidence links and remove the offending sentence pairs from our training corpus ,0,1,0
synchronous binarization zhang et al 2006 solves this problem by simultaneously binarizing both source and targetsides of a synchronous rule making sure of contiguous spans on both sides whenever possible ,1,0,0
decoding with an scfg eg translating from chinese to english using the above grammar can be cast as a parsing problem see section 3 for details in which case we need to binarize a synchronous rule with more than two nonterminals to achieve polynomial time algorithms zhang et al 2006 ,0,1,0
recent work by zhang et al 2006 shows a practically ef cient approach that binarizes linguistically scfg rules when possible ,1,0,0
this is the scenario considered by haghighi and klein 2006 for pos tagging how to construct an accurate tagger given a set of tags and a few example words for each of those tags ,0,1,0
haghighi and klein s 2006 prototypedriven approach requires just a few prototype examples for each pos tag exploiting these labeled words to constrain the labels of their distributionally similar words when training a generative loglinear model for pos tagging ,0,1,0
in fact we found that it doesnt do so badly at all the bitag hmm estimated by em achieves a mean 1to1 tagging accuracy of 40 which is approximately the same as the 413 reported by haghighi and klein 2006 for their sophisticated mrf model ,0,1,0
most previous work exploiting unsupervised training data for inferring pos tagging models has focused on semisupervised methods in the in which the learner is provided with a lexicon specifying the possible tags for each word merialdo 1994 smith and eisner 2005 goldwater and griffiths 2007 or a small number of prototypes for each pos haghighi and klein 2006 ,0,1,0
haghighi and klein 2006 propose constraining the mapping from hidden states to pos tags so that at most one hidden state maps to any pos tag ,0,1,0
haghighi and klein 2006 do the reverse for each class label y they ask the annotators to propose a few prototypical featuresf such thatpyf is as high as possible ,0,1,0
finally following haghighi and klein 2006 and johnson 2007 we can instead insist that at most one hmm state can be mapped to any partofspeech tag ,0,1,0
in addition a number of approaches have focused on developing discriminative approaches for unsupervised and semisupervised tagging smith and eisner 2005 haghighi and klein 2006 ,0,1,0
the 746 final accuracy on apartments is higher than any result obtained by haghighi and klein 2006 the highest is 741 higher than the supervised hmm results reported by grenager et al ,0,0,1
for example both haghighi and klein 2006 and mann and mccallum 2008 have demonstrated results better than 661 on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate ,0,1,0
similarly prototypedriven learning pdl haghighi and klein 2006 optimizes the joint marginal likelihood of data labeled with prototype input features for each label ,0,1,0
one other work that investigates the use of a limited lexicon is haghighiklein 2006 which develops a prototypedrive approach to propagate the categorical property using distributional similarity features using only three exemplars of each tag they achieve a tagging accuracy of 805 using a somewhat larger dataset but also the full penn tagset which is much larger ,0,1,0
this sparse information however can be propagated across all data based on distributional similarity haghighi and klein 2006 ,0,1,0
in this work we use the prototype lists originally defined by haghighi and klein 2006 hk06 and subsequently used by chang et al ,0,1,0
354 supervised induction techniques that have been successfully developed for english eg schutze 1995 clark 2003 including the recentlyproposed prototypedriven approach haghighi and klein 2006 and bayesian approach goldwater and griffiths 2007 ,0,1,0
still however such techniques often require seeds or prototypes cf haghighi and klein 2006 which are used to prune search spaces or direct learners ,0,1,0
haghighi and klein 2006 showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly ,1,0,0
for instance the frequency collected from the data can be used to bias initial transition and emission probabilities in an hmm model the tagged words in igt can be used to label the resulting clusters produced by the word clustering approach the frequent and unambiguous words in the target lines can serve as prototype examples in the prototypedriven approach haghighi and klein 2006 ,0,1,0
in particular knowing a little about the structure of a language can help in developing annotated corpora and tools since a little knowledge can go a long way in inducing accurate structure and annotations haghighi and klein 2006 ,0,1,0
haghighi and klein 2006 ask the user to suggest a few prototypes examples for each class and use those as features ,0,1,0
supervision for simple features has been explored in the literature raghavan et al 2006 druck et al 2008 haghighi and klein 2006 ,0,1,0
first we use the standard approach of greedily assigning each of the learned classes to the pos tag with which it has the greatest overlap and then computing tagging accuracy smith and eisner 2005 haghighi and klein 20068 additionally we compute the mutual information of the learned clusters with the gold tags and we compute the cluster fscore ghosh 2003 ,0,1,0
for comparison haghighi and klein 2006 report an unsupervised baseline of 413 and a best result of 805 from using handlabeled prototypes and distributional similarity ,0,1,0
in many cases improving semisupervised models was done by seeding these models with domain information taken from dictionaries or ontology cohen and sarawagi 2004 collins and singer 1999 haghighi and klein 2006 thelen and riloff 2002 ,0,1,0
grenager et al 2005 and haghighi and klein 2006 also report results for semisupervised learning for these domains ,0,1,0
haghighi and klein 2006 also worked on one of our data sets ,0,1,0
haghighi and klein 2006 extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity ,0,1,0
we implement some global constraints and include unary constraints which were largely imported from the list of seed words used in haghighi and klein 2006 ,0,1,0
haghighi and klein 2006 use a small list of labeled prototypes and no dictionary ,0,1,0
we also report stateoftheart results for hebrew full mor1another notable work though within a slightly different framework is the prototypedriven method proposed by haghighi and klein 2006 in which the dictionary is replaced with a very small seed of prototypical examples ,0,1,0
we achieve competitive performance in comparison to alternate model families in particular generative models such as mrfs trained with em haghighi and klein 2006 and hmms trained with soft constraints chang et al 2007 ,0,1,0
2005 and compare with results reported by hk06 haghighi and klein 2006 and crr07 chang et al 2007 ,0,1,0
1as do constraint relaxation tromble and eisner 2006 and forest reranking huang 2008 ,0,1,0
semantic features are used for classifying entities into semantic types such as name of person organization or place while syntactic features characterize the kinds of dependency 5it is worth noting that the present approach can be recast into one based on constraint relaxation tromble and eisner 2006 ,0,1,0
this approach will generally take advantage of languagespecific eg in freeman et al 2006 and domainspecific knowledge of any external resources eg database names dictionaries etc and of any information about the entities to process eg their type person name organization etc or internal structure eg in prager et al 2007 ,0,1,0
2 basic approaches 21 crosslingual approach our crosslingual approach called mlev is based on freeman et al 2006 who used a modified levenshtein string editdistance algorithm to match arabic script person names against their corresponding english versions ,0,1,0
for this study the levenshtein editdistance score where a perfect match scores zero is roman chinese pinyin alignment score lev ashburton ashenbodu a s h b u r t o n a s h e n b o d u 067 mlev ashburton ashenbodu a s h b u r t o n a s h e n b o d u 072 maline asvburton asecnpotu a sv b u r t o n a s ec n p o t u 048 3 normalized to a similarity score as in freeman et al 2006 where the score ranges from 0 to 1 with 1 being a perfect match ,0,1,0
in table 1 the maline row 3 shows that the english name has a palatoalveolar modification 2 as freeman et al 2006 point out these insights are not easy to come by these rules are based on first author dr andrew freemans experience with reading and translating arabic language texts for more than 16 years freeman et al 2006 p 474 ,0,1,0
featurebased methods jiang and zhai 2007 kambhatla 2004 zhou et al 2005 use predefined feature sets to extract features to train classification models ,0,1,0
jiang zhai 2007 gave a systematic examination of the efficacy of unigram bigram and trigram features drawn from different representations surface text constituency parse tree and dependency parse tree ,0,1,0
jiang and zhai 2007 then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence syntactic parse tree and dependency parse tree ,0,1,0
while transfer learning was proposed more than a decade ago thrun 1996 caruana 1997 its application in natural language processing is still a relatively new territory blitzer et al 2006 daume iii 2007 jiang and zhai 2007a arnold et al 2008 dredze and crammer 2008 and its application in relation extraction is still unexplored ,0,1,0
we systematically explored the feature space for relation extraction jiang and zhai 2007b kernel methods allow a large set of features to be used without being explicitly extracted ,0,1,0
a systematic exploration of a set of such features for proteinprotein interaction extraction was recently provided by jiang and zhai 2007 who also used features derived from the collins parser ,0,1,0
yates and etzioni 2007 4 ,0,1,0
following extraction ocrf applies the resolver algorithm yates and etzioni 2007 to find relation synonyms the various ways in which a relation is expressed in text ,0,1,0
in particular we have implemented an unsupervised morphological analyzer that outperforms goldsmith s 2001 linguistica and creutz and lagus s 2005 morfessor for our english and bengali datasets and compares favorably to the bestperforming morphological parsers in morphochallenge 20053 see dasgupta and ng 2007 ,1,0,0
letter successor variety lsv models hafer and weiss 1974 gaussier 1999 bernhard 2005 bordag 2005 669 keshava and pitler 2005 hammarstrom 2006 dasgupta and ng 2007 demberg 2007 use the hypothesis that there is less certainty when predicting the next character at morpheme boundaries ,0,1,0
relative frequencies of wordforms have been used in previous work to detect incorrect affix attachments in bengali and english dasgupta and ng 2007 ,0,1,0
there has been recent work on discovering allomorphic phenomena automatically dasgupta and ng 2007 demberg 2007 ,0,1,0
unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text and unsupervised morphological segmentation has been extensively studied for a number of languages brent et al 1995 goldsmith 2001 dasgupta and ng 2007 creutz and lagus 2007 ,0,1,0
some adopt a pipeline approach schone and jurafsky 2001 dasgupta and ng 2007 demberg 2007 which works by first extracting candidate affixes and stems and then segmenting the words based on the candidates ,0,1,0
allomorphs eg deni and deny are also automatically identified in dasgupta 2007 but the general problem of recognizing highly irregular forms is examined more extensively in yarowsky and wicentowski 2000 ,0,0,1
a second example of subtle language dependence comes from dasgupta and ng 2007 who present an unsupervised morphological segmentation algorithm meant to be languageindependent ,0,1,0
however it seems unrealistic to expect a onesizefitsall approach to be achieve uniformly high performance across varied languages and in fact it doesnt though the system presented in dasgupta and ng 2007 outperforms the best systems in the 2006 pascal challenge for turkish and finnish it still does significantly worse on these languages than english fscores of 662 and 665 compared to 794 ,0,0,1
dasgupta and ng 2007 improves over creutz 2003 by suggesting a simpler approach ,1,0,0
thus some research has been focused on deriving different wordsense groupings to overcome the finegrained distinctions of wn hearst and schutze 1993 peters et al 1998 mihalcea and moldovan 2001 agirre and lopezdelacalle 2003 navigli 2006 and snow et al 2007 ,0,1,0
in this way wikipedia provides a new very large source of annotated data constantly expanded mihalcea 2007 ,0,1,0
generally wsd methods use the context of a word for its sense disambiguation and the context information can come from either annotatedunannotated text or other knowledge resources such as wordnet fellbaum 1998 semcor semcor 2008 open mind word expert chklovski and mihalcea 2002 extended wordnet moldovan and rus 2001 wikipedia mihalcea 2007 parallel corpora ng wang and chan 2003 ,0,1,0
1mihalcea 2007 shows that wikipedia can indeed be used as a sense inventory for sense disambiguation ,0,1,0
mihalcea 2007 demonstrates that manual mappings can be created for a small number of words with relative ease but for a very large number of words the e ort involved in mapping would approach presented involves no be considerable ,0,1,0
they may rely only on this information eg turney 2002 whitelaw et al 2005 riloff and wiebe 2003 or they may combine it with additional information as well eg yu and hatzivassiloglou 2003 kim and hovy 2004 bloom et al 2007 wilson et al 2005a ,0,1,0
with the indepth study of opinion mining researchers committed their efforts for more accurate results the research of sentiment summarization philip et al 2004 hu et al kdd 2004 domain transfer problem of the sentiment analysis kanayama et al 2006 tan et al 2007 blitzer et al 2007 tan et al 2008 andreevskaia et al 2008 tan et al 2009 and finegrained opinion mining hatzivassiloglou et al 2000 takamura et al 2007 bloom et al 2007 wang et al 2008 titov et al 2008 are the main branches of the research of opinion mining ,1,0,0
a number of works in product review mining hu and liu 2004 popescu et al 2005 kobayashi et al 2005 bloom et al 2007 automatically find features of the reviewed products ,0,1,0
one of the most relevant work is bollegala et al 2007 which proposed to integrate various patterns in order to measure semantic similarity between words ,0,1,0
in addition to the classical windowbased technique some studies investigated the use of lexicosyntactic patterns eg x or y to get more accurate cooccurrence statistics chilovski and pantel 2004 bollegala et al 2007 ,1,0,0
however the most interesting work is certainly proposed by bollegala et al 2007 who extract patterns in two steps ,1,0,0
bleu automatic evaluation by bleu score papineni et al 2002 ,0,1,0
automated metrics such as bleu papineni et al 2002 red akiba et al 2001 weighted ngram model wnm babych 2004 syntactic relation semantic vector model rajman and hartley 2001 have been shown to correlate closely with scoring or ranking by different human evaluation parameters ,1,0,0
it was found to produce automated scores which strongly correlate with human judgements about translation fluency papineni et al 2002 ,1,0,0
this score measures the precision of unigrams bigrams trigrams and fourgrams with respect to a reference translation with a penalty for too short sentences papineni et al 2002 ,0,1,0
as an example of it s application ngram cooccurrence is used for evaluating machine translations papineni et al 2002 ,0,1,0
the following four metrics were used speci cally in this study bleu papineni et al 2002 a weighted geometric mean of the ngram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences ,0,1,0
 our evaluation metrics are bleu papineni et al 2002 and nist which are to perform caseinsensitive matching of ngrams up to n 4 ,0,1,0
52 experimental results following langkilde 2002 and other work on generalpurpose generators bleu score papineni et al 2002 average nist simple string accuracy ssa and percentage of exactly matched sentences are adopted as evaluation metrics ,0,1,0
the translation quality is evaluated by bleu metric papineni et al 2002 as calculated by mtevalv11bpl with caseinsensitive matching of ngrams where n 4 ,0,1,0
optimization and measurement were done with the nist implementation of caseinsensitive bleu 4n4r papineni et al 20024 41 baseline we compared translation by pattern matching with a conventional exact model representation using external prefix trees zens and ney 2007 ,0,1,0
we evaluated performance by measuring wer word error rate per positionindependent word error rate bleu papineni et al 2002 and ter translation error rate snover et al 2006 using multiple references ,0,1,0
the evaluation metric is casesensitive bleu4 papineni et al 2002 ,0,1,0
the stateoftheart methods for automatic mt evaluation are using an ngram based metric represented by bleu papineni et al 2002 and its variants ,1,0,0
translation quality is automatically evaluated by the ibmbleu metric papineni et al 2002 casesensitive using length of the closest reference translation on the following publicly 1148 chen ,0,1,0
in addition to the widely used bleu papineni et al 2002 and nist doddington 2002 scores we also evaluate translation quality with the recently proposed meteor banerjee and lavie 2005 and four editdistance style metrics word error rate wer positionindependent word error rate per tillmann et al 1997 cder which allows block reordering leusch et al 2006 and translation edit rate ter snover et al 2006 ,1,0,0
to counteract this we introduce two brevity penalty measures bp inspired by bleu papineni et al 2002 which we incorporate into the loss function using a product loss 1precbp bp1 exp1max1 rc 6 bp2 exp1maxcr rc where r is the reference length and c is the candidate length ,0,1,0
in our experiments using bleu papineni et al 2002 as the metric the interpolated synthetic model achieves a relative improvement of 117 over the best rbmt system that is used to produce the synthetic bilingual corpora ,0,1,0
the translation quality is evaluated using a wellestablished automatic measure bleu score papineni et al 2002 ,1,0,0
the translation quality is evaluated by bleu metric papineni et al 2002 as calculated by mtevalv11bpl 6 with casesensitive matching of ngrams ,0,1,0
all evaluation is in terms of the bleu score on our test set papineni et al 2002 ,0,1,0
this approach gave an improvement of 27 in bleu papineni et al 2002 score on the iwslt05 japanese to english evaluation corpus improving the score from 524 to 551 ,0,1,0
there exists a variety of different metrics eg word error rate positionindependent word error rate bleu score papineni et al 2002 nist score doddington 2002 meteor banerjee and lavie 2005 gtm turian et al 2003 ,0,1,0
a popular metric for evaluating machine translation quality is the bleu score papineni et al 2002 ,1,0,0
the algorithm is slightly different from other online training algorithms tillmann and zhang 2006 liang et al 2006 in that we keep and update oracle translations which is a set of good translations reachable by a decoder according to a metric ie bleu papineni et al 2002 ,0,1,0
for each training data size we report the size of the resulting language model the fraction of 5grams from the test data that is present in the language model and the bleu score papineni et al 2002 obtained by the machine translation system ,0,1,0
results in terms of worderrorrate wer and bleu score papineni et al 2002 are reported in table 4 for those sentences that contain at least one unknown word ,0,1,0
the translation quality is evaluated by bleu metric papineni et al 2002 as calculated by mtevalv11bpl with caseinsensitive matching of ngrams where n 4 ,0,1,0
in the following experiments the nist bleu score is used as the evaluation metric papineni et al 2002 which is reported as a percentage in the following sections ,0,1,0
there are however other similarity metrics eg bleu papineni et al 2002 which could be used equally well ,0,1,0
scored with lowercased tokenized nist bleu and exact match meteor papineni et al 2002 lavie and agarwal 2007 ,0,1,0
esen 63009 59209 6014 enes 63809 60510 5216 deen 71608 69009 3613 ende 75908 73509 3212 fren 62909 59210 5916 enfr 63409 60009 5414 bined in a loglinear fashion by adjusting a weight for each of them by means of the mert och 2003 procedure optimising the bleu papineni et al 2002 score obtained on the development partition ,0,1,0
to this purpose different authors papineni et al 1998 och and ney 2002 propose the use of the socalled loglinear models where the decision rule is given by the expression y argmax y msummationdisplay m1 mhmxy 3 where hmxy is a score function representing an important feature for the translation of x into y m is the number of models or features and m are the weights of the loglinear combination ,0,1,0
we also report the result of our translation quality in terms of both bleu papineni et al 2002 and ter snover et al 2006 against four human reference translations ,0,1,0
7 experiments to show the effectiveness of crosslanguage mention propagation information in improving mention detection system performance in arabic chinese and spanish we use three smt systems with very competitive performance in terms of bleu11 papineni et al 2002 ,0,1,0
performance is measured by computing the bleu scores papineni et al 2002 of the systems translations when compared against a single reference translation per sentence ,0,1,0
61 evaluation of translation performance we use the bleu score papineni et al 2002 to evaluate our systems ,0,1,0
all conditions were optimized using bleu papineni et al 2002 and evaluated using both bleu and translation edit rate ter snover et al 2006 ,0,1,0
21 the bleu metric the metric most often used with mert is bleu papineni et al 2002 where the score of a candidate c against a reference translation r is bleu bplenclenrexp 4summationdisplay n1 1 4 logpn where pn is the ngram precision2 and bp is a brevity penalty meant to penalize short outputs to discourage improving precision at the expense of recall ,1,0,0
it is the most widely reported metric in mt research and has been shown to correlate well with human judgment papineni et al 2002 coughlin 2003 ,1,0,0
we report bleu scores papineni et al 2002 on untokenized recapitalized output ,0,1,0
for efficiency reasons we report results on sentences of length 30 words or less10 the syntaxbased method gives a bleu papineni et al 2002 score of 2504 a 046 bleu point gain over pharoah ,0,1,0
for this we aligned 170863 pairs of arabicenglish newswire sentences from ldc trained a stateoftheart syntaxbased statistical machine translation system galley et al 2006 on these sentences and alignments and measured bleu scores papineni et al 2002 on a separate set of 1298 newswire test sentences ,0,1,0
instead researchers routinely use automatic metrics like bleu papineni et al 2002 as the sole evidence of improvement to translation quality ,0,1,0
52 translation in order to test the translation performance of the grammars induced by our model and the ghkm method6 we report bleu papineni et al 2002 scores on sentences of up to twenty words in length from the mt03 nist evaluation ,0,1,0
following the evaluation methodology of wong and mooney 2007 we performed 4 runs of the standard 10fold cross validation and report the averaged performance in this section using the standard automatic evaluation metric bleu papineni et al 2002 and nist doddington 20022 ,0,1,0
the full model yields a stateoftheart bleu papineni et al 2002 score of 08506 on section 23 of the ccgbank which is to our knowledge the best score reported to date 410 using a reversible corpusengineered grammar ,1,0,0
using the bleu metric papineni et al 2002 ,0,1,0
besides the the casesensitive bleu4 papineni et al 2002 used in the two experiments we design another evaluation metrics reordering accuracy racc for forced decoding evaluation ,0,1,0
similar to bleu score we also use the similar brevity penalty bp papineni et al 2002 to penalize the short translations in computing racc ,0,1,0
demonstrating the inadequacy of such approaches alonaizan and papineni 2006 showed that even given the words in the reference translation and their alignment to the source words a decoder of this sort charged with merely rearranging them into the correct targetlanguage order could achieve a bleu score papineni et al 2002 of at best 69and that only when restricted to keep most words very close to their source positions ,0,1,0
we evaluated the translation quality using caseinsensitive bleu metric papineni et al 2002 ,0,1,0
we show that our ddtm system provides significant improvements in bleu papineni et al 2002 and ter snover et al 2006 scores over the already extremely competitive dtm2 system ,0,1,0
the model weights of the transducer are tuned based on the development set using a gridbased line search and the translation results are evaluated based on a single chinese reference6 using bleu4 papineni et al 2002 ,0,1,0
we set all weights by optimizing bleu papineni et al 2002 using minimum error rate training mert och 2003 on a separate development set of 2000 sentences indonesian or spanish and we used them in a beam search decoder koehn et al 2007 to translate 2000 test sentences indonesian or spanish into english ,0,1,0
32 evaluation criteria wellestablished objective evaluation measures like the word error rate wer positionindependent word error rate per and the bleu score papineni et al 2002 were used to assess the translation quality ,1,0,0
stateoftheart measures such as bleu papineni et al 2002 or nist doddington 2002 aim at measuring the translation quality rather on the document level1 than on the level of single sentences ,1,0,0
1 introduction over the past five years progress in machine translation and to a lesser extent progress in natural language generation tasks such as summarization has been driven by optimizing against ngrambased evaluation metrics such as bleu papineni et al 2002 ,0,1,0
while studies have shown that ratings of mt systems by bleu and similar metrics correlate well with human judgments papineni et al 2002 doddington 2002 we are not aware of any studies that have shown that corpusbased evaluation metrics of nlg systems are correlated with human judgments correlation studies have been made of individual components bangalore et al 2000 but not of systems ,1,0,0
the bleu metric papineni et al 2002 in mt has been particularly successful for example mt05 the 2005 nist mt evaluation exercise used bleu4 as the only method of evaluation ,1,0,0
properly calculated bleu scores have been shown to correlate reliably with human judgments papineni et al 2002 ,1,0,0
some nlg researchers are impressed by the success of the bleu evaluation metric papineni et al 2002 in machine translation mt which has transformed the mt field by allowing researchers to quickly and cheaply evaluate the impact of new ideas algorithms and data sets ,1,0,0
for example in machine translation evaluation approaches such as bleu papineni et al 2002 use ngram overlap comparisons with a model to judge overall goodness with higher ngrams meant to capture fluency considerations ,0,1,0
the performance of pbsmt system is measured with bleu score papineni et al 2002 ,0,1,0
success is indicated by the proportion of the original sentence regenerated as measured by any string comparison method in our case using the bleu metric papineni et al 2002 ,0,1,0
51 evaluation of translation translations are evaluated on two automatic metrics bleu papineni et al 2002 and per position independent errorrate tillmann et al 1997 ,0,1,0
they reported that their method is superior to bleu papineni et al 2002 in terms of the correlation between human assessment and automatic evaluation ,0,0,1
we report case sensitive bleu papineni et al 2002scorebleucforallexperiments ,0,1,0
3 semantic representation 31 the need for dependencies perhaps the most common representation of text for assessing content is bagofwords or bagofngrams papineni et al 2002 ,0,1,0
unfortunately this is not the case for such widely used mt evaluation metrics as bleu papineni et al 2002 and nist doddington 2002 ,0,0,1
the feature weights are learned by maximizing the bleu score papineni et al 2002 on heldout datausingminimumerrorratetrainingoch2003 as implemented by koehn ,0,1,0
5 analysis over the last few years several automatic metrics for machine translation evaluation have been introduced largely to reduce the human cost of iterative system evaluation during the development cycle lin and och 2004 melamed et al 2003 papineni et al 2002 ,0,1,0
for extrinsic evaluation of machine translation we use the bleu metric papineni et al 2002 ,0,1,0
3 previous work the idea of employing ngram cooccurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the bleu metric for machine translation papineni et al 2002 ,1,0,0
we can incorporate each model into the system in turn and rank the results on a test corpus using bleu papineni et al 2002 ,0,1,0
1 introduction over the last few years several automatic metrics for machine translation mt evaluation have been introduced largely to reduce the human cost of iterative system evaluation during the development cycle papineni et al 2002 melamed et al 2003 ,0,1,0
however recent progress in machine translation and the continuous improvement on evaluation metrics such as bleu papineni et al 2002 suggest that smt systems are already very good at choosing correct word translations ,0,1,0
although there are various manualautomatic evaluation methods for these systems eg bleu papineni et al 2002 these methods are basically incapable of dealing with an mtsystem and a wpmtsystem at the same time as they have different output forms ,0,0,1
automatic measures like bleu papineni et al 2001 or nist doddington 2002 do so by counting sequences of words in such paraphrases ,0,1,0
the translations are evaluated in terms of bleu score papineni et al 2002 ,0,1,0
the horizontal axis represents the weight for the outofdomain translation model and the vertical axis 15 16 17 18 19 20 21 22 23 24 25 00 01 02 03 04 05 06 07 08 09 10weight for outofdomain translation model bleu sco re 400 k 800 k 12 m 16 m 25 m figure 2 results of data selection and linear interpolation bleu represents the automatic metric of translation quality bleu score papineni et al 2002 in fig ,0,1,0
this restriction is necessary because the problem of optimizing manytomany alignments 5 our preliminary experiments with ngrambased overlap measures such as bleu papineni et al 2002 and rouge lin and hovy 2003 show that these metrics do not correlate with human judgments on the fusion task when tested against two reference outputs ,0,0,1
5 translation performance was measured using the automatic bleu evaluation metric papineni et al 2002 on four reference translations ,0,1,0
for instance several studies have shown that bleu correlates with human ratings on machine translation quality papineni et al 2002 doddington 2002 coughlin 2003 ,0,1,0
however they can be usefully employed during system development for example for quickly assessing modeling ideas or for comparing across different system configurations papineni et al 2002 bangalore rambow and whittaker 2000 ,0,1,0
translation accuracy is measured in terms of the bleu score papineni et al 2002 which is computed here for translations generated by using the tuple ngram model alone in the case of table 2 and by using the tuple ngram model along with the additional four feature functions described in section 32 in the case of table 3 ,0,1,0
in our smt system implementation this optimization procedure is performed by using a tool developed inhouse which is based on a simplex method press et al 2002 and the bleu score papineni et al 2002 is used as a translation quality measurement ,0,1,0
the translation quality on the transtype2 task in terms of wer per bleu score papineni et al 2002 and nist score nist 2002 is given in table 4 ,0,1,0
finally the parameters i of the loglinear model 18 are learned by minimumerrorrate training och 2003 which tries to set the parameters so as to maximize the bleu score papineni et al 2002 of a development set ,0,1,0
this could for example aid machinetranslation evaluation where it has become common to evaluate systems by comparing their output against a bank of several reference translations for the same sentences papineni et al 2002 ,0,1,0
distance msd and the maximum swap segment size msss ranging from 0 to 10 and evaluated the translations with the bleu7 metric papineni et al 2002 ,0,1,0
7for details about the bleu evaluation metric see papineni et al 2002 ,0,1,0
expansion of the equivalent sentence set can be applied to automatic evaluation of machine translation quality papineni et al 2002 akiba et al 2001 for example ,0,1,0
for the evaluation of translation quality we used the bleu metric papineni et al 2002 which measures the ngram overlap between the translated output and one or more reference translations ,0,1,0
the third column reports the bleu score papineni et al 2002 along with 95 confidence interval ,0,1,0
441 ngram cooccurrence statistics for answer extraction ngram cooccurrence statistics have been successfully used in automatic evaluation papineni et al 2002 lin and hovy 2003 and more recently as training criteria in statistical machine translation och 2003 ,1,0,0
in machine translation the rankings from the automatic bleu method papineni et al 2002 have been shown to correlate well with human evaluation and it has been widely used since and has even been adapted for summarization lin and hovy 2003 ,1,0,0
word error rate wer which penalizes the edit distance against reference translations su et al 1992 bleu the geometric mean of ngram precision for the translation results found in reference translations papineni et al 2002 translation accuracy acc subjective evaluation ranks ranging from a to d a perfect b fair c acceptable and d nonsense judged blindly by a native speaker sumita et al 1999 in contrast to wer higher bleu and acc scores indicate better translations ,0,1,0
translation qualities are measured by uncased bleu papineni et al 2002 with 4 reference translations sysids ahb ahc ahd ahe ,0,1,0
to set the weights m we performed minimum error rate training och 2003 on the development set using bleu papineni et al 2002 as the objective function ,0,1,0
2 disperp and distortion corpora 21 defining disperp the ultimate reason for choosing one scm over another will be the performance of an mt system containing it as measured by a metric like bleu papineni et al 2002 ,0,1,0
mt output is evaluated using the standard mt evaluation metric bleu papineni et al 2002 ,0,1,0
in the nal step we score our translations with 4gram bleu papineni et al 2002 ,0,1,0
the pearson correlation is calculated over these ten pairs papineni et al 2002 stent et al 2005 ,0,1,0
our scores fall within the range of previous researchers papineni et al 2002 lin and och 2004 ,0,1,0
automatic evaluation measures a variety of automatic evaluation methods have been recently proposed in the machine translation community nist 2002 melamed et al 2003 papineni et al 2002 ,0,1,0
for evaluation we used the bleu metrics which calculates the geometric mean of ngram precision for the mt outputs found in reference translations papineni et al 2002 ,0,1,0
we optimized separately for both the nist doddington 2002 and the bleu metrics papineni et al 2002 ,0,1,0
in our research 23 scores namely bleu papineni et al 2002 with maximum ngram lengths of 1 2 3 and 4 nist nist 2002 with maximum ngram lengths of 1 2 3 4 and 5 gtm turian et al 2003 with exponents of 10 20 and 30 meteor exact banerjee and lavie 2005 wer niessen et al 2000 per leusch et al 2003 and rouge lin 2004 with ngram lengths of 1 2 3 and 4 and 4 variants lcs ssu w12 were used to calculate each similarity s i therefore the value of m in eq ,0,1,0
in recent years many researchers have tried to automatically evaluate the quality of mt and improve the performance of automatic mt evaluations niessen et al 2000 akiba et al 2001 papineni et al 2002 nist 2002 leusch et al 2003 turian et al 2003 babych and hartley 2004 lin and och 2004 banerjee and lavie 2005 gimenez et al 2005 because improving the performance of automatic mt evaluation is expected to enable us to use and improve mt systems efficiently ,0,1,0
2 three new features for mt evaluation since our sourcesentence constrained ngram precision and discriminative unigram precision are both derived from the normal ngram precision it is worth describing the original ngram precision metric bleu papineni et al 2002 ,0,1,0
the most commonly used metric bleu correlates well over large test sets with human judgments papineni et al 2002 but does not perform as well on sentencelevel evaluation blatz et al 2003 ,0,0,1
bleu papineni et al 2002 is a precision metric that assesses the quality of a translation in terms of the proportion of its word ngrams n 4 has become standard that it shares with several reference translations ,0,1,0
reference if our player 2 3 7 or 5 has the ball and the ball is close to our goal line pharaoh if player 3 has the ball is in 2 5 the ball is in the area near our goal line wasp1 if players 2 3 7 and 5 has the ball and the ball is near our goal line figure 4 sample partial system output in the robocup domain robocup geoquery bleu nist bleu nist pharaoh 03247 50263 02070 31478 wasp1 04357 54486 04582 59900 pharaoh 04336 59185 05354 63637 wasp1 06022 68976 05370 64808 table 1 results of automatic evaluation bold type indicates the best performing system or systems for a given domainmetric pair p 005 51 automatic evaluation weperformed4runsof10foldcrossvalidationand measured the performance of the learned generators using the bleu score papineni et al 2002 and the nist score doddington 2002 ,0,1,0
the nist bleu4 is a variant of bleu papineni et al 2002 and is computed as a49a51a50 a2a16a52a53a6 a0a9a8a10a0a12a11a54a13a55a15 a26a57a56a33a58a60a59 a43 a61a63a62 a64 a65a67a66a69a68 a28a71a70a46a72a74a73 a65 a6 a0a9a8a10a0a3a11a54a13a19a75a77a76 a6 a0a9a8a10a0a3a11a54a13 2 where a73 a65 a6 a0a78a8a10a0a3a11a54a13 is the precision of a79 grams in the hypothesis a0 given the reference a0 a11 and a76 a6 a0a78a8a10a0a3a11a54a13a81a80 a43 is a brevity penalty ,0,1,0
therefore having correct transliterations would give only small improvements in terms of bleu papineni et al 2002 and nist scores ,0,1,0
to set the weights m we carried out minimum error rate training och 2003 using bleu papineni et al 2002 as the objective function ,0,1,0
the baseline score using all phrase pairs was 5911 bleu papineni et al 2002 with a 95 confidence interval of 5713 6109 ,0,1,0
bleu score in order to measure the extent to which whole chunks of text from the prompt are reproduced in the student essays we used the bleu score known from studies of machine translation papineni et al 2002 ,0,1,0
we measure translation performance by the bleu score papineni et al 2002 with one reference for each hypothesis ,0,1,0
here we compare two similarity measures the familiar bleu score papineni et al 2002 and a score based on string kernels ,0,1,0
feature weights vector are trained discriminatively in concert with the language model weight to maximize the bleu papineni et al 2002 automatic evaluation metric via minimum error rate training mert och 2003 ,0,1,0
our evaluation metric is bleu papineni et al 2002 with caseinsensitive matching from unigram to fourgram ,0,1,0
the feature weights were tuned on a heldout development set so as to maximize an equally weighted linear combination of bleu and 1ter papineni et al 2002 snover et al 2006 using the minimum error training algorithm on a packed forest representation of the decoders hypothesis space macherey et al 2008 ,0,1,0
the final smt system performance is evaluated on a uncased test set of 3071 sentences using the bleu papineni et al 2002 nist doddington 2002 and meteor banerjee and lavie 2005 scores ,0,1,0
results are reported using lowercase bleu papineni et al 2002 ,0,1,0
in this case one is often required to find the translations in the hypergraph that are most similar to the desired translations with similarity computed via some automatic metric such as bleu papineni et al 2002 ,0,1,0
due to limited variations in the nbest list the nature of ranking and more importantly the nondifferentiable objective functions used for mt such as bleu papineni et al 2002 one often found only local optimal solutions to with no clue to walk out of the riddles ,0,0,1
including about 14 million sentence pairs extracted from the gigaword data we obtain a statistically significant improvement from 423 to 456 in bleu papineni et al 2002 ,0,1,0
day 1 day 2 no asr adaptation 2939 2741 unsupervised asr adaptation 3155 2766 supervised asr adaptation 3219 2765 table 2 impact of asr adaptation to smt table 2 shows the impact of asr adaptation on the performance of the translation system in bleu papineni et al 2002 ,0,1,0
bleu bleu score which computes the ratio of ngram for the translation results found in reference translations papineni et al 2002 ,0,1,0
performance is also measured by the bleu score papineni et al 2002 which measures similarity to the reference translation taken from the english side of the parallel corpus ,0,1,0
another current topic of machine translation is automatic evaluation of mt quality papineni et al 2002 yasuda et al 2001 akiba et al 2001 ,0,1,0
3 automatic evaluation of mt quality we utilize bleu papineni et al 2002 for the automatic evaluation of mt quality in this paper ,0,1,0
regressive flm rflm hflmej w1 flmejb regressive alm ralm halmej w1 almejb notice that h here is supposed to relate flm or alm to some independent evaluation metric such as bleu papineni et al 2002 not the log likelihood of a translation ,0,1,0
for comparison purposes we also computed the value of r 2 for fluency using the bleu score formula given in papineni et al 2002 for the 7 systems using the same one reference and we obtained a similar value 7852 computing the value of r 2 for fluency using the bleu scores computed with all 4 references available yielded a lower value for r 2 6496 although bleu scores obtained with multiple references are usually considered more reliable ,0,1,0
for comparison purposes we also computed the value of r 2 for adequacy using the bleu score formula given in papineni et al 2002 for the 7 systems using the same one reference and we obtain a similar value 8391 computing the value of r 2 for adequacy using the bleu scores computed with all 4 references available also yielded a lower value for r 2 6221 ,0,1,0
a similar observation was made in papineni et al 2002 313 ,0,1,0
some of them use human reference translations eg the bleu method papineni et al 2002 which is based on comparison of ngram models in mt output and in a set of human reference translations ,0,1,0
we evaluate accuracy performance using two automatic metrics an identity metric id which measures the percent of sentences recreated exactly and bleu papineni et al 2002 which gives the geometric average of the number of uni bi tri and fourgrams recreated exactly ,0,1,0
we calculated the translation quality using bleus modified ngram precision metric papineni et al 2002 for ngrams of up to length four ,0,1,0
our evaluation metric was bleu papineni et al 2002 as calculated by the nist script version 11a with its default settings which is to perform caseinsensitive matching of ngrams up to n 4 and to use the shortest as opposed to nearest reference sentence for the brevity penalty ,0,1,0
using our wsd model to constrain the translation candidates given to the decoder hurts translation quality as measured by the automated bleu metric papineni et al 2002 ,0,1,0
we use bleu scores papineni et al 2002 to measure translation accuracy ,0,1,0
our mt system was evaluated using the ngram based bleu papineni et al 2002 and nist machine translation evaluation software ,0,1,0
experimental results are reported in table 2 here cased bleu results are reported on mt03 arabicenglish test set papineni et al 2002 ,0,1,0
examples of monolingual parallel corpora that have been used are multiple translations of classical french novels into english and data created for machine translation evaluation methods such as bleu papineni et al 2002 which use multiple reference translations ,0,1,0
meteor was chosen since unlike the more commonly used bleu metric papineni et al 2002 it provides reasonably reliable scores for individual sentences ,0,0,1
other metrics assess the impact of alignments externally eg different alignments are tested by comparing the corresponding mt outputs using automated evaluation metrics eg bleu papineni et al 2002 or meteor banerjee and lavie 2005 ,0,1,0
mt output was evaluated using the standard evaluation metric bleu papineni et al 20022 the parameters of the mt system were optimized for bleu metric on nist mteval2002 test sets using minimum error rate training och 2003 and the systems were tested on nist mteval2003 test sets for both languages ,0,1,0
translation performance is measured using the automatic bleu papineni et al 2002 metric on one reference translation ,0,1,0
we evaluated the translation quality using the bleu metric papineni et al 2002 as calculated by mtevalv11bpl with its default setting except that we used casesensitive matching of ngrams ,0,1,0
we report results using the wellknown automatic evaluation metrics bleu papineni et al 2002 ,1,0,0
we show translation results in terms of the automatic bleu evaluation metric papineni et al 2002 on the mt03 arabicenglish darpa evaluation test set consisting of a212a89a212a89a87 sentences with a98a89a212a161a213a89a214a89a215 arabic words with a95 reference translations ,0,1,0
first we compared our system output to human reference translations using bleu papineni et al 2002 a widelyaccepted objective metric for evaluation of machine translations ,1,0,0
42 stringbased evaluation we evaluate the output of our generation system against the raw strings of section 23 using the simple string accuracy and bleu papineni et al 2002 evaluation metrics ,0,1,0
for evaluation we use ibms bleu score papineni et al 2002 to measure the performance of the sms normalization ,0,1,0
we use ibms bleu score papineni et al 2002 to measure the performance of sms text normalization ,0,1,0
2 recap of bleu rougew and meteor the most commonly used automatic evaluation metrics bleu papineni et al 2002 and nist doddington 2002 are based on the assumption that the closer a machine translation is to a promt1 life is like one nice chocolate in box ref life is just like a box of tasty chocolate ref life is just like a box of tasty chocolate mt2 life is of one nice chocolate in box figure 1 alignment example for rougew fessional human translation the better it is papineni et al 2002 ,0,1,0
the ongoing evaluationliteratureisperhapsmostobviousinthe machine translation communitys efforts to better bleu papineni et al 2002 ,0,0,1
one of the most successful metrics for judging machinegenerated text is bleu papineni et al 2002 ,1,0,0
4 experiment 41 evaluation method we evaluated each sentence compression method using word fmeasures bigram fmeasures and bleu scores papineni et al 2002 ,0,1,0
for word alignment accuracy fmeasure is reported ie the harmonic mean of precision and recall against a goldstandard reference set for translation quality bleu papineni et al 2002 and its variation of nist scores are reported ,0,1,0
we measure translation performance by the bleu score papineni et al 2002 and translation error rate ter snover et al 2006 with one reference for each hypothesis ,0,1,0
evaluation metrics we evaluated the generated translations using three different evaluation metrics bleu score papineni et al 2002 mwer multireference word error rate and mper multireference positionindependent word error rate nieen et al 2000 ,0,1,0
following chiang 2005 we used the version 11a nist bleu script with its default settings to calculate the bleu scores papineni et al 2002 based on caseinsensitive ngram matching where n is up to 4 ,0,1,0
the wellknown bleu papineni et al 2002 is based on the number of common ngrams between the translation hypothesis and human reference translations of the same sentence ,1,0,0
referencebased metrics such as bleu papineni et al 2002 have rephrased this subjective task as a somewhat more objective question how closely does the translation resemble sentences that are known to be good translations for the same source ,0,1,0
the quality of the translation output is evaluated using bleu papineni et al 2002 ,0,1,0
2 evaluation metrics currently the most widely used automatic mt evaluation metric is the nist bleu4 papineni et al 2002 ,1,0,0
bleu papineni et al 2002 is a canonical example in matching ngrams in a candidate translation text with those in a reference text the metric measures faithfulness by counting the matches and fluency by implicitly using the reference ngrams as a language model ,0,1,0
our evaluation metric is bleu4 papineni et al 2002 as calculated by the script mtevalv11bpl with its default setting except that we used casesensitive matching of ngrams ,0,1,0
casesensitive bleu4 papineni et al 2002 is used as the evaluation metric ,0,1,0
the parameters j were trained using minimum error rate training och 2003 to maximise the bleu score papineni et al 2002 on a 150 sentence development set ,0,1,0
using bleu papineni et al 2002 as a metric our method achieves an absolute improvement of 006 2213 relative as compared with the standard model trained with 5000 l f l e sentence pairs for frenchspanish translation ,0,1,0
since the introduction of bleu papineni et al 2002 the basic ngram precision idea has been augmented in a number of ways ,0,1,0
33 bleu score the bleu score papineni et al 2002 measures the agreement between a hypothesisei1 generated by the mt system and a reference translation ei1 ,0,1,0
it also contains tools for tuning these models using minimum error rate training och 2003 and evaluating the resulting translations using the bleu score papineni et al 2002 ,0,1,0
21 bleu bleu papineni et al 2002 is essentially a precisionbased metric and is currently the standard metric for automatic evaluation of mt performance ,0,1,0
42 automatic evaluation we first present our soft cohesion constraints effect on bleu score papineni et al 2002 for both our devtest and test sets ,0,1,0
we measure translation performance by the bleu papineni et al 2002 and meteor banerjee and lavie 2005 scores with multiple translation references ,0,1,0
other possibilities for the weighting include assigning constant one or the exponential of the final score etc one of the advantages of the proposed phrase training algorithm is that it is a parameterized procedure that can be optimized jointly with the trans82 lation engine to minimize the final translation errors measured by automatic metrics such as bleu papineni et al 2002 ,0,1,0
in addition to precision and recall we also evaluate the bleu score papineni et al 2002 changes before and after applying our measure word generation method to the smt output ,0,1,0
moreover the overall bleu papineni et al 2002 and meteor lavie and agarwal 2007 scores as well as numbers of exact string matches as measured against to the original sentences in the ccgbank are higher for the hypertaggerseeded realizer than for the preexisting realizer ,0,1,0
the evaluation metric is casesensitive bleu4 papineni et al 2002 ,0,1,0
for example in machine translation bleu score papineni et al 2002 is developed to assess the quality of machine translated sentences ,0,1,0
instead we report bleu scores papineni et al 2002 of the machine translation system using different combinations of wordand classbased models for translation tasks from english to arabic and arabic to english ,0,1,0
unfortunately as was shown by fraser and marcu 2007 aer can have weak correlation with translation performance as measured by bleu score papineni et al 2002 when the alignments are used to train a phrasebased translation system ,0,1,0
we use the standard nist mteval data sets for the years 2003 2004 and 2005 henceforth mt03 mt04 and mt05 respectively6 we report results in terms of caseinsensitive 4gram bleu papineni et al 2002 scores ,0,1,0
12 evaluation in this paper we report results using the bleu metric papineni et al 2002 however as the evaluation criterion in gale is hter snover et al 2006 we also report in ter snover et al 2005 ,0,1,0
our evaluation metric is bleu papineni et al 2002 ,0,1,0
in this paper we modify the method in albrecht and hwa 2007 to only prepare human reference translations for the training examples and then evaluate the translations produced by the subject systems against the references using bleu score papineni et al 2002 ,0,1,0
our evaluation metrics is casesensitive bleu4 papineni et al 2002 ,0,1,0
afterwards we select and remove a subset of highly informative sentences from u and add those sentences together with their humanprovided translations to l this process is continued iteratively until a certain level of translation quality is met we use the bleu score wer and per papineni et al 2002 ,0,1,0
since human evaluation is costly and difficult to do reliably a major focus of research has been on automatic measures of mt quality pioneered by bleu papineni et al 2002 and nist doddington 2002 ,1,0,0
1 introduction in statistical machine translation output translations are evaluated by their similarity to human reference translations where similarity is most often measured by bleu papineni et al 2002 ,0,1,0
we evaluated the translation quality using caseinsensitive bleu metric papineni et al 2002 ,0,1,0
for example kauchak and barzilay 2006 paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like bleu papineni et al 2002 ,0,1,0
we evaluate the string chosen by the loglinear model against the original treebank string in terms of exact match and bleu score papineni et al 821 syntactic feature type definites definite descriptions simple def simple definite descriptions poss def simple definite descriptions with a possessive determiner pronoun or possibly genitive name def attr adj definite descriptions with adjectival modifier def genarg definite descriptions with a genitive argument def ppadj definite descriptions with a pp adjunct def relarg definite descriptions including a relative clause def app definite descriptions including a title or job description as well as a proper name eg an apposition names proper combinations of positiontitle and proper name without article bare proper bare proper names demonstrative descriptions simple demon simple demonstrative descriptions mod demon adjectivally modified demonstrative descriptions pronouns pers pron personal pronouns expl pron expletive pronoun refl pron reflexive pronoun demon pron demonstrative pronouns not determiners generic pron generic pronoun man one da pron dapronouns darauf daruber dazu loc adv locationreferring pronouns temp advyear dates and times indefinites simple indef simple indefinites neg indef negative indefinites indef attr indefinites with adjectival modifiers indef contrast indefinites with contrastive modifiers einige some andere other weitere further indef ppadj indefinites with pp adjuncts indef rel indefinites with relative clause adjunct indef gen indefinites with genitive adjuncts indef num measurenumber phrases indef quant quantified indefinites table 5 an inventory of interesting syntactic characteristics in is phrases label 1 features label 2 features ba total dgivenpronoun indefrel 0 19 pers pron 39 indef attr 23 da pron 25 simple indef 17 demon pron 19 generic pron 11 dgivenpronoun dgivencataphor 01 11 pers pron 39 simple def 13 da pron 25 da pron 10 demon pron 19 generic pron 11 dgivenreflexive new 011 31 refl pron 54 simple indef 113 indef attr 53 indef num 32 indef ppadj 26 indef gen 25 table 6 is asymmetric pairs augmented with syntactic characteristics 822 2002 ,0,1,0
for automatic evaluation we employed bleu papineni et al 2002 by following unno et al 2006 ,0,1,0
methods have been proposed for automatic evaluation in mt eg bleu papineni et al 2002 ,0,1,0
43 experiments results our evaluation metric is bleu papineni et al 2002 which are to perform caseinsensitive matching of ngrams up to n 4 ,0,1,0
it could be shown that such methods of which bleu papineni et al 2002 is the most common can deliver evaluation results that show a high agreement with human judgments papineni et al 2002 coughlin 2003 koehn monz 2006 ,1,0,0
the measures are word overlap length difference in words bleu papineni et al 2002 dependency relation overlap ie r1 and r2 but not fr1r2 and dependency tree edit distance ,0,1,0
while recent proposals for evaluation of mt systems have involved multiparallel corpora thompson and brew 1996 papineni et al 2002 statistical mt algorithms typically only use oneparallel data ,0,1,0
52 bleu automatic evaluation bleu papineni et al 2002 is a system for automatic evaluation of machine translation ,0,1,0
bleu papineni et al 2002b is one of the methods for automatic evaluation of translation quality ,0,1,0
high correlation is reported between the bleu score and human evaluations for translations from arabic chinese french and spanish to english papineni et al 2002a ,1,0,0
2 background overview of bleu this section briefly describes the original bleu papineni et al 2002b1 which was designed for english translation evaluation so english sentences are used as examples in this section ,0,1,0
empirically the bleu score has a high correlation with human evaluation when n 4 for english translation evaluations papineni et al 2002b ,1,0,0
bleu score bleu is an automatic metric designed by ibm which uses several references papineni et al 2002 ,0,1,0
to evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans various metrics using ngram precision and word accuracy have been proposed word string precision hori and furui 2000b for summarization through word extraction rouge lin and hovy 2003 for abstracts and bleu papineni et al 2002 for machine translation ,0,1,0
work in this area includes that of lin and hovy 2003 and pastra and saggion 2003 both of whom inspect the use of bleulike metrics papineni et al 2002 in summarization ,0,1,0
the core technology of the proposed method ie the automatic evaluation of translations was developed in research aiming at the efficient development of machine translation mt technology su et al 1992 papineni et al 2002 nist 2002 ,0,1,0
the unit of utterance corresponds to the unit of segment in the original bleu and nist studies papineni et al 2002 nist 2002 ,0,1,0
of words person names 803 1749 organization names 312 867 location names 345 614 the bleu score papineni et al 2002 with a single reference translation was deployed for evaluation ,0,1,0
42 translation results the evaluation metrics used in our experiments are wer word error rate per positionindependent word error rate and bleu bilingual evaluation understudy papineni et al 2002 ,0,1,0
translation performance was measured using the bleu score papineni et al 2002 which measures ngram overlap with a reference translation ,0,1,0
this algorithm adjusts the loglinear weights so that bleu papineni et al 2002 is maximized over a given development set ,0,1,0
32 results and discussion the bleu scores papineni et al 2002 for 10 direct translations and 4 sets of heuristic selections 4admittedly in typical instances of such chains english would appear earlier ,0,1,0
52 evaluation criteria for the automatic evaluation we used the criteria from the iwslt evaluation campaign akiba et al 2004 namely word error rate wer positionindependent word error rate per and the bleu and nist scores papineni et al 2002 doddington 2002 ,0,1,0
we provide results using a range of automatic evaluation metrics bleu papineni et al 2002 precision and recall turian et al 2003 and wordand sentence error rates ,0,1,0
in order to create the necessary smt language and translation models they used giza och ney 20032 the cmucambridge statistical toolkit3 the isi rewrite decoder4 translation was performed from englishfrench and frenchenglish and the resulting translations were evaluated using a range of automatic metrics bleu papineni et al 2002 precision and recall 2httpwwwisieduochgizahtml 3httpmiengcamacukprc14toolkithtml 4httpwwwisiedulicensedswrewritedecoder 185 turian et al 2003 and wordand sentence error rates ,0,1,0
53 evaluation metric this paper focuses on the bleu metric as presented in papineni et al 2002 ,0,1,0
bleu and nist have been shown to correlate closely with human judgments in ranking mt systems with different qualities papineni et al 2002 doddington 2002 ,1,0,0
the most commonly used automatic evaluation metrics bleu papineni et al 2002 and nist doddington 2002 are based on the assumption that the closer a machine translation is to a professional human translation the better it is papineni et al 2002 ,0,1,0
this idea of employing ngram cooccurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the bleu metric for machine translation papineni et al 2002 and the rouge lin and hovy 2003 metric for summarization ,0,1,0
1 introduction automatic metrics for machine translation mt evaluation have been receiving significant attention in the past two years since ibms bleu metric was proposed and made available papineni et al 2002 ,1,0,0
2 the meteor metric 21 weaknesses in bleu addressed in meteor the main principle behind ibms bleu metric papineni et al 2002 is the measurement of the 66 overlap in unigrams single words and higher order ngrams of words between a translation being evaluated and a set of one or more reference translations ,0,1,0
text similarity has been also used for relevance feedback and text classification rocchio 1971 word sense disambiguation lesk 1986 and more recently for extractive summarization salton et al 1997b and methods for automatic evaluation of machine translation papineni et al 2002 or text summarization lin and hovy 2003 ,0,1,0
they are a bit controversial in a proper machine translation where the popular bleu score papineni et al 2002 although widely accepted as a measure of translation accuracy seems to favor stochastic approaches based on 91 an ngram model over other mt methods see the results in nist 2001 ,0,0,1
2 related work there is a number of publications dealing with various automatic evaluation measures for machine translation output some of them proposing new measures some proposing improvements and extensions of the existing ones doddington 2002 papineni et al 2002 babych and hartley 2004 matusov et al 2005 ,0,1,0
52 evaluation metrics the commonly used criteria to evaluate the translation results in the machine translation community are wer word error rate per positionindependent word error rate bleu papineni et al 2002 and nist doddington 2002 ,0,1,0
two error rates the sentence error rate ser and the word error rate wer that we seek to minimize and bleu papineni et al 2002 that we seek to maximize ,0,1,0
53 translation results for the translation experiments on the btec task we report the two accuracy measures bleu papineni et al 2002 and nist doddington 2002 as well as the two error rates word error rate wer and positionindependent word error rate per ,0,1,0
papineni et al 2002 ,0,1,0
the release has implementations for bleu papineni et al 2002 wer and per error criteria and it has decoding interfaces for phramer and pharaoh ,0,1,0
although phramer provides decoding functionality equivalent to pharaohs we preferred to use pharaoh for this task because it is much faster than phramer between 2 and 15 times faster depending on the configuration and preliminary tests showed that there is no noticeable difference between the output of these two in terms of bleu papineni et al 2002 score ,0,1,0
what therefore has to be explored are various similarity metrics defining similarity in a concrete way and evaluate the results against human annotations see papineni et al 2002 ,0,1,0
62 translation results for the translation experiments we report the two accuracy measures bleu papineni et al 2002 and nist doddington 2002 as well as the two error rates word error rate wer and positionindependent word error rate per ,0,1,0
results on the provided 2000sentence development set are reported using the bleu metric papineni et al 2002 ,0,1,0
22 weight optimization a common criterion to optimize the coefficients of the loglinear combination of feature functions is to maximize the bleu score papineni et al 2002 on a development set och and ney 2002 ,0,1,0
on the other hand both bleu papineni et al 2002 and nist doddington 2002 scores are higher for the baseline system mtevalv11bpl ,0,1,0
even the creators of bleu point out that it may not correlate particularly well with human judgment at the sentence level papineni et al 2002 ,0,0,1
6 experiments we evaluated the translation quality of the system using the bleu metric papineni et al 2002 ,0,1,0
bleu papineni et al 2002 was devised to provide automatic evaluation of mt output ,0,1,0
the most widely used are word error rate wer position independent word error rate per the bleu score papineni et al 2002 and the nist score doddington 2002 ,1,0,0
the bleu metric papineni et al 2002 and the closely related nist metric doddington 2002 along with wer and per 48 have been widely used by many machine translation researchers ,1,0,0
221 bleu evaluation the bleu score papineni et al 2002 was defined to measure overlap between a hypothesized translation and a set of human references ,0,1,0
1 introduction in recent years statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation papineni et al 2002 and errorbased optimization och 2003 ,1,0,0
84 52 machine translation on europarl corpus we further tested our wdhmm on a phrasebased machine translation system to see whether our improvement on word alignment can also improve mt accuracy measured by bleu score papineni et al 2002 ,0,1,0
the most widely known are the word error rate wer the position independent word error rate per the nist score doddington 2002 and especially in recent years the bleu score papineni et al 2002 and the translation error rate ter snover et al 2005 ,1,0,0
even the 3 a demo of the parser can be found at httplfgdemocomputingdcuielfgparserhtml creators of bleu point out that it may not correlate particularly well with human judgment at the sentence level papineni et al 2002 ,0,0,1
since this tradeoff is also affected by the settings of various pruning parameters we compared decoding time and translation quality as measured by bleu score papineni et al 2002 for the two models on our first test set over a broad range of settings for the decoder pruning parameters ,0,1,0
och showed thatsystemperformanceisbestwhenparametersare optimizedusingthesameobjectivefunctionthatwill be used for evaluation bleu papineni et al 2002 remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used eg banerjee and lavie 2005 snover et al 2006 ,0,1,0
translation scores are reported using caseinsensitive bleu papineni et al 2002 with a single reference translation ,0,1,0
the most commonly used mt evaluation metric in recent years has been ibms bleu metric papineni et al 2002 ,0,1,0
to further emphasize the importance of morphology in mt to czech we compare the standard bleu papineni et al 2002 of a baseline phrasebased translation with bleu which disregards word forms lemmatized mt output is compared to lemmatized reference translation ,0,1,0
we further assume that the degree of difficulty of a phrase is directly correlated with the quality of the translation produced by the mt system which can be approximated using an automatic evaluation metric such as bleu papineni et al 2002 ,0,1,0
caseinsensitive bleu4 papineni et al 2002 is used as the evaluation metric ,0,1,0
bleu for all translation tasks we report caseinsensitive nist bleu scores papineni et al 2002 using 4 references per sentence ,0,1,0
5httpopennlpsourceforgenet we use the standard fourreference nist mteval data sets for the years 2003 2004 and 2005 henceforth mt03 mt04 and mt05 respectively for testing and the 2002 data set for tuning6 bleu4 papineni et al 2002 meteor banerjee and lavie 2005 and multiplereference word error rate scores are reported ,0,1,0
the automatic metrics that were evaluated in this years shared task were the following bleu papineni et al 2002bleu remains the de facto standard in machine translation evaluation ,0,1,0
3 extending bleu and ter with flexible matching many widely used metrics like bleu papineni et al 2002 and ter snover et al 2006 are based on measuring string level similarity between the reference translation and translation hypothesis just like meteor most of them however depend on finding exact matches between the words in two strings ,1,0,0
the most commonly used mt evaluation metric in recent years has been ibms bleu metric papineni et al 2002 ,0,1,0
deen ende baseline 2695 2016 factored baseline 2743 2027 submitted system 2763 2046 table 1 bleu scores for europarl test2007 deen ende baseline 1954 1431 factored baseline 2016 1437 submitted system 2061 1477 table 2 bleu scores for news commentary nctest2007 5 results casesensitive bleu scores4 papineni et al 2002 for the europarl devtest set test2007 are shown in table 1 ,0,1,0
we used these weights in a beam search decoder to produce translations for the test sentences which we compared to the wmt07 gold standard using bleu papineni et al 2002 ,0,1,0
table 2 shows results in lowercase bleu papineni et al 2002 for both the baseline b and the improved baseline systems b5 on development and held151 out evaluation sets ,0,1,0
the results evaluated by bleu score papineni et al 2002 is shown in table 2 ,0,1,0
3 evaluation we trained our model parameters on a subset of the provided dev2006 development set optimizing for caseinsensitive ibmstyle bleu papineni et al 2002 with several iterations of minimum error rate training on nbest lists ,0,1,0
we report caseinsensitive scores for version 06 of meteor lavie and agarwal 2007 with all modules enabled version 104 of ibmstyle bleu papineni et al 2002 and version 5 of ter snover et al 2006 ,0,1,0
table 1 shows the evaluation of all the systems in terms of bleu score papineni et al 2002 with the best score highlighted ,0,1,0
the translation quality is measured by three mt evaluation metrics ter snover et al 2006 bleu papineni et al 2002 and meteor lavie and agarwal 2007 ,0,1,0
4 5 experiments 51 evaluation measures we evaluated the proposed method using four evaluation measures bleu papineni et al 2002 nist doddington 2002 werword error rate and perposition independent word error rate ,0,1,0
as shown in table 1 the java decoder without explicit parallelization is 22 times faster than the python decoder while achieving slightly better translation quality as measured by bleu4 papineni et al 2002 ,0,1,0
the translation output is measured using bleu papineni et al 2002 ,0,1,0
to compare the performance of system we recorded the total training time and the bleu score which is a standard automatic measurement of the translation qualitypapineni et al 2002 ,0,1,0
a summary of the differences between our proposed approach and that of papineni et al 2002 would include the reliance of bleu on the diversity of multiple reference translations in order to capture some of the acceptable alternatives in both word choice and word ordering that we have shown above ,0,1,0
following langkilde 2002 and other work on generalpurpose generators we adopt bleu score papineni et al 2002 average simple string accuracy ssa and percentage of exactly matched sentences for accuracy evaluation6 for coverage evaluation we measure the percentage of input fstructures that generate a sentence ,0,1,0
such metrics have been introduced in other fields including paradise walker et al 1997 for spoken dialogue systems bleu papineni et al 2002 for machine translation1 and rouge lin 2004 for summarisation ,0,1,0
to optimize the parameters of the decoder we performed minimum error rate training on iwslt04 optimizing for the ibmbleu metric papineni et al 2002 ,0,1,0
in this years shared task we evaluated a number of different automatic metrics bleu papineni et al 2002bleu remains the de facto standard in machine translation evaluation ,0,1,0
2 syntacticoriented evaluation metrics we investigated the following metrics oriented on the syntactic structure of a translation output posbleu the standard bleu score papineni et al 2002 calculated on pos tags instead of words posp pos ngram precision percentage of pos ngrams in the hypothesis which have a counterpart in the reference posr recall measure based on pos ngrams percentage of pos ngrams in the reference which are also present in the hypothesis posf pos ngram based fmeasure takes into account all pos ngrams which have a counter29 part both in the reference and in the hypothesis ,0,1,0
after a brief period following the introduction of generally accepted and widely used metrics bleu papineni et al 2002 and nist doddington 2002 when it seemed that this persistent problem has finally been solved the researchers active in the field of machine translation mt started to express their worries that although these metrics are simple fast and able to provide consistent results for a particular system during its development they are not sufficiently reliable for the comparison of different systems or different language pairs ,1,0,0
bleu papineni et al 2002 nist doddington 2002 ,0,1,0
for the wmt 2009 workshop we selected a linear combination of bleu papineni et al 2002 and ter snover et al 2006 as optimization criterion argmax2bleuter based on previous experience mauser et al 2008 ,0,1,0
of these only feature weights can be trained for which we used minimum error rate training with version 104 of ibmstyle bleu papineni et al 2002 in caseinsensitive mode ,0,1,0
we scored systems and our own output using caseinsensitive ibmstyle bleu 104 papineni et al 2002 meteor 06 lavie and agarwal 2007 with all modules and ter 5 snover et al 2006 ,0,1,0
the system combination weights one for each system lm weight and word and null insertion penalties were tuned to maximize the bleu papineni et al 2002 score on the tuning set newssyscomb2009 ,0,1,0
we set all feature weights by optimizing bleu papineni et al 2002 directly using minimum error rate training mert och 2003 on the tuning part of the development set devtest2009a ,0,1,0
instead of using a single system output as the skeleton we employ a minimum bayesrisk decoder to select the best single system output from the merged nbest list by minimizing the bleu papineni et al 2002 loss ,0,1,0
in this paper we report caseinsensitive bleu scores papineni et al 2002 unless otherwise stated calculated with the nist tool and caseinsensitive meteorranking scores without wordnet agarwal and lavie 2008 ,0,1,0
22 automatic evaluation metric since the official evaluation criterion for wmt09 is human sentence ranking we chose to minimize a linear combination of two common evaluation metrics bleu and ter papineni et al 2002 snover et al 2006 during system development and tuning terbleu 2 although we are not aware of any work demonstrating that this combination of metrics correlates better than either individually in sentence ranking yaser alonaizan personal communication reports that it correlates well with the human evaluation metric hter ,0,1,0
the loglinear model feature weights were learned using minimum error rate training mert och 2003 with bleu score papineni et al 2002 as the objective function ,0,1,0
as expected as we double the size of the data the bleu score papineni et al 2002 increases ,0,1,0
5httpwwwstatmtorgwmt08 185 the bleu score papineni et al 2002 and tested on test2008 ,0,1,0
1 introduction most empirical work in translation analyzes models and algorithms using bleu papineni et al 2002 and related metrics ,0,1,0
1 introduction since the introduction of the bleu metric papineni et al 2002 statistical mt systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics ,0,1,0
we compare terp with bleu papineni et al 2002 meteor banerjee and lavie 2005 and ter snover et al 2006 ,0,1,0
translation quality is reported using caseinsensitive bleu papineni et al 2002 ,0,1,0
to tune the decoder parameters we conducted minimum error rate training och 2003 with respect to the word bleu score papineni et al 2002 using 20k development sentence pairs ,0,1,0
the highest bleu score papineni et al 2002 was chosen as the optimization criterion ,0,1,0
52 impact on translation quality as reported in table 3 small increases in meteor banerjee and lavie 2005 bleu papineni et al 2002 and nist scores doddington 2002 suggest that smt output matches the references better after postprocessing or decoding with the suggested lemma translations ,0,1,0
4 features features used in our experiments are inspired by previous work on corpusbased approaches for discourse analysis marcu and echihabi 2002 lapata 2003 elsner et al 2007 ,0,1,0
antonyms often indicate the discourse relation of contrast marcu and echihabi 2002 ,0,1,0
as marcu and echihabi 2002 point out wordnet does not encode antonymy across partofspeech for example legallyembargo ,0,1,0
marcu and echihabi 2002 demonstrated that word pairs extracted from the respective text spans are a good signal of the discourse relation between arguments ,0,1,0
note that row 3 of table 3 corresponds to marcu and echihabi 2002s system which applies only word pair features ,0,1,0
2 related work one of the first works that use statistical methods to detect implicit discourse relations is that of marcu and echihabi 2002 ,0,1,0
syntactic criteria are relevant but clearly not decisive as can be observed in marcu and echihabi 2002 ,0,1,0
in showing how dltag and an interpretative process on its derivations operate we must of necessity gloss over how inference triggered by adjacency or associated with a structural connective provides the intended relation between adjacent discourse 578 computational linguistics volume 29 number 4 units it may be a matter simply of statistical inference as in marcu and echihabi 2002 or of more complex inference as in hobbs et al ,0,1,0
a similar approach has been advocated for the interpretation of discourse relations by marcu and echihabi 2002 ,0,1,0
apart from the fact that we present an alternative model our work differs from marcu and echihabi 2002 in two important ways ,0,1,0
marcu and echihabi 2002 proposed a method to identify discourse relations between text segments using nave bayes classifiers trained on a huge corpus ,0,1,0
when we consider the frequency of discourse relations ie 43 for elaboration 32 for contrast etc the weighted accuracy was 53 using only lexical information which is comparable to the similar experiment by marcu and echihabi 2002 of 497 ,0,1,0
the effectiveness of these features for recognition of discourse relations has been previously shown by marcu and echihabi 2002 ,0,1,0
since this relation can often be determined automatically for a given text marcu and echihabi 2002 we can readily use it to improve rank prediction ,0,1,0
we draw on and extend the work of marcu and echihabi 2002 ,0,1,0
marcu and echihabi 2002 use a patternbased approach in mining instances of rsrs such as contrast and elaboration from large unannotated corpora ,0,1,0
marcu and echihabi 2002 lter training instances based on partofspeech pos tags and soricut and marcu 2003 use syntactic features to identify sentenceinternal rst structure ,0,1,0
we adopt the approach of marcu and echihabi 2002 using a small set of patterns to build relation models and extend their work by re ning the training and classi cation process using parameter optimization topic segmentation and syntactic parsing ,0,1,0
3 the me framework we model two rsrs cause and contrast adopting the de nitions of marcu and echihabi 2002 henceforth me for their causeexplanationevidence and contrast relations respectively ,0,1,0
411 lexical cooccurrences lexical cooccurrences have previously been shown to be useful for discourse level learning tasks lapata and lascarides 2004 marcu and echihabi 2002 ,0,1,0
as such discourse markers play an important role in the parsing of natural language discourse forbes et al 2001 marcu 2000 and their correspondence with discourse relations can be exploited for the unsupervised learning of discourse relations marcu and echihabi 2002 ,0,1,0
for such cases unsupervised approaches have been developed for predicting relations by using sentences containing discourse connectives as training data marcu and echihabi 2002 lapata and lascarides 2004 ,0,1,0
presently there exist methods for learning oppositional terms marcu and echihabi 2002 and paraphrase learning has been thoroughly studied but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution ,0,0,1
although this study falls under the general topic of discourse modeling our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements mckeown 1985 marcu and echihabi 2002 ,0,1,0
a novel approach was described in marcu and echihabi 2002 which used an unsupervised training technique extracting relations that were explicitly and unamibiguously signalled and automatically labelling those examples as the training set ,1,0,0
2 previous work on sentiment analysis some prior studies on sentiment analysis focused on the documentlevel classification of sentiment turney 2002 pang et al 2002 where a document is assumed to have only a single sentiment thus these studies are not applicable to our goal ,0,0,1
much research is also being directed at acquiring affect lexica automatically turney 2002 turney and littman 2002 ,0,1,0
so can be used to classify reviews eg movie reviews as positive or negative turney 2002 and applied to subjectivity analysis such as recognizing hostile messages classifying emails mining reviews wiebe et al 2001 ,0,1,0
recent computational work either focuses on sentence subjectivity wiebe et al 2002 riloff et al 2003 concentrates just on explicit statements of evaluation such as of films turney 2002 pang et al 2002 or focuses on just one aspect of opinion eg hatzivassiloglou and mckeown 1997 on adjectives ,0,1,0
another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis turney 2002 takamura et al 2006 kaji and kitsuregawa 2006 ,0,1,0
pointwise mutual information pmi is commonly used for computing the association of two terms eg turney 2002 which is defined as nullnullnull null nullnull null nullnullnull nullnullnullnullnullnull nullnull null null null nullnullnullnullnull however we argue that pmi is not a suitable measure for our purpose ,0,0,1
sentiment classification at the document level investigates ways to classify each evaluative document eg product review as positive or negative pang et al 2002 turney 2002 ,0,1,0
the acquisition of clues is a key technology in these research efforts as seen in learning methods for documentlevel sa hatzivassiloglou and mckeown 1997 turney 2002 and for phraselevel sa wilson et al 2005 kanayama and nasukawa 2006 ,0,1,0
2007 apply the theory of hatzivassiloglou and mckeown 1997 and turney 2002 to emotion classification and propose a method based on the cooccurrence distribution over content words and six emotion words eg joy fear ,0,1,0
another possible comparison could be with a version of turneys 2002 sentiment classification method applied to chinese ,0,1,0
turney 2002 describes a method of sentiment classification using two humanselected seed words the words poor and excellent in conjunction with a very large text corpus the semantic orientation of phrases is computed as their association with the seed words as measured by pointwise mutual information ,0,1,0
2002 and turney 2002 classified sentiment polarity of reviews at the document level ,0,1,0
this direction has been forming the mainstream of research on opinionsensitive text processing pang et al 2002 turney 2002 etc ,0,1,0
this idea is the same as turney 2002 ,0,1,0
turney 2002 is one of the most famous work that discussed learning polarity from corpus ,1,0,0
typically a small set of seed polar phrases are prepared and new polar phrases are detected based on the strength of cooccurrence with the seeds hatzivassiloglous and mckeown 1997 turney 2002 kanayama and nasukawa 2006 ,0,1,0
in turneys work the cooccurrence is considered as the appearance in the same window turney 2002 ,0,1,0
for example if the lexicon contains an adjective excellent it matches every adjective phrase that includes excellent such as viewexcellent etc as a baseline we built lexicon similarly by using polarity value of turney 2002 ,0,1,0
its size is compatible to turney and littman 2002 ,0,1,0
turneys method did not work well although they reported 80 accuracy in turney and littman 2002 ,0,0,1
turney 2002 predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs which is denoted as the semantic oriented method ,0,1,0
sentiment summarization has been well studied in the past decade turney 2002 pang et al 2002 dave et al 2003 hu and liu 2004a 2004b carenini et al 2006 liu et al 2007 ,0,1,0
there are many research directions eg sentiment classification classifying an opinion document as positive or negative eg pang lee and vaithyanathan 2002 turney 2002 subjectivity classification determining whether a sentence is subjective or objective and its associated opinion wiebe and wilson 2002 yu and hatzivassiloglou 2003 wilson et al 2004 kim and hovy 2004 riloff and wiebe 2005 featuretopicbased sentiment analysis assigning positive or negative sentiments to topics or product features hu and liu 2004 popescu and etzioni 2005 carenini et al 2005 ku et al 2006 kobayashi inui and matsumoto 2007 titov and mcdonald ,0,1,0
one of the main directions is sentiment classification which classifies the whole opinion document eg a product review as positive or negative eg pang et al 2002 turney 2002 dave et al 2003 ng et al 2006 mcdonald et al 2007 ,0,1,0
they may rely only on this information eg turney 2002 whitelaw et al 2005 riloff and wiebe 2003 or they may combine it with additional information as well eg yu and hatzivassiloglou 2003 kim and hovy 2004 bloom et al 2007 wilson et al 2005a ,0,1,0
turneys 2002 work is perhaps one of the most notable examples of unsupervised polarity classification ,1,0,0
automatic methods for this often make use of lexicons of words tagged with positive and negative semantic orientation turney 2002 wilson et al 2005 pang and lee 2008 ,0,1,0
determining document orientation or polarity as in deciding if a given subjective text expresses a positive or a negative opinion on its subject matter pang and lee 2004 turney 2002 3 ,0,1,0
this problem will be solved by incorporating other resources such as thesaurus or a dictionaryorcombiningourmethodwithothermethods using external wider contexts suzuki et al 2006 turney 2002 baron and hirst 2004 ,0,1,0
turney 2002 applied an internetbased technique to the semantic orientation classification of phrases whichhadoriginallybeendevelopedforwordsentiment classification ,0,1,0
2 literature survey the task of sentiment analysis has evolved from document level analysis eg turney 2002 pang and lee 2004 to sentence level analysis eg hu and liu 2004 kim and hovy 2004 yu and hatzivassiloglou 2003 ,0,1,0
while other systems such as hu and liu 2004 turney 2002 have addressed these tasks to some degree opine is the first to report results ,0,0,1
pmi is an extended version of turney 2002s method for finding the so label of a phrase as an attempt to deal with contextsensitive words ,0,1,0
subjective phrases are used by turney 2002 pang and vaithyanathan 2002 kushal et al 2003 kim and hovy 2004 and others in order to classify reviews or sentences as positive or negative ,0,1,0
as a result the problem of opinion mining has seen increasing attention over the last three years from turney 2002 hu and liu 2004 and many others ,0,1,0
7 related work much work on sentiment analysis classifies documents by their overall sentiment for example determining whether a review is positive or negative eg turney 2002 dave et al 2003 pang and lee 2004 beineke et al 2004 ,0,1,0
a number of researchers have explored learning words and phrases with prior positive or negative polarity another term is semantic orientation eg hatzivassiloglou and mckeown 1997 kamps and marx 2002 turney 2002 ,0,1,0
6 conclusions and future directions in previous work statistical nlp computation over large corpora has been a slow of ine process as in knowitall etzioni et al 2005 and also in pmiir applications such as sentiment classi cation turney 2002 ,0,1,0
turney 2002 bai padman and airoldi 2004 beineke hastie and vaithyanathan 2003 mullen and collier 2003 pang and lee 2003 ,0,1,0
turney 2002 and wiebe 2000 focused on learning adjectives and adjectival phrases and wiebe et al ,0,1,0
dave et al 2003 pang and lee 2004 turney 2002 ,0,1,0
turney 2002 pang et al 2002 dave at al 2003 ,0,1,0
2 relatedwork 21 sentiment classification most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment turney 2002 pang et al 2002 dave at al 2003 ,0,1,0
but it is close to the paradigm described by yarowsky 1995 and turney 2002 as it also employs selftraining based on a relatively small seed data set which is incrementally enlarged with unlabelled samples ,0,1,0
measures of attributional similarity have been studied extensively due to their applications in problems such as recognizing synonyms landauer and dumais 1997 information retrieval deerwester et al 1990 determining semantic orientation turney 2002 grading student essays rehder et al 1998 measuring textual cohesion morris and hirst 1991 and word sense disambiguation lesk 1986 ,0,1,0
many 412 turney similarity of semantic relations researchers have argued that metaphor is the heart of human thinking lakoff and johnson 1980 hofstadter and the fluid analogies research group 1995 gentner et al 2001 french 2002 ,0,1,0
we are currently investigating more challenging problems like multiple category classification using the reuters21578 data set lewis 1992 and subjective sentiment classification turney 2002 ,0,1,0
such techniques are currently being applied in many areas including language identification authorship attribution stamatatos et al 2000 text genre classification kesseler et al 1997 stamatatos et al 2000 topic identification dumais et al 1998 lewis 1992 mccallum 1998 yang 1999 and subjective sentiment classification turney 2002 ,0,1,0
2002 and turney 2002 classified sentiment polarity of reviews at the document level ,0,1,0
turney 2002 applied an internetbased technique to the semantic orientation classification of phrases which had originally been developed for word sentiment classification ,0,1,0
2 related work sentiment classi cation traditionally categorization of opinion texts has been cast as a binary classication task pang et al 2002 turney 2002 yu and hatzivassiloglou 2003 dave et al 2003 ,0,1,0
1 introduction previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text pang et al 2002 turney 2002 yu and hatzivassiloglou 2003 ,0,1,0
turney 2002 has presented an unsupervised opinion classification algorithm called sopmi semantic orientation using pointwise mutual information ,0,1,0
2 details of the sopmi algorithm the sopmi algorithm turney 2002 is used to estimate the semantic orientation so of a phrase by 1httpwwwepinionscom 189 references peter d turney ,0,1,0
in related work chaovalit 2005 turney 2002 both supervised and unsupervised approaches have been shown to have their pros and cons ,0,1,0
2 related work there has been a large and diverse body of research in opinion mining with most research at the text pang et al 2002 pang and lee 2004 popescu and etzioni 2005 ounis et al 2006 sentence kim and hovy 2005 kudo and matsumoto 2004 riloff et al 2003 yu and hatzivassiloglou 2003 or word hatzivassiloglou and mckeown 1997 turney and littman 2003 kim and hovy 2004 takamura et al 2005 andreevskaia and bergler 2006 kaji and kitsuregawa 2007 level ,0,1,0
3 related work many methods have been developed for automatically identifying subjective opinion sentiment attitude affectbearing etc words eg turney 2002 riloff and wiebe 2003 kim and hovy 2004 taboada et al 2006 takamura et al 2006 ,0,1,0
the research of opinion mining began in 1997 the early research results mainly focused on the polarity of opinion words hatzivassiloglou et al 1997 and treated the textlevel opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text turney et al 2003 pang et al 2002 zagibalov et al 2008 ,0,1,0
examples of such early work include turney 2002 pang et al 2002 dave et al 2003 hu and liu 2004 popescu and etzioni 2005 ,0,1,0
1 introduction in the community of sentiment analysis turney 2002 pang et al 2002 tang et al 2009 transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work because sentiment expression often behaves with strong domainspecific nature ,0,1,0
in general previous work in opinion mining includes document level sentiment classification using supervised chaovalit and zhou 2005 and unsupervised methods turney 2002 machine learning techniques and sentiment classification considering rating scales pang lee and vaithyanathan 2002 and scoring of features dave lawrence and pennock 2003 ,0,1,0
a contrasting approach turney 2002 relies only upon documents whose labels are unknown ,0,1,0
second movie reviews are apparently harder to classify than reviews of other products turney 2002 dave lawrence and pennock 2003 ,0,1,0
2 motivation in the past work has been done in the area of characterizing words and phrases according to their emotive tone turney and littman 2003 turney 2002 kamps et al 2002 hatzivassiloglou and wiebe 2000 hatzivassiloglou and mckeown 2002 wiebe 2000 but in many domains of text the values of individual phrases may bear little relation to the overall sentiment expressed by the text ,0,1,0
in the present work the approach taken by turney 2002 is used to derive such values for selected phrases in the text ,0,1,0
also even the twocategory version of the ratinginference problem for movie reviews has proven quite challenging for many automated classi cation techniques pang lee and vaithyanathan 2002 turney 2002 ,0,1,0
most prior work on the speci c problem of categorizing expressly opinionated text has focused on the binary distinction of positive vs negative turney 2002 pang lee and vaithyanathan 2002 dave lawrence and pennock 2003 yu and hatzivassiloglou 2003 ,0,1,0
termbased versions of this premise have motivated much sentimentanalysis work for over a decade das and chen 2001 tong 2001 turney 2002 ,0,1,0
turney 2002 noted that the unigram unpredictable might have a positive sentiment in a movie review eg unpredictable plot but could be negative in the review of an automobile eg unpredictable steering ,0,1,0
turney 2002 ,0,1,0
the first is identifying words and phrases that are associated with subjectivity for example that think is associated with private states and that beautiful is associated with positive sentiments eg hatzivassiloglou and mckeown 1997 wiebe 2000 kamps and marx 2002 turney 2002 esuli and sebastiani 2005 ,0,1,0
the third exploits automatic subjectivity analysis in applications such as review classification eg turney 2002 pang and lee 2004 mining texts for product reviews eg yi et al 2003 hu and liu 2004 popescu and etzioni 2005 summarization eg kim and hovy 2004 information extraction eg riloff et al 2005 1note that sentiment the focus of much recent work in the area is a type of subjectivity specifically involving positive or negative opinion emotion or evaluation ,0,1,0
turney also reported good result without domain customization turney 2002 ,1,0,0
63 unsupervised sentiment classification turney proposed the unsupervised method for sentiment classification turney 2002 and similar method is utilized by many other researchers yu and hatzivassiloglou 2003 ,0,1,0
identifying subjectivity helps separate opinions from fact which may be useful in question answering summarization etc semantic orientation classification is a task of determining positive or negative sentiment of words hatzivassiloglou and mckeown 1997 turney 2002 esuli and sebastiani 2005 ,0,1,0
document level sentiment classification is mostly applied to reviews where systems assign a positive or negative sentiment for a whole review document pang et al 2002 turney 2002 ,0,1,0
for instance both pang and lee 2002 and turney 2002 consider the thumbs upthumbs down decision is a film review positive or negative ,0,1,0
however we do not rely on linguistic resources kamps and marx 2002 or on search engines turney and littman 2003 to determine the semantic orientation but rather rely on econometrics for this task ,0,1,0
to evaluate the polarity and strength of opinions most of the existing approaches rely either on training from humanannotated data hatzivassiloglou and mckeown 1997 or use linguistic resources hu and liu 2004 kim and hovy 2004 like wordnet or rely on cooccurrence statistics turney 2002 between words that are unambiguously positive eg excellent and unambiguously negative eg horrible ,0,1,0
furthermore these systems have tackled the problem at different levels of granularity from the document level pang et al 2002 sentence level pang and lee 2004 mao and lebanon 2006 phrase level turney 2002 choi et al 2005 as well as the speaker level in debates thomas et al 2006 ,0,1,0
the work most similar in spirit to ours that of turney 2002 ,0,1,0
while we do not have a direct comparison we note that turney 2002 performs worse on movie reviews than on his other datasets the same type of data as the polarity dataset ,0,0,1
1 introduction sentiment detection and classification has received considerable attention recently pang et al 2002 turney 2002 goldberg and zhu 2004 ,0,1,0
2 motivation automatic subjectivity analysis methods have been used in a wide variety of text processing applications such as tracking sentiment timelines in online forums and news lloyd et al 2005 balog et al 2006 review classification turney 2002 pang et al 2002 mining opinions from product reviews hu and liu 2004 automatic expressive texttospeech synthesis alm et al 2005 text semantic analysis wiebe and mihalcea 2006 esuli and sebastiani 2006 and question answering yu and hatzivassiloglou 2003 ,0,1,0
others such as turney 2002 pang and vaithyanathan 2002 have examined the positive or negative polarity rather than presence or absence of affective content in text ,0,1,0
much of the work in sentiment analysis in the computational linguistics domain has focused either on short segments such as sentences wilson et al 2005 or on longer documents with an explicit polarity orientation like movie or product reviews turney 2002 ,0,1,0
2002 turney 2002 kim and hovy 2004 and others however the research described in this paper uses the information retrieval ir paradigm which has also been used by some researchers ,0,1,0
291 31 level of analysis research on sentiment annotation is usually conducted at the text aue and gamon 2005 pang et al 2002 pang and lee 2004 riloff et al 2006 turney 2002 turney and littman 2003 or at the sentence levels gamon and aue 2005 hu and liu 2004 kim and hovy 2005 riloff et al 2006 ,0,1,0
for example it has been observed that texts often contain multiple opinions on different topics turney 2002 wiebe et al 2001 which makes assignment of the overall sentiment to the whole document problematic ,0,1,0
sentiment classification is a well studied problem wiebe 2000 pang et al 2002 turney 2002 and in many domains users explicitly 1we use the term aspect to denote properties of an object that can be rated by a user as in snyder and barzilay 2007 ,0,1,0
methods focussing on the use and generation of dictionaries capturing the sentiment of words have ranged from manual approaches of developing domaindependent lexicons das and chen 2001 to semiautomated approaches hu and liu 2004 zhuang et al 2006 kim and hovy 2004 and even an almost fully automated approach turney 2002 ,0,1,0
turneys 2002 work is perhaps one of the most notable examples of unsupervised polarity classification ,1,0,0
3 method 31 standard text classication approach we take our starting point from topicbased text classication dumais et al 1998 joachims 1998 and sentiment classication turney 2002 pang and lee 2008 ,0,1,0
1 introduction sentiment analysis have been widely conducted in several domains such as movie reviews product reviews news and blog reviews pang et al 2002 turney 2002 ,0,1,0
in terms of relative performance naive bayes tends to do the worst and svms tend to do the best although the 12httpwwwenglishbhamacukstafioliversoftwaretaggerindexhtm 13turneys 2002 unsupervised algorithm uses bigrams containing an adjective or an adverb ,0,1,0
turney 2002 makes a similar point noting that for reviews the whole is not necessarily the sum of the parts ,0,1,0
some of this work focuses on classifying the semantic orientation of individual words or phrases using linguistic heuristics or a preselected set of seed words hatzivassiloglou and mckeown 1997 turney and littman 2002 ,0,1,0
turneys 2002 work on classiflcation of reviews is perhaps the closest to ours2 he applied a speciflc unsupervised learning technique based on the mutual information between document phrases and the words excellent and poor where the mutual information is computed using statistics gathered by a search engine ,0,1,0
we also note that turney 2002 found movie reviews to be the most 2indeed although our choice of title was completely independent of his our selections were eerily similar ,0,1,0
researchers have focused on learning adjectives or adjectival phrases turney 2002 hatzivassiloglou and mckeown 1997 wiebe 2000 and verbs wiebe et al 2001 but no previous work has focused on learning nouns ,0,1,0
some existing resources contain lists of subjective words eg levins desire verbs 1993 and some empirical methods in nlp have automatically identified adjectives verbs and ngrams that are statistically associated with subjective language eg turney 2002 hatzivassiloglou and mckeown 1997 wiebe 2000 wiebe et al 2001 ,0,1,0
for example spertus 1997 developed a system to identify inflammatory texts and turney 2002 pang et al 2002 developed methods for classifying reviews as positive or negative ,0,1,0
turney 2002 showed that it is possible to use only a few of those semantically oriented words namely excellent and poor to label other phrases cooccuring with them as positive or negative ,0,1,0
for determining whether an opinion sentence is positive or negative we have used seed words similar to those produced by hatzivassiloglou and mckeown 1997 and extended them to construct a much larger set of semantically oriented words with a method similar to that proposed by turney 2002 ,0,1,0
our focus is on the sentence level unlike pang et al 2002 and turney 2002 we employ a significantly larger set of seed words and we explore as indicators of orientation words from syntactic classes other than adjectives nouns verbs and adverbs ,0,0,1
the approach is based on the hypothesis that positive words cooccur more than expected by chance and so do negative words this hypothesis was validated at least for strong positivenegative words in turney 2002 ,0,1,0
in earlier work turney 2002 only singletons were used as seed words varying their number allows us to test whether multiple seed words have a positive effect in detection performance ,0,1,0
1 introduction the field of sentiment classification has received considerable attention from researchers in recent years pang and lee 2002 pang et al 2004 turney 2002 turney and littman 2002 wiebe et al 2001 bai et al 2004 yu and hatzivassiloglou 2003 and many others ,0,1,0
movie and product reviews have been the main focus of many of the recent studies in this area pang and lee 2002 pang et al 2004 turney 2002 turney and littman 2002 ,0,1,0
accuracy training data turney 2002 66 unsupervised pang lee 2004 8715 supervised aue gamon 2005 914 supervised so 7395 unsupervised smso to increase seed words then so 7485 weakly supervised table 7 classification accuracy on the movie review domain turney 2002 achieves 66 accuracy on the movie review domain using the pmiir algorithm to gather association scores from the web ,0,1,0
c2005 association for computational linguistics automatic identification of sentiment vocabulary exploiting low association with known sentiment terms michael gamon anthony aue natural language processing group natural language processing group microsoft research microsoft research mgamonmicrosoftcom anthauemicrosoftcom abstract we describe an extension to the technique for the automatic identification and labeling of sentiment terms described in turney 2002 and turney and littman 2002 ,0,1,0
turney 2002 and turney and littman 2002 exploit the first two generalizations for unsupervised sentiment classification of movie reviews ,0,1,0
given a set of terms with unknown sentiment orientation turney 2002 then uses the pmiir algorithm turney 2001 to issue queries to the web and determine for each of these terms its pointwise mutual information pmi with the two seed words across a large set of documents ,0,1,0
we can then use this newly identified set to 1 use turneys method to find the orientation for the terms and employ the terms and their scores in a classifier and 2 use turneys method to find the orientation for the terms and add the new terms as additional seed terms for a second iteration as opposed to turney 2002 we do not use the web as a resource to find associations rather we apply the method directly to indomain data ,0,1,0
2002 turney 2002 dave et al ,0,1,0
in analyzing opinions cardie et al 2003 wilson et al 2004 judging documentlevel subjectivity pang et al 2002 turney 2002 and answering opinion questions cardie et al 2003 yu and hatzivassiloglou 2003 the output of a sentencelevel subjectivity classification can be used without modification ,0,1,0
the ve partofspeech pos patterns from turney 2002 were used for the extraction of indicators all involving at least one adjective or adverb ,0,1,0
2002 and turney 2002 ,0,1,0
in particular since we treat each individual speech within a debate as a single document we are considering a version of documentlevel sentimentpolarity classification namely automatically distinguishing between positive and negative documents das and chen 2001 pang et al 2002 turney 2002 dave et al 2003 ,0,1,0
2002 turney 2002 dave et al ,0,1,0
c3btc5 and cccdca were used in kamps and marx 2002 and turney and littman 2003 respectively ,0,1,0
the token precision is higher than 90 in all of the corpora including the movie domain which is considered to be difficult for sa turney 2002 ,0,1,0
in the thriving area of research on automatic analysis and processing of product reviews hu and liu 2004 turney 2002 pang and lee 2005 little attention has been paid to the important task studied here assessing review helpfulness ,0,0,1
2002 and turney 2002 classified sentiment polarity of reviews at the document level ,0,1,0
lexical cues of differing complexities have been used including single words and ngrams eg mullen and collier 2004 pang et al 2002 turney 2002 yu and hatzivassiloglou 2003 wiebe et al 2004 as well as phrases and lexicosyntactic patterns eg kim and hovy 2004 hu and liu 2004 popescu and etzioni 2005 riloff and wiebe 2003 whitelaw et al 2005 ,0,1,0
also pmiir is useful for calculating semantic orientation and rating reviews turney 2002 ,0,1,0
for example researchers turney 2002 yu and hatzivassiloglou 2003 have identified semantic correlation between words and views positive words tend to appear more frequently in positive movie and product reviews and newswire article sentences that have a positive semantic orientation and vice versa for negative reviews or sentences with a negative semantic orientation ,0,1,0
1 introduction sentiment analysis of text documents has received considerable attention recently shanahan et al 2005 turney 2002 dave et al 2003 hu and liu 2004 chaovalit and zhou 2005 ,0,1,0
theauthorsapplysopmiirturney 2002 to extract and determine the polarity of adjectives ,0,1,0
as comparison turney and littman 2003 used seed sets consisting of 7 words in their word valence annotation experiments while turney 2002 used minimal seed sets consisting of only one positive and one negative word excellent and poor in his experiments on review classification ,0,1,0
2 related work our approach for emotion classification is based on the idea of hatzivassiloglou and mckeown 1997 and is similar to those of turney 2002 and turney and littman 2003 ,0,1,0
the idea of tracing polarity through adjective cooccurrence is adopted by turney 2002 for the binary positive and negative classification of text reviews ,0,1,0
following hatzivassiloglou and mckeown 1997 and turney 2002 we decided to observe how often the words from the headline cooccur with each one of the six emotions ,0,1,0
some of the differences between our approach and those of turney 2002 are mentioned below objectives turney 2002 aims at binary text classification while our objective is six class classification of oneliner headlines ,0,1,0
word class turney 2002 measures polarity using only adjectives however in our approach we consider the noun the verb the adverb and the adjective content words ,0,0,1
search engines turney 2002 uses the altavista web browser while we consider and combine the frequency information acquired from three web search engines ,0,1,0
word proximity for the web searches turney 2002 uses the near operator and considers only those documents that contain the adjectives within a specific proximity ,0,1,0
queries the queries of turney 2002 are made up of a pair of adjectives and in our approach the query contains the content words of the headline and an emotion ,0,1,0
our work builds upon turneys work on semantic orientation turney 2002 and synonym learning turney 2001 in which he used a pmiir algorithm to measure the similarity of words and phrases based on web queries ,0,1,0
turney turney 2001 turney 2002 reported that the near operator outperformed simple page cooccurrence for his purposes our early experiments informally showed the same for this work ,0,1,0
finally we plan to apply the model to other paraphrasing tasks including fully abstractive document summarisation daume iii and marcu 2002 ,0,1,0
between these two extremes there has been a relatively modest amount of work in sentence simplification chandrasekar doran and bangalore 1996 mahesh 1997 carroll et al 1998 grefenstette 1998 jing 2000 knight and marcu 2002 and document compression daume iii and marcu 2002 daume iii and marcu 2004 zajic dorr and schwartz 2004 in which words phrases and sentences are selected in an extraction process ,0,1,0
the third baseline comp is the document compression system developed by daume iii and marcu 2002 which compresses documents by cutting out constituents in a combined syntax and discourse tree ,0,1,0
a few researchers have focused on other aspects of summarization including single sentence knight and marcu 2002 paragraph or short document daume iii and marcu 2002 queryfocused berger and mittal 2000 or speech hori et al 2003 ,0,1,0
1 introduction hyponymy relations can play a crucial role in various nlp systems and there have been many attempts to develop automatic methods to acquire hyponymy relations from text corpora hearst 1992 caraballo 1999 imasumi 2001 fleischman et al 2003 morin and jacquemin 2003 ando et al 2003 ,0,1,0
the row labelled precision shows the precision of the extracted information ie how many entries are correct according to a human annotator estimated by random sampling and manual evaluation of 1 of the data for each table similar to fleischman et al 2003 ,0,1,0
fleischman et al 2003 jijkoun et al 2003 ,0,1,0
in our future work we plan to investigate the effect of more sophisticated and probably more accurate filtering methods fleischman et al 2003 on the qa results ,1,0,0
the recall problem is usually addressed by increasing the amount of text data for extraction taking larger collections fleischman et al 2003 or by developing more surface patterns soubbotin and soubbotin 2002 ,0,1,0
the usual recall and precision metrics eg how many of the interesting bits of information were detected and how many of the found bits were actually correct require either a test corpus previously annotated with the required information or manual evaluation fleischman et al 2003 ,0,1,0
32 questions and corpus to get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases similarly to fleischman et al 2003 we focused only on questions about persons taken from the trec8 through trec 2003 question sets ,0,1,0
2 related work question answering has attracted much attention from the areas of natural language processing information retrieval and data mining fleischman et al 2003 echihabi et al 2003 yang et al 2003 hermjakob et al 2002 dumais et al 2002 hermjakob et al 2000 ,0,1,0
1 motivation question answering has emerged as a key area in natural language processing nlp to apply question parsing information extraction summarization and language generation techniques clark et al 2004 fleischman et al 2003 echihabi et al 2003 yang et al 2003 hermjakob et al 2002 dumais et al 2002 ,0,1,0
1 introduction the goal of this study has been to automatically extract a large set of hyponymy relations which play a critical role in many nlp applications such as qa systems fleischman et al 2003 ,0,1,0
we do not use particular lexicosyntactic patterns as previous attempts have hearst 1992 caraballo 1999 imasumi 2001 fleischman et al 2003 morin and jacquemin 2003 ando et al 2003 ,0,1,0
we compared our system with the concepts in wordnet and fleischman et als instanceconcept relations fleischman et al 2003 ,0,1,0
2 previous work there have been several approaches to automatically discovering lexicosemantic information from text hearst 1992 riloff and shepherd 1997 riloff and jones 1999 berland and charniak 1999 pantel and lin 2002 fleischman et al 2003 girju et al 2003 ,0,1,0
in contrast the idea of bootstrapping for relation and information extraction was first proposed in riloff and jones 1999 and successfully applied to the construction of semantic lexicons thelen and riloff 2002 named entity recognition collins and singer 1999 extraction of binary relations agichtein and gravano 2000 and acquisition of structured data for tasks such as question answering lita and carbonell 2004 fleischman et al 2003 ,1,0,0
after that several million instances of people locations and other facts were added fleischman et al 2003 ,0,1,0
alternatively one can train them with respect to the final translation quality measured by some error criterion och 2003 ,0,1,0
to simulate real world scenario we use nbest lists from isis stateoftheart statistical machine translation system altemp och 2003 and the 2002 nist chineseenglish evaluation corpus as the test corpus ,0,1,0
for example a statistical machine translation system such as isis altemp smt system och 2003 can generate a list of nbest alternative translations given a source sentence ,0,1,0
 a natural fit to the existing statistical machine translation framework a metric that ranks a good translation high in an nbest list could be easily integrated in a minimal error rate statistical machine translation training framework och 2003 ,0,1,0
the training of ibm model 4 was implemented by the giza package och and ney 2003 ,0,1,0
we adopted an nbest hypothesis approach och 2003 to train ,0,1,0
indeed the proposed speech translation paradigm of loglinear models have been shown e ective in many applications beyerlein 1998 vergyri 2000 och 2003 ,0,1,0
the powells algorithm used in this work is similar as the one from press et al 2000 but we modi ed the line optimization codes a subroutine of powells algorithm with reference to och 2003 ,0,1,0
imum error rate training mert och 2003 to maximize bleu score papineni et al 2002 ,0,1,0
by introducing the hidden word alignment variable a brown et al 1993 the optimal translation can be searched for based on the following criterion 1 arg max m mm m ea eh efa 1 where is a string of phrases in the target language e f fa is the source language string of phrases he are feature functions weights m m are typically optimized to maximize the scoring function och 2003 ,0,1,0
our mt baseline system is based on moses decoder koehn et al 2007 with word alignment obtained from giza och et al 2003 ,0,1,0
we use minimum error rate training och 2003 to tune the feature weights for the loglinear model ,0,1,0
except where noted each system was trained on 27 million words of newswire data aligned with giza och and ney 2003 and symmetrized with the growdiagfinaland heuristic koehn et al 2003 ,0,1,0
in all experiments that follow each system configuration was independently optimized on the nist 2003 chineseenglish test set 919 sentences using minimum error rate training och 2003 and tested on the nist 2005 chineseenglish task 1082 sentences ,0,1,0
this may be because their system was not tuned using minimum error rate training och 2003 ,0,1,0
5we use deterministic sampling which is useful for reproducibility and for minimum error rate training och 2003 ,0,1,0
our baseline uses giza alignments och and ney 2003 symmetrized with the growdiagfinaland heuristic koehn et al 2003 ,0,1,0
proceedings of the 22nd international conference on computational linguistics coling 2008 pages 585592 manchester august 2008 random restarts in minimum error rate training for statistical machine translation robert c moore and chris quirk microsoft research redmond wa 98052 usa bobmooremicrosoftcom chrisqmicrosoftcom abstract ochs 2003 minimum error rate training mert procedure is the most commonly used method for training feature weights in statistical machine translation smt models ,0,1,0
then we use both moses decoder and its suppo we run the decoder with its d then use moses implementation of minimum error rate training och 2003 to tune the feature weights on the development set ,0,1,0
3 baseline mt system the phrasebased smt system used in our experiments is moses phrase translation pro ing probabilities and languag ties are combined in the loglinear model to obtain the best translation best e of the source sentence f m p maxarg fee ebest 2 m mm h 1 maxarg fe e the weights are set by a discriminative training method using a heldout data set as describ in och 2003 ,0,1,0
this wrong translation of content words is similar to the incorrect omission reported in och et al 2003 which both hurt translation adequacy ,0,1,0
starting with bilingualphrasepairsextractedfromautomatically aligned parallel text och and ney 2004 koehn et al 2003 these pscfg approaches augment each contiguous in source and target words phrase pair with a lefthandside symbol like the vp in the example above and perform a generalization procedure to form rules that include nonterminal symbols ,0,1,0
the kbest list is also frequently used in discriminative learning to approximate the whole set of candidates which is usually exponentially large och 2003 mcdonald et al 2005 ,0,1,0
our human word alignments do not distinguish between sure and probable links och and ney 2003 ,0,1,0
such an approach contrasts with the loglinear hmmmodel4 combination proposed by och and ney 2003 ,0,1,0
2 word alignment framework a statistical translation model brown et al 1993 och and ney 2003 describes the relationship between a pair of sentences in the source and target languages f fj1e ei1 using a translation probability pfe ,0,1,0
2 we note that these posterior probabilities can be computed efficiently for some alignment models such as the hmm vogel et al 1996 och and ney 2003 models 1 and 2 brown et al 1993 ,1,0,0
high quality word alignments can yield more accurate phrasepairs which improve quality of a phrasebased smt system och and ney 2003 fraser and marcu 2006b ,0,1,0
much of the recent work in word alignment has focussed on improving the word alignment quality through better modeling och and ney 2003 deng and byrne 2005 martin et al 2005 or alternative approaches to training fraser and marcu 2006b moore 2005 ittycheriah and roukos 2005 ,0,1,0
42 experiments to build all alignment systems we start with 5 iterations of model 1 followed by 4 iterations of hmm vogel et al 1996 as implemented in giza och and ney 2003 ,0,1,0
for all nonleaf systems we take the best performing of the union refined and intersection symmetrization heuristics och and ney 2003 to combine the 1ton and mto1 directions resulting in a mton alignment ,0,1,0
och and ney 2003 presented results suggesting that the additional parameters required to ensure that a model is not deficient result in inferior performance but we plan to study whether this is the case for our generative model in future work ,0,1,0
22 unsupervised parameter estimation we can perform maximum likelihood estimation of the parameters of this model in a similar fashion to that of model 4 brown et al 1993 described thoroughly in och and ney 2003 ,1,0,0
we use viterbi training brown et al 1993 but neighborhood estimation alonaizan et al 1999 och and ney 2003 or pegging brown et al 1993 could also be used ,0,1,0
och and ney 2003 discussed efficient implementation ,1,0,0
the phrase bilexicon is derived from the intersection of bidirectional ibm model 4 alignments obtained with giza och and ney 2003 augmented to improve recall using the growdiagfinal heuristic ,0,1,0
the loglinear model weights are learned using chiangs implementation of the maximum bleu training algorithm och 2003 both for the baseline and the wsdaugmented system ,0,1,0
3 s in equation 1 are the weights of different feature functions learned to maximize development set bleu scores using a method similar to och 2003 ,0,1,0
smt has evolved from the original wordbased approach brown et al 1993 into phrasebased approaches koehn et al 2003 och and ney 2004 and syntaxbased approaches wu 1997 alshawi et al 2000 yamada and knignt 2001 chiang 2005 ,0,1,0
we run the decoder with its default settings maximum phrase length 7 and then use koehns implementation of minimum error rate training och 2003 to tune the feature weights on the de2 the full name of htrdp is national high technology research and development program of china also named as 863 program ,0,1,0
for the loglinear model training we take minimumerrorrate training method as described in och 2003 ,0,1,0
the translation models were pharsebased zen et al 2002 created using the giza toolkit och et al 2003 ,0,1,0
for tuning of the decoders parameters including the language model weight minimum error training och 2003 with respect to the bleu score using was conducted using the development corpus ,0,1,0
note that the minimum error rate training och 2003 uses only the target sentence with the maximum posterior probability whereas here the whole probability distribution is taken into account ,0,0,1
we will show that some achieve significantly better results than the standard minimum error rate training of och 2003 ,0,0,1
the current stateoftheart is to optimize these parameters with respect to the final evaluation criterion this is the socalled minimum error rate training och 2003 ,1,0,0
the current stateoftheart is to use minimum error rate training mert as described in och 2003 ,1,0,0
therefore och and ney 2002 och 2003 defined the translation candidate with the minimum worderror rate as pseudo reference translation ,0,1,0
however as pointed out in och 2003 there is no reason to believe that the resulting parameters are optimal with respect to translation quality measured with the bleu score ,0,1,0
33 features similar to the default features in pharaoh koehn och and marcu 2003 we used following features to estimate the weight of our grammar rules ,0,1,0
we just assign these rules a constant score trained using our implementation of minimum error rate training och 2003b which is 07 in our system ,0,1,0
6 training similar to most stateoftheart phrasebased smt systems we use the sri toolkit stolcke 2002 for language model training and giza toolkit och and ney 2003 for word alignment ,0,1,0
based on the word alignment results if the aligned target words of any two adjacent foreign linguistic phrases can also be formed into two valid adjacent phrase according to constraints proposed in the phrase extraction algorithm by och 2003a they will be extracted as a reordering training sample ,0,1,0
a superset of the parallel data was word aligned by giza union och and ney 2003 and emd fraser and marcu 2006 ,0,1,0
the hierarchical phrase translation pairs are extracted in a standard way chiang 2005 first the bilingual data are word alignment annotated by running giza och and ney 2003 in two directions ,0,1,0
the baseline hierarchical phrasebased system is trained using standard maxbleu training mert without sparse features och 2003 ,0,1,0
2 statistical machine translation we use a loglinear approach och 2003 in which a foreign language sentence f is translated into another language for example english e by seeking a maximum solution e argmax e wt h f e 1 where h f e is a largedimension feature vector ,0,1,0
1 introduction the recent advances in statistical machine translation have been achieved by discriminatively training a small number of realvalued features based either on hierarchical phrasebased translation och and ney 2004 koehn et al 2003 chiang 2005 or syntaxbased translation galley et al 2006 ,0,1,0
to model ptas we use a standard loglinear approach ptas exp bracketleftbiggsummationdisplay i ifista bracketrightbigg where each fista is a feature function and weights i are set using ochs algorithm och 2003 to maximize the systems bleu score papineni et aal ,0,1,0
on test bleu bp bleu bp pairci 95 bleu bp 3 01 03 3298 092 3303 093 023 034 3360 093 4 01 04 3344 093 3346 093 026 029 3497 094 5 01 05 3307 092 3314 093 029 043 3433 093 6 01 06 3286 092 3353 093 026 108 3443 093 7 01 07 3308 093 3351 093 004 082 3449 093 8 01 08 3312 093 3347 093 006 075 3450 094 9 01 09 3315 093 3322 093 035 051 3468 093 10 01 10 3301 093 3359 094 018 096 3479 094 11 01 11 3284 094 3340 094 013 098 3476 094 12 01 12 3273 093 3349 094 034 118 3483 094 13 01 13 3271 093 3354 094 039 126 3491 094 14 01 14 3266 093 3369 094 058 147 3497 094 15 01 15 3247 093 3357 094 063 157 3499 094 16 01 16 3251 093 3362 094 062 159 3500 094 32 nonuniform system prior weights as pointed out in section 21 a useful property of the mbrlike system selection method is that system prior weights can easily be trained using the minimum error rate training och 2003 ,0,1,0
note that all systems were optimized using a nondeterministic implementation of the minimum error rate training described in och 2003 ,0,1,0
for instance word alignment models are often trained using the giza toolkit och and ney 2003 error minimizing training criteria such as the minimum error rate training och 2003 are employed in order to learn feature function weights for loglinear models and translation candidates are produced using phrasebased decoders koehn et al 2003 in combination with ngram language models brants et al 2007 ,0,1,0
for instance changing the training procedure for word alignment models turned out to be most beneficial for details see och and ney 2003 ,1,0,0
 using the components of the rowvector bm as feature function values for the candidate translation em m a16 1m the system prior weights can easily be trained using the minimum error rate training described in och 2003 ,1,0,0
when different decoder settings are applied to the same model mert weights och 2003 from the unprojected single pass setup are used and are kept constant across runs ,0,1,0
we benchmark our results against a model hiero which was directly trained to optimise bleunist using the standard mert algorithm och 2003 and the full set of translation and lexical weight features described for the hiero model chiang 2007 ,0,1,0
most work on discriminative training for smt has focussed on linear models often with margin based algorithms liang et al 2006 watanabe et al 2006 or rescaling a product of submodels och 2003 ittycheriah and roukos 2007 ,0,1,0
1 introduction since its introduction by och 2003 minimum error rate training mert has been widely adopted for training statistical machine translation mt systems ,1,0,0
51 baseline system we trained moses on all spanishenglish europarl sentences up to length 20 177k sentences using giza model 4 word alignments and the growdiagfinaland combination heuristic koehn et al 2007 och and ney 2003 koehn 2002 which performed better than any alternative combination heuristic13 the baseline estimates heuristic come fromextractingphrasesuptolength7fromtheword alignment ,0,1,0
an important contribution to interactive cat technology was carried out around the transtype tt project langlais et al 2002 foster et al 2002 foster 2002 och et al 2003 ,1,0,0
in the present work we decided to use wsr instead of key stroke ratio ksr which is used in other works on imt such as och et al 2003 ,0,1,0
esen 63009 59209 6014 enes 63809 60510 5216 deen 71608 69009 3613 ende 75908 73509 3212 fren 62909 59210 5916 enfr 63409 60009 5414 bined in a loglinear fashion by adjusting a weight for each of them by means of the mert och 2003 procedure optimising the bleu papineni et al 2002 score obtained on the development partition ,0,1,0
in och et al 2003 the use of a word graph is proposed as interface between an alignmenttemplate smt model and the imt engine ,0,1,0
this tolerant search uses the well known concept of levenshtein distance in order to obtain the most similar string for the given prefix see och et al 2003 for more details ,0,1,0
the standard minimum error rate training och 2003 was applied to tune the weights for all feature types ,0,1,0
for each language pair we use two development sets one for minimum error rate training och 2003 macherey et al 2008 and the other for tuning the scale factor for mbr decoding ,0,1,0
we then train word alignment models och and ney 2003 using 6 model1 iterations and 6 hmm iterations ,0,1,0
the f are optimized by minimumerror training mert och 2003 ,0,1,0
for evaluation we use a stateoftheart baseline system moses hoang and koehn 2008 which works with a loglinear interpolation of feature functions optimized by mert och 2003 ,0,1,0
koehn et al 2003 och and ney 2004 ,0,1,0
these heuristics define a phrase pair to consist of a source and target ngrams of a wordaligned sourcetarget sentence pair such that if one end of an alignment is in the one ngram the other end is in the other ngram and there is at least one such alignment och and ney 2004 koehn et al 2003 ,0,1,0
a class of training criteria that provides a tighter connection between the decision rule and the final error metric is known as minimum error rate training mert and has been suggested for smt in och 2003 ,0,1,0
6 related work as suggested in och 2003 an alternative method for the optimization of the unsmoothed error count is powells algorithm combined with a gridbased line optimization press et al 2007 p 509 ,0,1,0
assuming that the corpusbased error count for some translations es1 is additively decomposable into the error counts of the individual sentences ie ed4rs1 es1d5 ag ewss ag1 ed4rsesd5the mert criterion is given as m1 ag argmin m1 az s f4 sag1 ea0rsed4fsm1 d5a8 b7 3 ag argmin m1 az s f4 sag1 k f4 kag1 ed4rseskd5a0ed4fsm1 d5eska8 b7 with e d4fsm1 d5 ag argmaxe az m f4 mag1 mhmd4efsd5 b7 4 in och 2003 it was shown that linear models can effectively be trained under the mert criterion using a special line optimization algorithm ,0,1,0
the upper envelope is a convex hull and can be inscribed with a convex polygon whose edges are the segments of a piecewise linear function in papineni 1999 och 2003 envd4fd5 ag max ec8c awa d4efd5 a0 a4 bd4efd5 c8 rb4 6 726 score error count 0 0 e1 e2 e5 e6 e8 e1e 2 e3 e4 e5e6e 7 e8 figure 1 the upper envelope bold red curve for a set of lines is the convex hull which consists of the topmost line segments ,0,1,0
this operation can be used in applications like minimum error rate training och 2003 or optimizing system combination as described by hillard et al ,0,1,0
parameters were tuned with minimum errorrate training och 2003 on the nist evaluation set of 2006 mt06 for both ce and ae ,0,1,0
1 introduction statistical phrasebased systems och and ney 2004 koehn et al 2003 have consistently delivered stateoftheart performance in recent machine translation evaluations yet these systems remain weak at handling word order changes ,0,1,0
moreover rather than predicting an intrinsic metric such as the parseval fscore the metric that the predictor learns to predict can be chosen to better fit the final metric on which an endtoend system is measured in the style of och 2003 ,0,1,0
72 minimumrisk training adjusting or changes the distribution p minimum error rate training mert och 2003 tries to tune to minimize the bleu loss of a decoder that chooses the most probable output according to p ,0,1,0
och 2003 shows that setting those weights should take into account the evaluation metric by which the mt system will eventually be judged ,0,1,0
1 och 2003 provides evidence that should be chosen by optimizing an objective function basd on the evaluation metric of interest rather than likelihood ,0,1,0
10both pharoah and our system have weights trained using mert och 2003 on sentences of length 30 words or less to ensure that training and test conditions are matched ,0,1,0
however the approach raises two major challenges 7in practice mert training och 2003 will be used to train relative weights for the different model components ,0,1,0
our approach permits an alternative to minimum errorrate training mert och 2003 it is discriminativebuthandleslatentstructureandregularization in more principled ways ,0,1,0
we perform word alignment using giza och and ney 2003 symmetrize the alignments using the growdiagfinaland heuristic and extract phrases up to length 3 ,0,1,0
the same probabilities are also included using 50 hard word classes derived from the parallel corpus using the giza mkcls utility och and ney 2003 ,0,1,0
the rules are then treated as events in a relative frequency estimate4 we used giza model 4 to obtain word alignments och and ney 2003 using the growdiagfinaland heuristic to symmetrise the two directional predictions koehn et al 2003 ,0,1,0
no artificial gluerules or rule span limits were employed7 the parameters of the translation system were trained to maximize bleu on the mt02 test set och 2003 ,0,1,0
there has been some previous work on accuracydriven training techniques for smt such as mert och 2003 and the simplex armijo downhill method zhao and chen 2009 which tune the parameters in a linear combination of various phrase scores according to a heldout tuning set ,0,1,0
feature weights were set with minimum error rate training och 2003 on a development set using bleu papineni et al 2002 as the objective function ,0,1,0
furthermore wasp1 employs minimum error rate training och 2003 to directly optimize the evaluation metrics ,0,1,0
for the mer training och 2003 we modify koehns mer trainer koehn 2004 to train our system ,0,1,0
1 introduction phrasebased method koehn et al 2003 och and ney 2004 koehn et al 2007 and syntaxbased method wu 1997 yamada and knight 2001 eisner 2003 chiang 2005 cowan et al 2006 marcu et al 2006 liu et al 2007 zhang et al 2007c 2008a 2008b shen et al 2008 mi and huang 2008 represent the stateoftheart technologies in statistical machine translation smt ,1,0,0
for practical reasons the maximum size of a token was set at three for chinese andfour forkorean2 minimum error rate training och 2003 was run on each system afterwardsand bleu score papineni et al 2002 was calculated on the test sets ,0,1,0
we train our feature weights using maxbleu och 2003 and decode with a ckybased decoder that supports language model scoring directly integrated into the search ,0,1,0
we held out 300 sentences for minimum error rate training mert och 2003 and optimised the parameters of the feature functions of the decoder for each experimental run ,0,1,0
we used giza och and ney 2003 to align approximately 751000 sentences from the germanenglish portion of the europarl corpus koehn 2005 in both the germantoenglish and englishtogerman directions ,0,1,0
moses used the development data for minimum errorrate training och 2003 of its small number of parameters ,0,1,0
the feature weights are tuned by the modified koehns mer och 2003 koehn 2007 trainer ,0,1,0
however this is not unprecedented discriminatively weighted generative models have been shown to outperform purely discriminative competitors in various nlp classification tasks raina et al 2004 toutanova 2006 and remain the standard approach in statistical translation modeling och 2003 ,1,0,0
since we also adopt a linear scoring function in equation 3 the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard minimum error rate training mert algorithm och 2003 ,0,1,0
parameters were tuned with mert algorithm och 2003 on the nist evaluation set of 2003 mt03 for both the baseline systems and the system combination model ,0,1,0
giza toolkit och and ney 2003 is used to perform word alignment in both directions with default settings and the intersectdiaggrow method is used to generate symmetric word alignment refinement ,0,1,0
we then built separate directed word alignments for englishx andxenglish xindonesian spanish using ibm model 4 brown et al 1993 combined them using the intersectgrow heuristic och and ney 2003 and extracted phraselevel translation pairs of maximum length seven using the alignment template approach och and ney 2004 ,0,1,0
we set all weights by optimizing bleu papineni et al 2002 using minimum error rate training mert och 2003 on a separate development set of 2000 sentences indonesian or spanish and we used them in a beam search decoder koehn et al 2007 to translate 2000 test sentences indonesian or spanish into english ,0,1,0
we extract a phrase table using the moses pipeline based on model 4 word alignments generated from giza och and ney 2003 ,0,1,0
phrases are then extracted from the word alignments using the method described in och and ney 2003 ,0,1,0
the remaining six entries were all fully automatic machine translation systems in fact they were all phrasebased statistical machine translation system that had been trained on the same parallel corpus and most used bleubased minimum error rate training och 2003 to optimize the weights of their log linear models feature functions och and ney 2002 ,1,0,0
for example work which failed to detect improvements in translation quality with the integration of word sense disambiguation carpuat and wu 2005 or work which attempted to integrate syntactic information but which failed to improve bleu charniak et al 2003 och et al 2004 may deserve a second look with a more targeted manual evaluation ,0,1,0
the statistical machine translation community relies on the bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training och 2003 ,1,0,0
this preprocessing step can be accomplished by applying the giza toolkit och and ney 2003 that provides viterbi alignments based on ibm model4 ,0,1,0
starting from the parallel training corpus provided with direct and inverted alignments the socalled union alignment och and ney 2003 is computed ,0,1,0
och 2003 has described an efficient exact onedimensional accuracy maximization technique for a similar search problem in machine translation ,1,0,0
due to space we do not describe step 8 in detail see och 2003 ,0,1,0
287 system train base test base 1 baseline 8789 8789 2 contrastive 8870 082 8845 056 5 trialsfold 3 contrastive 8882 093 8855 066 greedy selection table 1 average f1 of 7way crossvalidation to generate the alignments we used model 4 brown et al 1993 as implemented in giza och and ney 2003 ,0,1,0
53 baseline system we conducted experiments using different segmenters with a standard loglinear pbsmt model giza implementation of ibm word alignment model 4 och and ney 2003 the refinement and phraseextraction heuristics described in koehn et al 2003 minimumerrorrate training och 2003 a 5gram language model with kneserney smoothing trained with srilm stolcke 2002 on the english side of the training data and moses koehn et al 2007 dyer et al 2008 to translate both single best segmentation and word lattices ,0,1,0
the tools used are the moses toolkit koehn et al 2007 for decoding and training giza for word alignment och and ney 2003 and srilm stolcke 2002 for language models ,0,1,0
to tune feature weights minimum error rate training is used och 2003 optimized against the neva metric forsbom 2003 ,0,1,0
current state of the art machine translation systems och 2003 use phrasal ngram features extracted automatically from parallel corpora ,1,0,0
5 phrase pair induction a common approach to phrasebased translation is to extract an inventory of phrase pairs ppi from bitext koehn et al 2003 for example in the phraseextract algorithm och 2002 a word alignment am1 is generated over the bitext and all word subsequences ei2i1 and fj2j1 are found that satisfy am1 aj i1i2 iff j j1j2 ,0,1,0
pooling the sets to form two large ce and ae test sets the ae system improvements are significant at a 95 level och 2003 the ce systems are only equivalent ,0,1,0
the hallucination process is motivated by the use of null alignments into markov alignment models as done by och and ney 2003 ,0,1,0
the line search is an extension of that described in och 2003 quirk et al 2005 ,0,1,0
33 grid line search our implementation of a grid search is a modified version of that proposed in och 2003 ,0,1,0
msr thus adopts the method proposed by och 2003 ,0,1,0
this is analogous and in a certain sense equivalent to empirical risk minimization which has been used successfully in related areas such as speech recognition rahim and lee 1997 language modeling paciorek and rosenfeld 2000 and machine translation och 2003 ,0,1,0
a first family of libraries was based on a word alignment a produced using the refined method described in och and ney 2003 combination of two ibmviterbi alignments we call these the a libraries ,0,1,0
the first is to align the words using a standard word alignement technique such as the refined method described in och and ney 2003 the intersection of two ibm viterbi alignments forward and reverse enriched with alignments from the union and then generate biphrases by combining together individual alignments that cooccur in the same pair of sentences ,0,1,0
this is the strategy that is usually adopted in other phrasebased mt approaches zens and ney 2003 och and ney 2004 ,0,1,0
instead and as suggested by och 2003 we chose to maximize directly the quality of the translations produced by the system as measured with a machine translation evaluation metric ,0,1,0
1 introduction possibly the most remarkable evolution of recent years in statistical machine translation is the step from wordbased models to phrasebased models och et al 1999 marcu and wong 2002 yamada and knight 2002 tillmann and xia 2003 ,0,1,0
nowadays most of the stateoftheart smt systems are based on bilingual phrases bertoldi et al 2004 koehn et al 2003 och and ney 2004 tillmann 2003 vogel et al 2004 zens and ney 2004 ,0,1,0
the model scaling factors 15 and the word and phrase penalties are optimized with respect to some evaluation criterion och 2003 eg bleu score ,0,1,0
the feature weights are learned by maximizing the bleu score papineni et al 2002 on heldout datausingminimumerrorratetrainingoch2003 as implemented by koehn ,0,1,0
it has a lower bound of 0 no upper bound better scores indicate better translations and it tends to be highly correlated with the adequacy of outputs mwer och 2003 or multiple word error rate is the edit distance in words between the system output and the closest reference translation in a set ,0,1,0
in the training phase bilingual parallel sentences are preprocessed and aligned using alignment algorithms or tools such as giza och and ney 2003 ,0,1,0
the weights for the various components of the model phrase translation model language model distortion model etc are set by minimum error rate training och 2003 ,0,1,0
the corresponding weight is trained through minimum error rate method och 2003 ,0,1,0
2003 bilingual sentences are trained by giza och and ney 2003 in two directions from source to target and target to source ,0,1,0
we used the preprocessed data to train the phrasebased translation model by using giza och and ney 2003 and the pharaoh tool kit koehn et al 2003 ,0,1,0
322 features we used eight features och and ney 2003 koehn et al 2003 and their weights for the translations ,0,1,0
target language model probability weight 05 according to a previous study the minimum error rate training mert och 2003 which is the optimization of feature weights by maximizing the bleu score on the development set can improve the performance of a system ,1,0,0
a comparison of the two approaches can be found in koehn och and marcu 2003 ,0,1,0
looking at the results of the recent machine translation evaluations this approach seems currently to give the best results and an increasing number of researchers are working on different methods for learning phrase translation lexica for machine translation purposes marcu and wong 2002 venugopal vogel and waibel 2003 tillmann 2003 koehn och and marcu 2003 ,0,1,0
an alternative training criterion therefore directly optimizes translation quality as measured by an automatic evaluation criterion och 2003 ,0,1,0
1993 and och and ney 2003 ,0,1,0
the alignment a j 1 that has the highest probability under a certain model is also called the viterbi alignment of that model a j 1 argmax a j 1 p f j 1 a j 1 e i 1 8 a detailed comparison of the quality of these viterbi alignments for various statistical alignment models compared to humanmade word alignments can be found in och and ney 2003 ,0,1,0
using this alignment strategy we follow och and ney 2003 and compute one alignment for each translation direction f e and e f and then combine them ,0,1,0
all our mt systems were trained using a variant of the alignment template model described in och 2003 ,0,1,0
it is also related to loglinear models described in berger della pietra and della pietra 1996 xue 2003 och 2003 and peng feng and mccallum 2004 ,0,1,0
furthermore statistical generation systems lapata 2003 barzilay and lee 2004 karamanis and manurung 2002 mellish et al 1998 could use as a means of directly optimizing information ordering much in the same way mt systems optimize model parameters using bleu as a measure of translation quality och 2003 ,0,1,0
nowadays most stateoftheart smt systems are based on bilingual phrases och tillmann and ney 1999 koehn och and marcu 2003 tillmann 2003 bertoldi et al 2004 vogel et al 2004 zens and ney 2004 chiang 2005 ,0,1,0
the model scaling factors 1 5 and the word and phrase penalties are optimized with respect to some evaluation criterion och 2003 such as bleu score ,0,1,0
the rules extracted from the training bitext have the following features a114 p andp the latter of which is not found in the noisychannel model but has been previously found to be a helpful feature och and ney 2002 210 chiang hierarchical phrasebased translation a114 the lexical weights p w andp w which estimate how well the words in translate the words in koehn och and marcu 2003 4 a114 a penalty exp1 for extracted rules analogous to koehns phrase penalty koehn 2003 which allows the model to learn a preference for longer or shorter derivations ,0,1,0
but koehn och and marcu 2003 find that phrases longer than three words improve performance little for training corpora of up to 20 million words suggesting that the data may be too sparse to learn longer phrases ,0,1,0
above the phrase level some models perform no reordering zens and ney 2004 kumar deng and byrne 2006 some have a simple distortion model that reorders phrases independently of their content koehn och and marcu 2003 och and ney 2004 and some for example the alignment template system och et al 2004 thayer et al 2004 hereafter ats and the ibm phrasebased system tillmann 2004 tillmann and zhang 2005 have phrasereordering models that add some lexical sensitivity ,0,1,0
phrases of up to 10 in length on the french side were extracted from the parallel text and minimumerrorrate training och 2003 was 8 we can train on the full training data shown if tighter constraints are placed on rule extraction for the united nations data ,0,1,0
other insights borrowed from the current state of the art include minimumerrorrate training of loglinear models och and ney 2002 och 2003 and use of an mgram language model ,1,0,0
some of the alignment sets also have links which are not sure links but are possible links och and ney 2003 ,0,1,0
we also have an additional heldout translation set the development set which is employed by the mt system to train the weights of its loglinear model to maximize bleu och 2003 ,0,1,0
the training data for the frenchenglish data set is taken from the ldc canadian hansard data set from which the word aligned data presented in och and ney 2003 was also taken ,0,1,0
294 fraser and marcu measuring word alignment quality for statistical machine translation 22 measuring translation performance changes caused by alignment in phrasedbased smt koehn och and marcu 2003 the knowledge sources which vary with the word alignment are the phrase translation lexicon which maps source phrases to target phrases using counts from the word alignment and some of the word level translation parameters sometimes called lexical smoothing ,0,1,0
to generate word alignments we use giza och and ney 2003 which implements both the ibm models of brown et al ,0,1,0
word alignment quality metrics 31 alignment error rate is not a useful measure we begin our study of metrics for word alignment quality by testing aer och and ney 2003 ,0,1,0
och and ney 2003 state that aer is derived from fmeasure ,0,1,0
however certain properties of the bleu metric can be exploited to speed up search as described in detail by och 2003 ,0,1,0
for all performance metrics we show the 70 confidence interval with respect to the map baseline computed using bootstrap resampling press et al 2002 och 2003 ,0,1,0
och 2003 developed a training procedure that incorporates various mt evaluation criteria in the training procedure of loglinear mt models ,0,1,0
the minimum error training och 2003 was used on the development data for parameter estimation ,0,1,0
six features from och 2003 were used as baseline features ,0,1,0
och 2003 described the use of minimum error training directly optimizing the error rate on automatic mt evaluation metrics such as bleu ,0,1,0
smt team 2003 also used minimum error training as in och 2003 but used a large number of feature functions ,0,1,0
by reranking a 1000best list generated by the baseline mt system from och 2003 the bleu papineni et al 2001 score on the test dataset was improved from 316 to 329 ,0,1,0
the model scaling factors are optimized on the development corpus with respect to mwer similar to och 2003 ,0,1,0
this method has the advantage that it is not limited to the model scaling factors as the method described in och 2003 ,0,0,1
alternatively one can train them with respect to the final translation quality measured by some error criterion och 2003 ,0,1,0
word alignments were produced by giza och and ney 2003 with a standard training regimen of five iterations of model 1 five iterations of the hmm model and five iterations of model 4 in both directions ,0,1,0
finally we trained model weights by maximizing bleu och 2003 and set decoder optimization parameters nbest list size timeouts 14 etc on a development test set of 200 heldout sentences each with a single reference translation ,0,1,0
we used the heuristic combination described in och and ney 2003 and extracted phrasal translation pairs from this combined alignment as described in koehn et al 2003 ,0,1,0
2 the problem of coverage in smt statistical machine translation made considerable advances in translation quality with the introduction of phrasebased translation marcu and wong 2002 koehn et al 2003 och and ney 2004 ,0,1,0
weights on the components were assigned using the och 2003 method for maxbleu training on the development set ,0,1,0
1 introduction defining scms the work presented here was done in the context of phrasebased mt koehn et al 2003 och and ney 2004 ,0,1,0
the parameters of the mt system were optimized on mteval02 data using minimum error rate training och 2003 ,0,1,0
in a later study och and ney 2003 present a loglinear combination of the hmm and ibm model 4 that produces better alignments than either of those ,1,0,0
1 introduction word alignmentdetection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation mt brown et al 1993 och and ney 2003 koehn et al 2003 but also has been shown useful for other applications such as construction of bilingual lexicons wordsense disambiguation projection of resources and crosslanguage information retrieval ,0,1,0
maximum entropy me models have been used in bilingual sense disambiguation word reordering and sentence segmentation berger et al 1996 parsing pos tagging and pp attachment ratnaparkhi 1998 machine translation och and ney 2002 and framenet classification fleischman et al 2003 ,0,1,0
number of words in target string these statistics are combined into a loglinear model whose parameters are adjusted by minimum error rate training och 2003 ,0,1,0
minimumerrorrate training was done using koehns implementation of ochs 2003 minimumerrorrate model ,0,1,0
2003 and component weights are adjusted by minimum error rate training och 2003 ,0,1,0
1 introduction recent approaches to statistical machine translation smt piggyback on the central concepts of phrasebased smt och et al 1999 koehn et al 2003 and at the same time attempt to improve some of its shortcomings by incorporating syntactic knowledge in the translation process ,0,1,0
this is also true for reranking and discriminative training where the kbest list of candidates serves as an approximation of the full set collins 2000 och 2003 mcdonald et al 2005 ,0,1,0
many methods for calculating the similarity have been proposed niessen et al 2000 akiba et al 2001 papineni et al 2002 nist 2002 leusch et al 2003 turian et al 2003 babych and hartley 2004 lin and och 2004 banerjee and lavie 2005 gimenez et al 2005 ,0,1,0
in recent years many researchers have tried to automatically evaluate the quality of mt and improve the performance of automatic mt evaluations niessen et al 2000 akiba et al 2001 papineni et al 2002 nist 2002 leusch et al 2003 turian et al 2003 babych and hartley 2004 lin and och 2004 banerjee and lavie 2005 gimenez et al 2005 because improving the performance of automatic mt evaluation is expected to enable us to use and improve mt systems efficiently ,0,1,0
for example och reported that the quality of mt results was improved by using automatic mt evaluation measures for the parameter tuning of an mt system och 2003 ,0,1,0
this type of direct optimization is known as minimum error rate training och 2003 in the mt community and is an essential component in building the stateofart mt systems ,1,0,0
2003 a trigram target language model an order model word count phrase count average phrase size functions and wholesentence ibm model 1 logprobabilities in both directions och et al 2004 ,0,1,0
the weights of these models are determined using the maxbleu method described in och 2003 ,0,1,0
most stateoftheart smt systems treat grammatical elements in exactly the same way as content words and rely on generalpurpose phrasal translations and target language models to generate these elements eg och and ney 2002 koehn et al 2003 quirk et al 2005 chiang 2005 galley et al 2006 ,0,1,0
unlike maxent training the method och 2003 used for estimating the weight vector for bleu maximization are not computationally scalable for a large number of feature functions ,0,1,0
the f are trained using a heldout corpus using maximum bleu training och 2003 ,0,1,0
the model parameters are trained using minimum errorrate training och 2003 ,0,1,0
in wasp giza och and ney 2003 is used to obtain the best alignments from the training examples ,0,1,0
the modified powells method has been previously used in optimizing the weights of a standard featurebased mt decoder in och 2003 where a more efficient algorithm for loglinear models was proposed ,1,0,0
if the alignments are not available they can be automatically generated eg using giza och and ney 2003 ,0,1,0
this is the shared task baseline system for the 2006 naaclhlt workshop on statistical machine translation koehn and monz 2006 and consists of the pharaoh decoder koehn 2004 srilm stolcke 2002 giza och and ney 2003 mkcls och 1999 carmel1 and a phrase model training code ,0,1,0
2 phrasebased smt we use a phrasebased smt system pharaoh koehn et al 2003 koehn 2004 which is based on a loglinear formulation och and ney 2002 ,0,1,0
to set the weights m we carried out minimum error rate training och 2003 using bleu papineni et al 2002 as the objective function ,0,1,0
the model scaling factors are optimized using minimum error rate training och 2003 ,0,1,0
parameters used to calculate pd are trained using mer training och 2003 on development data ,0,1,0
feature function weights in the loglinear model are set using ochs minium error rate algorithm och 2003 ,0,1,0
unsupervised systems och and ney 2003 liang et al 2006 are based on generative models trained with the em algorithm ,0,1,0
whilst the parameters for the maximum entropy model are developed based on the minimum error rate training method och 2003 ,0,1,0
finally to estimate the parameters i of the weighted linear model we adopt the popular minimum error rate training procedure och 2003 which directly optimizes translation quality as measured by the bleu metric ,1,0,0
met och 2003 was carried out using a development set and the bleu score evaluated on two test sets ,0,1,0
the way a decoder constructs translation hypotheses is directly related to the weights for different model features in a smt system which are usually optimized for a given set of models with minimum error rate training mert och 2003 to achieve better translation performance ,0,1,0
the models are trained using the margin infused relaxed algorithm or mira crammer et al 2006 instead of the standard minimumerrorrate training or mert algorithm och 2003 ,0,1,0
feature weights vector are trained discriminatively in concert with the language model weight to maximize the bleu papineni et al 2002 automatic evaluation metric via minimum error rate training mert och 2003 ,0,1,0
we obtain aligned parallel sentences and the phrase table after the training of moses which includes running giza och and ney 2003 growdiagonalfinal symmetrization and phrase extraction koehn et al 2005 ,0,1,0
to tune all lambda weights above we perform minimum error rate training och 2003 on the development set described in section 7 ,0,1,0
their weights are optimized wrt bleu score using the algorithm described in och 2003 ,0,1,0
standard met och 2003 iterative parameter estimation under ibm bleu papineni et al 2001 is performed on the corresponding development set ,0,1,0
the component features are weighted to minimize a translation error criterion on a development set och 2003 ,0,1,0
3 experiments we built baseline systems using giza och and ney 2003 moses phrase extraction with growdiagfinalend heuristic koehn et al 2007 a standard phrasebased decoder vogel 2003 the sri lm toolkit stolcke 2002 the suffixarray language model zhang and vogel 2005 a distancebased word reordering model algorithm 5 rich interruption constraints coh5 input source tree t previous phrase fh current phrase fh1 coverage vector hc 1 interruption false 2 icountverbcountnouncount 0 3 f the left and rightmost tokens of fh 4 for each of f f do 5 climb the dependency tree from f until you reach the highest node n such that fh1 tn ,0,1,0
all model weights were trained on development sets via minimumerror rate training mert och 2003 with 200 unique nbest lists and optimizing toward bleu ,0,1,0
starting from a nbest list generated from a translation decoder an optimizer such as minimum error rate mer och 2003 training proposes directions to search for a better weightvector to combine feature functions ,0,1,0
it is also related to loglinear models for machine translation och 2003 ,0,1,0
for each feature function there is a model parameter i the best word segmentation w is determined by the decision rule as m i ii w m w wsfwsscorew 0 0 maxargmaxarg 2 below we describe how to optimize s our method is a discriminative approach inspired by the minimum error rate training method proposed in och 2003 ,0,1,0
1 introduction with the introduction of the bleu metric for machine translation evaluation papineni et al 2002 the advantages of doing automatic evaluation for various nlp applications have become increasingly appreciated they allow for faster implementevaluate cycles by bypassing the human evaluation bottleneck less variation in evaluation performance due to errors in human assessor judgment and not least the possibility of hillclimbing on such metrics in order to improve system performance och 2003 ,0,1,0
we ran the trainer with its default settings maximum phrase length 7 and then used koehns implementation of minimumerrorrate training och 2003 to tune the feature weights to maximize the systems bleu score on our development set yielding the values shown in table 2 ,0,1,0
above the phrase level these models typically have a simple distortion model that reorders phrases independently of their content och and ney 2004 koehn et al 2003 or not at all zens and ney 2004 kumar et al 2005 ,0,1,0
for our experiments we used the following features analogous to pharaohs default feature set p and p the latter of which is not found in the noisychannel model but has been previously found to be a helpful feature och and ney 2002 the lexical weights pw and pw koehn et al 2003 which estimate how well the words in translate the words in 2 a phrase penalty exp1 which allows the model to learn a preference for longer or shorter derivations analogous to koehns phrase penalty koehn 2003 ,0,1,0
2003 which is based on that of och and ney 2004 ,0,1,0
to do this we first identify initial phrase pairs using the same criterion as previous systems och and ney 2004 koehn et al 2003 definition 1 ,0,1,0
we used giza package och and ney 2003 to train ibm translation models ,0,1,0
after that we used three types of methods for performing a symmetrization of ibm models intersection union and refined methods och and ney 2003 ,0,1,0
studies reveal that statistical alignment models outperform the simple dice coefficient och and ney 2003 ,1,0,0
it is promising to optimize the model parameters directly with respect to aer as suggested in statistical machine translation och 2003 ,1,0,0
och and ney 2003 proposed model 6 a loglinear combination of ibm translation models and hmm model ,0,1,0
in practice when training the parameters of an smt system for example using the discriminative methods of och 2003 the cost for skips of this kind is typically set to a very high value ,0,1,0
for this reason there is currently a great deal of interest in methods which incorporate syntactic information within statistical machine translation systems eg see alshawi 1996 wu 1997 yamada and knight 2001 gildea 2003 melamed 2004 graehl and knight 2004 och et al 2004 xia and mccord 2004 ,0,1,0
more recently phrasebased models och et al 1999 marcu and wong 2002 koehn et al 2003 have been proposed as a highly successful alternative to the ibm models ,0,1,0
reranking methods have also been proposed as a method for using syntactic information koehn and knight 2003 och et al 2004 shen et al 2004 ,0,1,0
1 introduction recent research on statistical machine translation smt has lead to the development of phrasebased systems och et al 1999 marcu and wong 2002 koehn et al 2003 ,0,1,0
instead of directly minimizing error as in earlier work och 2003 we decompose the decoding process into a sequence of local decision steps based on eq ,0,1,0
as far as the loglinear combination of float features is concerned similar training procedures have been proposed in och 2003 ,0,1,0
decoding weights are optimized using ochs algorithm och 2003 to set weights for the four components of the loglinear model language model phrase translation model distortion model and wordlength feature ,0,1,0
2 related work starting with the ibm models brown et al 1993 researchers have developed various statistical word alignment systems based on different models such as hidden markov models hmm vogel et al 1996 loglinear models och and ney 2003 and similaritybased heuristic methods melamed 2000 ,0,1,0
mt output was evaluated using the standard evaluation metric bleu papineni et al 20022 the parameters of the mt system were optimized for bleu metric on nist mteval2002 test sets using minimum error rate training och 2003 and the systems were tested on nist mteval2003 test sets for both languages ,0,1,0
several nonlinear objective functions such as fscore for text classification gao et al 2003 and bleuscore and some other evaluation measures for statistical machine translation och 2003 have been introduced with reference to the framework of mce criterion training ,0,1,0
nbest results for phrasal alignment and ordering models in the decoder were optimized by lambda training via maximum bleu along the lines described in och 2003 ,0,1,0
one is distortion model och and ney 2004 koehn et al 2003 which penalizes translations according to their jump distance instead of their content ,0,1,0
line 4 and 5 are similar to the phrase extraction algorithm by och 2003b ,0,1,0
the kbest list is very important for the minimum error rate training och 2003a which is used for tuning the weights for our model ,0,1,0
51 pharaoh the baseline system we used for comparison was pharaoh koehn et al 2003 koehn 2004 a freely available decoder for phrasebased translation models pef pfe plmelm pdefd lengthewe 10 we ran giza och and ney 2000 on the training corpus in both directions using its default setting and then applied the refinement rule diagand described in koehn et al 2003 to obtain a single manytomany word alignment for each sentence pair ,0,1,0
to perform minimum error rate training och 2003 to tune the feature weights to maximize the systems bleu score on development set we used optimizev5ibmbleum venugopal and vogel 2005 ,0,1,0
1 introduction phrasebased translation models marcu and wong 2002 koehn et al 2003 och and ney 2004 which go beyond the original ibm translation models brown et al 1993 1 by modeling translations of phrases rather than individual words have been suggested to be the stateoftheart in statistical machine translation by empirical evaluations ,1,0,0
the current approach does not use specialized probability features as in och 2003 in any stage during decoder parameter training ,0,1,0
the novel algorithm differs computationally from earlier work in discriminative training algorithms for smt och 2003 as follows a90 no computationally expensive a57 best lists are generated during training for each input sentence a single block sequence is generated on each iteration over the training data ,0,1,0
although the training algorithm can handle realvalued features as used in och 2003 tillmann and zhang 2005 the current paper intentionally excludes them ,0,1,0
we tuned pharaohs four parameters using minimum error rate training och 2003 on dev12 we obtained an increase of 08 9as in the pos features we map each phrase pair to its majority constellation ,0,1,0
the first approach is to reuse the components of a generative model but tune their relative weights in a discriminative fashion och and ney 2002 och 2003 chiang 2005 ,0,1,0
however union and rened alignments which are manytomany are what are used to build competitive phrasal smt systems because intersection performs poorly despite having been shown to have the best aer scores for the frenchenglish corpus we are using och and ney 2003 ,0,1,0
for each training direction we run giza och and ney 2003 specifying 5 iterations of model 1 4 iterations of the hmm model vogel et al 1996 and 4 iterations of model 4 ,0,1,0
1 introduction the most widely applied training procedure for statistical machine translation ibm model 4 brown et al 1993 unsupervised training followed by postprocessing with symmetrization heuristics och and ney 2003 yields low quality word alignments ,0,0,1
och 2003 has described an ef cient exact onedimensional error minimization technique for a similar search problem in machine translation ,1,0,0
feature function scaling factors m are optimized based on a maximum likely approach och and ney 2002 or on a direct error minimization approach och 2003 ,0,1,0
manytomany word alignments are induced by running a onetomany word alignment model such as giza och and ney 2003 in both directions and by combining the results based on a heuristic koehn et al 2003 ,0,1,0
when evaluated against the stateoftheart phrasebased decoder pharaoh koehn 2004 using the same experimental conditions translation table trained on the fbis corpus 72m chinese words and 92m english words of parallel text trigram language model trained on 155m words of english newswire interpolation weights a65 equation 2 trained using discriminative training och 2003 on the 2002 nist mt evaluation set probabilistic beam a90 set to 001 histogram beam a58 set to 10 and bleu papineni et al 2002 as our metric the widlnglmaa86 a129 algorithm produces translations that have a bleu score of 02570 while pharaoh translations have a bleu score of 02635 ,0,1,0
the interpolation weights a65 equation 2 are trained using discriminative training och 2003 using rougea129 as the objective function on the development set ,0,1,0
in the postediting step a prediction engine helps to decrease the amount of human interaction och et al 2003 ,0,1,0
for instance the resulting word graph can be used in the prediction engine of a cat system och et al 2003 ,0,1,0
to find the optimal coefficients for a loglinear combination of these experts we use separate development data using the following procedure due to och 2003 1 ,0,1,0
despite these difficulties some work has shown it worthwhile to minimize error directly och 2003 bahl et al 1988 ,0,1,0
och 2003 found that such smoothing during training gives almost identical results on translation metrics ,0,1,0
the solution we employ here is the discriminative training procedure of och 2003 ,0,1,0
there are two necessary ingredients to implement ochs 2003 training procedure ,0,1,0
in contrast more recent research has focused on stochastic approaches that model discourse coherence at the local lexical lapata 2003 and global levels barzilay and lee 2004 while preserving regularities recognized by classic discourse theories barzilay and lapata 2005 ,0,1,0
to perform translation stateoftheart mt systems use a statistical phrasebased approach marcu and wong 2002 koehn et al 2003 och and ney 2004 by treating phrases as the basic units of translation ,0,1,0
61 hiero results using the mt 2002 test set we ran the minimumerror rate training mert och 2003 with the decoder to tune the weights for each feature ,0,1,0
the nist mt03 test set is used for development particularly for optimizing the interpolation weights using minimum error rate training och 2003 ,0,1,0
firstly rather than induce millions of xrs rules from parallel data we extract phrase pairs in the standard way och ney 2003 and associate with each phrasepair a set of target language syntactic structures based on supertag sequences ,0,1,0
the bidirectional word alignment is used to obtain phrase translation pairs using heuristics presented in 2httpwwwfjochcomgizahtml 289 och ney 2003 and koehn et al 2003 and the moses decoder was used for phrase extraction and decoding3 let t and s be the target and source language sentences respectively ,0,1,0
the bidirectional word alignmentisusedtoobtainlexicalphrasetranslationpairs using heuristics presented in och ney 2003 and koehn et al 2003 ,0,1,0
running words 1864 14437 vocabulary size 569 1081 table 2 chineseenglish corpus statistics och 2003 using phramer olteanu et al 2006 a 3gram language model with kneserney smoothing trained with srilm stolcke 2002 on the english side of the training data and pharaoh koehn 2004 with default settings to decode ,0,1,0
 there is want to need not in front of as soon as look at figure 2 examples of entries from the manually developed dictionary 4 experimental setting 41 evaluation the intrinsic quality of word alignment can be assessed using the alignment error rate aer metric och and ney 2003 that compares a systems alignment output to a set of goldstandard alignment ,0,1,0
the same powells method has been used to estimate feature weights of a standard featurebased phrasal mt decoder in och 2003 ,0,1,0
in matusov et al 2006 different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using giza och and ney 2003 ,0,1,0
we present two approaches to smtbased query expansion both of which are implemented in the framework of phrasebased smt och and ney 2004 koehn et al 2003 ,0,1,0
4 smtbased query expansion our smtbased query expansion techniques are based on a recent implementation of the phrasebased smt framework koehn et al 2003 och and ney 2004 ,0,1,0
as follows psyni1trgi1 iproductdisplay i1 psynitrgi 4 pprimetrgisyniprime pwsynitrgiw pwprimetrgisyniwprime pdsynitrgid lwsyni1l csyni1c plmsyni1lm for estimation of the feature weights vector defined in equation 4 we employed minimum error rate mer training under the bleu measure och 2003 ,0,1,0
to perform minimum error rate training och 2003 to tune the feature weights to maximize the systems bleu score on development set we used the script optimizev5ibmbleum venugopal and vogel 2005 ,0,1,0
all the feature weights s were trained using our implementation of minimum error rate training och 2003 ,0,1,0
we use the stanford parser klein and manning 2003 with its default chinese grammar the giza och and ney 2000 alignment package with its default settings and the me tool developed by zhang 2004 ,0,1,0
the parameters j were trained using minimum error rate training och 2003 to maximise the bleu score papineni et al 2002 on a 150 sentence development set ,0,1,0
the translation models and lexical scores were estimated on the training corpus whichwasautomaticallyalignedusinggizaoch et al 1999 in both directions between source and target and symmetrised using the growing heuristic koehn et al 2003 ,0,1,0
a single translation is then selected by finding the candidate that yields the best overall score och and ney 2001 utiyama and isahara 2007 or by cotraining callisonburch and osborne 2003 ,0,1,0
as an alternative to linear interpolation we also employ a weighted product for phrasetable combination pst productdisplay j pjstj 3 this has the same form used for loglinear training of smt decoders och 2003 which allows us to treateachdistributionasafeatureandlearnthemixing weights automatically ,0,1,0
1 introduction for statistical machine translation smt phrasebased methods koehn et al 2003 och and ney 2004 and syntaxbased methods wu 1997 alshawi et al 2000 yamada and knignt 2001 melamed 2004 chiang 2005 quick et al 2005 mellebeek et al 2006 outperform wordbased methods brown et al 1993 ,0,1,0
we run the decoder with its default settings and then use koehns implementation of minimum error rate training och 2003 to tune the feature weights on the development set ,0,1,0
we want to avoid training a metric that as5or in a less adversarial setting a system may be performing minimum errorrate training och 2003 signs a higher than deserving score to a sentence that just happens to have many ngram matches against the targetlanguage reference corpus ,0,1,0
metrics in the rouge family allow for skip ngrams lin and och 2004a kauchak and barzilay 2006 take paraphrasing into account metrics such as meteor banerjee and lavie 2005 and gtm melamed et al 2003 calculate both recall and precision meteor is also similar to sia liu and gildea 2006 in that word class information is used ,0,1,0
the model scaling factors m1 are optimized with respect to the bleu score as described in och 2003 ,0,1,0
it also contains tools for tuning these models using minimum error rate training och 2003 and evaluating the resulting translations using the bleu score papineni et al 2002 ,0,1,0
moses uses standard external tools for some of the tasks to avoid duplication such as giza och and ney 2003 for word alignments and srilm for language modeling ,0,1,0
1 introduction raw parallel data need to be preprocessed in the modern phrasebased smt before they are aligned by alignment algorithms one of which is the wellknown tool giza och and ney 2003 for training ibm models 14 ,1,0,0
word alignments are provided by giza och and ney 2003 with growdiagfinal combination with infrastructure for alignment combination and phrase extraction provided by the shared task ,0,1,0
candidate translations are scored by a linear combination of models weighted according to minimum error rate training or mert och 2003 ,0,1,0
early experiments with syntacticallyinformed phrases koehn et al 2003 and syntactic reranking of kbest lists och et al 2004 produced mostly negative results ,0,1,0
we also trained a baseline model with giza och and ney 2003 following a regimen of 5 iterations of model 1 5 iterations of hmm and 5 iterations of model 4 ,0,1,0
minimum error rate training och 2003 over bleu was used to optimize the weights for each of these models over the development test data ,0,1,0
we use the standard minimum errorrate training och 2003 to tune the feature weights to maximize the systems bleu score on the dev set ,0,1,0
however while discriminative models promise much they have not been shown to deliver significant gains 1we class approaches using minimum error rate training och 2003 frequency count based as these systems rescale a handful of generative features estimated from frequency counts and do not support large sets of nonindependent features ,0,1,0
once we obtain the augmented phrase table we should run the minimumerrorrate training och 2003 with the augmented phrase table such that the model parameters are properly adjusted ,0,1,0
the feature functions are combined under a loglinear framework andtheweights aretuned bytheminimumerrorrate training och 2003 using bleu papineni et al 2002 as the optimization metric ,0,1,0
452 bleu on nist mt test sets we use mt02 as the development set4 for minimum error rate training mert och 2003 ,0,1,0
the features are similar to the ones used in phrasal systems and their weights are trained using maxbleu training och 2003 ,0,1,0
for the mer training och 2003 we modified koehns mer trainer koehn 2004 for our tree sequencebased system ,0,1,0
1 introduction phrasebased modeling method koehn et al 2003 och and ney 2004a is a simple but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well ,0,1,0
given sentencealigned bilingual training data we first use giza och and ney 2003 to generate word level alignment ,0,1,0
following och 2003 the kbest results are accumulated as the input of the optimizer ,0,1,0
hierarchical rules were extracted from a subset which has about 35m41m words5 and the rest of the training data were used to extract phrasal rules as in och 2003 chiang 2005 ,0,1,0
the weights are trained using minimum error rate training och 2003 with bleu score as the objective function ,0,1,0
a greek model was trained on 440082 aligned sentences of europarl v3 tuned with minimum error training och 2003 ,0,1,0
to obtain their corresponding weights we adapted the minimumerrorrate training algorithm och 2003 to train the outsidelayer model ,0,1,0
each i is a weight associated with feature i and these weights are typically optimized using minimum error rate training och 2003 ,0,1,0
this shows that hypothesis features are either not discriminative enough or that the reranking model is too weak this performance gap can be mainly attributed to two problems optimization error and modeling error see figure 11 much work has focused on developing better algorithms to tackle the optimization problem eg mert och 2003 since mt evaluation metrics such as bleu and per are riddled with local minima and are difficult to differentiate with respect to reranker parameters ,0,1,0
we perform minimumerrorrate training och 2003 to tune the feature weights of the translation model to maximize the bleu score on development set ,0,1,0
sp and pt are feature weights set by performing minimum error rate training as described in och 2003 ,0,1,0
two popular techniques that incorporate the error criterion are minimum error rate training mert och 2003 and minimum bayesrisk mbr decoding kumar and byrne 2004 ,1,0,0
a path in a translation hypergraph induces a translation hypothesis e along with its sequence of scfg rules d r1r2rk which if applied to the start symbol derives e the sequence of scfg rules induced by a path is also called a derivation tree for e 3 minimum error rate training given a set of source sentences fs1 with corresponding reference translations rs1 the objective of mert is to find a parameter set m1 which minimizes an automated evaluation criterion under a linear model m1 argmin m1 sx s1 errrs efs m1 ff efs m1 argmax e sx s1 mhme fs ff in the context of statistical machine translation the optimization procedure was first described in och 2003 for nbest lists and later extended to phraselattices in macherey et al ,0,1,0
for the mer training och 2003 koehns mer trainer koehn 2007 is modified for our system ,0,1,0
ueffing et al 2007 haffari et al 2009 show that treating u as a source for a new feature function in a loglinear model for smt och and ney 2004 allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum errorrate training mert och 2003 ,0,1,0
these constituent matchingviolation counts are used as a feature in the decoders loglinear model and their weights are tuned via minimal error rate training mert och 2003 ,0,1,0
the model was trained using minimum error rate training for arabic och 2003 and mira for chinese chiang et al 2008 ,0,1,0
4 extended minimum error rate training minimum error rate training och 2003 is widely used to optimize feature weights for a linear model och and ney 2002 ,1,0,0
instead of computing all intersections och 2003 only computes critical intersections where highestscore translations will change ,0,1,0
we obtained word alignments of training data by first running giza och and ney 2003 and then applying the refinement rule growdiagfinaland koehn et al 2003 ,0,1,0
 as multiple derivations are used for finding optimal translations we extend the minimum error rate training mert algorithm och 2003 to tune feature weights with respect to bleu score for maxtranslation decoding section 4 ,0,1,0
while they train the parameters using a maximum a posteriori estimator we extend the mert algorithm och 2003 to take the evaluation metric into account ,0,1,0
in the geometric interpolation above the weight n controls the relative veto power of the ngram approximation and can be tuned using mert och 2003 or a minimum risk procedure smith and eisner 2006 ,0,1,0
the nist mt03 set is used to tune model weights eg those of 16 and the scaling factor 17we have also experimented with mert och 2003 and found that the deterministic annealing gave results that were more consistent across runs and often better ,1,0,0
parametertuningwasdonewithminimum error rate training och 2003 which was used to maximize bleu papineni et al 2001 ,0,1,0
1 introduction hierarchical approaches to machine translation have proven increasingly successful in recent years chiang 2005 marcu et al 2006 shen et al 2008 and often outperform phrasebased systems och and ney 2004 koehn et al 2003 on targetlanguage fluency and adequacy ,0,0,1
we use the giza implementation of ibm model 4 brown et al 1993 och and ney 2003 coupled with the phrase extraction heuristics of koehn et al ,0,1,0
the parameters of the nist systems were tuned using ochs algorithm to maximize bleu on the mt02 test set och 2003 ,0,1,0
tuning learning the values discussed in section 41 was done using minimum error rate training och 2003 ,0,1,0
36 parameter estimation to estimate parameters k1 k k lm and um we adopt the approach of minimum error rate training mert that is popular in smt och 2003 ,1,0,0
the weights of feature functions are optimized to maximize the scoring measure och 2003 ,0,1,0
2006 2008 proposed using giza och and ney 2003 to align words between the backbone and hypothesis ,0,1,0
uses for kbest lists include minimum bayes risk decoding goodman 1998 kumar and byrne 2004 discriminative reranking collins 2000 charniak and johnson 2005 and discriminative training och 2003 mcclosky et al 2006 ,0,1,0
we also use minimum errorrate training och 2003 to tune our feature weights ,0,1,0
2003 grow the set of word links by appending neighboring points while och and hey 2003 try to avoid both horizontal and vertical neighbors ,0,1,0
we train ibm model4 using giza toolkit och and ney 2003 in two translation directions and perform different word alignment combination ,0,1,0
the next two methods are heuristic h in och and ney 2003 and growdiagonal gd proposed in koehn et al 2003 ,0,1,0
we tune all feature weights automatically och 2003 to maximize the bleu papineni et al 2002 score on the dev set ,0,1,0
by having the advantage of leveraging large parallel corpora the statistical mt approach outperforms the traditional transfer based approaches in tasks for which adequate parallel corpora is available och 2003 ,1,0,0
we applied the union intersection and refined symmetrization metrics och and ney 2003 to the final alignments output from training as well as evaluating the two final alignments directly ,0,1,0
we wish to minimize this error function so we select accordingly argmin summationdisplay a eaa argmax a pa fe 4 maximizing performance for all of the weights at once is not computationally tractable but och 2003 has described an efficient onedimensional search for a similar problem ,1,0,0
the discriminative training regimen is otherwise similar to och 2003 ,0,1,0
the system used for baseline experiments is two runs of ibm model 4 brown et al 1993 in the giza och and ney 2003 implementation which includes smoothing extensions to model 4 ,0,1,0
for symmetrization we found that och and neys refined technique described in och and ney 2003 produced the best aer for this data set under all experimental conditions ,1,0,0
the field of statistical machine translation has been blessed with a long tradition of freely available software tools such as giza och and ney 2003 and parallel corpora such as the canadian hansards2 ,1,0,0
in addition we also made a word alignment available which was derived using a variant of the current default method for word alignment och and ney 2003s refined method ,0,1,0
2004 better languagespecific preprocessing koehn and knight 2003 and restructuring collins et al 2005 additional feature functions such as word class language models and minimum error rate training och 2003 to optimize parameters ,0,1,0
s to set weights on the components of the loglinear model we implemented ochs algorithm och 2003 ,0,1,0
koehn et al 2003 och 2003 ,0,1,0
more details on these standard criteria can be found for instance in och 2003 ,0,1,0
och et al 2003 ,0,1,0
the model scaling factors are optimized with respect to some evaluation criterion och 2003 ,0,1,0
21 minimum error rate training the predominant approach to reconciling the mismatch between the map decision rule and the evaluation metric has been to train the parameters of the exponential model to correlate the map choice with the maximum score as indicated by the evaluation metric on a development set with known references och 2003 ,0,1,0
in the following we summarize the optimization algorithm for the unsmoothed error counts presented in och 2003 and the implementation detailed in venugopal and vogel 2005 ,0,1,0
the translations were generated by the alignment template system of och 2003 ,0,1,0
in the area of statistical machine translation smt recently a combination of the bleu evaluation metric papineni et al 2001 and the bootstrap method for statistical significance testing efron and tibshirani 1993 has become popular och 2003 kumar and byrne 2004 koehn 2004b zhang et al 2004 ,0,1,0
our system is a reimplementation of the phrasebased system described in koehn 2003 and uses publicly available components for word alignment och and ney 20031 decoding koehn 2004a2 language modeling stolcke 20023 and finitestate processing knight and alonaizan 19994 ,0,1,0
for example och 2003 shows how to train a loglinear translation model not by maximizing the likelihood of training data but maximizing the bleu score among other metrics of the model on 53 the data ,0,1,0
the weights of the models are computed automatically using a variant of the maximum bleu training procedure proposed by och 2003 ,0,1,0
the decoder is capable of producing nbest derivations and nbest lists knight and graehl 2005 which are used for maximum bleu training och 2003 ,0,1,0
we concatenate the lists and we learn a new combination of weights that maximizes the bleu score of the combined nbest list using the same development corpus we used for tuning the individual systems och 2003 ,0,1,0
1 introduction during the last four years various implementations and extentions to phrasebased statistical models marcu and wong 2002 koehn et al 2003 och and ney 2004 have led to significant increases in machine translation accuracy ,1,0,0
to model ptas we use a standard loglinear approach ptas exp bracketleftbiggsummationdisplay i ifista bracketrightbigg where each fista is a feature function and weights i are set using ochs algorithm och 2003 to maximize the systems bleu score papineni et al 2001 on a development corpus ,0,1,0
in fact a limitation of the experiments described in this paper is that the loglinear weights for the glassbox techniques were optimized for bleu using ochs algorithm och 2003 while the linear weights for 55 blackbox techniques were set heuristically ,0,1,0
the weights for these models are determined using the method described in och 2003 ,0,1,0
furthermore endtoend systems like speech recognizers roark et al 2004 and automatic translators och 2003 use increasingly sophisticated discriminative models which generalize well to new data that is drawn from the same distribution as the training data ,1,0,0
alternatively one can train them with respect to the final translation quality measured by an error criterion och 2003 ,0,1,0
the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion och 2003 ,0,1,0
the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion och 2003 ,0,1,0
we train ibm model 4 with giza och and ney 2003 in both translation directions ,0,1,0
then the alignments are symmetrized using a refined heuristic as described in och and ney 2003 ,0,1,0
the model scaling factors m1 are trained with respect to the final translation quality measured by an error criterion och 2003 ,0,1,0
feature function scaling factors m are optimized based on a maximum likelihood approach och and ney 2002 or on a direct error minimization approach och 2003 ,0,1,0
first manytomany word alignments are induced by running a onetomany word alignment model such as giza och and ney 2003 in both directions and by combining the results based on a heuristic och and ney 2004 ,0,1,0
given a source sentence f the preferred translation output is determined by computing the lowestcost derivation combination of hierarchical and glue rules yielding f as its source side where the cost of a derivation r1 rn with respective feature vectors v1vn rm is given by msummationdisplay i1 i nsummationdisplay j1 vji here 1m are the parameters of the loglinear model which we optimize on a heldout portion of the training set 2005 development data using minimumerrorrate training och 2003 ,0,1,0
the software also required giza word alignment tooloch and ney 2003 ,0,1,0
in this paper we present phramer an opensource system that embeds a phrasebased decoder a minimum error rate training och 2003 module and various tools related to machine translation mt ,0,1,0
the size of the development set used to generate 1 and 2 1000 sentences compensates the tendency of the unsmoothed mert algorithm to overfit och 2003 by providing a high ratio between number of variables and number of parameters to be estimated ,0,0,1
feature weights of both systems are tuned on the same data set3 for pharaoh we use the standard minimum errorrate training och 2003 and for our system since there are only two independent features as we always fix 1 we use a simple gridbased lineoptimization along the languagemodel weight axis ,0,1,0
2 previous work it is helpful to compare this approach with recent efforts in statistical mt phrasebased models koehn et al 2003 och and ney 2004 are good at learning local translations that are pairs of consecutive substrings but often insufficient in modeling the reorderings of phrases themselves especially between language pairs with very different wordorder ,0,1,0
alternatively one can train them with respect to the final translation quality measured by an error criterion och 2003 ,0,1,0
here we train word alignments in both directions with giza och and ney 2003 ,0,1,0
we report precision recall and balanced fmeasure och and ney 2003 ,0,1,0
weights for the loglinear model are set using the 500sentence tuning set provided for the shared task with minimum error rate training och 2003 as implemented by venugopal and vogel 2005 ,0,1,0
the surface heuristic can define consistency according to any word alignment but most often the alignment is provided by giza och and ney 2003 ,0,1,0
manytomany alignments can be created by combining two giza alignments one where english generates foreign and another with those roles reversed och and ney 2003 ,0,1,0
different optimization techniques are available like the simplex algorithm or the special minimum error training as described in och 2003 ,0,1,0
the comparison phrasal system was constructed using the same giza alignments and the heuristic combination described in och ney 2003 ,0,1,0
the factored translation model combines features in a loglinear fashion och 2003 ,0,1,0
weights on the loglinear features are set using ochs algorithm och 2003 to maximize the systems bleu score on a development corpus ,0,1,0
we use the nbest generation scheme interleaved with optimization as described in och 2003 ,0,1,0
73 224 minimum error rate training a good way of training is to minimize empirical top1 error on training data och 2003 ,1,0,0
1 introduction in recent years statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation papineni et al 2002 and errorbased optimization och 2003 ,1,0,0
in the experiment only the first 500 sentences were used to train the loglinear model weight vector where minimum error rate mer training was used och 2003 ,0,1,0
still a confidence range for bleu can be estimated by bootstrapping och 2003 zhang and vogel 2004 ,0,1,0
the feature weights for the overall translation models were trained using ochs 2003 minimumerrorrate training procedure ,0,1,0
och 2003 introduced minimum error rate training mert a technique for optimizing loglinear modelparametersrelativetoameasureoftranslation quality ,0,1,0
initial phrase pairs are identified following the procedure typically employed in phrase based systems koehn et al 2003 och and ney 2004 ,0,1,0
oncetraininghastakenplaceminimumerrorrate training och 2003 is used to tune the parameters i finally decoding in hiero takes place using a cky synchronous parser with beam search augmented to permit efficient incorporation of language model scores chiang 2007 ,0,1,0
to model ptas we use a standard loglinear approach ptas exp bracketleftbiggsummationdisplay i ifista bracketrightbigg 1 where each fista is a feature function and weights i are set using ochs algorithm och 2003 to maximize the systems bleu score papineni et al 2001 on a development corpus ,0,1,0
their weights are optimized wrt bleu score using the algorithm described in och 2003 ,0,1,0
3see httpwwwstatmtorgmoses 194 4 implementation details 41 alignment of mt output the input text and the output text of the mt systems was aligned by means of giza och and ney 2003 a tool with which statistical models for alignment of parallel texts can be trained ,0,1,0
to optimize the system towards a maximal bleu or nist score we use minimum error rate mer training as described in och 2003 ,0,1,0
feature weight tuning was carried out using minimum error rate training maximizing bleu scores on a heldout development set och 2003 ,0,1,0
unfortunately longer sentences up to 100 tokens rather than 40 longer phrases up to 10 tokens rather than 7 two lms rather than just one higherorder lms order 7 rather than 3 multiple higherorder lexicalized reordering models up to 3 etc all contributed to increased systems complexity and as a result time limitations prevented us from performing minimumerrorrate training mert och 2003 for ucb3 ucb4 and ucb5 ,0,0,1
the feature weights i are trained in concert with the lm weight via minimum error rate mer training och 2003 ,0,1,0
bleu is fast and easy to run and it can be used as a target function in parameter optimization training procedures that are commonly used in stateoftheart statistical mt systems och 2003 ,1,0,0
31 evaluation measure and mert we evaluate our experiments using the lowercase tokenized bleu metric and estimate the empirical confidence using the bootstrapping method described in koehn 2004b6 we report the scores obtained on the test section with model parameters tuned using the tuning section for minimum error rate training mert och 2003 ,0,1,0
we also plan to employ this evaluation metric as feedback in building dialogue coherence models as is done in machine translation och 2003 ,0,1,0
minimum errorrate mer training och 2003 was applied to obtain weights m in equation 2 for these features ,0,1,0
the weights 1m are typically learned to directly minimize a standard evaluation criterion on development data eg the bleu score papineni et al 2002 using numerical search och 2003 ,0,1,0
the mixture coefficients are trained in the usual way minimum errorrate training och 2003 so that the additional context is exploited when it is useful and ignored when it isnt the paper proceeds as follows ,0,1,0
to combine the many differentlyconditioned features into a single model we provide them as features to the linear model equation 2 and use minimum errorrate training och 2003 to obtain interpolation weights m this is similar to an interpolation of backedoff estimates if we imagine that all of the different contextsaredifferentlybackedoffestimatesofthe complete context ,0,1,0
och 2003 claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective o lscript ,0,1,0
2003 of running giza och ney 2003 in both directions and then merging the alignments using the growdiagfinal heuristic ,0,1,0
the first is a novel stochastic search strategy that appears to make better use of och 2003s algorithm for finding the global minimum along any given search direction than either coordinate descent or powells method ,0,1,0
however by exploiting the fact that the underlying scores assigned to competing hypotheses wehf vary linearly wrt changes in the weight vector w och 2003 proposed a strategy for finding the global minimum along any given search direction ,0,1,0
1 introduction och 2003 introduced minimum error rate training mert as an alternative training regime to the conditional likelihood objective previously used with loglinear translation models och ney 2002 ,0,1,0
this is seen in that each time we check for the nearest intersection to the current 1best for some nbest list l we algorithm 1 och 2003s line search method to find the global minimum in the loss lscript when starting at the point w and searching along the direction d using the candidate translations given in the collection of nbest lists l input l w d lscript i for l l do for e l do me efeatures d be efeatures w end for bestn argmaxel mebe breaks ties loop bestn1 argminel max parenleftbig 0 bbestnbemembestn parenrightbig intercept max parenleftbig 0 bbestnbbestn1mbestn1mbestn parenrightbig if intercept 0 then addi intercept else break end if end loop end for addi maxi2epsilon1 ibest argminii evallscriptlwiepsilon1d return wibest epsilon1d must calculate its intersection with all other candidate translations that have yet to be selected as the 1best ,0,1,0
while the former is piecewise constant and thus cannot be optimized using gradient techniques och 2003 provides an approach that performs such training efficiently ,1,0,0
moses uses standard external tools for some of these tasks such as giza och and ney 2003 for word alignments and srilm stolcke 2002 for language modeling ,0,1,0
we show that link 1for a complete discussion of alignment symmetrization heuristics including union intersection and refined refer to och and ney 2003 ,0,1,0
giza och and ney 2003 an implementation of the ibm brown et al 1993 and hmm ,0,1,0
after maximum bleu tuning och 2003a on a heldout tuning set we evaluate translation quality on a heldout test set ,0,1,0
32 evaluation metrics aer alignment error rate och and ney 2003 is the most widely used metric of alignment quality but requires goldstandard alignments labelled with surepossible annotations to compute lacking such annotations we can compute alignment fmeasure instead ,0,0,1
the feature weights are tuned using minimum error rate training och and ney 2003 to optimize bleu score on a heldout development set ,0,1,0
the word alignments were created with giza och and ney 2003 applied to a parallel corpus containing the complete europarl training data plus sets of 4051 sentence pairs created by pairing the test sentences with the reference translations and the test sentences paired with each of the system translations ,0,1,0
a large database of human judgments might also be useful as an objective function for minimum error rate training och 2003 or in other system development tasks ,0,1,0
translation systems och and ney 2004 koehn et al 2003 and use moses koehn et al 2007 to search for the best target sentence ,0,1,0
bleu is fast and easy to run and it can be used as a target function in parameter optimization training procedures that are commonly used in stateoftheart statistical mt systems och 2003 ,1,0,0
word alignments were generated using giza och and ney 2003 over a stemmed version of the parallel text ,0,1,0
31 system tuning minimum error training och 2003 under bleu papineni et al 2001 was used to optimise the feature weights of the decoder with respect to the dev2006 development set ,0,1,0
we use the minimumerror rate training procedure by och 2003 as implemented in the moses toolkit to set the weights of the various translation and language models optimizing for bleu ,0,1,0
we set the feature weights by optimizing the bleu score directly using minimum error rate training och 2003 on the development set ,0,1,0
following the guidelines of the workshop we built baseline systems using the lowercased europarl parallel corpus restricting sentence length to 40 words giza och and ney 2003 moses koehn et al 2007 and the sri lm toolkit stolcke 2002 to build 5gram lms ,0,1,0
instead of interpolating the two language models we explicitly used them in the decoder and optimized their weights via minimumerrorrate mer training och 2003 ,0,1,0
for example in ibm model 1 the lexicon probability of source word f given target word e is calculated as och and ney 2003 pfe summationtext k cfee kfk summationtext kf cfee kfk 1 cfeekfk summationdisplay ekfk pekfksummationdisplay a paekfk 2 summationdisplay j ffkj eekaj therefore the distribution of pekfk will affect the alignment results ,0,1,0
assuming that the parameters petkfsk are known the most likely alignment is computed by a simple dynamicprogramming algorithm1 instead of using an expectationmaximization algorithm to estimate these parameters as commonly done when performing word alignment brown et al 1993 och and ney 2003 we directly compute these parameters by relying on the information contained within the chunks ,0,1,0
we tuned our system on the development set devtest2006 for the europarl tasks and on nctest2007 for czechenglish using minimum errorrate training och 2003 to optimise bleu score ,0,1,0
this set of 800 sentences was used for minimum error rate training och 2003 to tune the weights of our system with respect to bleu score ,0,1,0
this setup provides an elegant solution to the fairly complex task of integrating multiple mt results that may differ in word order using only standard software modules in particular giza och and ney 2003 for the identification of building blocks and moses for the recombination but the authors were not able to observe improvements in 1see httpwwwstatmtorgmoses terms of bleu score ,0,1,0
decoding conditions for tuning of the decoders parameters minimum error training och 2003 with respect to the bleu score using was conducted using the respective development corpus ,0,1,0
the feature weights were optimized against the bleu scores och 2003 ,0,1,0
we build phrase translations by first acquiring bidirectional giza och and ney 2003 alignments and using moses growdiag alignment symmetrization heuristic1 we set the maximum phrase length to a large value 10 because some segmenters described later in this paper will result in shorter 1in our experiments this heuristic consistently performed better than the default growdiagfinal ,0,1,0
for tuning of decoder parameters we conducted minimum error training och 2003 with respect to the bleu score using 916 development sentence pairs ,0,1,0
one of the popular statistical machine translation paradigms is the phrasebased model pbsmt marcu et al 2002 koehn et al 2003 och et al 2004 ,0,1,0
we use the giza toolkit och and ney 2000 a suffixarray architecture lopez 2007 the srilm toolkit stolcke 2002 and minimum error rate training och et al 2003 to obtain wordalignments a translation model language models and the optimal weights for combining these models respectively ,0,1,0
furthermore techniques such as iterative minimum errorrate training och et al 2003 as well as webbased mt services require the decoder to translate a large number of sourcelanguage sentences per unit time ,0,1,0
minimumerrorrate training och 2003 are conducted on devset to optimize feature weights maximizing the bleu score up to 4grams and the obtained feature weights are blindly applied on the testset ,0,1,0
the decision rule was based on the standard loglinear interpolation of several models with weights tunedbymertonthedevelopmentsetoch2003 ,0,1,0
43 baselines 431 word alignment we used the giza implementation of ibm word alignment model 4 brown et al 1993 och and ney 2003 for word alignment and the heuristics described in och and ney 2003 to derive the intersection and refined alignment ,0,1,0
73 ment and phraseextraction heuristics described in koehn et al 2003 minimumerrorrate training och 2003 a trigram language model with kneserney smoothing trained with srilm stolcke 2002 on the english side of the training data and moses koehn et al 2007 to decode ,0,1,0
slightly differently from och and ney 2003 we use possible alignments in computing recall ,0,1,0
since manual word alignment is an ambiguous task we also explicitly allow for ambiguous alignments ie the links are marked as sure s or possible p och and ney 2003 ,0,1,0
giza och and ney 2003 is a very popular system within smt for creating word alignment from parallel corpus in fact the moses training scripts uses it ,1,0,0
however this may still be too expensive as part of an mt model that directly optimizes some performance measure eg minimum error rate training och 2003 ,0,1,0
then we run giza och and ney 2003 on the corpus to obtain word alignments in both directions ,0,1,0
we use minimal error rate training och 2003 to maximize bleu on the complete development data ,0,1,0
we then built separate englishtospanish and spanishtoenglish directed word alignments using ibm model 4 brown et al 1993 combined them using the intersectgrow heuristic och and ney 2003 and extracted phraselevel translation pairs of maximum length 7 using the alignment template approach och and ney 2004 ,0,1,0
we set all feature weights by optimizing bleu papineni et al 2002 directly using minimum error rate training mert och 2003 on the tuning part of the development set devtest2009a ,0,1,0
the features we used are as follows word posterior probability fiscus 1997 3 4gram target language model word length penalty null word length penalty also we use mert och 2003 to tune the weights of confusion network ,0,1,0
26 tuning procedure the mosesbased systems were tuned using the implementation of minimum error rate training mert och 2003 distributed with the moses decoder using the development corpus dev2009a ,0,1,0
41 baseline our baseline system is a fairly typical phrasebased machine translation system finch and sumita 2008a built within the framework of a featurebased exponential model containing the following features table 1 language resources corpus train dev eval nc spanish sentences 74k 2001 2007 words 2048k 49116 56081 vocab 61k 9047 8638 length 276 245 279 oov 52 29 14 09 english sentences 74k 2001 2007 words 1795k 46524 49693 vocab 47k 8110 7541 length 242 232 248 oov 52 29 12 09 perplexity 349 381 348 458 ep spanish sentences 1404k 1861 2000 words 41003k 50216 61293 vocab 170k 7422 8251 length 292 270 306 oov 24 01 24 02 english sentences 1404k 1861 2000 words 39354k 48663 59145 vocab 121k 5869 6428 length 280 261 296 oov 18 01 19 01 perplexity 210 72 305 125 table 2 testset 2009 corpus test nc spanish sentences 3027 words 80591 vocab 12616 length 266 sourcetarget phrase translation probability inverse phrase translation probability sourcetarget lexical weighting probability inverse lexical weighting probability phrase penalty language model probability lexical reordering probability simple distancebased distortion model word penalty for the training of the statistical models standard word alignment giza och and ney 2003 and language modeling srilm stolcke 2002 tools were used ,0,1,0
minimum error rate training mert with respect to bleu score was used to tune the decoders parameters and performed using the technique proposed in och 2003 ,0,1,0
the translation system is a factored phrasebased translation system that uses the moses toolkit koehn et al 2007 for decoding and training giza for word alignment och and ney 2003 and srilm stolcke 2002 for language models ,0,1,0
deterministic annealing in this system instead of using the regular mert och 2003 whose training objective is to minimize the onebest error we use the deterministic annealing training procedure described in smith and eisner 2006 whose objective is to minimize the expected error together with the entropy regularization technique ,0,1,0
the toolkit also implements suffixarray grammar extraction callisonburch et al 2005 lopez 2007 and minimum error rate training och 2003 ,0,1,0
the toolkit also implements suffixarray grammar extraction lopez 2007 and minimum error rate training och 2003 ,0,1,0
the search across a dimension uses the efficient method of och 2003 ,1,0,0
32 translation scores the translation scores for four different systems are reported in table 15 baseline in this system we use the giza toolkit och and ney 2003 a suffixarray architecture lopez 2007 the srilm toolkit stolcke 2002 and minimum error rate training och 2003 to obtain wordalignments a translation model language models and the optimal weights for combining these models respectively ,0,1,0
the preprocessed training data was filtered for length and aligned using the giza implementation of ibm model 4 och and ney 2003 in both directions and symmetrized using the growdiagfinaland heuristic ,0,1,0
23 forest minimum error training to tune the feature weights of our system we used a variant of the minimum error training algorithm och 2003 that computes the error statistics from the target sentences from the translation search space represented by a packed forest that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space macherey et al 2008 ,0,1,0
the loglinear model feature weights were learned using minimum error rate training mert och 2003 with bleu score papineni et al 2002 as the objective function ,0,1,0
tuning is done for each experimental condition using ochs minimum error training och 2003 ,0,1,0
4 experiment our baseline system is a popular phrasebased smt system moses koehn et al 2007 with 5gram srilm language model stolcke 2002 tuned with minimum error training och 2003 ,0,1,0
parameter tuning is done with minimum error rate training mert och 2003 ,0,1,0
the corpus was aligned with giza och and ney 2003 and symmetrized with the growdiagfinaland heuristic koehn et al 2003 ,0,1,0
systems were optimized on the wmt08 frenchenglish development data 2000 sentences using minimum error rate training och 2003 and tested on the wmt08 test data 2000 sentences ,0,1,0
ochs procedure is the most widelyused version of mert for smt och 2003 ,1,0,0
although they obtained consistent and stable performance gains for mt these were inferior to the gains yielded by ochs procedure in och 2003 ,1,0,0
32 translation performance for the experiments reported in this section we used feature weights trained with minimum error rate training mert och 2003 because mert ignores the denominator in equation 1 it is invariant with respect to the scale of the weight vector the moses implementation simply normalises the weight vector it finds by its lscript1norm ,0,1,0
parameter tuning is done with minimum error rate training mert och 2003 ,0,1,0
a popular statistical machine translation paradigms is the phrasebased model koehn et al 2003 och and ney 2004 ,0,1,0
for phrasebased translation model training we used the giza toolkit och and ney 2003 and 10m bilingual sentences ,0,1,0
to tune the decoder parameters we conducted minimum error rate training och 2003 with respect to the word bleu score papineni et al 2002 using 20k development sentence pairs ,0,1,0
this involves running giza och and ney 2003 on the corpus in both directions and applying renement rules the variant they designate is naland to obtain a single manytomany word alignment for each sentence ,0,1,0
as to analysis of nps there have been a lot of work on statistical techniques for lexical dependency parsing of sentences collins and roark 2004 mcdonald et al 2005 and these techniques potentially can be used for analysis of nps if appropriate resources for nps are available ,0,1,0
in fact when the perceptron update rule of dekel et al 2004 which modifies the weights of every divergent node along the predicted and true paths is used in the ranking framework it becomes virtually identical with the standard flat ranking perceptron of collins 20025 in contrast our approach shares the idea of cesabianchi et al 2006a that if a parent class has been predicted wrongly then errors in the children should not be taken into account we also view this as one of the key ideas of the incremental perceptron algorithm of collins and roark 2004 which searches through a complex decision space stepbystep and is immediately updated at the first wrong move ,0,1,0
7 discussion as we mentioned there are some algorithms similar to ours collins and roark 2004 daume iii and marcu 2005 mcdonald and pereira 2006 liang et al 2006 ,0,1,0
collinsandroark2004proposedanapproximate incremental method for parsing ,0,1,0
collins and roark 2004 used the averaged perceptron collins 2002a ,0,1,0
with regard to the local update b in algorithm 42 early updates collins and roark 2004 and ygood requirement in daume iii and marcu 2005 resemble our local update in that they tried to avoid the situation where the correct answer cannot be output ,0,1,0
recently severalmethodscollins and roark 2004 daume iii and marcu 2005 mcdonald and pereira 2006 have been proposed with similar motivation to ours ,0,1,0
variants of this method have been successfully used in many nlp tasks like shallow processing daume iii and marcu 2005 parsing collins and roark 2004 shen and joshi 2005 and word alignment moore 2005 ,1,0,0
we still use complex structures to represent the partial analyses so as to employ both topdown and bottomup information as in collins and roark 2004 shen and joshi 2005 ,0,1,0
beamsearch has been successful in many nlp tasks koehn et al 2003 562 inputs training examples xiyi initialization set vectorw 0 algorithm r training iterations n examples for t 1r i 1n zi argmaxygenxi y vectorw if zi negationslash yi vectorw vectorw yizi outputs vectorw figure 1 the perceptron learning algorithm collins and roark 2004 and can achieve accuracy that is close to exact inference ,0,1,0
during training the early update strategy of collins and roark 2004 is used when the correct state item falls out of the beam at any stage parsing is stopped immediately and the model is updated using the current best partial item ,0,1,0
incremental topdown and leftcorner parsers have been shown to effectively and efficiently make use of nonlocal features from the leftcontext to yield very high accuracy syntactic parses roark 2001 henderson 2003 collins and roark 2004 and we will use such rich models to derive our scores ,1,0,0
these findings are in line with collins roarks 2004 results with incremental parsing with perceptrons where it is suggested that a generative baseline feature provides the perceptron algorithm with a much better starting point for learning ,0,1,0
following collins and roark 2004 we also use the earlyupdate strategy where an update happens whenever the goldstandard actionsequence falls off the beam with the rest of the sequence neglected ,0,1,0
online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment moore 2005 sequence analysis daume and marcu 2005 mcdonald et al 2005a and phrasestructure parsing collins and roark 2004 ,0,1,0
we also employ the voted perceptron algorithm freund and schapire 1999 and the early update technique as in collins and roark 2004 ,0,1,0
another alternative for future work is to compare the dynamic programming approach taken here with the beamsearch approach of collins and roark 2004 which allows more global features ,0,1,0
thelistsmaybeused withannotation and a tuning process such as in collins and roark 2004 to iteratively alter feature weights and improve results ,0,1,0
our approach is related to those of collins and roark 2004 and taskar et al ,0,1,0
collins and roark 2004 presented a linear parsing model trained with an averaged perceptron algorithm ,0,1,0
discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated riezler et al 2002 clark and curran 2004 collins and roark 2004 taskar et al 2004 ,0,1,0
in particular most of the work on parsing with kernel methods has focussed on kernels over parse trees collins and duffy 2002 shen and joshi 2003 shen et al 2003 collins and roark 2004 ,0,1,0
for comparison to previous results table 2 lists the results on the testing set for our best model topefficientfreq20 and several other statistical parsers collins 1999 collins and duffy 2002 collins and roark 2004 henderson 2003 charniak 2000 collins 2000 shen and joshi 2004 shen et al 2003 henderson 2004 bod 2003 ,0,1,0
22 perceptronbased training to tune the parameters w of the model we use the averaged perceptron algorithm collins 2002 because of its efficiency and past success on various nlp tasks collins and roark 2004 roark et al 2004 ,1,0,0
the lefttoright parser would likely improve if we were to use a leftcorner transform collins roark 2004 ,0,1,0
collins and roark 2004 and taskar et al ,0,1,0
although generating training examples in advance without a working parser turian melamed 2005 is much faster than using inference collins roark 2004 henderson 2004 taskar et al 2004 our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor ,0,0,1
3for decoding loc is averaged over the training iterations as in collins and roark 2004 ,0,1,0
similar models have been successfully applied in the past to other tasks including parsing collins and roark 2004 chunking daume and marcu 2005 and machine translation cowan et al 2006 ,1,0,0
this linear model is learned using a variant of the incremental perceptron algorithm collins and roark 2004 daume and marcu 2005 ,0,1,0
in daume iii and marcu 2005 as well as other similar works collins 2002 collins and roark 2004 shen and joshi 2005 only lefttoright search was employed ,0,1,0
we proposed a perceptron like learning algorithm collins and roark 2004 daume iii and marcu 2005 for guided learning ,0,1,0
hence we use a beamsearch decoder during training and testing our idea is similar to that of collins and roark 2004 who used a beamsearch decoder as part of a perceptron parsing model ,0,1,0
it is an online training algorithm and has been successfully used in many nlp tasks such as pos tagging collins 2002 parsing collins and roark 2004 chinese word segmentation zhang and clark 2007 jiang et al 2008 and so on ,1,0,0
to tackle this problem we defined 2the best results of collins and roark 2004 lr884 lp891 and f888 are achieved when the parser utilizes the information about the final punctuation and the lookahead ,0,1,0
2 incremental parsing this section gives a description of collins and roarks incremental parser collins and roark 2004 and discusses its problem ,0,1,0
3 incremental parsing method based on adjoining operation in order to avoid the problem of infinite local ambiguity the previous works have adopted the following approaches 1 a beam search strategy collins and roark 2004 roark 2001 roark 2004 2 limiting the allowable chains to those actually observed in the treebank collins and roark 2004 and 3 transforming the parse trees with a selective leftcorner transformation johnson and roark 2000 before inducing the allowable chains and allowable triples collins and roark 2004 ,0,1,0
several incremental parsing methods have been proposed so far collins and roark 2004 roark 2001 roark 2004 ,0,1,0
the limited contexts used in this model are similar to the previous methods collins and roark 2004 roark 2001 roark 2004 ,0,1,0
to achieve efficient parsing we use a beam search strategy like the previous methods collins and roark 2004 roark 2001 roark 2004 ,1,0,0
each queue hi stores the only nbest 43 table 1 parsing results lr lp f roark 2004 864 868 866 collins and roark 2004 865 868 867 no adjoining 863 868 866 nonmonotonic adjoining 861 871 866 monotonic adjoining 872 877 874 partial parse trees ,0,1,0
this approach has been shown to be accurate relatively efficient and robust using both generative and discriminative models roark 2001 roark 2004 collins and roark 2004 ,1,0,0
beamsearch parsing using an unnormalized discriminative model as in collins and roark 2004 requires a slightly different search strategy than the original generative model described in roark 2001 2004 ,0,1,0
a generative parsing model can be used on its own and it was shown in collins and roark 2004 that a discriminative parsing model can be used on its own ,0,1,0
1 introduction statistical parsing models have been shown to be successful in recovering labeled constituencies collins 2003 charniak and johnson 2005 roark and collins 2004 and have also been shown to be adequate in recovering dependency relationships collins et al 1999 levy and manning 2004 dubey and keller 2003 ,1,0,0
its also worth noting that collins and roark 2004 saw a lfms improvement of 08 over their baseline discriminative parser after adding punctuation features one of which encoded the sentencefinal punctuation ,1,0,0
training discriminative parsers is notoriously slow especially if it requires generating examples by repeatedly parsing the treebank collins roark 2004 taskar et al 2004 ,0,1,0
this combination of the perceptron algorithm with beamsearch is similar to that described by collins and roark 20045 the perceptron algorithm is a convenient choice because it converges quickly usually taking only a few iterations over the training set collins 2002 collins and roark 2004 ,0,1,0
using a variant of the voted perceptron collins 2002 collins and roark 2004 crammer and singer 2003 we discriminatively trained our parser in an online fashion ,0,1,0
3 online learning again following mcdonald et al 2005 we have used the single best mira crammer and singer 2003 which is a variant of the voted perceptron collins 2002 collins and roark 2004 for structured prediction ,0,1,0
although generating training examples in advance without a working parser sagae lavie 2005 is much faster than using inference collins roark 2004 henderson 2004 taskar et al 2004 our training time can probably be decreased further by choosing a parsing strategy with a lower branching factor ,0,0,1
parsing research has also begun to adopt discriminative methods from the machine learning literature such as the perceptron freund and schapire 1999 collins and roark 2004 and the largemargin methods underlying support vector machines taskar et al 2004 mcdonald 2006 ,0,1,0
the existing work most similar to ours is collins and roark 2004 ,0,1,0
1 introduction a recent development in datadriven parsing is the use of discriminative training methods riezler et al 2002 taskar et al 2004 collins and roark 2004 turian and melamed 2006 ,0,1,0
51 relationship to supervised training to illustrate the relationship between the above symbolic training method for preference scoring and corpusbased methods perhaps the easiest way is to compare it to an adaptation collins and roark 2004 of the perceptron training method to the problem of obtaining a best parse either directly or for parse reranking because the two methods are analogous in a number of ways ,0,1,0
here it might be useful to relax the strict linear control regime by exploring beam search strategies eg along the lines of collins and roark 2004 ,0,1,0
it is possible to prove that provided the training set xizi is separable with margin 0 the algorithm is assured to converge after a finite number of iterations to a model with zero training errors collins and roark 2004 ,0,1,0
albeit simple the algorithm has proven to be very efficient and accurate for the task of parse selection collins and roark 2004 collins 2004 zettlemoyer and collins 2005 zettlemoyer and collins 2007 ,0,1,0
6 related work evidence from the surrounding context has been used previously to determine if the current sentence should be subjectiveobjective riloff et al 2003 pang and lee 2004 and adjacency pair information has been used to predict congressional votes thomas et al 2006 ,0,1,0
moviedomainsubjectivitydatasetmovie pang and lee 2004 used a collection of labeled subjective and objective sentences in their work on review classification5 the data set contains 5000 subjective sentences extracted from movie reviews collected from the rotten tomatoes web formed best ,0,1,0
2 related work there has been extensive research in opinion mining at the document level for example on product and movie reviews pang et al 2002 pang and lee 2004 dave et al 2003 popescu and etzioni 2005 ,0,1,0
2003 pang and lee 2004 2005 ,0,1,0
2007 we introduced the movie review polarity dataset enriched with annotator rationales8 it is based on the dataset of pang and lee 20049 which consists of 1000 positive and 1000 negative movie reviews tokenized and divided into 10 folds f0f9 ,0,1,0
we collect substring rationales for a sentiment classification task pang and lee 2004 and use them to obtain significant accuracy improvements for each annotator ,0,1,0
2002 various classification models and linguistic features have been proposed to improve the classification performance pang and lee 2004 mullen and collier 2004 wilson et al 2005a read 2005 ,0,1,0
with this model we can provide not only qualitative textual summarization such as good food and bad service but also a numerical scoring of sentiment ie how good the food is and how bad the service is 2 related work there have been many studies on sentiment classification and opinion summarization pang and lee 2004 2005 gamon et al 2005 popescu and etzioni 2005 liu et al 2005 zhuang et al 2006 kim and hovy 2006 ,0,1,0
others use sentence cohesion pang and lee 2004 agreementdisagreement between speakers thomas et al 2006 bansal et al 2008 or structural adjacency ,0,1,0
et al 2007 and unigrams used by many researchers eg pang and lee 2004 ,0,1,0
second benefits for sentiment analysis can be realized by decomposing the problem into so or neutral versus polar and polarity classification yu and hatzivassiloglou 2003 pang and lee 2004 wilson et al 2005a kim and hovy 2006 ,1,0,0
this amounts to performing binary text categorization under categories objective and subjective pang and lee 2004 yu and hatzivassiloglou 2003 2 ,0,1,0
determining document orientation or polarity as in deciding if a given subjective text expresses a positive or a negative opinion on its subject matter pang and lee 2004 turney 2002 3 ,0,1,0
2 literature survey the task of sentiment analysis has evolved from document level analysis eg turney 2002 pang and lee 2004 to sentence level analysis eg hu and liu 2004 kim and hovy 2004 yu and hatzivassiloglou 2003 ,0,1,0
mincuts have been used 4as of this writing wordnet is available for more than 40 world languages httpwwwglobalwordnetorg figure 2 semisupervised classification using mincuts in semisupervised learning for various tasks including document level sentiment analysis pang and lee 2004 ,0,1,0
this formulation is similar to the energy minimization framework which is commonly used in image analysis besag 1986 boykov et al 1999 and has been recently applied in natural language processing pang and lee 2004 ,0,1,0
7 related work much work on sentiment analysis classifies documents by their overall sentiment for example determining whether a review is positive or negative eg turney 2002 dave et al 2003 pang and lee 2004 beineke et al 2004 ,0,1,0
2003 pang and lee 2004 ,0,1,0
2003 pang and lee 2004 wilson et al ,0,1,0
2003 pang and lee 2004 ,0,1,0
2004 pang and lee 2004 wilson et al ,0,1,0
dave et al 2003 pang and lee 2004 turney 2002 ,0,1,0
there has also been previous work on determining whether a given text is factual or expresses opinion yu hatzivassiloglu 2003 pang lee 2004 again this work uses a binary distinction and supervised rather than unsupervised approaches ,0,1,0
for examples see erkan and radev 2004 mihalcea and tarau 2004 pang and lee 2004 ,0,1,0
2 background several graphbased learning techniques have recently been developed and applied to nlp problems minimum cuts pang and lee 2004 random walks mihalcea 2005 otterbacher et al 2005 graph matching haghighi et al 2005 and label propagation niu et al 2005 ,0,1,0
2002 and pang and lee 2004 in merely using binary unigram features corresponding to the 17744 unstemmed word or punctuation types with count 4 in the full 2000document corpus ,0,1,0
sentiment analysis includes a variety of different problems including sentiment classification techniques to classify reviews as positive or negative based on bag of words pang et al 2002 or positive and negative words turney 2002 mullen and collier 2004 classifying sentences in a document as either subjective or objective riloff and wiebe 2003 pang and lee 2004 identifying or classifying appraisal targets nigam and hurst 2004 identifying the source of an opinion in a text choi et al 2005 whether the author is expressing the opinion or whether he is attributing the opinion to someone else and developing interactive and visual opinion mining methods gamon et al 2005 popescu and etzioni 2005 ,0,1,0
2 related work there has been a large and diverse body of research in opinion mining with most research at the text pang et al 2002 pang and lee 2004 popescu and etzioni 2005 ounis et al 2006 sentence kim and hovy 2005 kudo and matsumoto 2004 riloff et al 2003 yu and hatzivassiloglou 2003 or word hatzivassiloglou and mckeown 1997 turney and littman 2003 kim and hovy 2004 takamura et al 2005 andreevskaia and bergler 2006 kaji and kitsuregawa 2007 level ,0,1,0
graphbased algorithms for classification into subjectiveobjective or positivenegative language units have been mostly used at the sentence and document level pang and lee 2004 agarwal and bhattacharyya 2005 thomas et al 2006 instead of aiming at dictionary annotation as we do ,0,1,0
we also cannot use prior graph construction methods for the document level such as physical proximity of sentences used in pang and lee 2004 at the word sense level ,0,1,0
wst summationdisplay usvt wuv globally optimal minimum cuts can be found in polynomial time and nearlinear running time in practice using the maximum flow algorithm pang and lee 2004 cormen et al 2002 ,0,1,0
in fact researchers in sentiment analysis have realized benefits by decomposing the problem into so and polarity classification yu and hatzivassiloglou 2003 pang and lee 2004 wilson et al 2005 kim and hovy 2006 ,0,1,0
the description of the minimum cut framework in section 41 was inspired by pang and lee 2004 ,0,1,0
all reviews were automatically preprocessed to remove both explicit rating indicators and objective sentences the motivation for the latter step is that it has previously aided positive vs negative classi cation pang and lee 2004 ,1,0,0
interestingly previous sentiment analysis research found that a minimumcut formulation for the binary subjectiveobjective distinction yielded good results pang and lee 2004 ,1,0,0
a later study pang and lee 2004 found that performance increased to 872 when considering only those portions of the text deemed to be subjective ,1,0,0
the third exploits automatic subjectivity analysis in applications such as review classification eg turney 2002 pang and lee 2004 mining texts for product reviews eg yi et al 2003 hu and liu 2004 popescu and etzioni 2005 summarization eg kim and hovy 2004 information extraction eg riloff et al 2005 1note that sentiment the focus of much recent work in the area is a type of subjectivity specifically involving positive or negative opinion emotion or evaluation ,0,1,0
41 experimental setup like several previous work eg mullen and collier 2004 pang and lee 2004 whitelaw et al ,0,1,0
next we learn our polarity classifier using positive and negative reviews taken from two movie 611 review datasets one assembled by pang and lee 2004 and the other by ourselves ,0,1,0
indeed recent work has shown that benefits can be made by first separating facts from opinions in a document eg yu and hatzivassiloglou 2003 and classifying the polarity based solely on the subjective portions of the document eg pang and lee 2004 ,1,0,0
finally other approaches rely on reviews with numeric ratings from websites pang and lee 2002 dave et al 2003 pang and lee 2004 cui et al 2006 and train semisupervised learning algorithms to classify reviews as positive or negative or in more finegrained scales pang and lee 2005 wilson et al 2006 ,0,1,0
in both cases there 1alternatively decisions from the sentence classifier can guide which input is seen by the document level classifier pang and lee 2004 ,0,1,0
in fact it has already been established that sentence level classification can improve document level analysis pang and lee 2004 ,0,1,0
cascaded models for finetocoarse sentiment analysis were studied by pang and lee 2004 ,0,1,0
for instance in pang and lee 2004 yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective ,0,1,0
the local dependencies between sentiment labels on sentences is similar to the work of pang and lee 2004 where soft local consistency constraints were created between every sentence in adocument and inference wassolved using a mincut algorithm ,0,1,0
previous workonsentimentanalysishascoveredawiderange of tasks including polarity classification pang et al 2002 turney 2002 opinion extraction pang and lee 2004 and opinion source assignment choi et al 2005 choi et al 2006 ,0,1,0
furthermore these systems have tackled the problem at different levels of granularity from the document level pang et al 2002 sentence level pang and lee 2004 mao and lebanon 2006 phrase level turney 2002 choi et al 2005 as well as the speaker level in debates thomas et al 2006 ,0,1,0
first even when sentiment is the desired focus researchers in sentiment analysis have shown that a twostage approach is often beneficial in which subjective instances are distinguished from objective ones and then the subjective instances are further classified according to polarity yu and hatzivassiloglou 2003 pang and lee 2004 wilson et al 2005 kim and hovy 2006 ,1,0,0
it is worth noting that we observed the same relation between subjectivity detection and polarity classification accuracy as described by pang and lee 2004 and eriksson 2006 ,0,1,0
pang and lee 2004 applied two different classifiers to perform sentiment annotation in two sequential steps the first classifier separated subjective sentimentladen texts from objective neutral ones and then they used the second classifier to classify the subjective texts into positive and negative ,0,1,0
table 1 datasets 33 establishing a baseline for a corpusbased system cbs supervised statistical methods have been very successful in sentiment tagging of texts on movie review texts they reach accuracies of 8590 aue and gamon 2005 pang and lee 2004 ,1,0,0
in many applications it has been shown that sentences with subjective meanings are paid more attention than factual onespang and lee 2004esuli and sebastiani 2006 ,0,1,0
wilson et al 2005 pang and lee 2004 and emotion studies eg ,0,1,0
a twotier scheme pang and lee 2004 where sentences are rst classi ed as subjective versus objective and then applying the sentiment classi er on only the subjective sentences further improves performance ,1,0,0
and 20ng is a collection of approximately 20000 20category documents 1 in sentiment text classification we also use two data sets one is the widely used cornell moviereview dataset2 pang and lee 2004 and one dataset from product reviews of domain dvd3 blitzer et al 2007 ,1,0,0
1 introduction the field of sentiment classification has received considerable attention from researchers in recent years pang and lee 2002 pang et al 2004 turney 2002 turney and littman 2002 wiebe et al 2001 bai et al 2004 yu and hatzivassiloglou 2003 and many others ,0,1,0
movie and product reviews have been the main focus of many of the recent studies in this area pang and lee 2002 pang et al 2004 turney 2002 turney and littman 2002 ,0,1,0
accuracy training data turney 2002 66 unsupervised pang lee 2004 8715 supervised aue gamon 2005 914 supervised so 7395 unsupervised smso to increase seed words then so 7485 weakly supervised table 7 classification accuracy on the movie review domain turney 2002 achieves 66 accuracy on the movie review domain using the pmiir algorithm to gather association scores from the web ,0,1,0
2003 pang and lee 2004 ,0,1,0
2004 pang and lee 2004 wilson et al ,0,1,0
for process 2 existing methods aim to distinguish between subjective and objective descriptions in texts kim and hovy 2004 pang and lee 2004 riloff and wiebe 2003 ,0,1,0
for process 3 machinelearning methods are usually used to classify subjective descriptions into bipolar categories dave et al 2003 beineke et al 2004 hu and liu 2004 pang and lee 2004 or multipoint scale categories kim and hovy 2004 pang and lee 2005 ,0,1,0
the focus of much of the automatic sentiment analysis research is on identifying the affect bearing words words with emotional content and on measurement approaches for sentiment turney littman 2003 pang lee 2004 wilson et al 2005 ,0,1,0
in contrast to the opinion extracts produced by pang and lee 2004 our summaries are not text extracts but rather explicitly identify and 337 characterize the relations between opinions and their sources ,0,1,0
intersentential contexts as in our approach were used as a clue also for subjectivity analysis riloff and wiebe 2003 pang and lee 2004 which is twofold classification into subjective and objective sentences ,0,1,0
3 clacnb system nave bayes supervised statistical methods have been very successful in sentiment tagging of texts and in subjectivity detection at sentence level on movie review texts they reach an accuracy of 8590 aue and gamon 2005 pang and lee 2004 and up to 92 accuracy on classifying movie review snippets into subjective and objective using both nave bayes and svm pang and lee 2004 ,1,0,0
33 language model lm as a second baseline we use the classification based on the language model using overlapping ngram sequences n was set to 8 as suggested by pang lee 2004 2005 for the english language ,0,1,0
pang lee 2004 propose the use of language models for sentiment analysis task and subjectivity extraction ,0,1,0
previous research has focused on classifying subjectiveversusobjective expressions wiebe et al 2004 and also on accurate sentiment polarity assignment turney 2002 yi et al 2003 pang and lee 2004 sindhwani and melville 2008 melville et al 2009 ,0,1,0
pang and lee 2004 frame the problem of detecting subjective sentences as finding the minimum cut in a graph representation of the sentences ,0,1,0
our approach is datadriven following the methodology in cahill et al 2004 guo et al 2007 we automatically convert the english pennii treebank and the chinese penn treebank xue et al 2005 into fstructure banks ,0,1,0
1999 openccg white 2004 and xle crouch et al 2007 or created semiautomatically belz 2007 or fully automatically extracted from annotated corpora like the hpsg nakanishi et al 2005 lfg cahill and van genabith 2006 hogan et al 2007 and ccg white et al 2007 resources derived from the pennii treebank ptb marcus et al 1993 ,0,1,0
in addition to cfgoriented approaches a number of richer treebankbased grammar acquisition and parsing methods based on hpsg miyao et al 2003 ccg clark and hockenmaier 2002 lfg riezler et al 2002 cahill et al 2004 and dependency grammar nivre and nilsson 2005 incorporate nonlocal dependencies into their deep syntactic or semantic representations ,0,1,0
f cahill et al 2004 overall 9598 5786 7220 7300 4028 5191 9016 5435 6782 6554 3616 4661 args only 9864 4203 5894 8269 3054 4460 8636 3680 5161 6608 2440 3564 basic model overall 9244 9128 9185 6387 6215 6300 6312 6233 6272 4269 4154 4210 args only 8942 9295 9115 6089 6345 6215 4792 4981 4884 3141 3273 3206 basic model with subject path constraint overall 9216 9136 9176 6372 6220 6295 7596 7530 7563 5082 4961 5021 args only 8904 9308 9102 6069 6352 6207 6615 6915 6762 4277 4476 4476 table 7 evaluation of trace insertion and antecedent recovery for c04 algorithm our basic algorithm and basic algorithm with the subject path constraint ,0,1,0
inspired by cahill et al 2004s methodology which was originally designed for english and pennii treebank our approach to chinese nonlocal dependency recovery is based on lexicalfunctional grammar lfg a formalism that involves both phrase structure trees and predicateargument structures ,0,1,0
our method revises and considerably extends the approach of cahill et al 2004 originally designed for english and to the best of our knowledge is the first nld recovery algorithm for chinese ,0,1,0
the evaluation shows that our algorithm considerably outperforms cahill et al 2004s with respect to chinese data ,0,0,1
in section 3 we review cahill et al 2004s method for recovering english nlds in treebankbased lfg approximations ,0,1,0
cahill et al 2004s approach for english resolves three ldd types in parser output trees without traces and coindexation figure 2b ie topicalisation topic whmovement in relative clauses topic rel and interrogatives focus ,0,1,0
inspired by cahill et al 2004 burke et al 2004 we have implemented an fstructure annotation algorithm to automatically obtain fstructures from cfgtrees in the ctb51 ,0,1,0
42 adaptation to chinese cahill et al 2004s algorithm section 32 only resolves certain nlds with known types of antecedents topic topic rel and focus at fstructures ,0,1,0
in order to resolve all chinese nlds represented in the ctb we modify and substantially extend the cahill et al 2004 henceforth c04 for short algorithm as follows given the set of subcat frames s for the word w and a set of paths p for the trace t the algorithm traverses the fstructure f to predict a dislocated argument t at a subfstructure h by comparing the local predw to ws subcat frames s t can be inserted at h if h together with t is complete and coherent relative to subcat frame s traverse f starting from t along the path p link t to its antecedent a if ps ending gf a exists in a subfstructure within f or leave t without an antecedent if an empty path for t exists in the modified algorithm we condition the probability of nld path p including the empty path without an antecedent on the gf associated of the trace t rather than the antecedent a as in c04 ,0,1,0
it has also obtained competitive scores on general gr evaluation corpora cahill et al 2004 ,0,1,0
32 the parsers the parsers that we chose to evaluate are the cc ccg parser clark and curran 2007 the enju hpsg parser miyao and tsujii 2005 the rasp parser briscoe et al 2006 the stanford parser klein and manning 2003 and the dcu postprocessor of ptb parsers cahill et al 2004 based on lfg and applied to the output of the charniak and johnson reranking parser ,0,1,0
most of this work has so far focused either on postprocessing to recover nonlocal dependencies from contextfree parse trees johnson 2002 jijkoun and de rijke 2004 levy and manning 2004 campbell 2004 or on incorporating nonlocal dependency information in nonterminal categories in constituency representations dienes and dubey 2003 hockenmaier 2003 cahill et al 2004 or in the categories used to label arcs in dependency representations nivre and nilsson 2005 ,0,1,0
however more recent work cahill et al 2002 cahill mccarthy et al 2004 has presented efforts in evolving and scaling up annotation techniques to the pennii treebank marcus et al 1994 containing more than 1000000 words and 49000 sentences ,0,1,0
our approach is based on earlier work on lfg semantic form extraction van genabith sadler and way 1999 and recent progress in automatically annotating the pennii and penniii treebanks with lfg fstructures cahill et al 2002 cahill mccarthy et al 2004 ,0,1,0
we have also applied our more general unification grammar acquisition methodology to the tiger treebank brants et al 2002 and penn chinese treebank xue chiou and palmer 2002 extracting widecoverage probabilistic lfg grammar 361 computational linguistics volume 31 number 3 approximations and lexical resources for german cahill et al 2003 and chinese burke lam et al 2004 ,0,1,0
because treebank annotation for individual formalisms is prohibitively expensive there have been a number of efforts to extract tags lfgs and more recently hpsgs from the penn treebank xia 1999 chen and vijayshanker 2000 xia palmer and joshi 2000 xia 2001 cahill et al 2002 miyao ninomiya and tsujii 2004 odonovan et al 2005 shen and joshi 2005 chen bangalore and vijayshanker 2006 ,0,1,0
for the penn treebank our research and the work of others xia 1999 chen and vijayshanker 2004 chiang 2000 cahill et al 2002 have shown that such a correspondence exists in most cases ,0,1,0
statistical parsers have been developed for tag chiang 2000 sarkar and joshi 2003 lfg riezler et al 2002 kaplan et al 2004 cahill et al 2004 and hpsg toutanova et al 2002 toutanova markova and manning 2004 miyao and tsujii 2004 malouf and van noord 2004 among others ,0,1,0
the feasibility of such postparse deepening for a statistical parser is demonstrated by cahill et al 2004 ,0,1,0
in this paper we show how the extraction process can be scaled to the complete wall street journal wsj section of the pennii treebank with about 1 million words in 50000 sentences based on the automatic lfg fstructure annotation algorithm described in cahill et al 2004b ,0,1,0
we are already using the extracted semantic forms in parsing new text with robust widecoverage pcfgbased lfg grammar approximations automatically acquired from the fstructure annotated pennii treebank cahill et al 2004a ,0,1,0
we utilise the automatic annotation algorithm of cahill et al 2004b to derive a version of pennii where each node in each tree is annotated with an lfg functional annotation ie an attribute value structure equation ,0,1,0
cahill et al 2004b provide four sets of annotation principles one for noncoordinate configurations one for coordinate configurations one for traces long distance dependencies and a final catch all and clean up phase ,0,1,0
the algorithm of cahill et al 2004b translates the traces into corresponding reentrancies in the fstructure representation figure 1 ,0,1,0
cahill et al 2004b measure annotation quality in terms of precision and recall against manually constructed goldstandard fstructures for 105 randomly selected trees from section 23 of the wsj section of pennii ,0,1,0
finally since nonprojective constructions often involve longdistance dependencies the problem is closely related to the recovery of empty categories and nonlocal dependencies in constituencybased parsing johnson 2002 dienes and dubey 2003 jijkoun and de rijke 2004 cahill et al 2004 levy and manning 2004 campbell 2004 ,0,1,0
recent work on the automatic acquisition of multilingual lfg resources from treebanks for chinese german and spanish burke et al 2004 cahill et al 2005 odonovan et al 2005 has shown that given a suitable treebank it is possible to automatically acquire high quality lfg resources in a very short space of time ,0,1,0
c2006 association for computational linguistics robust pcfgbased generation using automatically acquired lfg approximations aoife cahill1 and josef van genabith12 1 national centre for language technology nclt school of computing dublin city university dublin 9 ireland 2 center for advanced studies ibm dublin ireland acahilljosefcomputingdcuie abstract we present a novel pcfgbased architecture for robust probabilistic generation based on widecoverage lfg approximations cahill et al 2004 automatically extracted from treebanks maximising the probability of a tree given an fstructure ,0,1,0
in this paper we present a novel pcfgbased architecture for probabilistic generation based on widecoverage robust lexical functional grammar lfg approximations automatically extracted from treebanks cahill et al 2004 ,0,1,0
1 introduction the research presented in this paper forms part of an ongoing effort to develop methods to induce widecoverage multilingual lexicalfunctional grammar lfg bresnan 2001 resources from treebanks by means of automatically associating lfg fstructure information with constituency trees produced by probabilistic parsers cahill et al 2004 ,0,1,0
our approach is to use finitestate approximations of longdistance dependencies as they are described in schneider 2003a for dependency grammar dg and cahill et al 2004 for lexical functional grammar lfg ,0,1,0
the translation and reference files are analyzed by a treebankbased probabilistic lfg parser cahill et al 2004 which produces a set of dependency triples for each input ,0,1,0
1 introduction a recent theme in parsing research has been the application of statistical methods to linguistically motivated grammars for example lfg kaplan et al 2004 cahill et al 2004 hpsg toutanova et al 2002 malouf and van noord 2004 tag sarkar and joshi 2003 and ccg hockenmaier andsteedman2002 clarkandcurran2004b ,0,1,0
methods for doing so for stochastic parser output are described by johnson 2002 and cahill et al 2004 ,0,1,0
2004 and cahill et al ,0,1,0
6 related work the most relevant previous works include word sense translation and translation disambiguation li li 2003 cao li 2002 koehn and knight 2000 kikui 1999 fung et al 1999 frame semantic induction green et al 2004 fung chen 2004 and bilingual semantic mapping fung chen 2004 huang et al 2004 ploux ji 2003 ngai et al 2002 palmer wu 1995 ,0,1,0
it would be necessary to apply either semiautomatic or automatic methods such as those in burchardt et al 2005 green et al 2004 to extend framenet coverage for final application to machine translation tasks ,0,1,0
this paper demonstrates several of the characteristics and benefits of semframe green et al 2004 green and dorr 2004 a system that produces such a resource ,1,0,0
when the data has distinct substructures models that exploit hidden state variables are advantageous in learning matsuzaki et al 2005 petrov et al 2007 ,0,1,0
more recently em has been used to learn hidden variables in parse trees these can be headchildannotationschiangandbikel 2002 latent head features matsuzaki et al 2005 prescher 2005 dreyer and eisner 2006 or hierarchicallysplit nonterminal states petrov et al 2006 ,0,1,0
6 discussion noting that adding latent features to nonterminals in unlexicalized contextfree parsing has been very successful chiang and bikel 2002 matsuzaki et al 2005 prescher 2005 dreyer and eisner 2006 petrov et al 2006 we were surprised not to see a 3czech experiments were not done since the number of features more than 14 million was too high to multiply out by clusters ,1,0,0
we compare an ordinary pcfg estimated with maximum likelihood matsuzaki et al 2005 and the hdppcfg estimated using the variational inference algorithm described in section 26 ,0,1,0
unlexicalized methods refine the grammar in a more conservative fashion splitting each nonterminal or preterminal symbol into a much smaller number of subsymbols klein and manning 2003 matsuzaki et al 2005 petrov et al 2006 ,0,1,0
the resulting memory limitations alone can prevent the practical learning of highly split grammars matsuzaki et al 2005 ,0,1,0
1 introduction in latent variable approaches to parsing matsuzaki et al 2005 petrov et al 2006 one models an observed treebank of coarse parse trees using a grammar over more refined but unobserved derivation trees ,0,1,0
2 parsing model the berkeley parser petrov et al 2006 petrov and klein 2007 is an efficient and effective parser that introduces latent annotations matsuzaki et al 2005 to refine syntactic categories to learn better pcfg grammars ,1,0,0
this was recently followed by matsuzaki et al 2005 petrov et al 2006 who introduce stateoftheart nearly unlexicalized pcfg parsers ,1,0,0
then some manual and automatic symbol splitting methods are presented which get comparable performance with lexicalized parsers klein and manning 2003 matsuzaki et al 2005 ,0,1,0
the latentannotation model matsuzaki et al 2005 petrov et al 2006 is one of the most effective unlexicalized models ,1,0,0
in general they can be divided into two major categories namely lexicalized models collins 1997 1999 charniak 1997 2000 and unlexicalized models klein and manning 2003 matsuzaki et al 2005 petrov et al 2006 petrov and klein 2007 ,0,1,0
1 introduction when data have distinct substructures models exploiting latent variables are advantageous in learning matsuzaki et al 2005 petrov and klein 2007 blunsom et al 2008 ,1,0,0
previous work has shown that highquality unlexicalized pcfgs can be learned from a treebank either by manual annotation klein and manning 2003 or automatic state splitting matsuzaki et al 2005 petrov et al 2006 ,1,0,0
rather than explicit annotation we could use latent annotations to split the pos tags similarly to the introduction of latent annotations to pcfg grammars matsuzaki et al 2005 petrov et al 2006 ,0,1,0
building upon the large body of research to improve tagging performance for various languages using various models eg thede and harper 1999 brants 2000 tseng et al 2005b huang et al 2007 and the recent work on pcfg grammars with latent annotations matsuzaki et al 2005 petrov et al 2006 we will investigate the use of finegrained latent annotations for chinese pos tagging ,0,1,0
as one can see in table 4 the resulting parser ranks among the best lexicalized parsers beating those of collins 1999 and charniak and johnson 20058 its f1 performance is a 27 reduction in error over matsuzaki et al ,0,0,1
2005 866 867 119 611 collins 1999 887 885 092 667 charniak and johnson 2005 901 901 074 701 this paper 903 900 078 685 all sentences lp lr cb 0cb klein and manning 2003 863 851 131 572 matsuzaki et al ,0,1,0
5 related work there has not been much previous work on graphical models for full parsing although recently several latent variable models for parsing have been proposed koo and collins 2005 matsuzaki et al 2005 riezler et al 2002 ,0,1,0
5httpnlpcsberkeleyedumainhtmlparsing 47 figure 3 predicate argument structure timized automatically by assigning latent variables to each nonterminal node and estimating the parameters of the latent variables by the em algorithm matsuzaki et al 2005 ,0,1,0
previous research in this area includes several models which incorporate hidden variables matsuzaki et al 2005 koo and collins 2005 petrov et al 2006 titov and henderson 2007 ,0,1,0
cfgs extracted from such structures were then annotated with hidden variables encoding the constraints described in the previous section and trained until convergence by means of the insideoutside algorithm defined in pereira and schabes 1992 and applied in matsuzaki et al 2005 ,0,1,0
such methods stand in sharp contrast to partially supervised techniques that have recently been proposed to induce hidden grammatical representations that are finergrained than those that can be read off the parsed sentences in treebanks henderson 2003 matsuzaki et al 2005 prescher 2005 petrov et al 2006 ,0,1,0
instead researchers condition parsing decisions on many other features such as parent phrasemarker and famously the lexicalhead of the phrase magerman 1995 collins 1996 collins 1997 johnson 1998 charniak 2000 henderson 2003 klein and manning 2003 matsuzaki et al 2005 and others ,0,1,0
in retrospect however there are perhaps even greater similarities to that of magerman 1995 henderson 2003 matsuzaki et al 2005 ,0,1,0
matsuzaki et al 2005 koo and collins 2005 ,0,1,0
compared to a basic treebank grammar charniak 1996 the grammars of highaccuracy parsers weaken independence assumptions by splitting grammar symbols and rules with either lexical charniak 2000 collins 1999 or nonlexical klein and manning 2003 matsuzaki et al 2005 conditioning information ,1,0,0
in matsuzaki et al 2005 nonterminals in a standard pcfg model are augmented with latent variables ,0,1,0
while the model of matsuzaki et al 2005 significantly outperforms the constrained model of prescher 2005 they both are well below the stateoftheart in constituent parsing ,0,0,1
31 a note on statesplits recent studies klein and manning 2003 matsuzaki et al 2005 prescher 2005 petrov et al 2006 suggest that categorysplits help in enhancing the performance of treebank grammars and a previous study on mh tsarfaty 2006 outlines specific postags splits that improve mh parsing accuracy ,0,1,0
2 latent variable parsing in latent variable parsing matsuzaki et al 2005 prescher 2005 petrov et al 2006 we learn rule probabilities on latent annotations that when marginalized out maximize the likelihood of the unannotated training trees ,0,1,0
this leads to 49 methods that use semisupervised techniques on a treebankinfered grammar backbone such as matsuzaki et al 2005 petrov et al 2006 ,0,1,0
solving this first methodological issue has led to solutions dubbed hereafter as unlexicalized statistical parsing johnson 1998 klein and manning 2003a matsuzaki et al 2005 petrov et al 2006 ,0,1,0
a further development has been first introduced by matsuzaki et al 2005 who recasts the problem of adding latent annotations as an unsupervised learning problem given an observed pcfg induced from the treebank the latent grammar is generated by combining every non terminal of the observed grammar to a predefined set h of latent symbols ,0,1,0
in this paper stanford named entity recognizer finkel et al 2005 is used to classify noun phrases into four semantic categories person location organizarion and misc ,0,1,0
although several methods have already been proposed to incorporate nonlocal features sutton and mccallum 2004 bunescu and mooney 2004 finkel et al 2005 roth and yih 2005 krishnan and manning 2006 nakagawa and matsumoto 2006 these present a problem that the types of nonlocal features are somewhat constrained ,0,0,1
the performance of the related work finkel et al 2005 krishnan and manning 2006 is listed in table4 ,0,1,0
method dev test finkel et al 2005 finkel et al 2005 baseline crf 8551 nonlocal features 8686 krishnan and manning 2006 krishnan and manning 2006 baseline crf 8529 nonlocal features 8724 table 5 summary of performance with poschunk tags by tagchunk ,0,1,0
however the achieved accuracy was not better than that of related work finkel et al 2005 krishnan and manning 2006 based on crfs ,0,1,0
the namedentity features are generated by the freely available stanford ner tagger finkel et al 2005 ,0,1,0
however due to the lack of a fine grained ner tool at hand we employ the stanford ner package finkel et al 2005 which identifies only four types of named entities ,0,0,1
51 conll named entities presence feature we use stanford named entity recognizer ner finkel et al 2005 to identify conll style nes7 as possible answer strings in a candidate sentence for a given type of question ,0,1,0
semantic 1 the named entity ne tag of wi obtained using the stanford crfbased ne recognizer finkel et al 2005 ,0,1,0
instead we opt to utilize the stanford ner tagger finkel et al 2005 over the sentences in a document and annotate each np with the ner label assigned to that mention head ,0,1,0
41 ner features we used the features generated by the crf package finkel et al 2005 ,0,1,0
fme 1 cbcner system m 7167 2347 3536cbcner system a 7066 3286 4486 2 xip ner 7777 5655 6548 xip cbc m 7841 6026 6815 xip cbc a 7631 6048 6748 3 stanford ner 6794 6801 6797 stanford cbc m 6940 7107 7023 stanford cbc a 7009 7293 7148 4 gate ner 6330 5688 5992 gate cbc m 6643 6179 6403 gate cbc a 6651 6310 6476 5 stanford xip 7285 7587 7433 stanford xip cbc m 7294 7770 7524 stanford xip cbc a 7355 7893 7615 6 gate xip 6938 6604 6767 gate xip cbc m 6962 6779 6869 gate xip cbc a 6987 6910 6948 7 gate stanford 6312 6932 6607 gate stanford cbc m 6509 7205 6839 gate stanford cbc a 6566 7325 6925 table 1 results given by different hybrid ner systems and coupled with the cbcner system corpora conll muc6 muc7 and ace nerengiecrf3all2008distsimsergz finkel et al 2005 line 3 in table 1 gate ner or in short gate cunningham et al 2002 line 4 in table 1 and several hybrid systems which are given by the combination of pairs taken among the set of the three lastmentioned ner systems lines 5 to 7 in table 1 ,0,1,0
we parse the data using the collins parser collins 1997 and then tag person location and organization names using the stanford named entity recognizer finkel et al 2005 ,0,1,0
some stem from work on graphical modelsincludingloopybeliefpropagationsuttonand mccallum 2004 smith and eisner 2008 gibbs sampling finkel et al 2005 sequential monte carlo methods such as particle filtering levy et al 2008 and variational inference jordan et al 1999 mackay 1997 kurihara and sato 2006 ,0,1,0
in all the experiments our source side language is english and the stanford named entity recognizer finkel et al 2005 was used to extract nes from the source side article ,0,1,0
in the first approach heuristic rules are used to find the dependencies bunescu and mooney 2004 or penalties for label inconsistency are required to handset adhoc finkel et al 2005 ,0,1,0
corpus time period size articles words new indian express english 20070101 to 20070831 2359 347050 dinamani tamil 20070101 to 20070831 2359 256456 table 1 statistics on comparable corpora from the above corpora we first extracted all the nes from the english side using the stanford ner tool finkel et al 2005 ,0,1,0
that is a significant shortcoming because in many domains hard or soft global constraints on the label sequence are motivated by common sense for named entity recognition a phrase that appears multiple times should tend to get the same label each time finkel et al 2005 ,0,1,0
such techniques include gibbs sampling finkel et al 2005 a generalpurpose monte carlo method and integer linear programming ilp roth and yih 2005 a generalpurpose exact framework for npcomplete problems ,0,1,0
for the named entity features we used a fairly standard feature set similar to those described in finkel et al 2005 ,0,1,0
our features were based on those in finkel et al 2005 ,0,1,0
many of the previous studies of bioner tasks have been based on machine learning techniques including hidden markov models hmms bikel et al 1997 the dictionary hmm model kou et al 2005 and maximum entropy markov models memms finkel et al 2004 ,0,1,0
however other types of nonlocal information have also been shown to be effective finkel et al 2005 and we will examine the effectiveness of other nonlocal information which can be embedded into label information ,0,1,0
information about the previous state finkel et al 2005 ,0,1,0
in a recent study by finkel et al 2005 nonlocal information is encoded using an independence model and the inference is performed by gibbs sampling which enables us to use a stateoftheart factored model and carry out training efficiently but inference still incurs a considerable computational cost ,0,0,1
global information is known to be useful in other nlp tasks especially in the named entity recognition task and several studies successfully used global features chieu and ng 2002 finkel et al 2005 ,0,1,0
 most existing work to capture labelconsistency has attempted to create all parenleftbign2parenrightbig pairwise dependencies between the different occurrences of an entity finkel et al 2005 sutton and mccallum 2004 where n is the number of occurrences of the given entity ,0,1,0
 most work has looked to model nonlocal dependencies only within a document finkel 1125 et al 2005 chieu and ng 2002 sutton and mccallum 2004 bunescu and mooney 2004 ,0,1,0
additionally our approach makes it possible to do inference in just about twice the inference time with a single sequential crf in contrast approaches like gibbs sampling that model the dependencies directly can increase inference time by a factor of 30 finkel et al 2005 ,0,1,0
a very common case of this in the conll dataset is that of documents containing references to both the china daily a newspaper and china the country finkel et al 2005 ,0,1,0
an additional consistent edge of a linearchain conditional random field crf explicitly models the dependencies between distant occurrences of similar words sutton and mccallum 2004 finkel et al 2005 ,0,1,0
starting out with a chunking pipeline which uses a classical combination of tagger and chunker with the stanford pos tagger toutanova et al 2003 the yamcha chunker kudoh and matsumoto 2000 and the stanford named entity recognizer finkel et al 2005 the desire to use richer syntactic representations led to the development of a parsing pipeline which uses charniak and johnsons reranking parser charniak and johnson 2005 to assign pos tags and uses base nps as chunk equivalents while also providing syntactic trees that can be used by feature extractors ,0,1,0
we perform named entity tagging using the stanford fourclass named entity tagger finkel et al 2005 ,0,1,0
to implement this method we rst use the stanford named entity recognizer4 finkel et al 2005toidentifythesetofpersonandorganisation entities e from each article in the corpus ,0,1,0
most previous work with crfs containing nonlocal dependencies used approximate probabilistic inference techniques including trp sutton and mccallum 2004 and gibbs sampling finkel et al 2005 ,0,1,0
4 relation to previous work there is a significant volume of work exploring the use of crfs for a variety of chunking tasks including namedentity recognition gene prediction shallow parsing and others finkel et al 2005 culotta et al 2005 sha and pereira 2003 ,0,1,0
we extract the named entities from the web pages using the stanford named entity recognizer finkel et al 2005 ,0,1,0
one of the steps in the analysis of english is named entity recognition using stanford named entity recognizer finkel et al 2005 ,0,1,0
ner proves to be a knowledgeintensive task and it was reassuring to observe that system resources used f1 lbjner wikipedia nonlocal features wordclass model 9080 suzuki and isozaki 2008 semisupervised on 1gword unlabeled data 8992 ando and zhang 2005 semisupervised on 27mword unlabeled data 8931 kazama and torisawa 2007a wikipedia 8802 krishnan and manning 2006 nonlocal features 8724 kazama and torisawa 2007b nonlocal features 8717 finkel et al 2005 nonlocal features 8686 table 7 results for conll03 data reported in the literature ,0,1,0
use of global features for structured prediction problem has been explored by several nlp applications such as sequential labeling finkel et al 2005 krishnan and manning 2006 kazama and torisawa 2007 and dependency parsing nakagawa 2007 with a great deal of success ,0,1,0
kbest suffix arrays have been used in autocomplete applications church and thiesson 2005 ,0,1,0
of the methods we compare against only the wordnetbased similarity measures mihalcea and moldovan 2001 and navigli 2006 provide a method for predicting verb similarities our learned measure widely outperforms these methods achieving a 136 fscore improvement over the lesk similarity measure ,0,1,0
navigli 2006 presents an automatic approach for mapping between sense inventories here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between wordnet senses and distinctions made within the coarsergrained oxford english dictionary ,0,1,0
finally we use as a feature the mappings produced in navigli 2006 of wordnet senses to oxford english dictionary senses ,0,1,0
several researchers eg palmer et al 2004 navigli 2006 snow et al 2007 hovy et al 2006 work on reducing the granularity of sense inventories for wsd ,0,1,0
such coarsegrained inventories can be produced manually from scratch hovy et al 2006 or by automatically relating mccarthy 2006 or clustering navigli 2006 navigli et al 2007 existing word senses ,0,1,0
wordnet has been criticized for being overly finegrained navigli et al 2007 ide and wilks 2006 we are using it here because it is the sense inventory used by erk et al ,0,1,0
wordnet sense information has been criticized to be too fine grained agirre and lopez de lacalle lekuona 2003 navigli 2006 ,0,1,0
thus some research has been focused on deriving different wordsense groupings to overcome the finegrained distinctions of wn hearst and schutze 1993 peters et al 1998 mihalcea and moldovan 2001 agirre and lopezdelacalle 2003 navigli 2006 and snow et al 2007 ,0,1,0
the firstsense heuristic can be thought of as striving for maximal specificity at the risk of precluding some admissible senses reduced recall 7allowing for multiple finegrained senses to be judged as appropriate in a given context goes back at least to sussna 1993 discussed more recently by eg navigli 2006 ,0,1,0
wsd systems have been far more successful in distinguishing coarsegrained senses than finegrained ones navigli 2006 but does that approach neglect necessary meaning differences ,0,0,1
this clustering was created automatically with the aid of a methodology described in navigli 2006 ,0,1,0
22 creation of a coarsegrained sense inventory to tackle the granularity issue we produced a coarsergrained version of the wordnet sense inventory3 based on the procedure described by navigli 2006 ,0,1,0
however in the coarsegrained task the sense inventory was first clustered semiautomatically with each cluster representing an equivalence class over senses navigli 2006 ,0,1,0
in particular abney defines a function k that is an upper bound on the negative loglikelihood and shows his bootstrapping algorithms locally minimize k we now present a generalization of abneys k function and relate it to another semisupervised learning technique entropy regularization brand 1999 grandvalet and bengio 2005 jiao et al 2006 ,0,1,0
we thus introduce a multiplier to form the actual objective function that we minimize with respect to 4 summationdisplay il logpiyi nsummationdisplay inegationslashl hpi 4 one may regard as a lagrange multiplier that is used to constrain the classifiers uncertainty h to be low as presented in the work on entropy regularization brand 1999 grandvalet and bengio 2005 jiao et al 2006 ,0,1,0
in fact many attempts have recently been made to develop semisupervised sol methods zhu et al 2003 li and mccallum 2005 altun et al 2005 jiao et al 2006 brefeld and scheffer 2006 ,0,1,0
53 comparison with sscrfmer when we consider semisupervised sol methods sscrfmer jiao et al 2006 is the most competitive with hysol since both methods are defined based on crfs ,0,1,0
in fact we still have a question as to whether sscrfmer is really scalable in practical time for such a large amount of unlabeled data as used in our experiments which is about 680 times larger than that of jiao et al 2006 ,0,1,0
semisupervised conditional random fields crfs based on a minimum entropy regularizer sscrfmer have been proposed in jiao et al 2006 ,0,1,0
recent work includes improved model variants eg jiao et al 2006 okanohara et al 2006 and applications such as web data extraction pinto et al 2003 scientific citation extraction peng and mccallum 2004 and word alignment blunsom and cohn 2006 ,1,0,0
the variance semiring is essential for many interesting training paradigms such as deterministic 40 annealing rose 1998 minimum risk smith and eisner 2006 active and semisupervised learning grandvalet and bengio 2004 jiao et al 2006 ,0,1,0
we use entropy regularization er jiao et al 2006 to leverage unlabeled instances7 we weight the er term by choosing the best8 weight in 103102101110 multiplied by labeledunlabeled for each data set and query selection method ,0,1,0
reported work includes improved model variants eg jiao et al 2006 and applications such as web data extraction pinto et al 2003 scientific citation extraction peng and mccallum 2004 word alignment blunsom and cohn 2006 and discourselevel chunking feng et al 2007 ,0,1,0
high values of fall into the minimal entropy trap while low values ofhave no effect on the model see jiao et al 2006 for an example ,0,1,0
jiao et al propose semisupervised conditional random fields jiao et al 2006 that try to maximize the conditional loglikelihood on the training data and simultaneously minimize the conditional entropy of the class labels on the unlabeled data ,0,1,0
suzuki et al 2006 8802 082 unlabeled data 17m 27m words 8841 039 supplied gazetters 8890 049 add dev ,0,1,0
suzuki et al 2006 9436 006 table 8 the hysol performance with the fscore optimization technique on chunking conll2000 experiments from unlabeled data appear different from each other ,0,1,0
more specialized methods also exist for example for support vector machines musicant et al 2003 and for conditional random fields gross et al 2007 suzuki et al 2006 ,0,1,0
we follow gao et al 2006 suzuki et al 2006 and approximate the metrics using the sigmoid function ,0,1,0
dubey et al proposed an unlexicalized pcfg parser that modied pcfg probabilities to condition the existence of syntactic parallelism dubey et al 2006 ,0,1,0
the results have demonstrated the existence of priming effects in corpus data they occur for specific syntactic constructions gries 2005 szmrecsanyi 2005 consistent with the experimental literature but also generalize to syntactic rules across the board which repeated more often than expected by chance reitter et al 2006b dubey et al 2006 ,0,1,0
the other intriguing issue is how our anchorbased method for shared argument identification can benefit from recent advances in coreference and zeroanaphora resolution iida et al 2006 komachi et al 2007 etc ,0,1,0
we follow yang et al 2006 iida et al 2006 in using a tree kernel to represent structural information using the subtree that covers a pronoun and its antecedent candidate ,0,1,0
one possible approach is to employ stateoftheart techniques for coreference and zeroanaphora resolution iida et al 2006 komachi et al 2007 etc in preprocessing cooccurrence samples ,1,0,0
both liang et al 2006 and tillmann and zhang 2006 report on effective machine translation mt models involving large numbers of features with discriminatively trained weights ,0,1,0
the algorithm is slightly different from other online training algorithms tillmann and zhang 2006 liang et al 2006 in that we keep and update oracle translations which is a set of good translations reachable by a decoder according to a metric ie bleu papineni et al 2002 ,0,1,0
tillmann and zhang 2006 avoided the problem by precomputing the oracle translations in advance ,0,1,0
tillmann and zhang 2006 used a different update style based on a convex loss function le e etmax parenleftbig 0 1 parenleftbig si f t esi f t e parenrightbigparenrightbig 768 table 1 experimental results obtained by varying normalized tokens used with surface form ,0,1,0
tillmann and zhang 2006 and liang et al ,0,1,0
tillmann and zhang 2006 trained their feature set using an online discriminative algorithm ,0,1,0
online discriminative training has already been studied by tillmann and zhang 2006 and liang et al ,0,1,0
this paper continues a line of research on online discriminative training tillmann and zhang 2006 liang et al 2006 arun and koehn 2007 extending that of watanabe et al ,0,1,0
the second uses the decoder to search for the highestb translation tillmann and zhang 2006 which arun and koehn 2007 call maxb updating ,0,1,0
tillmann and zhang 2006 present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations ,0,1,0
2 related work this method is similar to blockorientation modeling tillmann and zhang 2005 and maximum entropy based phrase reordering model xiong et al 2006 in which local orientations leftright of phrase pairs blocks are learned via maxent classifiers ,0,1,0
the use of structured prediction to smt is also investigated by liang et al 2006 tillmann and zhang 2006 watanabe et al 2007 ,0,1,0
recently there have been several discriminative approaches at training large parameter sets including tillmann and zhang 2006 and liang et al 2006 ,0,1,0
in tillmann and zhang 2006 the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level bleu ,0,1,0
others have introduced alternative discriminative training methods tillmann and zhang 2006 liang et al 2006 turian et al 2007 blunsom et al 2008 macherey et al 2008 in which a recurring challenge is scalability to train many features we need many train218 ing examples and to train discriminatively we need to search through all possible translations of each training example ,0,1,0
discriminative training has been used mainly for translation model combination och and ney 2002 and with the exception of wellington et al 2006 tillmann and zhang 2006 has not been used to directly train parameters of a translation model ,0,1,0
if the input consists of sevwe also adopt the approximation that treats every sentence with its reference as a separate corpus tillmann and zhang 2006 so that ngram counts are not accumulated and parallel processing of sentences becomes possible ,0,1,0
tillmann and zhang 2006 use a bleu oracle decoder for discriminative training of a local reordering model ,0,1,0
they can be used for discriminative training of reordering models tillmann and zhang 2006 ,0,1,0
where they are expected to be maximally discriminative tillmann and zhang 2006 ,0,1,0
this might prove beneficial for various discriminative training methods tillmann and zhang 2006 ,0,1,0
this makes it suitable for discriminative smt training which is still a challenge for large parameter sets tillmann and zhang 2006 liang et al 2006 ,0,1,0
however at the short term the incorporation of these type of features will force us to either build a new decoder or extend an existing one or to move to a new mt architecture for instance in the fashion of the architectures suggested by tillmann and zhang 2006 or liang et al ,0,1,0
several studies have shown that largemargin methods can be adapted to the special complexities of the task liang et al 2006 tillmann and zhang 2006 cowan et al 2006 however the capacity of these algorithms to improve over stateoftheart baselines is currently limited by their lack of robust dimensionality reduction ,0,1,0
both liang et al 2006 and tillmann and zhang 2006 report on effective machine translation mt models involving large numbers of features with discriminatively trained weights ,1,0,0
the algorithm is slightly different from other online training algorithms tillmann and zhang 2006 liang et al 2006 in that we keep and update oracle translations which is a set of good translations reachable by a decoder according to a metric ie bleu papineni et al 2002 ,0,1,0
tillmann and zhang 2006 and liang et al ,0,1,0
online discriminative training has already been studied by tillmann and zhang 2006 and liang et al ,0,1,0
in this method each training sentence is decoded and weights are updated at every iteration liang et al 2006 ,0,1,0
tillmann and zhang 2006 liang et al ,0,1,0
this paper continues a line of research on online discriminative training tillmann and zhang 2006 liang et al 2006 arun and koehn 2007 extending that of watanabe et al ,0,1,0
sentencelevel approximations to b exist lin and och 2004 liang et al 2006 but we found it most effective to perform b computations in the context of a setoof previouslytranslated sentences following watanabe et al ,0,0,1
moreover this evaluation concern dovetails with a frequent engineering concern that sentencelevel scores are useful at various points in the mt pipeline for example minimum bayes risk decoding kumar and byrne 2004 selecting oracle translations for discriminative reranking liang 614 et al 2006 watanabe et al 2007 and sentencebysentence comparisons of outputs during error analysis ,0,1,0
one is to use a stochastic gradient descent sgd or perceptron like online learning algorithm to optimize the weights of these features directly for mt shen et al 2004 liang et al 2006 tillmann and zhang 2006 ,0,1,0
by building the entire system on the derivation level we sidestep issues that can occur when perceptron training with hidden derivations liang et al 2006 but we also introduce the need to transform our training sourcetarget pairs into training derivations ,0,1,0
so we will engineer more such features especially with lexicalization and soft alignments liang et al 2006 and study the impact of alignment quality on parsing improvement ,0,1,0
alignment is often used in training both generative and discriminative models brown et al 1993 blunsom et al 2008 liang et al 2006 ,0,1,0
item form ijueve goal ijue rules ijue rfifiprimeejejprime iprimejejejprime ijueejve ij 1ueejve ej1 rj1 logic monotonealign under the boolean semiring this minimal logic decides if a training example is reachable by the model which is required by some discriminative training regimens liang et al 2006 blunsom et al 2008 ,0,1,0
recently there have been several discriminative approaches at training large parameter sets including tillmann and zhang 2006 and liang et al 2006 ,0,1,0
liang et al 2006 demonstrates a discriminatively trained system for machine translation that has the following characteristics 1 requires a varying update strategy local vs bold depending on whether the reference sentence is reachable or not 2 uses sentence level bleu as a criterion for selecting which output to update towards and 3 only trains on limited length 515 words sentences ,0,1,0
this latter point is a critical difference that contrasts to the major weakness of the work of liang et al 2006 which uses a topn list of translations to select the maximum bleu sentence as a target for training so called local update ,0,0,1
others have introduced alternative discriminative training methods tillmann and zhang 2006 liang et al 2006 turian et al 2007 blunsom et al 2008 macherey et al 2008 in which a recurring challenge is scalability to train many features we need many train218 ing examples and to train discriminatively we need to search through all possible translations of each training example ,0,1,0
work on learning with hidden variables can be used for both crfs quattoni et al 2004 and for inference based learning algorithms like those used in this work liang et al 2006 ,0,1,0
these algorithms are usually applied to sequential labeling or chunking but have also been applied to parsing taskar et al 2004 mcdonald et al 2005 machine translation liang et al 2006 and summarization daume iii et al 2006 ,0,1,0
for this reason to our knowledge all discriminative models proposed to date either sidestep the problem by choosing simple model and feature structures such that spurious ambiguity is lessened or removed entirely ittycheriah and roukos 2007 watanabe et al 2007 or else ignore the problem and treat derivations as translations liang et al 2006 tillmann and zhang 2007 ,0,1,0
to our knowledge no systems directly address problem 1 instead choosing to ignore the problem by using one or a small handful of reference derivations in an nbest list liang et al 2006 watanabe et al 2007 or else making local independence assumptions which sidestep the issue ittycheriah and roukos 2007 tillmann and zhang 2007 wellington et al 2006 ,0,0,1
forced decoding arises in online discriminative training where model updates are made toward the most likely derivation of a gold translation liang et al 2006 ,0,1,0
this makes it suitable for discriminative smt training which is still a challenge for large parameter sets tillmann and zhang 2006 liang et al 2006 ,0,1,0
however at the short term the incorporation of these type of features will force us to either build a new decoder or extend an existing one or to move to a new mt architecture for instance in the fashion of the architectures suggested by tillmann and zhang 2006 or liang et al ,0,1,0
in general agold acandidates following collins 2000 and charniak and johnson 2005 for parse reranking and liang et al 2006 for translation reranking we define aoracle as alignment in acandidates that is most similar to agold8 we update each feature weight i as follows i i haoraclei ha1besti 9 following moore 2005 after each training pass we average all the feature weight vectors seen during the pass and decode the discriminative training set using the vector of averaged feature weights ,0,1,0
9liang et al 2006 report that for translation reranking such local updates towards the oracle outperform bold updates towards the gold standard ,0,1,0
research have also been made into alternatives to the current loglinear scoring model such as discriminative models with millions of features liang et al 2006 or kernel based models wang et al 2007 ,0,1,0
we observe that aer is loosely correlated to bleu 081 though the relation is weak as observed earlier by fraser and marcu 2006a ,0,1,0
high quality word alignments can yield more accurate phrasepairs which improve quality of a phrasebased smt system och and ney 2003 fraser and marcu 2006b ,0,1,0
we compare semisupervised leaf with a previous state of the art semisupervised system fraser and marcu 2006b ,1,0,0
fraser and marcu 2006b described symmetrized training of a 1ton loglinear model and a mto1 loglinear model ,0,1,0
we use the semisupervised emd algorithm fraser and marcu 2006b to train the model ,0,1,0
fraser and marcu 2006a established that it is important to tune the tradeoff between precision and recall to maximize performance ,0,1,0
for an alignment model most of these use the aachen hmm approach vogel et al 1996 the implementation of ibm model 4 in giza och and ney 2000 or more recently the semisupervised emd algorithm fraser and marcu 2006 ,0,1,0
if humanaligned data is available the emd algorithm provides higher baseline alignments than giza that have led to better mt performance fraser and marcu 2006 ,0,1,0
we follow the approach of bootstrapping from a model with a narrower parameter space as is done in eg och and ney 2000 and fraser and marcu 2006 ,0,1,0
fmeasure with an appropriate setting of will be useful during the development process of new alignment models or as a maximization criterion for discriminative training of alignment models cherry and lin 2003 ayan dorr and monz 2005 ittycheriah and roukos 2005 liu liu and lin 2005 fraser and marcu 2006 lacostejulien et al 2006 moore yih and bode 2006 ,0,1,0
2 related work recently several successful attempts have been made at using supervised machine learning for word alignment liu et al 2005 taskar et al 2005 ittycheriah and roukos 2005 fraser and marcu 2006 ,1,0,0
with the exception of fraser and marcu 2006 these previous publications do not entirely discard the generative models in that they integrate ibm model predictions as features ,0,1,0
85 recently some alignment evaluation metrics have been proposed which are more informative when the alignments are used to extract translation units fraser and marcu 2006 ayan and dorr 2006 ,0,1,0
along similar lines fraser and marcu 2006 combine a generative model of word alignment with a loglinear discriminative model trained on a small set of hand aligned sentences ,0,1,0
consequently considerable effort has gone into devising and improving automatic word alignment algorithms and into evaluating their performance eg och and ney 2003 taskar et al 2005 moore et al 2006 fraser and marcu 2006 among many others ,0,1,0
method prec rec fmeasure giza intersect 967 530 685 giza union 825 690 751 giza gdf 840 682 752 phrasal itg 507 803 622 phrasal itg ncc 754 780 767 following the lead of fraser and marcu 2006 we handaligned the first 100 sentence pairs of our training set according to the blinker annotation guidelines melamed 1998 ,0,1,0
fraser and marcu 2006 have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional expectationmaximization algorithm used in ibm models ,0,1,0
emd training fraser and marcu 2006 combines generative and discriminative elements ,0,1,0
the majority of this research was done on extending the tree structure finding new synsets snow et al 2006 or enriching wn with new relationships cuadros and rigau 2008 rather than improving the quality of existing conceptsynset nodes ,0,1,0
although some early systems for webpage analysis induce rules at characterlevel eg such as wien kushmerick et al 1997 and dipre brin 1998 most recent approaches for set expansion have used either tokenized andor parsed freetext carlson et al 2009 talukdar et al 2006 snow et al 2006 pantel and pennacchiotti 2006 or have incorporated heuristics for exploiting html structures that are likely to encode lists and tables nadeau et al 2006 etzioni et al 2005 ,0,1,0
1510 5 related work in recent years many research has been done on extracting relations from free text eg pantel and pennacchiotti 2006 agichtein and gravano 2000 snow et al 2006 however almost all of them require some languagedependent parsers or taggers for english which restrict the language of their extractions to english only or languages that have these parsers ,0,1,0
beyond wordnet fellbaum 1998 a wide range of resources has been developed and utilized including extensions to wordnet moldovan and rus 2001 snow et al 2006 and resources based on automatic distributional similarity methods lin 1998 pantel and lin 2002 ,0,1,0
finally methods in the literature more focused on a specific disambiguation task include statistical methods for the attachment of hyponyms under the most likely hypernym in the wordnet taxonomy snow et al 2006 structural approaches based on semantic clusters and distance metrics pennacchiotti and pantel 2006 supervised machine learning methods for the disambiguation of meronymy relations girju et al 2003 etc 6 conclusions in this paper we presented a novel approach to disambiguate the glosses of computational lexicons and machinereadable dictionaries with the aim of alleviating the knowledge acquisition bottleneck ,0,1,0
other researchers pantel and pennacchiotti 2006 snow et al 2006 use clustering techniques coupled with syntactic dependency features to identify isa relations in large text collections ,0,1,0
recently snow jurafsky and ng 2005 generated tens of thousands of hypernym patterns and combined these with noun clusters to generate highprecision suggestions for unknown noun insertion into wordnet snow et al 2006 ,0,1,0
4 related work 41 acquisition of classes of instances although some researchers focus on reorganizing or extending classes of instances already available explicitly within manuallybuilt resources such as wikipedia ponzetto and strube 2007 or wordnet snow et al 2006 or both suchanek et al 2007 a large body of previous work focuses on compiling sets of instances not necessarily labeled from unstructured text ,0,1,0
1 introduction current methods for largescale information extraction take advantage of unstructured text available from either web documents banko et al 2007 snow et al 2006 or more recently logs of web search queries pasca 2007 to acquire useful knowledge with minimal supervision ,0,1,0
ponzetto and strube 2007 snow et al 2006 can be summarized as c such asincluding i and where i is a potential instance eg venezuelan equine encephalitis and c is a potential class label for the instance eg zoonotic diseases for example in the sentence the expansion of the farms increased the spread of zoonotic diseases such as venezuelan equine encephalitis ,0,1,0
since hearst 1992 numerous works have used patterns for discovery and identification of instances of semantic relationships eg girju et al 2006 snow et al 2006 banko et al 2007 ,0,1,0
some work has been done on adding new terms and relations to wordnet snow et al 2006 and factotum ohara and wiebe 2003 ,0,1,0
21 relationship types there is a large body of related work that deals with discovery of basic relationship types represented in useful resources such as wordnet including hypernymy hearst 1992 pantel et al 2004 snow et al 2006 synonymy davidov and rappoport 2006 widdows and dorow 2002 and meronymy berland and charniak 1999 girju et al 2006 ,0,1,0
we compare system performance between snow et al 2006 and our framework in section 5 ,0,1,0
53 snow et al 2006 snow snow et al 2006 has extended the wordnet 21 by adding thousands of entries synsets at a relatively high precision ,0,1,0
we have also illustrated that asia outperforms three other english systems kozareva et al 2008 pasca 2007b snow et al 2006 even though many of these use more input than just a semantic class name ,0,0,1
we also compare asia on twelve additional benchmarks to the extended wordnet 21 produced by snow et al snow et al 2006 and show that for these twelve sets asia produces more than five times as many set instances with much higher precision 98 versus 70 ,0,0,1
snow etal snow et al 2006 use known hypernymhyponym pairs to generate training data for a machinelearning system which then learns many lexicosyntactic patterns ,0,1,0
43 scoring alln rules we observed that the likelihood of nouns mentioned in a definition to be referred by the concept title depends greatly on the syntactic path connecting them which was exploited also in snow et al 2006 ,0,1,0
41 judging rule correctness following the spirit of the finegrained human evaluation in snow et al 2006 we randomly sampled 800 rules from our rulebase and presented them to an annotator who judged them for correctness according to the lexical reference notion specified above ,0,1,0
6 related work a large body of previous work exists on extending wordnet with additional concepts and instances snow et al 2006 suchanek et al 2007 these methods do not address attributes directly ,0,1,0
currently the bestperforming english np interpretation methods in computational linguistics focus mostly on two consecutive noun instances noun compounds and are either weakly supervised knowledgeintensive rosario and hearst 2001 rosario et al 2002 moldovan et al 2004 pantel and pennacchiotti 2006 pennacchiotti and pantel 2006 kim and baldwin 2006 snow et al 2006 girju et al 2005 girju et al 2006 or use statistical models on large collections of unlabeled data berland and charniak 1999 lapata and keller 2004 nakov and hearst 2005 turney 2006 ,1,0,0
given the probabilistic taxonomy learning model introduced by snow et al 2006 we leverage on the computation of logistic regression to exploit singular value decomposition svd as unsupervised feature selection ,0,1,0
first we need to determine whether or not the positive effect of svd feature selection is preserved in more complex feature spaces such as syntactic feature spaces as those used in snow et al 2006 ,0,1,0
in section 3 we then describe the probabilistic taxonomy learning model introduced by snow et al 2006 ,0,1,0
34 31 probabilistic model in the probabilistic formulation snow et al 2006 the task of learning taxonomies from a corpus is seen as a probability maximization problem ,0,1,0
given a set of evidences e over all the relevant word pairs in snow et al 2006 the probabilistic taxonomy learning task is defined as the problem of finding the taxonomy hatwidet that maximizes the 67 probability of having the evidences e ie hatwidet arg max t pet in snow et al 2006 this maximization problem is solved with a local search ,0,1,0
this increase of probabilities is defined as multiplicative change n as follows n petprimepet 2 the main innovation of the model in snow et al 2006 is the possibility of adding at each step the best relation n rijas well as n irij that is rij with all the relations by the existing taxonomy ,1,0,0
the last important fact is that it is possible to demonstrate that eij k prijt eij 1prijteij koddsrij where k is a constant see snow et al 2006 that will be neglected in the maximization process ,0,1,0
because of this property vector space models have been used successfully both in computational linguistics manning et al 2008 snow et al 2006 gorman and curran 2006 schutze 1998 and in cognitive science landauer and dumais 1997 lowe and mcdonald 2000 mcdonald and ramscar 2001 ,1,0,0
we have adopted the evaluation method of snow et al 2006 compare the generated hypernyms with hypernyms present in a lexical resource in our case the dutch part of eurowordnet 1998 ,0,1,0
6 related work several works attempt to extend wordnet with additional lexical semantic information moldovan and rus 2001 snow et al 2006 suchanek et al 2007 clark et al 2008 ,0,1,0
thus it may not suffer from the issues of nonisomorphic structure alignment and nonsyntactic phrase usage heavily wellington et al 2006 ,0,1,0
1 introduction translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning wellington et al 2006 ,0,1,0
however to what extent that assumption holds is tested only on a small number of language pairs using hand aligned data fox 2002 hwa et al 2002 wellington et al 2006 ,0,0,1
wellington et al 2006 argue that these restrictions reduce our ability to model translation equivalence effectively ,0,1,0
figure 1b shows several orders of the sentence which violate this constraint1 previous studies have shown that if both the source and target dependency trees represent linguistic constituency the alignment between subtrees in the two languages is very complex wellington et al 2006 ,0,1,0
more importantly the ratio of binarizability as expected decreases on freer wordorder languages wellington et al 2006 ,0,1,0
it is for all three reasons ie translation induction from alignment structures and induction of alignment structures important that the synchronous grammars are expressive enough to induce all the alignment structures found in handaligned gold standard parallel corpora wellington et al 2006 ,0,1,0
2006 and chiang 2007 in terms of what alignments they induce has been discussed in wu 1997 and wellington et al ,0,1,0
2 insideout alignments wu 1997 identified socalled insideout alignments two alignment configurations that cannot be induced by binary synchronous contextfree grammars these alignment configurations while infrequent in language pairs such as englishfrench cherry and lin 2006 wellington et al 2006 have been argued to be frequent in other language pairs incl ,0,1,0
englishchinese wellington et al 2006 and englishspanish lepage and denoual 2005 ,0,1,0
one of the theoretical problems with phrase based smt models is that they can not effectively model the discontiguous translations and numerous attempts have been made on this issue simard et al 2005 quirk and menezes 2006 wellington et al 2006 bod 2007 zhang et al 2007 ,0,1,0
gibbs sampling is not new to the natural language processing community teh 2006 johnson et al 2007 ,0,1,0
nonparametricmodels teh 2006 may be appropriate ,1,0,0
the relationship between kneserney smoothing to the bayesian approach have been explored in goldwater et al 2006 teh 2006 using pitmanyor processes ,0,1,0
the most direct comparison is between our system and those presented in cahill and van genabith 2006 and hogan et al ,0,1,0
298 within lfg includes the xle3 cahill and van genabith 2006 hogan et al ,0,1,0
cahill and van genabith 2006 and hogan et al ,0,1,0
1999 openccg white 2004 and xle crouch et al 2007 or created semiautomatically belz 2007 or fully automatically extracted from annotated corpora like the hpsg nakanishi et al 2005 lfg cahill and van genabith 2006 hogan et al 2007 and ccg white et al 2007 resources derived from the pennii treebank ptb marcus et al 1993 ,0,1,0
aoifecahillimsunistuttgartde and van genabith 2006 which do not rely on handcrafted grammars and thus can easily be ported to new languages ,0,1,0
as in cahill and van genabith 2006 fstructures are generated from the now altered treebank and from this data along with the treebank trees the pcfgbased grammar which is used for training the generation model is extracted ,0,1,0
in the lfgbased generation algorithm presented by cahill and van genabith 2006 complex named entities ie those consisting of more than one word token and other multiword units can be fragmented in the surface realization ,0,1,0
we take the generator of cahill and van genabith 2006 as our baseline generator ,0,1,0
these rules can be handcrafted grammar rules such as those of langkildegeary 2002 carroll and oepen 2005 created semiautomatically belz 2007 or alternatively extracted fully automatically from treebanks bangalore and rambow 2000 nakanishi et al 2005 cahill and van genabith 2006 ,0,1,0
the uparrows and downarrows are shorthand for mni ni where ni is the cstructure node annotated with the equation2 treebest argmaxtreeptreefstr 1 ptreefstr productdisplay x y in tree feats aivjxai vj px y x feats 2 the generation model of cahill and van genabith 2006 maximises the probability of a tree given an fstructure eqn ,0,1,0
cahill and van genabith 2006 note that conditioning fstructure annotated generation rules on local features eqn ,0,1,0
to solve the problem cahill and van genabith 2006 apply an automatic generation grammar transformation to their training data they automatically label cfg nodes with additional case information and the model now learns the new improved generation rules of tables 4 and 5 ,0,1,0
fstruct feats grammar rules predpronumsg per3 genfem prpnom she predpronumsg per3 genfem prpacc her table 5 lexical item rules with case markings 4 a historybased generation model the automatic generation grammar transform presented in cahill and van genabith 2006 provides a solution to coarsegrained and in fact inappropriate independence assumptions in the basic generation model ,0,1,0
this additional conditioning has the effect of making the choice of generation rules sensitive to the history of the generation process and we argue provides a simpler more uniform general intuitive and natural probabilistic generation model obviating the need for cfggrammar transforms in the original proposal of cahill and van genabith 2006 ,0,0,1
note that for our example the effect of the uniform additional conditioning on mother grammatical function has the same effect as the generation grammar transform of cahill and van genabith 2006 but without the need for the gramfstruct feats grammar rules predpronumsg per3 genfem subj prp she predpronumsg per3 genfem obj prp her table 7 lexical item rules ,0,1,0
in addition uniform conditioning on mother grammatical function is more general than the casephenomena specific generation grammar transform of cahill and van genabith 2006 in that it applies to each and every subpart of a recursive input fstructure driving generation making available relevant generation history context to guide local generation decisions ,0,0,1
existing statistical nlg i uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation varges and mellish 2001 white 2004 paiva and evans 2005 ii applies ngram models to select the overall most likely realisation after generation halogen family or iii reuses an existing parsing grammar or treebank for surface realisation velldal et al 2004 cahill and van genabith 2006 ,0,1,0
there are other approaches in which the generation grammars are extracted semiautomatically belz 2007 or automatically such as hpsg nakanishi and miyao 2005 lfg cahill and van genabith 2006 hogan et al 2007 and ccg white et al 2007 ,0,1,0
2005 and cahill and van genabith 2006 with hpsg and lfg grammars ,0,1,0
one possible strategy is to exploit a widecoverage realizer that aims for applicability in multiple application domains white et al 2007 cahill and van genabith 2006 zhong and stent 2005 langkildegeary 2002 langkilde and knight 1998 elhadad 1991 ,0,1,0
from the same treebank cahill and van genabith 2006 automatically extracted widecoverage lfg approximations for a pcfgbased generation model ,0,1,0
our model improves the baseline provided by cahill and van genabith 2006 i accuracy is increased by creating a lexicalised pcfg grammar and enriching conditioning context with parent fstructure features and ii coverage is increased by providing lexical smoothing and fuzzy matching techniques for rule smoothing ,0,0,1
tbest argmax t ptf 1 ptf productdisplay x y in t feats aiai x px yxfeats 2 3 disambiguation models the basic generation model presented in cahill and van genabith 2006 used simple probabilistic contextfree grammars ,0,1,0
2007 presented a historybased generation model to overcome some of the inappropriate independence assumptions in the basic generation model of cahill and van genabith 2006 ,0,0,1
the generator used in our experiments is an instance of the second type using a probability model defined over lexical functional grammar cstructure and fstructure annotations cahill and van genabith 2006 hogan et al 2007 ,0,1,0
2 background the natural language generator used in our experiments is the wsjtrained system described in cahill and van genabith 2006 and hogan et al ,0,1,0
cahill and van genabith 2006 attain 982 coverage and a bleu score of 06652 on the standard wsj test set section 23 ,0,1,0
from this lfg annotated treebank largescale unification grammar resources were automatically extracted and used in parsing cahill and al 2008 and generation cahill and van genabith 2006 ,0,1,0
this approach was subsequently extended to other languages including german cahill and al 2003 chinese burke 2004 guo and al 2007 spanish odonovan 2004 chrupala and van genabith 2006 and french schluter and van genabith 2008 ,0,1,0
c2009 association for computational linguistics automatic treebankbased acquisition of arabic lfg dependency structures lamia tounsi mohammed attia nclt school of computing dublin city university ireland lamiatounsi mattia josefcomputingdcuie josef van genabith abstract a number of papers have reported on methods for the automatic acquisition of largescale probabilistic lfgbased grammatical resources from treebanks for english cahill and al 2002 cahill and al 2004 german cahill and al 2003 chinese burke 2004 guo and al 2007 spanish odonovan 2004 chrupala and van genabith 2006 and french schluter and van genabith 2008 ,0,1,0
1 introduction word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research for example brown et al 1993 ittycheriah and roukos 2005 fraser and marcu 2007 including work leveraging syntactic parse trees eg cherry and lin 2006 denero and klein 2007 fossum et al 2008 ,0,1,0
the second alternative used berkeleyaligner liang et al 2006 denero and klein 2007 which shares information between the two alignment directions to improve alignment quality ,0,1,0
denero and klein 2007 use a syntaxbased distance in an hmm word alignment model to favor syntaxfriendly alignments ,0,1,0
when we trained external chinese models we used the same unlabeled data set as denero and klein 2007 including the bilingual dictionary ,0,1,0
for example the hmm aligner achieves an aer of 207 when using the competitive thresholding heuristic of denero and klein 2007 ,0,1,0
we also trained an hmm aligner as described in denero and klein 2007 and used the posteriors of this model as features ,0,1,0
thresholding denero and klein 2007 ,0,1,0
lopez and resnik 2005 and denero and klein 2007 modify the distortion model of the hmm alignment model vogel et al 1996 to reflect tree distance rather than string distance cherry and lin 2006 modify an itg aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints ,0,1,0
for example the word alignment computed by giza and used as a basis to extract the tts templates in most ssmt systems has been observed to be a problem for ssmt denero and klein 2007 may and knight 2007 due to the fact that the wordbased alignment models are not aware of the syntactic structure of the sentences and could produce many syntaxviolating word alignments ,0,1,0
approaches have been proposed recently towards getting better word alignment and thus better tts templates such as encoding syntactic structure information into the hmmbased word alignment model denero and klein 2007 and build62 ing a syntaxbased word alignment model may and knight 2007 with tts templates ,0,1,0
denero and klein 2007 focus on alignment and do not present mt results while may and knight 2007 takesthesyntacticrealignmentasaninputtoanem algorithm where the unaligned target words are insertedintothetemplatesandminimumtemplatesare combinedintobiggertemplatesgalleyetal2006 ,0,0,1
we use the version extracted and preprocessed by daume iii and campbell 2007 ,0,1,0
one way of obtaining a suitable granularity of nodes is to introduce latent classes such as the semimarkov class model okanohara and tsujii 2007 ,0,1,0
more recently however okanohara and tsujii 2007 showed that a 1 conditional maximum entropy models rosenfeld 1996 provide somewhat of a counterexample but there too many kinds of global and nonlocal features are difficult to use rosenfeld 1997 ,0,1,0
as shown in okanohara and tsujii 2007 using this representation a linear classifier cannot distinguish sentences sampled from a trigram and real sentences ,0,1,0
artificial ungrammaticalities have been used in various nlp tasks smith and eisner 2005 okanohara and tsujii 2007 the idea of an automatically generated ungrammatical treebank was proposed by foster 2007 ,0,1,0
examples are andersen 2006 2007 okanohara and tsujii 2007 sun et al ,0,1,0
both okanohara and tsujii 2007 and wagner et al ,0,1,0
okanohara and tsujii 2007 generate illformed sentences by sampling a probabilistic language model and end up with pseudonegative examples which resemble machine translation output more than they do learner texts ,0,1,0
adaptations to the algorithms in the presence of ngram lms are discussed in chiang 2007 venugopal et al 2007 huang and chiang 2007 ,0,1,0
huang and chiang 2007 searches with the full model but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration ,0,1,0
the forest concept is also used in machine translation decoding for example to characterize the search space of decoding with integrated language models huang and chiang 2007 ,0,1,0
these include cube pruning chiang 2007 cube growing huang and chiang 2007 early pruning moore and quirk 2007 closing spans roark and hollingshead 2008 roark and hollingshead 2009 coarsetofine methods petrov et al 2008 pervasive laziness pust and knight 2009 and many more ,0,1,0
in the smt research community the second step has been well studied and many methods have been proposed to speed up the decoding process such as nodebased or spanbased beam search with different pruning strategies liu et al 2006 zhang et al 2008a 2008b and cube pruning huang and chiang 2007 mi et al 2008 ,1,0,0
to circumvent these computational limitations various pruning techniques are usually needed eg huang and chiang 2007 ,0,1,0
to speed our computations we use the cube pruning method of huang and chiang 2007 with a fixed beam size ,1,0,0
31 translation model form we first assume the general hypergraph setting of huang and chiang 2007 namely that derivations under our translation model form a hypergraph ,0,1,0
hiero search refinements huang and chiang 2007 offer several refinements to cube pruning to improve translation speed ,1,0,0
13huang and chiang 2007 give an informal example but do not elaborate on it ,0,0,1
recent innovations have greatly improved the efficiency of language model integration through multipass techniques such as forest reranking huang and chiang 2007 local search venugopal et al 2007 and coarsetofine pruning petrov et al 2008 zhang and gildea 2008 ,1,0,0
as an alternative huang and chiang 2007 describes a forestbased reranking algorithm called cube growing which also employs beam search but focuses computation only where necessary in a topdown pass through a parse forest ,0,1,0
3huang and chiang 2007 describes the cube growing algorithm in further detail including the precise form of the successor function for derivations ,0,1,0
in a second topdown pass similar to huang and chiang 2007 we can recalculate psynd for alternative derivations in the hypergraph potentially correcting search errors made in the first pass ,0,1,0
433 hiero search refinements huang and chiang 2007 offer several refinements to cube pruning to improve translation speed ,1,0,0
taken together with cube pruning chiang 2007 kbest tree extraction huang and chiang 2005 and cube growing huang and chiang 2007 these results provide evidence that lazy techniques may penetrate deeper yet into mt decoding and other nlp search problems ,0,1,0
huang and chiang 2007 de143 5x108 1x109 15x109 2x109 25x109 3x109 edges created 42000 43000 44000 45000 model cost lazy cube generation exhaustive cube generation figure 3 number of edges produced by the decoder versus model cost of 1best decodings ,0,1,0
for 1best search we use the cube pruning technique chiang 2007 huang and chiang 2007 which approximately intersects the translation forest with the lm ,0,1,0
but we did not use any lm estimate to achieve early stopping as suggested by huang and chiang 2007 ,0,1,0
the cubepruning by chiang 2007 and the lazy cubepruning of huang and chiang 2007 turn the computation of beam pruning of cyk decoders into a topk selection problem given two columns of translation hypotheses that need to be combined ,0,1,0
so we propose forest reranking a technique inspired by forest rescoring huang and chiang 2007 that approximately reranks the packed forest of exponentially many parses ,0,1,0
for nonlocal features we adapt cube pruning from forest rescoring chiang 2007 huang and chiang 2007 since the situation here is analogous to machine translation decoding with integrated language models we can view the scores of unit nonlocal features as the language model cost computed onthefly when combining subconstituents ,0,1,0
we also use cube pruning algorithm huang and chiang 2007 to speed up the translation process ,1,0,0
6 related work in machine translation the concept of packed forest is first used by huang and chiang 2007 to characterize the search space of decoding with language models ,0,1,0
3a hypergraph is analogous to a parse forest huang and chiang 2007 ,0,1,0
4 sub translation combining for sub translation combining we mainly use the bestfirst expansion idea from cube pruning huang and chiang 2007 to combine subtranslations and generate the whole kbest translations ,0,1,0
decoding time of our experiments h means hours language model for rescoring huang and chiang 2007 ,0,1,0
recent work has explored twostage decoding which explicitly decouples decoding into a source parsing stage and a target language model integration stage huang and chiang 2007 ,0,1,0
we rerank derivations with cube growing a lazy beam search algorithm huang and chiang 2007 ,0,1,0
forest reranking with a language model can be performed over this nary forest using the cube growing algorithm of huang and chiang 2007 ,0,1,0
however with the algorithms proposed in huang and chiang 2005 chiang 2007 huang and chiang 2007 it is possible to develop a generalpurpose decoder that can be used by all the parsingbased systems ,0,1,0
in our decoder we incorporate two pruning techniques described by chiang 2007 huang and chiang 2007 ,0,1,0
note that this early discarding is related to ideas behind cube pruning huang and chiang 2007 which generates the top n most promising hypotheses but in our method the decision not to generate hypotheses is guided by the quality of hypotheses on the result stack ,0,1,0
decoding used beam search with the cube pruning algorithm huang and chiang 2007 ,0,1,0
the dependency trees induced when each rewrite rule in an ith order lcfrs distinguish a unique head can similarly be characterized by being of gapdegree i so that i is the maximum number of gaps that may appear between contiguous substrings of any subtree in the dependency tree kuhlmann and mohl 2007 ,0,1,0
kuhlmann and mohl 2007 mcdonald and nivre 2007 nivre et al 2007 hindi is a verb final flexible word order language and therefore has frequent occurrences of nonprojectivity in its dependency structures ,0,1,0
in the supervised setting a recent paper by daume iii 2007 shows that using a very simple feature augmentation method coupled with support vector machines he is able to effectively use both labeled target and source data to provide the best results in a number of nlp tasks ,1,0,0
in order to build models that perform well in new target domains we usually find two settings daume iii 2007 in the semisupervised setting the goal is to improve the system trained on the source domain using unlabeled data from the target domain and the baseline is that of the system c2008 ,0,1,0
there are two tasksdaume iii 2007 for the domain adaptation problem ,0,1,0
this is comparable to the accuracy of 9629 reported by daume iii 2007 on the newswire domain ,0,1,0
5 combining indomain and outofdomain data for training in this section we will first introduce the augment technique of daume iii 2007 before showing the performance of our wsd system with and without using this technique ,0,1,0
51 the augment technique for domain adaptation the augment technique introduced by daume iii 2007 is a simple yet very effective approach to performing domain adaptation ,1,0,0
in the english allwords task of the previous senseval evaluations senseval2 senseval3 semeval2007 the best performing english allwords task systems with the highest wsd accuracy were trained on semcor mihalcea and moldovan 2001 decadt et al 2004 chan et al 2007b ,0,1,0
many adaptation methods operate by simple augmentations of the target feature space as we have doneheredaumeiii2007 ,0,1,0
blitzer et al 2006 jiang and zhai 2007 daume iii 2007 finkel and manning 2009 or st where no labeled target domain data is available eg ,0,1,0
the last row shows the results for the feature augmentation algorithm daume iii 2007 ,0,1,0
in order to build models that perform well in new target domains we usually find two settings daume iii 2007 ,0,1,0
the problem itself has started to get attention only recently roark and bacchiani 2003 hara et al 2005 daume iii and marcu 2006 daume iii 2007 blitzer et al 2006 mcclosky et al 2006 dredze et al 2007 ,0,1,0
we distinguish two main approaches to domain adaptation that have been addressed in the literature daume iii 2007 supervised and semisupervised ,0,1,0
in supervised domain adaptation gildea 2001 roark and bacchiani 2003 hara et al 2005 daume iii 2007 besides the labeled source data we have access to a comparably small but labeled amount of target data ,0,1,0
there are many possible methods for combining unlabeled and labeled data daume iii 2007 but we simply concatenate unlabeled data with labeled data to see the effectiveness of the selected reliable parses ,0,1,0
for the multilingual dependency parsing track which was the other track of the shared task nilsson et al achieved the best performance using an ensemble method hall et al 2007 ,0,1,0
daume iii 2007 further augments the feature space on the instances of both domains ,0,1,0
because daume iii 2007 views the adaptation as merely augmenting the feature space each of his features has the same prior mean and variance regardless of whether it is domain specific or independent ,0,1,0
trained and tested using the same technique as daume iii 2007 ,0,1,0
5 related work we already discussed the relation of our work to daume iii 2007 in section 24 ,0,1,0
we also show that the domain adaptation work of daume iii 2007 which is presented as an adhoc preprocessing step is actually equivalent to our formal model ,0,1,0
we demonstrate that allowing different values for these hyperparameters significantly improves performance over both a strong baseline and daume iii 2007 within both a conditional random field sequence model for named entity recognition and a discriminatively trained dependency parser ,0,1,0
24 formalization of daume iii 2007 as mentioned earlier our model is equivalent to that presented in daume iii 2007 and can be viewed as a formal version of his model2 in his presentation the adapation is done through feature augmentation ,0,1,0
recall that the log likelihood of our model is d parenleftbigg lorigddd i di i2 2 2d parenrightbigg i i2 2 2 we now introduce a new variable d d and plug it into the equation for log likelihood d parenleftbigg lorigddd i di2 2 2d parenrightbigg i i2 2 2 the result is the model of daume iii 2007 where the d are the domainspecific feature weights and d are the domainindependent feature weights ,0,1,0
other techniques have tried to quantify the generalizability of certain features across domains daume iii and marcu 2006 jiang and zhai 2006 or tried to exploit the common structure of related problems bendavid et al 2007 scholkopf et al 2005 ,0,1,0
daume allows an extra degree of freedom among the features of his domains implicitly creating a twolevel feature hierarchy with one branch for general features and another for domain specific ones but does not extend his hierarchy further daume iii 2007 ,0,1,0
unlike our technique in most cases researchers have focused on the scenario where labeled training data is available in both the source and the target domain eg daume iii 2007 chelba and acero 2004 daume iii and marcu 2006 ,0,1,0
2006 and daume iii 2007 and see below for discussions so in this paper we focus on the less studied but equally important problem of annotationstyle adaptation ,0,1,0
ald 2008 and is also similar to the pred baseline for domain adaptation in daume iii and marcu 2006 daume iii 2007 ,0,1,0
for example daume iii 2007 shows that training a learning algorithm on the weighted union of different data sets which is basically what we did performs almost as well as more involved domain adaptation approaches ,1,0,0
the model presented above is based on our previous work jiang and zhai 2007c which bears the same spirit of some other recent work on multitask learning ando and zhang 2005 evgeniou and pontil 2004 daume iii 2007 ,0,1,0
while transfer learning was proposed more than a decade ago thrun 1996 caruana 1997 its application in natural language processing is still a relatively new territory blitzer et al 2006 daume iii 2007 jiang and zhai 2007a arnold et al 2008 dredze and crammer 2008 and its application in relation extraction is still unexplored ,0,1,0
daume iii 2007 proposed a simple feature augmentation method to achieve domain adaptation ,0,1,0
also the aspect of generalizing features across different products is closely related to fully supervised domain adaptation daume iii 2007 and we plan to combine our approach with the idea from daume iii 2007 to gain insights into whether the composite backoff features exhibit different behavior in domaingeneral versus domainspecific feature subspaces ,0,1,0
1 introduction word sense disambiguation wsd competitions have focused on general domain texts as attested in the last senseval and semeval competitions kilgarriff 2001 mihalcea et al 2004 pradhan et al 2007 ,0,1,0
for instance daume iii 2007 shows that a simple feature augmentation method for svm is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of nlp tasks ,1,0,0
this technique is called system combination bangalore et al 2001 matusov et al 2006 sim et al 2007 rosti et al 2007a rosti et al 2007b ,0,1,0
redecoding rosti et al 2007a based regeneration redecodes the source sentence using original lm as well as new trans105 lation and reordering models that are trained on the sourcetotarget nbest translations generated in the first pass ,0,1,0
confusion network and redecoding have been well studied in the combination of different mt systems bangalore et al 2001 matusov et al 2006 sim et al 2007 rosti et al 2007a rosti et al 2007b ,0,1,0
rosti et al 2007a also used redecoding to do system combination by extracting sentencespecific phrase translation tables from the outputs of different mt systems and running a phrasebased decoding with this new translation table ,0,1,0
31 regeneration with redecoding one way of regeneration is by running the decoding again to obtain new hypotheses through a redecoding process rosti et al 2007a ,0,1,0
2007 rosti et al ,0,1,0
2007 rosti et al ,0,1,0
2007a and rosti et al ,0,1,0
2007 and rosti et al ,0,1,0
similar to rosti et al 2007 each word in the confusion network is associated with a word posterior probability ,0,1,0
2 confusionnetworkbased mt system combination the current stateoftheart is confusionnetworkbased mt system combination as described by 98 rosti and colleagues rosti et al 2007a rosti et al 2007b ,1,0,0
recently confusionnetworkbased system combination algorithms have been developed to combine outputs of multiple machine translation mt systems to form a consensus output bangalore et al 2001 matusov et al 2006 rosti et al 2007 sim et al 2007 ,0,1,0
although various approaches to smt system combination have been explored including enhanced combination model structure rosti et al 2007 better word alignment between translations ayan et al 2008 he et al 2008 and improved confusion network construction rosti et al 2008 most previous work simply used the ensemble of smt systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way ,0,0,1
psarc is increased by 1110 1k1 if the hypothesis ranking k in the system s contains the arc rosti et al 2007a he et al 2008 ,0,1,0
in recent several years the system combination methods based on confusion networks developed rapidly bangalore et al 2001 matusov et al 2006 sim et al 2007 rosti et al 2007a rosti et al 2007b rosti et al 2008 he et al 2008 which show stateoftheart performance in benchmarks ,1,0,0
al 2006 rosti et al 2007a ,0,1,0
it is very likely that even greater gains can be achieved by more complicated combination schemes rosti et al 2007 although significantly more effort in tuning would be required ,1,0,0
53 comparison with system combination we reimplemented a stateoftheart system combination method rosti et al 2007 ,0,1,0
in machine translation confusionnetwork based combination techniques eg rosti et al 2007 he et al 2008 have achieved the stateoftheart performance in mt evaluations ,1,0,0
among the four steps the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different mt systems rosti et al 2007 ,0,1,0
similar to rosti et al 2007a each word in the hypothesis is assigned with a rankbased score of 11 r where r is the rank of the hypothesis ,0,1,0
2007 rosti et al ,0,1,0
confusion network based system combination for machine translation has shown promising advantage compared with other techniques based system combination such as sentence level hypothesis selection by voting and source sentence redecoding using the phrases or translation models that are learned from the source sentences and target hypotheses pairs rosti et al 2007a huang and papineni 2007 ,1,0,0
as in rosti et al 2007 confusion networks built around all skeletons are joined into a lattice which is expanded and rescored with language models ,0,1,0
other scores for the word arc are set as in rosti et al 2007 ,0,1,0
the recent approaches used pairwise alignment algorithms based on symmetric alignments from a hmm alignment model matusov et al 2006 or edit distance alignments allowing shifts rosti et al 2007 ,0,1,0
the availability of the ter software has made it easy to build a high performance system combination baseline rosti et al 2007 ,1,0,0
the hypothesis scores and tuning are identical to the setup used in rosti et al 2007 ,0,1,0
besides continued research on improving mt techniques one line of research is dedicated to better exploitation of existing methods for the combination of their respective advantages macherey and och 2007 rosti et al 2007a ,0,1,0
this can be seen as a simplified version of rosti et al 2007b ,0,1,0
214 model features our mst models are based on the features described in hall 2007 specifically we use features based on a dependency nodes form lemma coarse and fine partofspeech tag and morphologicalstring attributes ,0,1,0
the treebased reranker includes the features described in hall 2007 as well as features based on nonprojective edge attributes explored in havelka 2007a havelka 2007b ,0,1,0
3 results and analysis hall 2007 shows that the oracle parsing accuracy of a kbest edgefactored mst parser is considerably higher than the onebest score of the same parser even when k is small ,0,1,0
nakagawa 2007 and hall 2007 also showed the effectiveness of global features in improving the accuracy of graphbased parsing using the approximate gibbs sampling method and a reranking approach respectively ,0,1,0
an existing method to combine multiple parsing algorithms is the ensemble approach sagae and lavie 2006a which was reported to be useful in improving dependency parsing hall et al 2007 ,0,1,0
thus nakagawa 2007 and hall 2007 both try to overcome the limited feature scope of graphbased models by adding global features in the former case using gibbs sampling to deal with the intractable inference problem in the latter case using a reranking scheme ,0,1,0
mcdonald et al 2007 ivan et al 2008 proposed a structured model based on crfs for jointly classifying the sentiment of text at varying levels of granularity ,0,1,0
there are many research directions eg sentiment classification classifying an opinion document as positive or negative eg pang lee and vaithyanathan 2002 turney 2002 subjectivity classification determining whether a sentence is subjective or objective and its associated opinion wiebe and wilson 2002 yu and hatzivassiloglou 2003 wilson et al 2004 kim and hovy 2004 riloff and wiebe 2005 featuretopicbased sentiment analysis assigning positive or negative sentiments to topics or product features hu and liu 2004 popescu and etzioni 2005 carenini et al 2005 ku et al 2006 kobayashi inui and matsumoto 2007 titov and mcdonald ,0,1,0
one of the main directions is sentiment classification which classifies the whole opinion document eg a product review as positive or negative eg pang et al 2002 turney 2002 dave et al 2003 ng et al 2006 mcdonald et al 2007 ,0,1,0
another important direction is classifying sentences as subjective or objective and classifying subjective sentences or clauses as positive or negative wiebe et al 1999 wiebe and wilson 2002 yu and hatzivassiloglou 2003 wilson et al 2004 kim and hovy 2004 riloff and wiebe 2005 gamon et al 2005 mcdonald et al 2007 ,0,1,0
several researchers also studied featuretopicbased sentiment analysis eg hu and liu 2004 popescu and etzioni 2005 ku et al 2006 carenini et al 2006 mei et al 2007 ding liu and yu 2008 titov and r mcdonald 2008 stoyanov and cardie 2008 lu and zhai 2008 ,0,1,0
domain adaptation deals with these feature distribution changes blitzer et al 2007 jiang and zhai 2007 ,0,1,0
we use five sentiment classification datasets including the widelyused movie review dataset mov pang et al 2002 as well as four datasets containing reviews of four different types of products from amazon books boo dvds dvd electronics ele and kitchen appliances kit blitzer et al 2007 ,0,1,0
however such methods require the existence of either a parallel corpusmachine translation engine for projectingtranslating annotationslexica from a resourcerich language to the target language banea et al 2008 wan 2008 or a domain that is similar enough to the target domain blitzer et al 2007 ,0,1,0
the problem itself has started to get attention only recently roark and bacchiani 2003 hara et al 2005 daume iii and marcu 2006 daume iii 2007 blitzer et al 2006 mcclosky et al 2006 dredze et al 2007 ,0,1,0
2 motivation and prior work while several authors have looked at the supervised adaptation case there are less and especially less successful studies on semisupervised domain adaptation mcclosky et al 2006 blitzer et al 2006 dredze et al 2007 ,0,0,1
while scl has been successfully applied to pos tagging and sentiment analysis blitzer et al 2006 blitzer et al 2007 its effectiveness for parsing was rather unexplored ,1,0,0
similarly structural correspondence learning blitzer et al 2006 blitzer et al 2007 blitzer 2008 has proven to be successful for the two tasks examined pos tagging and sentiment classification ,1,0,0
4 structural correspondence learning scl structural correspondence learning blitzer et al 2006 blitzer et al 2007 blitzer 2008 is a recently proposed domain adaptation technique which uses unlabeled data from both source and target domain to learn correspondences between features from different domains ,0,1,0
on a separate note previous research has explicitly studied sentiment analysis as an application of transfer learning blitzer et al 2007 ,0,1,0
as the training data from dvds is much more similar to books than that from kitchen blitzer et al 2007 we should give the data from dvds a higher weight ,0,1,0
various machine learning strategies have been proposed to address this problem including semisupervised learning zhu 2007 domain adaptation wu and dietterich 2004 blitzer et al 2006 blitzer et al 2007 arnold et al 2007 chan and ng 2007 daume 2007 jiang and zhai 2007 reichart and rappoport 2007 andreevskaia and bergler 2008 multitask learning caruana 1997 reichart et al 2008 arnold et al 2008 selftaught learning raina et al 2007 etc a commonality among these methods is that they all require the training data and test data to be in the same feature space ,0,1,0
training set labeled english reviews there are many labeled english corpora available on the web and we used the corpus constructed for multidomain sentiment classification blitzer et al 2007 9 because the corpus was largescale and it was within similar domains as the test set ,0,1,0
we will employ the structural correspondence learning scl domain adaption algorithm used in blitzer et al 2007 for linking the translated text and the natural text ,0,1,0
finally recent efforts have also looked at transfer learning mechanisms for sentiment analysis eg see blitzer et al 2007 ,0,1,0
and 20ng is a collection of approximately 20000 20category documents 1 in sentiment text classification we also use two data sets one is the widely used cornell moviereview dataset2 pang and lee 2004 and one dataset from product reviews of domain dvd3 blitzer et al 2007 ,0,1,0
4 evaluation 41 experimental setup for evaluation we use five sentiment classification datasets including the widelyused movie review dataset mov pang et al 2002 as well as four datasets that contain reviews of four different types of product from amazon books boo dvds dvd electronics ele and kitchen appliances kit blitzer et al 2007 ,0,1,0
the second one needs no labeled data for the new domain blitzer et al 2007 tan et al 2007 andreevskaia and bergler 2008 tan et al 2008 tan et al 2009 ,0,1,0
we also compare our algorithm to structural correspondence learning scl blitzer et al 2007 ,0,1,0
seen from table 2 our result about scl is in accord with that in blitzer et al 2007 on the whole ,0,1,0
3 experiments we evaluated the effect of random feature mixing on four popular learning methods perceptron mira crammer et al 2006 svm and maximum entropy with 4 nlp datasets 20 newsgroups1 reuters lewis et al 2004 sentiment blitzer et al 2007 and spam bickel 2006 ,1,0,0
5 conclusions and future work the paper compares structural correspondence learning blitzer et al 2006 with various instances of selftraining abney 2007 mcclosky et al 2006 for the adaptation of a parse selection model to wikipedia domains ,0,1,0
we examine structural correspondence learning scl blitzer et al 2006 for this task and compare it to several variants of selftraining abney 2007 mcclosky et al 2006 ,0,1,0
2 previous work so far structural correspondence learning has been applied successfully to pos tagging and sentiment analysis blitzer et al 2006 blitzer et al 2007 ,1,0,0
the techniques examined are structural correspondence learning scl blitzer et al 2006 and selftraining abney 2007 mcclosky et al 2006 ,0,1,0
scl for discriminative parse selection so far pivot features on the word level were used blitzer et al 2006 blitzer et al 2007 ,0,1,0
wehope the present work will together with talbot and osborne 2007 establish the bloom filter as a practical alternative to conventional associative data structures used in computational linguistics ,0,1,0
in this paper we build on recent work talbot and osborne 2007 that demonstrated how the bloom filter bloom 1970 bf a spaceefficient randomised data structure for representing sets could be used to store corpus statistics efficiently ,0,1,0
our framework makes use of the logfrequency bloom filter presented in talbot and osborne 2007 and described briefly below to compute smoothed conditional ngram probabilities on the fly ,0,1,0
3 language modelling with bloom filters recentworktalbotandosborne 2007presenteda scheme for associating static frequency information with a set of ngrams in a bf efficiently1 31 logfrequency bloom filter the efficiency of the scheme for storing ngram statistics within a bf presented in talbot and osborne 2007 relies on the zipflike distribution of ngramfrequencies mosteventsoccuranextremely small number of times while a small number are very frequent ,1,0,0
321 proxy items there is a potential risk of redundancy if we represent related statistics using the logfrequency bf scheme presented in talbot and osborne 2007 ,0,1,0
also the use of lossy data structures based on bloom filters has been demonstrated to be effective for lms talbot and osborne 2007a talbot and osborne 2007b ,0,1,0
there also have been prior work on maintaining approximate counts for higherorder language models lms talbot and osborne 2007a talbot and osborne 2007b talbot and brants 2008 operates under the model that the goal is to store a compressed representation of a diskresident table of counts and use this compressed representation to answer count queries approximately ,1,0,0
since the use of cluster of machines is not always practical talbot and osborne 2007b talbot and osborne 2007a showed a randomized data structure called bloom filter that can be used to construct space efficient language models 513 for smt ,1,0,0
3 spaceefficient approximate frequency estimation prior work on approximate frequency estimation for language models provide a nofalsenegative guarantee ensuring that counts for ngrams in the model are returned exactly while working to make sure the falsepositive rate remains small talbot and osborne 2007a ,1,0,0
following talbot and osborne 2007a we can avoid unnecessary false positives by not querying for the longer ngram in such cases ,1,0,0
however if we are willing to accept that occasionally our model will be unable to distinguish between distinct ngrams then it is possible to store each parameter in constant space independent of both n and the vocabulary size carter et al 1978 talbot and osborne 2007a ,0,1,0
note that unlike the constructions in talbot and osborne 2007b and church et al 2007 no errors are possible for ngrams stored in the model ,0,1,0
we have also implemented a bloom filter lm in joshua following talbot and osborne 2007 ,0,1,0
1 introduction supervised statistical parsers attempt to capture patterns of syntactic structure from a labeled set of examples for the purpose of annotating new sentences with their structure bod 2003 charniak and johnson 2005 collins and koo 2005 petrov et al 2006 titov and henderson 2007 ,0,1,0
we use a recently proposed dependency parser titov and henderson 2007b1 which has demonstrated stateoftheart performance on a selection of languages from the 1the isbn parser will be soon made downloadable from the authors webpage ,1,0,0
when conditioning on words we treated each word feature individually as this proved to be useful in titov and henderson 2007b ,1,0,0
isbns originally proposed for constituent parsing in titov and henderson 2007a use vectors of binary latent variables to encode information about the parse history ,0,1,0
in fact in titov and henderson 2007a it was shown that this neural network can be viewed as a coarse approximation to the corresponding isbn model ,0,1,0
in our experiments we use the same definition of structural locality as was proposed for the isbn dependency parser in titov and henderson 2007b ,0,1,0
unlike titov and henderson 2007b in the shared task we used only the simplest feedforward approximation which replicates the computation of a neural network of the type proposed in henderson 2003 ,0,1,0
to search for the most probable parse we use the heuristic search algorithm described in titov and henderson 2007b which is a form of beam search ,0,1,0
as was demonstrated in titov and henderson 2007b even a minimal set of local explicit features achieves results which are nonsignificantly different from a carefully chosen set of explicit features given the language independent definition of locality described in section 2 ,0,1,0
this curve plots the average labeled attachment score over basque chinese english and turkish as a function of parsing time per token4 accuracy of only 1 below the maximum can be achieved with average processing time of 17 ms per token or 60 tokens per second5 we also refer the reader to titov and henderson 2007b for more detailed analysis of the isbn dependency parser results where among other things it was shown that the isbn model is especially accurate at modeling long dependencies ,0,1,0
51 the statistical parser the parsing model is the one proposed in merlo and musillo 2008 which extends the syntactic parser of henderson 2003 and titov and henderson 2007 with annotations which identify semantic role labels and has competitive performance ,0,1,0
previous research in this area includes several models which incorporate hidden variables matsuzaki et al 2005 koo and collins 2005 petrov et al 2006 titov and henderson 2007 ,0,1,0
as discussed in titov and henderson 2007 computing the conditional probabilities which we need for parsing is in general intractable with isbns but they can be approximated efficiently in several ways ,1,0,0
we expect that the mean field approximation should demonstrate better results than feedforward approximation on this task as it is theoretically expected and confirmed on the constituent parsing task titov and henderson 2007 ,0,1,0
as discussed in titov and henderson 2007 undirected graphical models do not seem to be suitable for historybased parsing models ,0,1,0
the extension of dynamic sbns with incrementally specified model structure ie incremental sigmoid belief networks used in this paper was proposed and applied to constituent parsing in titov and henderson 2007 ,0,1,0
145 2 the latent variable architecture in this section we will begin by briefly introducing the class of graphical models we will be using incremental sigmoid belief networks titov and henderson 2007 ,0,1,0
they are latent variable models which are not tractable to compute exactly but two approximations exist which have been shown to be effective for constituent parsing titov and henderson 2007 ,1,0,0
incremental sigmoid belief networks titov and henderson 2007 differ from simple dynamic sbns in that they allow the model structure to depend on the output variable values ,0,1,0
146 23 approximating isbns titov and henderson 2007 proposes two approximations for inference in isbns both based on variational methods ,0,1,0
titov and henderson 2007 proposes two approximate models based on the variational approach ,0,1,0
the second approximation proposed in titov and henderson 2007 takes into consideration the fact that after each decision is made all the preceding latent variables should have their means i updated ,0,1,0
following titov and henderson 2007 we describe the original parsing architecture and our modifications to it as a dynamic bayesian network ,0,1,0
for more detail explanations and experiments see titov and henderson 2007 ,0,1,0
parsing titov and henderson 2007 ,0,1,0
our probabilistic model is based on incremental sigmoid belief networks isbns a recently proposed latent variable model for syntactic structure prediction which has shown very good behaviour for both constituency titov and henderson 2007a and dependency parsing titov and henderson 2007b ,1,0,0
21 synchronous derivations the derivations for syntactic dependency trees are the same as specified in titov and henderson 2007b which are based on the shiftreduce style parser of nivre et al 2006 ,0,1,0
pctdc1ct1 producttextipdiddbtdd di1d c1ct1 3 the actions are also sometimes split into a sequence of elementary decisions di di1din as discussed in titov and henderson 2007a ,0,1,0
as with many dependency parsers nivre et al 2006 titov and henderson 2007b we handle nonprojective ie crossing arcs by transforming them into noncrossing arcs with augmented labels1 because our syntactic derivations are equivalent to those of nivre et al 2006 we use their head methods to projectivise the syntactic dependencies ,0,1,0
3 the learning architecture the synchronous derivations described above are modelled with an incremental sigmoid belief network isbn titov and henderson 2007a ,0,1,0
neural networks have been used in nlp in the past eg for machine translation asuncion castano et al 1997 and constituent parsing titov and henderson 2007 ,0,1,0
recently some generic methods were proposed to handle contextsensitive inference dagan et al 2006 pantel et al 2007 downey et al 2007 connor and roth 2007 but these usually treat only a single aspect of context matching see section 6 ,0,1,0
downey et al 2007 use hmmbased similarity for the same purpose ,0,1,0
realm uses an hmm trained on a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type downey et al 2007 ,0,1,0
the idea of bidirectional parsing is related to the bidirectional sequential classification method described in shen et al 2007 ,0,1,0
the learning algorithm for level0 dependency is similar to the guided learning algorithm for labelling as described in shen et al 2007 ,0,1,0
the stateofthe art taggers are using feature sets discribed in the corresponding articles collins 2002 gimenez and marquez 2004 toutanova et al 2003 and shen et al 2007 morce supervised and morce semisupervised are using feature set desribed in section 4 ,1,0,0
the combination is significantly better than shen et al 2007 at a very high level but more importantly shens results currently representing the replicable stateoftheart in pos tagging have been significantly surpassed also by the semisupervised morce at the 99 confidence level ,0,0,1
finally it would be nice to merge some of the approaches by toutanova et al 2003 and shen et al 2007 with the ideas of semisupervised learning introduced here since they seem orthogonal in at least some aspects eg to replace the rudimentary lookahead features with full bidirectionality ,0,1,0
as a result of this tuning our fully supervised version of the morce tagger gives the best accuracy among all single taggers for czech and also very good results for english being beaten only by the tagger shen et al 2007 by 010 absolute and not significantly by toutanova et al 2003 ,1,0,0
3 the data 31 the supervised data for english we use the same data division of penn treebank ptb parsed section marcus et al 1994 as all of collins 2002 toutanova et al 2003 gimenez and marquez 2004 and shen et al 2007 do for details see table 1 ,0,1,0
in the following sections we present the best performing set of feature templates as determined on the development data set using only the supervised training setting our feature templates have thus not been influenced nor extended by the unsupervised data13 11the full list of tags as used by shen et al 2007 also makes the underlying viterbi algorithm unbearably slow ,0,1,0
most recently suzuki and isozaki 2008 published their semisupervised sequential labelling method whose results on pos tagging seem to be optically better than shen et al 2007 but no significance tests were given and the tool is not available for download ie for repeating the results and significance testing ,0,0,1
for english we use three stateoftheart taggers the taggers of toutanova et al 2003 and shen et al 2007 in step 1 and the svm tagger gimenez and marquez 2004 in step 3 ,1,0,0
test additional resources jesscm crfhmm 9735 9740 1gword unlabeled data shen et al 2007 9728 9733 toutanova et al 2003 9715 9724 crude company name detector sup ,0,1,0
5 comparison with previous top systems and related work in pos tagging the previous best performance was reported by shen et al 2007 as summarized in table 7 ,1,0,0
for our pos tagging experiments we used the wall street journal in ptb iii marcus et al 1994 with the same data split as used in shen et al 2007 ,0,1,0
5 bidirectional sequence classification bidirectional pos tagging shen et al 2007 the current state of the art for english has some properties that make it appropriate for icelandic ,0,1,0
shen et al 2007 report an accuracy of 9733 on the same data set using a perceptronbased bidirectional tagging model ,0,1,0
networks toutanova et al 2003 9724 svm gimenez and marquez 2003 9705 me based a bidirectional inference tsuruoka and tsujii 2005 9715 guided learning for bidirectional sequence classification shen et al 2007 9733 adaboostsdf with candidate features 21100 wdist 9732 adaboostsdf with candidate features 21010 fdist 9732 svm with candidate features c01 d2 9732 text chunking f1 regularized winnow full parser output zhang et al 2001 9417 svmvoting kudo and matsumoto 2001 9391 aso unlabeled data ando and zhang 2005 9439 crfrerankingkudo et al 2005 9412 me based a bidirectional inference tsuruoka and tsujii 2005 9370 laso approximate large margin update daume iii and marcu 2005 944 hysol suzuki et al 2007 9436 adaboostsdf with candidate featuers 21 wdist 9432 adaboostsdf with candidate featuers 21010wdist 9430 svm with candidate features c1 d2 9431 one of the reasons that boostingbased classifiers realize faster classification speed is sparseness of rules ,0,1,0
lscript1regularized loglinear models lscript1llms on the other hand provide sparse solutions in which weights of irrelevant features are exactly zero by assumingalaplacianpriorontheweightstibshirani 1996 kazama and tsujii 2003 goodman 2004 gao et al 2007 ,0,1,0
in other words learning with l1 regularization naturally has an intrinsic effect of feature selection which results in an 97 efficient and interpretable inference with almost the same performance as l2 regularization gao et al 2007 ,0,1,0
there is usually not a considerable difference between the two methods in terms of the accuracy of the resulting model gao et al 2007 but l1 regularization has a significant advantage in practice ,1,0,0
since soon soon et al 2001 started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving coreference resolution problem many machine learningbased systems have been built using both supervised and unsupervised learning methods haghighi and klein 2007 ,0,1,0
crpbased samplers have served the communitywellinrelatedlanguagetaskssuchaswordsegmentation and coreference resolution goldwater et al 2006 haghighi and klein 2007 ,0,1,0
first the addition of each modification improves the fscore for both true and system mentions 9the hk results shown here are not directly comparable with those reported in haghighi and klein 2007 since hk evaluated their system on the ace 2004 coreference corpus ,0,1,0
experimental results indicate that our model outperforms haghighi and kleins 2007 coreference model by a large margin on the ace data sets and compares favorably to a modified version of their model ,0,0,1
for comparison purposes we revisit haghighi and kleins 2007 fullygenerative bayesian model for unsupervised coreference resolution discuss its potential weaknesses and consequently propose three modifications to their model ,0,0,1
3 haghighi and kleins coreference model to gauge the performance of our model we compare it with a bayesian model for unsupervised coreference resolution that was recently proposed by haghighi and klein 2007 ,0,1,0
more recently haghighi and klein 2007 use the distinction between pronouns nominals and proper nouns 660 in their unsupervised generative model for coreference resolution for their model this is absolutely critical for achieving better accuracy ,0,1,0
this therefore suggests that better parameters are likely to be learned in the 2haghighi and kleins 2007 generative coreference model mirrors this in the posterior distribution which it assigns to mention types given their salience see their table 1 ,0,1,0
12poon and domingos 2008 outperformed haghighi and klein 2007 ,0,0,1
1153 while much research ng and cardie 2002 culotta et al 2007 haghighi and klein 2007 poon and domingos 2008 finkel and manning 2008 has explored how to reconcile pairwise decisions to form coherent clusters we simply take the transitive closure of our pairwise decision as in ng and cardie 2002 and bengston and roth 2008 which can and does cause system errors ,0,1,0
the probabilities are ordered according to at least my intuition with pronoun being the most likely 0094 followed by proper nouns 0057 followed by common nouns 0032 a fact also noted by haghighi and klein 2007 ,0,1,0
in addition their system does not classify nonanaphoric pronouns a third paper that has significantly influenced our work is that of haghighi and klein 2007 ,1,0,0
the model of haghighi and klein 2007 incorporated a latent variable for named entity class ,0,1,0
5 discussion as stated above we aim to build an unsupervised generative model for named entity clustering since such a model could be integrated with unsupervised coreference models like haghighi and klein 2007 for joint inference ,0,1,0
named entities also pose another problem with the haghighi and klein 2007 coreference model since it models only the heads of nps it will fail to resolve some references to named entities ford motor co ford while erroneously merging others ford motor co lockheed martin co ,0,1,0
our system improves over the latent namedentity tagging in haghighi and klein 2007 from 61 to 87 ,0,0,1
like haghighi and klein 2007 we give our model information about the basic types of pronouns in english ,0,1,0
secondly while most pronoun resolution evaluations simply exclude nonreferential pronouns recent unsupervised approaches cherry and bergsma 2005 haghighi and klein 2007 must deal with all pronouns in unrestricted text and therefore need robust modules to automatically handle nonreferential instances ,0,0,1
in terms of applying nonparametric bayesian approaches to nlp haghighi and klein 2007 evaluated the clustering properties of dpmms by performing anaphora resolution with good results ,1,0,0
recent work has applied bayesian nonparametric models to anaphora resolution haghighi and klein 2007 lexical acquisition goldwater 2007 and language modeling teh 2006 with good results ,1,0,0
richman and schone 2008 used a method similar to nothman et al ,0,1,0
521 generate english annotated corpus from wikipedia wikipedia provides a variety of data resources for ner and other nlp research richman and schone 2008 ,0,1,0
2008 and distributional methods eg bergsma et al ,0,1,0
2 related work given its potential usefulness in coreference resolution anaphoricity determination has been studied fairly extensively in the literature and can be classified into three categories heuristic rulebased eg paice and husk 1987 lappin and leass 1994 kennedy and boguraev 1996 denber 1998 vieira and poesio 2000 statisticsbased eg bean and riloff 1999 cherry and bergsma 2005 bergsma et al 2008 and learningbased eg evans 2001 ng and cardie 2002a ng 2004 yang et al 2005 denis and balbridge 2007 ,0,1,0
bergsma et al 2008 proposed a distributional method in detecting nonanaphoric pronouns by first extracting the surrounding textual context of the pronoun then gathering the distribution of words that occurred within that context from a large corpus and finally learning to classify these distributions as representing either anaphoric and nonanaphoric pronoun instances ,0,1,0
2008 and distributional methods eg bergsma et al ,0,1,0
instead of analyzing sentences directly aucontraire relies on the textrunner open information extraction system banko et al 2007 banko and etzioni 2008 to map each sentence to one or more tuples that represent the entities in the sentences and the relationships between them eg was born inmozartsalzburg ,0,1,0
recent research in open information extraction banko and etzioni 2008 davidov and rappaport 2008 has shown that we can extract large amounts of relational data from opendomain text with high accuracy ,0,1,0
1 introduction motivation sharing basic intuitions and longterm goals with other tasks within the area of webbased information extraction banko and etzioni 2008 davidov and rappoport 2008 the task of acquiring class attributes relies on unstructured text available on the web as a data source for extracting generallyuseful knowledge ,0,1,0
banko and etzioni 2008 studied open domain relation extraction for which they manually identified several common relation patterns ,0,1,0
2005 choi et al 2006 ku et al 2006 titov and mcdonald 2008 ,0,1,0
specifically aspect rating as an interesting topic has also been widely studied titov and mcdonald 2008a snyder and barzilay 2007 goldberg and zhu 2006 ,0,1,0
titov and mcdonald 2008b proposed a joint model of text and aspect ratings which utilizes a modified lda topic model to build topics that are representative of ratable aspects and builds a set of sentiment predictors ,0,1,0
several researchers also studied featuretopicbased sentiment analysis eg hu and liu 2004 popescu and etzioni 2005 ku et al 2006 carenini et al 2006 mei et al 2007 ding liu and yu 2008 titov and r mcdonald 2008 stoyanov and cardie 2008 lu and zhai 2008 ,0,1,0
for example aspects of a digital camera could include picture quality battery life size color value etc finding such aspects is a challenging research problem that has been addressed in a number of ways hu and liu 2004b gamon et al 2005 carenini et al 2005 zhuang et al 2006 branavan et al 2008 blairgoldensohn et al 2008 titov and mcdonald 2008b titov and mcdonald 2008a ,0,1,0
in recent years sentiment classification has drawn much attention in the nlp field and it has many useful applications such as opinion mining and summarization liu et al 2005 ku et al 2006 titov and mcdonald 2008 ,0,1,0
aspectbased sentiment analysis summarizes sentiments with diverse attributes so that customers may have to look more closely into analyzed sentiments titov and mcdonald 2008 ,0,1,0
identifying transliteration pairs is an important component in many linguistic applications which require identifying outofvocabulary words such as machine translation and multilingual information retrieval klementiev and roth 2006b hermjakob et al 2008 ,0,1,0
we finally also include as alignment candidates those word pairs that are transliterations of each other to cover rare proper names hermjakob et al 2008 which is important for language pairs that dont share the same alphabet such as arabic and english ,0,1,0
identification of terms tobe transliterated ttt must not be confused with recognition of named entities ne hermjakob et al 2008 ,0,1,0
there are many techniques for transliteration and backtransliteration and they vary along a number of dimensions phoneme substitution vs character substitution heuristic vs generative vs discriminative models manual vs automatic knowledge acquisition we explore the third dimension where we see several techniques in use manuallyconstructed transliteration models eg hermjakob et al 2008 ,0,1,0
snow et al 2006 nakov hearst 2008 ,0,1,0
nakov and hearst 2008 solved relational similarity problems using the web as a corpus ,1,0,0
nakov and hearst 2005 gledson and keane 2008 ,0,1,0
the patterns will be manually constructed following the approach of hearst 1992 and nakov and hearst 20086 the example collection for each relation r will be passed to two independent annotators ,0,1,0
they propose a twolevel hierarchy with 5 classes at the first level and 30 classes at the second one other researchers kim and baldwin 2005 nakov and hearst 2008 nastase et al 2006 turney 2005 turney and littman 2005 have used their class scheme and data set ,0,1,0
as a first step semeval2007 task 4 offered many useful insights into the performance of different approaches to semantic relation classification it has also motivated followup research davidov and rappoport 2008 katrenkoandadriaans 2008 nakovandhearst 2008 o seaghdha and copestake 2008 ,0,1,0
pearsons correlation coefficient is a standard measure of the correlation strength between two distributions it can be calculated as follows exy exey radicalbigex2 ex2radicalbigey 2 ey 2 1 where x x1xn and y y1yn are vectors of numerical scores for each paraphrase provided by the humans and the competing systems respectively n is the number of paraphrases to score and ex is the expectation of x cosine correlation coefficient is another popular alternative and was used by nakov and hearst 2008 it can be seen as an uncentered version of pearsons correlation coefficient xybardblxbardblbardblybardbl 2 spearmans rank correlation coefficient is suitable for comparing rankings of sets of items it is a special case of pearsons correlation derived by considering rank indices 12 as item scores it is defined as follows n summationtextx iyi summationtextx i summationtexty iradicalbig nsummationtextx2i summationtextxi2 radicalbig nsummationtexty2i summationtextyi2 3 one problem with using spearmans rank coefficient for the current task is the assumption that swapping any two ranks has the same effect ,0,1,0
the semeval2010 task we present here builds on thework ofnakov nakovand hearst 2006 nakov 2007 nakov 2008b where ncs are paraphrased by combinations of verbs and prepositions ,0,1,0
paraphrasesofthiskind have been shown to be useful in applications such as machine translation nakov 2008a and as an intermediate step in inventorybased classification of abstract relations kim and baldwin 2006 nakov and hearst 2008 ,0,1,0
3the usefulness of position varies significantly in different genres penn and zhu 2008 ,0,1,0
this obviously does not preclude using the audiobased system together with other features such as utterance position length speakers roles and most others used in the literature penn and zhu 2008 ,0,1,0
these models have achieved stateoftheart performance in transcriptbased speech summarization zechner 2001 penn and zhu 2008 ,1,0,0
audio data amenable to summarization include meeting recordings murray et al 2005 telephone conversations zhu and penn 2006 zechner 2001 news broadcasts maskey and hirschberg 2005 christensen et al 2004 presentations he et al 2000 zhang et al 2007 penn and zhu 2008 etc although extractive summarization is not as ideal as abstractive summarization it outperforms several comparable alternatives ,0,1,0
1 introduction summarizing spoken documents has been extensively studied over the past several years penn and zhu 2008 maskey and hirschberg 2005 murray et al 2005 christensen et al 2004 zechner 2001 ,0,1,0
both were 5gram models with modified kneserney smoothing lossily compressed using a perfecthashing scheme similar to that of talbot and brants 2008 but using minimal perfect hashing botelho et al 2005 ,0,1,0
this fact along with the observation that machine translation quality improves as the amount of monolingual training material increases has lead to the introduction of randomised techniques for representing large lms in small space talbot and osborne 2007 talbot and brants 2008 ,0,1,0
we set our space usage to match the 308 bytes per ngram reported in talbot and brants 2008 and held out just over 1m unseen ngrams to test the error rates of our models ,0,1,0
it is a variant of the batchbased bloomier filter lm of talbot and brants 2008 which we refer to as the tblm henceforth ,0,1,0
any encoding scheme such as the packed representation of talbot and brants 2008 is viable here ,0,1,0
as with other randomised models we construct queries with the appropriate sanity checks to lower the error rate efficiently talbot and brants 2008 ,0,1,0
talbot and brants 2008 used a bloomier filter to encode a lm ,0,1,0
the bloomier filter lm talbot and brants 2008 has a precomputed matching of keys shared between a constant number of cells in the filter array ,0,1,0
there also have been prior work on maintaining approximate counts for higherorder language models lms talbot and osborne 2007a talbot and osborne 2007b talbot and brants 2008 operates under the model that the goal is to store a compressed representation of a diskresident table of counts and use this compressed representation to answer count queries approximately ,0,1,0
a problem mentioned in talbot and brants 2008 is that the algorithm that computes the compressed representation might need to retain the entire database in memory in their paper they design strategies to work around this problem ,0,1,0
either pruning stolcke 1998 church et al 2007 or lossy randomizing approaches talbot and brants 2008 may result in a compact representation for the application runtime ,0,1,0
by using 8bit floating point quantization 1 ngram language models are compressed into 10 gb which is comparable to a lossy representation talbot and brants 2008 ,0,1,0
talbot and brants 2008 show that bloomier filters chazelle et al 2004 can be used to create perfect hash functions for language models ,0,1,0
although some work has been done on syllabifying orthographic forms muller et al 2000 bouma 2002 marchand and damper 2007 bartlett et al 2008 syllables are technically speaking phonological entities that can only be composed of strings of phonemes ,0,1,0
jiampojamarn et al 2008 and bartlett et al 2008 do worse on the english test data than they do on german dutch or french ,0,1,0
thus we can compute the source dependency lm score in the same way we compute the target side score using a procedure described in shen et al 2008 ,0,1,0
due to the lack of a good arabic parser compatible with the sakhr tokenization that we used on the source side we did not test the source dependency lm for arabictoenglish mt when extracting rules with source dependency structures we applied the same wellformedness constraint on the source side as we did on the target side using a procedure described by shen et al 2008 ,0,1,0
in post and gildea 2008 shen et al 2008 target trees were employed to improve the scoring of translation theories ,0,1,0
73 122 baseline system and experimental setup we take bbns hierdec a stringtodependency decoder as described in shen et al 2008 as our baseline for the following two reasons it provides a strong baseline which ensures the validity of the improvement we would obtain ,0,1,0
2 linguistic and context features 21 nonterminal labels in the original stringtodependency model shen et al 2008 a translation rule is composed of a string of words and nonterminals on the source side and a wellformed dependency structure on the target side ,0,1,0
early examples of this work include alshawi 1996 wu 1997 more recent models include yamada and knight 2001 eisner 2003 melamed 2004 zhang and gildea 2005 chiang 2005 quirk et al 2005 marcu et al 2006 zollmann and venugopal 2006 nesson et al 2006 cherry 2008 mi et al 2008 shen et al 2008 ,0,1,0
other factors that distinguish us from previous work are the use of all phrases proposed by a phrasebased system and the use of a dependency language model that also incorporates constituent information although see charniak et al 2003 shen et al 2008 for related approaches ,0,1,0
there is also substantial work in the use of targetside syntax galley et al 2006 marcu et al 2006 shen et al 2008 ,1,0,0
features that consider only targetside syntax and words without considering s can be seen as syntactic language model features shen et al 2008 ,0,1,0
1 introduction phrasebased method koehn et al 2003 och and ney 2004 koehn et al 2007 and syntaxbased method wu 1997 yamada and knight 2001 eisner 2003 chiang 2005 cowan et al 2006 marcu et al 2006 liu et al 2007 zhang et al 2007c 2008a 2008b shen et al 2008 mi and huang 2008 represent the stateoftheart technologies in statistical machine translation smt ,1,0,0
wordaligned corpora have been found to be an excellent source for translationrelated knowledge not only for phrasebased models och and ney 2004 koehn et al 2003 but also for syntaxbased models eg chiang 2007 galley et al 2006 shen et al 2008 liu et al 2006 ,0,1,0
recently shen et al 2008 introduced an approach for incorporating a dependencybased language model into smt ,0,1,0
firstly shen et al 2008 resorted to heuristics to extract the stringtodependency trees whereas our approach employs the well formalized ccg grammatical theory ,0,1,0
this is in direct contrast to recent reported results in which other filtering strategies lead to degraded performance shen et al 2008 zollmann et al 2008 ,0,0,1
extensions to hiero several authors describe extensions to hiero to incorporate additional syntactic information zollmann and venugopal 2006 zhang and gildea 2006 shen et al 2008 marton and resnik 2008 or to combine it with discriminative latent models blunsom et al 2008 ,0,1,0
dependency representation has been used for language modeling textual entailment and machine translation haghighi et al 2005 chelba et al 1997 quirk et al 2005 shen et al 2008 to name a few tasks ,0,1,0
they can be roughly divided into three categories stringtotree models eg galley et al 2006 marcu et al 2006 shen et al 2008 treetostring models eg liu et al 2006 huang et al 2006 and treetotree models eg eisner 2003 ding and palmer 2005 cowan et al 2006 zhang et al 2008 ,0,1,0
on the contrary a stringtotree decoder eg galley et al 2006 shen et al 2008 is a parser that applies stringtotree rules to obtain a target parse for the source string ,0,1,0
this provides a compelling advantage over previous dependency language models for mt shen et al 2008whichusea5gramlmonlyduringreranking ,0,0,1
dependency models have recently gained considerable interest in many nlp applications including machine translation ding and palmer 2005 quirk et al 2005 shen et al 2008 ,0,1,0
1 introduction hierarchical approaches to machine translation have proven increasingly successful in recent years chiang 2005 marcu et al 2006 shen et al 2008 and often outperform phrasebased systems och and ney 2004 koehn et al 2003 on targetlanguage fluency and adequacy ,1,0,0
finally we are investigating several avenues for using this system output for machine translation mt including 1 aiding word alignment for other mt system wang et al 2007 and 2 aiding the creation various mt models involving analyzed text eg gildea 2004 shen et al 2008 ,0,1,0
in the concept extension part of our algorithm we adapt our concept acquisition framework davidov and rappoport 2006 davidov et al 2007 davidov and rappoport 2008a davidov and rappoport 2008b to suit diverse languages including ones without explicit word segmentation ,0,1,0
in computational linguistics our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs hearst 1998 chklovski and pantel 2004 etzioni et al 2004 turney 2006 davidov and rappoport 2008 inter alia ,0,1,0
this approach is similar to that of seed words eg hearst 1998 or hook words eg davidov and rappoport 2008 in previous work ,0,1,0
note that apart from previous work ding et al 2008 we use complete skipchain contextanswer edges in hcxy ,0,1,0
we made use of the same data set as introduced in cong et al 2008 ding et al 2008 ,0,1,0
the suffixes c and v denote the models using incomplete skipchain edges and vertical sequential edges proposed in ding et al 2008 as shown in figures 2a and 2c ,0,1,0
in comparison the 2d model in figure 2c used in previous work ding et al 2008 can only model the interaction between adjacent questions ,0,0,1
our graphical representation has two advantages over previous work ding et al 2008 unifying sentence relations and incorporating question interactions ,0,0,1
previous work ding et al 2008 performs the extraction of contexts and answers in multiple passes of the thread with each pass corresponding to one question which cannot address the interactions well ,0,0,1
we design special inference algorithms instead of generalpurpose inference algorithms used in previous works cong et al 2008 ding et al 2008 by taking advantage of special properties of our task ,0,1,0
1 introduction recently extracting questions contexts and answers from post discussions of online forums incurs increasing academic attention cong et al 2008 ding et al 2008 ,0,1,0
even for many unsupervised situations this is available from a lexicon eg banko and moore 2004 goldberg et al 2008 ,0,1,0
thus an orthogonal line of research can involve inducing classes for words which are more general than single categories ie something akin to ambiguity classes see eg the discussion of ambiguity class guessers in goldberg et al 2008 ,0,1,0
41 complete ambiguity classes ambiguity classes capture the relevant property we are interested in words with the same category possibilities are grouped together4 and ambiguity classes have been shown to be successfully employed in a variety of ways to improve pos tagging eg cutting et al 1992 daelemans et al 1996 dickinson 2007 goldberg et al 2008 tseng et al 2005 ,1,0,0
traditionally such unsupervised emtrained hmm taggers are thought to be inaccurate but goldberg et al 2008 showed that by feeding the em process with sufficiently good initial probabilities accurate taggers 91 accuracy can be learned for both english and hebrew based on a possibly incomplete lexicon and large amount of raw text ,0,1,0
6 smaller tagset and incomplete dictionaries previously researchers working on this task have also reported results for unsupervised tagging with a smaller tagset smith and eisner 2005 goldwater and griffiths 2007 toutanova and johnson 2008 goldberg et al 2008 ,0,1,0
some previous approaches toutanova and johnson 2008 goldberg et al 2008 handle unknown words explicitly using ambiguity class components conditioned on various morphological features and this has shown to produce good tagging results especially when dealing with incomplete dictionaries ,0,1,0
due to its popularity for unsupervised pos induction research eg goldberg et al 2008 goldwater and griffiths 2007 toutanova and johnson 2008 and its oftenused tagset for our initial research we use the wall street journal wsj portion of the penn treebank marcus et al 1993 with 36 tags plus 9 punctuation tags and we use sections 0018 leaving heldout data for future experiments4 defining frequent frames as those occurring at 4even if we wanted childdirected speech the childes database macwhinney 2000 uses coarse pos tags ,0,1,0
regression has also been used to order sentences in extractive summarization biadsy et al 2008 ,0,1,0
instead we follow a simplified form of previous work on biography creation where a classifier is trained to distinguish biographical text zhou et al 2004 biadsy et al 2008 ,0,1,0
instances of this work include information extraction ontology induction and resource acquisition wu and weld 2007 biadsy et al 2008 nastase 2008 nastase and strube 2008 ,0,1,0
citation texts have also been used to create summaries of single scientific articles in qazvinian and radev 2008 and mei and zhai 2008 ,0,1,0
nenkova and louis 2008 investigate how summary length and the characteristics of the input influence the summary quality in multidocument summarization ,0,1,0
for the first set of experiments we divide all inputs based on the mean value of the average system scores as in nenkova and louis 2008 ,0,1,0
only recently the issue has drawn attention nenkova and louis 2008 present an initial analysis of the factors that influence system performance in content selection ,0,1,0
4 features for our experiments we use the features proposed motivated and described in detail by nenkova and louis 2008 ,0,1,0
09595 09590 09611 09085 09134 09152 table 8 comparison of f1 results of our baseline model with nakagawa and uchimoto 2007 and zhang and clark 2008 on ctb 30 ,0,1,0
zhang and clark 2008 indicated that their results cannot directly compare to the results of shi and wang 2007 due to different experimental settings ,0,1,0
2008a 2008b on ctb 50 and zhang and clark 2008 on ctb 40 since they reported the best performances on joint word segmentation and pos tagging using the training materials only derived from the corpora ,0,1,0
following zhang and clark 2008 we first generated ctb 30 from ctb 40 using sentence ids 110364 ,0,1,0
table 8 compares the f1 results of our baseline model with nakagawa and uchimoto 2007 and zhang and clark 2008 on ctb 30 ,0,1,0
word segmentation and pos tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion ng and low 2004 nakagawa and uchimoto 2007 zhang and clark 2008 jiang et al 2008a jiang et al 2008b ,1,0,0
in this paper we used ctb 50 ldc2005t01 as our main corpus defined the training development and test sets according to jiang et al 2008a jiang et al 2008b and designed our experiments to explore the impact of the training corpus size on our approach ,0,1,0
jiang et al 2008a jiang et al 2008b ,0,1,0
for example a perceptron algorithm is used for joint chinese word segmentation and pos tagging zhang and clark 2008 jiang et al 2008a jiang et al 2008b ,0,1,0
in addition the performance of the adapted model for joint st obviously surpass that of jiang et al 2008 which achieves an f1 of 9341 for joint st although with more complicated models and features ,0,0,1
following our previous work jiang et al 2008 ,0,1,0
many nlp systems use the output of supervised parsers eg kwok et al 2001 for qa moldovan et al 2003 for ie punyakanok et al 2008 for srl srikumar et al 2008 for textual inference and avramidis and koehn 2008 for mt ,0,1,0
1 introduction and motivation detecting contradictory statements is an important and challenging nlp task with a wide range of potential applications including analysis of political discourse of scientific literature and more de marneffe et al 2008 condoravdi et al 2003 harabagiu et al 2006 ,0,1,0
automatically determining the degree of antonymy between words has many uses including detecting and generating paraphrases the dementors caught sirius black black could not escape the dementors and detecting contradictions marneffe et al 2008 voorhees 2008 kyoto has a predominantly wet climate it is mostly dry in kyoto ,0,1,0
some other researchers also work on detecting negative cases ie contradiction instead of entailment de marneffe et al 2008 ,0,1,0
this can be the base of a principled method for detecting structural contradictions de marneffe et al 2008 ,0,1,0
by associating natural language with concepts as they are entered into a knowledge a model of semantic analysis all of the following discussion is based on a model of semantic analysis similar to that proposed in hobbs 1985 ,0,1,0
hobbs jerry 1985 ontological promiscuity proceedings of the 23rd annual meeting of the association for computational linguistics chicago illinois pp ,0,1,0
stage 2 processing is then free to assign to the compound any bracketing for which it 3the design of this level of lucy is influenced by hobbs 1985 which advocates a level of surfaey logical form with predicates close to actual english words and a structure similar to the syntactic structure of the sentence ,0,1,0
thus we are lead to an ontologically promiscuous semantics hobbs 1985 ,0,1,0
independently in ai an effort arose to encode large amounts of commonsense knowledge hayes 1979 hobbs and moore 1985 hobbs et al 1985 ,0,1,0
we are encoding the knowledge as axioms in what is for the most part firstorder logic described in hobbs 1985a although quantification over predicates is sometimes convenient ,0,1,0
since so many concepts used in discourse are qaindependent a theory of granularity is also fundamental see hobbs 1985b ,0,1,0
essentially we follow hobbs 1985 in using a rich ontology and a representation scheme that makes explicit all the individuals and abstract objects ie propositions factsbeliefs and eventualities asher 1993 involved in the lf interpretation of an utterance ,0,1,0
we can stipulate the time line to be linearly ordered although it is not in approaches that build ignorance of relative times into the representation of time eg hobbs 1974 nor in approaches employing branching futures eg mcdermott 1985 and we can stipulate it to be dense although it is not in the situation calculus ,0,1,0
independently in artificial intelligence an effort arose to encode large amounts of commonsense knowledge hayes 1979 hobbs and moore 1985 hobbs et al 1985 ,0,1,0
we are encoding the knowledge as axioms in what is for the most part a firstorder logic described by hobbs 1985a although quantification over predicates is sometimes convenient ,0,1,0
since so many concepts used in discourse are graindependent a theory of granularity is also fundamental see hobbs 1985b ,0,1,0
the separation of these two requirements 7 a more precise account of what it means to be able to identify an object is beyond the scope of this paper for further details see the discussions by hobbs 1985 appelt 1985 kronfeld 1986 1990 and morgenstern 1988 ,0,1,0
independently in ai an effort arose to encode large amounts of commonsense knowledge hayes 1979 hobbs and moore 1985 hobbs et al 1985 ,0,1,0
we can stipulate the time line to be linearly ordered although it is not in approaches that build ignorance of relative times into the representation of time eg hobbs 1974 nor in approaches using branching futures eg mcdermott 1985 and we can stipulate it to be dense although it is not in the situation calculus ,0,1,0
since so many concepts used in discourse are graindependent a theory of granularity is also fundamental see hobbs 1985b ,0,1,0
ssee hobbs 1985a for explanation of this notation for events ,0,1,0
4for justification for this kind of logical form for sentences with quantifiers and inteusional operators see hobbs1983 and hobbs 1985a ,0,1,0
they have made semantic formalisms like those now usually associated with davison davidson 1980 parsons 1990 attractive in artificial intelligence for many years hobbs 1985 kay 1970 ,0,1,0
first we adopt an ontologically promiscuous representation hobbs 1985 that includes a wide variety of types of entities ,0,1,0
we do not completely rule out the possibility that some more sophisticated ontologically promiscuous firstorder analysis perhaps along the lines of hobbs 1985 might account for these kinds of monotonicity inferences ,1,0,0
more sophisticated firstorder accounts hirst 1991 hobbs 1985 may be extendable to bear this load ,1,0,0
the mlfs use reification to achieve flat expressions very much in the line of davidson 1967 hobbs 1985 and copestake et al ,0,1,0
note that the predicate language representation utilized by carmeltools is in the style of davidsonian event based semantics hobbs 1985 ,0,1,0
after the parser produces a semantic feature structure representation of the sentence predicate mapping rules then match against that representation in order to produce a predicate language representation in the style of davidsonian event based semantics davidson 1967 hobbs 1985 as mentioned above ,0,1,0
we adopt their idea of an utterance as a description generated from a communicative goal and also use an ontologically promiscuous formalism for representing meaning hobbs 1985 ,0,1,0
moreover as stated in hobbs 1985 we assume that the alleged predicate is existentially opaque in its second argument ,0,1,0
doing inference with representations close to natural language has also been advocated by jerry hobbs as in hobbs 1985 ,0,1,0
second in keeping with ontological promiscuity hobbs 1985 we represent the importance of attributes by the salience of events and states in the discourse modelthese states and events now have the same status in the discourse model as any other entities ,0,1,0
first as originally advocated by hobbs 1985 we adopt an ontologically promiscuous representation that includes a wide variety of types of entities ,0,1,0
previous literature on gb parsing wehrli 1984 sharp 1985 kashket 1986 kuhns 1986 abney 1986has not addressed the issue of implementation of the binding theory the present paper intends in part to fill this gap ,0,0,1
formal complexity analysis has not been carried out but my algorithm is simpler at least conceptually than the variablewordorder parsers of johnson 1985 kashket 1986 and abramson and dahl 1989 ,0,0,1
although the parser is not yet complete we expect that its breath of coverage of the language will be substantially larger than that of other governmentbinding parsers recently reported in the literature kashket 1986 kuhns 1986 sharp 1985 and wehrli 1984 ,0,0,1
there are similarities with dependency grammars here because such constraint graphs are also produced by dependency grammars covington 1990 kashket 1986 ,0,1,0
the construction is defined in fillmores 1988 construction grammar as a pairing of a syntactic pattern with a meaning structure they are similar to signs in hpsg pollard sag 1987 and patternconcept pairs wilensky arens 1980 wilensky et al 1988 ,0,1,0
see also kaplan et al 1988 on the latter point ,0,1,0
recently an elegant approach to inference in discourse interpretation has been developed at a number of sites eg ltobbs et al 1988 charniak and goldman 1988 norvig 1987 all based on tim notion of abduction and we have begun to explore its potential application to machine translation ,1,0,0
probability based commensurability charniak and goldman 1988 started out with a model very similar to hobbs et al but became concerned with 227 the lack of theoretical grounding for ihe number in rules much as we were ,0,1,0
we suggest two ways to do it a version of obbs et als 1988 1990 generation as abduction and the interactive defaults strategy introduced by aoshi et al 1984a 1984b 1986 ,0,1,0
modulo more minor differences these notions are close to the ideas of interpretation as abduction hobbs et al 1988 and generation as abduction ltobbs et al 19902628 where we take abduction in the former case for instance to be a process returning a temporalcausal structure which can explain the utterance in context ,0,1,0
ordinary prologstyle backchaining deduction is augmented with the capability of making assumptions and of factoring two goal literals that are unifiable see hobbs et al 1988 ,0,1,0
we borrow this useful term from the core language engine project alshawi et al 1988 1989 ,0,1,0
it also has close links with theoretical work in situation semantics pollard and sag 1988 fenstad et al 1987 ,0,1,0
walker et al forthcoming and boguraev and briscoe 1988 ,0,1,0
hobbs et al 1988 charniak and goldman 1988 ,0,1,0
1980 walker 1978 fink and biermann 1986 mudler and paulus 1988 carbonell and pierrel 1988 young 1990 and young et al ,0,1,0
2005 is to translate dependency parses into neodavidsonianstyle quasilogical forms and to perform weighted abductive theorem proving in the tradition of hobbs et al 1988 ,0,1,0
volume 17 number 1 march 1991 references lakoff george and johnson mark metaphors we live 8y university of chicago press 1980 madcow committee hirschman lynette et al multisite data collection for a spoken language corpus in proceedings speech and natural language workshop february 1992 grice h p logic and conversation in p cole and j l morgan speech acts new york academic press 1975 pustejovsky james the generative lexicon computational linguistics volume 17 number 4 december 1991 hobbs jerry r and stickel mark interpretation as abduction in proceedings of the 26th acl june 1988 bobrow r ingria r and stallard d the mapping unit approach to subcategorization in proceedings speech and natural language workshop february 1991 hobbs jerry r and martin paul local pragmatics in proceedings 10th international joint conference on artificial intelligence ijcai87 ,0,1,0
this is known as costbased abduction hobbs et al 1988 ,0,1,0
two main extensions from that work that we are making use of are 1 proofs falling below a user defined cost threshold halt the search 2 a simple variable typing system reduces the number of axioms written and the size of the search space hobbs et al 1988 pg 102 ,0,1,0
the domain axioms will bind the body variables to their most likely referents during unification with facts and previously assumed and proven propositions similarly to hobbs et al 1988 ,0,1,0
1 is a set of assumptions sufficient to support the inirlnlation given s and r in other words this is hcrlrctal ion as abduction itobbs et al 1988 since ilion not deduction is needed to arrive at the d h ii itioiis4 ,0,1,0
in hindle1990 zernik 1989 webster el marcus 1989 cooccurrence analyses augmented with syntactic parsing is used for the purpose of word classification ,0,1,0
hindle 1990 hindle and rooths1991 and smadja 1991 use syntactic markers to increase the significance of the data ,0,1,0
combining statistical and parsing methods has been done by hindle 1990 hindle and rooths1991 and smadja and mckewon 1990 smadja1991 ,0,1,0
hindle 1990 classified nouns on the basis of cooccurring patterns of subjectverb and verbobject pairs ,0,1,0
probably the most widely used association weight function is pointwise mutual information mi church et al 1990 hindle 1990 lin 1998 dagan 2000 defined by log 2 fpwp fwpfwmi a known weakness of mi is its tendency to assign high weights for rare features ,1,0,0
1 introduction distributional similarity has been an active research area for more than a decade hindle 1990 ruge 1992 grefenstette 1994 lee 1997 lin 1998 dagan et al 1999 weeds and weir 2003 ,0,1,0
22 cooccurrencebased approaches the second class of algorithms uses cooccurrence statistics hindle 1990 lin 1998 ,0,1,0
hindle 1990 used nounverb syntactic relations and hatzivassiloglou and mckeown 1993 used coordinated adjectiveadjective modifier pairs ,0,1,0
others proposed distributional similarity measures between words hindle 1990 lin 1998 lee 1999 weeds et al 2004 ,0,1,0
there bas recently been work in the detection of semantically related nouns via for example shared argument structures hindle 1990 and shared dictionary definition context wilks e al 1990 ,0,1,0
among the applications of collocational analysis for lexical acquisition are the derivation of syntactic disambiguation cues basili et al 1991 1993a hindle and rooths 19911993 sekine 1992 bogges et al 1992 sense preference yarowski 1992 acquisition of selectional restrictions basili et al 1992b 1993b utsuro et al 1993 lexical preference in generation smadjia 1991 word clustering pereira 1993 hindle 1990 basili et al 1993c etc in the majority of these papers even though the precedent or subsequent statistical processing reduces the number of accidental associations very large corpora 10000000 words are necessary to obtain reliable data on a large enough number of words ,0,0,1
have been proposed hindle 1990 brown et al 1992 pereira et al 1993 tokunaga et al 1995 ,0,1,0
4 towards an adequate similarity esfimatation for the building of ontologies the comparison with the similarity score of hindle 1990 shows that syclade similarity indicator is specifically relevant for ontology bootstrap and tuning ,0,1,0
hindle uses the observed frequencies within a specific syntactic pattern subjectverb and verbobject to derive a cooccu rence score which is an estimate of mutual information church and hanks 1990 ,0,1,0
in the past five years important research on the automatic acquisition of word classes based on lexical distribution has been published church and hanks 1990 hindle 1990 smadja 1993 greinstette 1994 grishman and sterling 1994 ,0,1,0
section 4 compares our results to itindles ones hindle 1990 ,0,1,0
itowever harris methodology implies also to simplify and transform each parse tree 2 so as to obtain socalled elementary sentences exhibiting the main conceptual classes for the domain sager liaor instance hindle hindle 1990 needs a six million word corpus in order to extract noun similarities from predicateargunlent structures ,0,1,0
23 measuring the similarity between classes step 3 in step 3 we measure the similarity between two primitive classes by using the method given by hindle hindle 1990 ,0,1,0
since a handmade thesaurus is not slfitahle for machine use and expensive to compile automatical construction ofa thesaurus has been attempted using corpora hindle 1990 ,0,1,0
distributional approaches on the other hand rely on text corpora and model relatedness by comparing the contexts in which two words occur assuming that related words occur in similar context eg hindle 1990 lin 1998 mohammad and hirst 2006 ,0,1,0
semantic collocations are harder to extract than cooccurrence patternsthe state of the art does not enable us to find semantic collocations automatically t this paper however argues that if we take advantage of lexicai paradigmatic behavior underlying the lexicon we can at least achieve semiautomatic extraction of semantic collocations see also calzolari and bindi 1990 i but note the important work by hindle hindlego on extracting semantically similar nouns based on their substitutability in certain verb contexts ,1,0,0
words appearing in similax grammatical contexts are assumed to be similar and therefore classified into the same class lin 1998 grefenstette 1994 grefenstette 1992 ruge 1992 hindle 1990 ,0,1,0
some researchers hindle 1990 grefenstette 1994 lin 1998 classify terms by similarities based on their distributional syntactic patterns ,0,1,0
a wide range of contextual information such as surrounding words lowe and mcdonald 2000 curran and moens 2002a dependency or case structure hindle 1990 ruge 1997 lin 1998 and dependency path lin and pantel 2001 pado and lapata 2007 has been utilized for similarity calculation and achieved considerable success ,1,0,0
features identified using distributional similarity have previously been used for syntactic and semantic disambiguation hindle 1990 dagan pereira and lee 1994 and to develop lexical resources from corpora lin 1998 riloff and jones 1999 ,0,1,0
similaritybased smoothing hindle 1990 brown et al 1992 dagan marcus and markovitch 1993 pereira tishby and lee 1993 dagan lee and pereira 1999 provides an intuitively appealing approach to language modeling ,0,1,0
45 hindles measure hindle 1990 proposed an mibased measure which he used to show that nouns could be reliably clustered based on their verb cooccurrences ,0,1,0
this hypothesized relationship between distributional similarity and semantic similarity has given rise to a large body of work on automatic thesaurus generation hindle 1990 grefenstette 1994 lin 1998a curran and moens 2002 kilgarriff 2003 ,0,1,0
most work on corpora of naturally occurring language 244 michael r brent from grammar to lexicon either uses no a priori grammatical knowledge brill and marcus 1992 ellison 1991 finch and chater 1992 pereira and schabes 1992 or else it relies on a large and complex grammar hindle 1990 1991 ,0,1,0
many other projects have used statistics in a way that summarizes facts about the text but does not draw any explicit conclusions from them finch and chater 1992 hindle 1990 ,0,1,0
we have found however that collocational evidence can be employed to suggest which noun compounds reflect taxonomic relationships using a strategy similar to that employed by hindle 1990 for detecting synonyms ,0,1,0
hindle 1990 ,0,1,0
using techniques described in church and hindle 1990 church and hanks 1990 and hindle and rooth 1991 figure 4 shows some examples of the most frequent vo pairs from the ap corpus ,0,1,0
hindle 1990 reports interesting results of this kind based on literal collocations where he parses the corpus hindle 1983 into predicateargument structures and applies a mutual information measure fano 1961 magerman and marcus 1990 to weigh the association between the predicate and each of its arguments ,0,1,0
the use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research church and hanks 1990 zernik and jacobs 1990 hindle 1990 smadja 1993 ,0,1,0
more specifically two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment hindle and rooth 1991 and pronoun references dagan and itai 1990 1991 ,0,1,0
predicate argument structures which consist of complements case filler nouns and case markers and verbs have also been used in the task of noun classification hindle 1990 ,0,1,0
3239 proceedings of hltnaacl 2003 similar distribution patterns hindle 1990 peraira et al 1993 grefenstette 1994 ,0,1,0
there have been many approaches to compute the similarity between words based on their distribution in a corpus hindle 1990 landauer and dumais 1997 lin 1998 ,0,1,0
one approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus hindle 1990 lin 1998 ,0,1,0
this second source of evidence is sometimes referred to as distributional similarity hindle 1990 ,0,1,0
researchers have mostly looked at representing words by their surrounding words lund and burgess 1996 and by their syntactical contexts hindle 1990 lin 1998 ,0,1,0
4 building noun similarity lists a lot of work has been done in the nlp community on clustering words according to their meaning in text hindle 1990 lin 1998 ,0,1,0
to date researchers have harvested with varying success several resources including concept lists lin and pantel 2002 topic signatures lin and hovy 2000 facts etzioni et al 2005 and word similarity lists hindle 1990 ,0,1,0
for example hindle 1990 used cooccurrences between verbs and their subjects and objects and proposed a similarity metric based on mutual information but no exploration concerning the effectiveness of other kinds of word relationship is provided although it is extendable to any kinds of contextual information ,1,0,0
various methods hindle 1990 lin 1998 hagiwara et al 2005 have been proposed for synonym acquisition ,0,1,0
32 mncousin classification the classifier for learning coordinate terms relies on the notion of distributional similarity ie the idea that two words with similar meanings will be used in similar contexts hindle 1990 ,0,1,0
many methods have been proposed to compute distributional similarity between words eg hindle 1990 pereira et al 1993 grefenstette 1994 and lin 1998 ,0,1,0
we use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space hindles 1990 measure the weighted lin measure wu and zhou 2003 the skew divergence measure lee 1999 the jensenshannon js divergence measure lin 1991 jaccards coef cient van rijsbergen 1979 and the confusion probability essen and steinbiss 1992 ,0,1,0
we will be using the similarity metrics shown in table 1 cosine the dice and jaccard coefficients and hindles 1990 and lins 1998 mutual informationbased metrics ,0,1,0
hindle 1990 uses a mutualinformation based metric derived from the distribution of subject verb and object in a large corpus to classify nouns ,0,1,0
our method is thus related to previous work based on harris 1985s distributional hypothesis2 it has been used to determine both word and syntactic path similarity hindle 1990 lin 1998a lin and pantel 2001 ,0,1,0
hindle 1990 grouped nouns into thesauruslike lists based on the similarity of their syntactic contexts ,0,1,0
syntactic context information is used hindle 1990 ruge 1992 lin 1998 to compute term similarities based on which similar words to a particular word can directly be returned ,0,1,0
this has been now an active research area for a couple of decades hindle 1990 lin 1998 weeds and weir 2003 ,0,1,0
the use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research church and hanks 1990 zernik and jacobs 1990 hindle 1990 ,0,1,0
more specifically two recent works have suggested to use statistical data on lexical relations for resolving ambiguity cases of ppattachment hindle and rooth 1990 and pronoun references dagan and itai 1990a dagan and itai 1990b ,0,1,0
his results may be improved if more sophisticated techniques and larger corpora are used to establish similarity between words such as in hindle 1990 ,0,1,0
three recent papers in this area are church and hanks 1990 hindle 1990 and smadja and mckeown 1990 ,0,1,0
1 a i expected nv the man who smoked np to eat icecream h i doubted np the man who liked to eat icecream np current highcoverage parsers tend to use either custom handgenerated lists of subcategorization frames eg hindle 1983 or published handgenerated lists like the ozford advanced learners dictionary of contemporary english hornby and covey 1973 eg demarcken 1990 ,0,1,0
8interestingly in work on the automated classification of nouns hindle 1990 also noted problems with empty words that depend on their complements for meaning ,1,0,0
in comparison most corpusbased algorithms employ substantially larger corpora eg 1 million words de marcken 1990 25 million words brent 1991 6 million words hindle 1990 13 million words hindle rooth 1991 ,0,1,0
statistical data about these various cooccurrence relations is employed for a variety of applications such as speech recognition jelinek 1990 language generation smadja and mckeown 1990 lexicography church and hanks 1990 machine translation brown et al sadler 1989 information retrieval maarek and smadja 1989 and various disambiguation tasks dagan et al 1991 hindle and rooth 1991 grishman et al 1986 dagan and itai 1990 ,0,1,0
hindle 1990 proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of similar events that have been seen ,0,1,0
some researchers apply shallow or partial parsers smadja 1991 hindle 1990 to acquiring specific patterns from texts ,0,1,0
in fact we are considering word usage rather than word meanin zernik 1990 following in this the distributional point of view see harris 1968 hindle 1990 ,0,1,0
statistical or probabilistic methods are often used to extract semantic clusters from corpora in order to build lexical resources for anlp tools hindle 1990 zernik 1990 resnik 1993 or for automatic thesaurus generation grefenstette 1994 ,0,1,0
in hindle 1990 a small set of sample results are presented ,0,1,0
when the value of ilw rwll is unknown we assume that a and c are conditionally independent given b the probability of a b and c cooccurring is estimated by pmle b pmle ab pmle cb where pmle is the maximum likelihood estimation of a probability distribution and pleb iill p eaib iill p lecib when the value of hw r wh is known we can obtain pmlea b c directly pmlea b c w r wll h let iwrw denote the amount information contained in hw rwc its value can be corn769 simgindzewl w2 rwetcwlntcw2aresubjofobjof miniwl r w iw2 r w simhindte wl w2 rwetwntw2 miniwl r w iw2 r w twlntw2i simcosinewlw2 xizwllzw2l 2x itwlnzw2l simdicewl w2 itwllltw2 i simjacard wl w2 twl otw2l twl tw2litwlrltw2l figure 1 other similarity measures puted as follows iwrw iogpmlebpmleabpmlecib log pmlea b c log iiwrwflllrll iiwrll xllrwll it is worth noting that iwrw is equal to the mutual information between w and w hindle 1990 ,0,1,0
the measure simhinate is the same as the similarity measure proposed in hindle 1990 except that it does not use dependency triples with negative mutual information ,0,1,0
arguably the most widely used is the mutual information hindle 1990 church and hanks 1990 dagan et al 1995 luk 1995 d lin 1998a ,1,0,0
the most frequently used resource for synonym extraction is large monolingual corpora hindle 1990 crouch and yang 1992 grefenstatte 1994 park and choi 1997 gasperin et al 2001 and lin 1998 ,0,1,0
for example the words corruption and abuse are similar because both of them can be subjects of verbs like arouse become betray cause continue cost exist force go on grow have increase lead to and persist etc and both of them can modify nouns like accusation act allegation appearance and case etc many methods have been proposed to compute distributional similarity between words eg hindle 1990 pereira et al 1993 grefenstette 1994 and lin 1998 ,0,1,0
for example the words test and exam are similar because both of them follow verbs such as administer cancel cheat on conduct and both of them can be preceded by adjectives such as academic comprehensive diagnostic difficult many methods have been proposed to compute distributional similarity between words hindle 1990 pereira et al 1993 grefenstette 1994 lin 1998 ,0,1,0
for example the words test and exam are similar because both of them can follow verbs such as administer cancel cheat on conduct etc many methods have been proposed to compute distributional similarity between words eg hindle 1990 pereira et al 1993 grefenstette 1994 lin 1998 ,0,1,0
more recent papers hindle 1990 pereira and tishby 1992 proposed to cluster nouns on the basis of a metric derived from the distribution of subject verb and object in the texts ,0,1,0
a number of knowledgerich jacobs and rau 1990 calzolari and bindi 1990 mauldin 1991 and knowledgepoor brown et al 1992 hindle 1990 ruge 1991 grefenstette 1992 methods have been proposed for recognizing when words are similar ,0,1,0
this criticism leads us to automatic approaches for building thesauri from large corpora hirschman et al 1975 hindle 1990 hatzivassiloglou and mckeown 1993 pereira et al 1993 tokunaga et al 1995 ushioda 1996 ,0,1,0
our predicateargument structurebased thesatmis is based on the method proposed by hindie hindle 1990 although hindle did not apply it to information retrieval ,0,0,1
this is similar to work by several other groups which aims to induce semantic classes through syntactic cooccurrence analysis riloff and jones 1999 pereira et al 1993 dagan et al 1993 hirschman et al 1975 although in our case the contexts are limited to selected patterns relevant to the scenario ,0,1,0
iagan el al proposed a similaritybased model in which each word is generalized not to its own specific class but to a set of words which are most similar to it dagan et al 1993 ,0,1,0
we say that wv and nq are semantically related if wi and nq are semantically related and wp nq and wi nq are semantically similar dagan et al 1993 ,0,1,0
similaritybased smoothing hindle 1990 brown et al 1992 dagan marcus and markovitch 1993 pereira tishby and lee 1993 dagan lee and pereira 1999 provides an intuitively appealing approach to language modeling ,1,0,0
52 pseudodisambiguation task pseudodisambiguation tasks have become a standard evaluation technique gale church and yarowsky 1992 sch utze 1992 pereira tishby and lee 1993 sch utze 1998 lee 1999 dagan lee and pereira 1999 golding and roth 1999 rooth et al 1999 evenzohar and roth 2000 lee 2001 clark and weir 2002 and in the current setting we may use a nouns neighbors to decide which of two cooccurrences is the most likely ,0,1,0
a promising approach may be to use aligned bilingual corpora especially for augmenting existing lexicons with domainspecific terminology brown et al 1993 dagan church and gale 1993 ,1,0,0
related work the recent availability of large amounts of bilingual data has attracted interest in several areas including sentence alignment gale and church 1991b brown lai and mercer 1991 simard foster and isabelle 1992 gale and church 1993 chen 1993 word alignment gale and church 1991a brown et al 1993 dagan church and gale 1993 fung and mckeown 1994 fung 1995b alignment of groups of words smadja 1992 kupiec 1993 van der eijk 1993 and statistical translation brown et al 1993 ,0,1,0
successful approaches aimed at trying to overcome the sparse data limitation include backoff katz 1987 turinggood variants good 1953 church and gale 1991 interpolation jelinek 1985 deleted estimation jelinek 1985 church and gale 1991 similaritybased models dagan pereira and lee 1994 essen and steinbiss 1992 poslanguage models derouault and merialdo 1986 and decision tree models bahl et al 1989 black garside and leech 1993 magerman 1994 ,1,0,0
this can be done by smoothing the observed frequencies 7 church and mercer 1993 or by classbased methods brown et al 1991 pereira and tishby 1992 pereira tishby and lee 1993 hirschman 1986 resnik 1992 brill et al 1990 dagan marcus and markovitch 1993 ,0,1,0
regardless of whether it takes the form of dictionaries lesk 1986 guthrie et al 1991 dagan itai and schwall 1991 karov and edelman 1996 thesauri yarowsky 1992 walker and amsler 1986 bilingual corpora brown et al 1991 church and gale 1991 or handlabeled training sets hearst 1991 leacock towell and voorhees 1993 niwa and nitta 1994 bruce and wiebe 1994 providing information for sense definitions can be a considerable burden ,0,0,1
in the similaritybased approaches dagan et al 1993 1994 grishman et al 1993 rather than a class each word is modelled by its own set of similar words derived from statistical data collected from corpora ,0,1,0
in dagan et al 1993 and pereira et al 993 clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time ,0,1,0
this can be done by smoothing the observed frequencies church and mercer 1993 or by classbased methods brown et al 1991 pereira and tishby 1992 pereira et ah 1993 hirschman 1986 resnik 1992 brill et ah 1990 dagan et al 1993 ,0,1,0
several authors have used mutual information and similar statistics as an objective function for word clustering dagan et al 1993 brown et al 1992 pereira et al 1993 wang et al 1996 for automatic determination of phonemic baseforms lucassen mercer 1984 and for language modeling for speech recognition ries ct al 1996 ,0,1,0
1991 yarowsky 1995 ,0,1,0
a broadcoverage word sense tagger dekang lin department of computer science university of manitoba winnipeg manitoba canada r3t 2n2 lindekcsumanitobaca previous corpusbased word sense disambiguation wsd algorithms hearst 1991 bruce and wiebe 1994 leacock et al 1996 ng and lee 1996 yarowsky 1992 yarowsky 1995 determine the meanings of polysemous words by exploiting their local contexts ,0,1,0
statistical techniques both supervised learning from tagged corpora yarowsky 1992 ng and lee 1996 and unsupervised learning yarowsky 1995 resnik 1997 have been investigated ,0,1,0
the model can be seen as a bootstrapping learning process tbr disambiguation where the information gained from one part selectional preference is used to improve tile other disambiguation and vice versa reminiscent of the work by riloff and jones 1999 and yarowsky 1995 ,0,1,0
however the best performing statistical approaches to lexical ambiguity resolution llmmselves rely on complex infornmtion sources such as lemmas inflected forms parts of speech and arbitrary word classes if local and distant collocations trigram sequences and predicate mgument association yarowsky 1995 p 190 or large contextwindows up to 1000 neighboring words schitze 1992 ,0,1,0
a variety of unsupervised wsd methods which use a machinereadable dictionary or thesaurus in addition to a corpus have also been proposed yarowsky 1992 yarowsky 1995 karov and edelman 1998 ,0,1,0
david yarowsky 1995 showed it was accurate in the word sense disambiguation ,0,1,0
distance from a target word is used for this purpose and it is calculated by the assumption that the target words in the context window have the same sense yarowsky 1995 ,0,1,0
recent work emphasizes corpusbased unsupervised approach dagon and itai 1994 yarowsky 1992 yarowsky 1995 that avoids the need for costly truthed training data ,1,0,0
many techniques which have been studied for the purpose of machine translation such as word sense disambiguation dagan and itai 1994 yarowsky 1995 anaphora resolution mitamura et al 2002 and automatic pattern extraction from corpora watanabe et al 2003 can accelerate the further enhancement of sentiment analysis or other nlp tasks ,0,1,0
however following the work of yarowsky 1992 yarowsky 1995 many supervised wsd systems use minimal information about syntactic structures for the most part restricting the notion of context to topical and local features ,0,1,0
yarowsky 1995 describes a semiunsupervised approach to the problem of sense disambiguation of words also using a set of initial seeds in this case a few high quality sense annotations ,0,1,0
an alternative method we considered was to estimate certain conditional probabilities similarly to the formula used in yarowsky 1995 swt log pp c at ft afa log 2 pp c rt ft rfl here fa is an estimate of the probability that any given candidate phrase will be accepted by the spotter and fr is the probability that this phrase is rejected ie fr lf a ,0,1,0
in his analysis of yarowsky 1995 abney 2004 formulates several variants of bootstrapping ,0,1,0
our observation is that this situation is ideal for socalled bootstrapping cotraining or minimally supervised learning methods yarowsky 1995 blum and mitchell 1998 yarowsky and wicentowski 2000 ,0,1,0
an alternative approach to extracting the informal phrases is to use a bootstrapping algorithm eg yarowsky 1995 ,0,1,0
our intuition comes from an observation by yarowsky 1995 regarding multiple tokens of words in documents ,0,1,0
the benefits of using grammatical information for automatic wsd were first explored by yarowsky 1995 and resnik 1996 in unsupervised approaches to disambiguating single words in context ,0,1,0
one heuristic approach is to adapt the selftraining algorithm yarowsky 1995 to our model ,0,1,0
this process is repeated for a number of iterations in a selftraining fashion yarowsky 1995 ,0,1,0
we propose a method similar to yarowsky 1995 to generalize beyond the training set ,0,1,0
although previous work yarowsky 1995 blum and mitchell 1998 abney 2000 zhang 2004 has tackled the bootstrapping approach from both the theoretical and practical point of view many key problems still remain unresolved such as the selection of initial seed set ,0,0,1
this can be done in a supervised yarowsky 1994 a semisupervised yarowsky 1995 or a fully unsupervised way pantel lin 2002 ,0,1,0
yarowsky 1995 and mihalcea and moldovan 2001 utilized bootstrapping for word sense disambiguation ,0,1,0
our method is based on a decision list proposed by yarowsky yarowsky 1994 yarowsky 1995 ,0,1,0
within the machine learning paradigm il has been incorporated as a technique for bootstrapping an extensional learning algorithm as in yarowsky 1995 collins and singer 1999 liu et al 2004 ,0,1,0
it is possible to recognize a common structure of these works based on a typical bootstrap schema yarowsky 1995 collins and singer 1999 step 1 initial unsupervised categorization ,0,1,0
there has of course been a large amount of work on the more general problem of wordsense disambiguation eg yarowsky 1995 kilgarriff and edmonds 2002 ,0,1,0
we extracted all examples of each word from the 14millionword english portion of the hansards8 note that this is considerably smaller than yarowskys 1995 corpus of 460 million words so bootstrapping will not perform as well and may be more sensitive to the choice of seed ,0,1,0
in the supervised condition we used just 2 additional task instances plant and tank each with 4000 handannotated instances drawn from a large balanced corpus yarowsky 1995 ,0,1,0
6 conclusions in this paper we showed that it is sometimes possible indeed preferableto eliminate the initial bit of supervision in bootstrapping algorithms such as the yarowsky 1995 algorithm for word sense disambiguation ,0,0,1
21 the yarowsky algorithm yarowsky 1995 sparked considerable interest in bootstrapping with his successful method for word sense disambiguation ,1,0,0
yarowsky 1995 used this method for word sense disambiguation ,0,1,0
it is appreciated that multisense words appearing in the same document tend to be tagged with the same word sense if they belong to the same common domain in the semantic hierarchy yarowsky 1995 ,0,1,0
but it is close to the paradigm described by yarowsky 1995 and turney 2002 as it also employs selftraining based on a relatively small seed data set which is incrementally enlarged with unlabelled samples ,0,1,0
yarowsky 1995 dealt with this problem largely by producing an unsupervised learning algorithm that generates probabilistic decision list models of word senses from seed collocates ,0,1,0
currently machine learning methods yarowsky 1995 rigau atserias and agirre 1997 and combinations of classifiers mcroy 1992 have been popular ,0,1,0
1991 yarowsky 1995 and others ,0,1,0
since then this idea has been applied to several tasks including word sense disambiguation yarowsky 1995 and namedentity recognition cucerzan and yarowsky 1999 ,0,1,0
this approach to minimally supervised classifier construction has been widely studied yarowsky 1995 especially in cases in which the features of interest are orthogonal in some sense eg blum and mitchell 1998 abney 2002 ,0,1,0
yarowsky 1995 has proposed a bootstrapping method for word sense disambiguation ,0,1,0
this implementation is exactly the one proposed in yarowsky 1995 ,0,1,0
we viewed the seed word as a classified sentence following a similar proposal in yarowsky 1995 ,0,1,0
note that the results of mbd here cannot be directly compared with those in yarowsky 1995 because the data used are different ,0,1,0
after line 17 we can employ the onesenseperdiscourse heuristic to further classify unclassified data as proposed in yarowsky 1995 ,0,1,0
a variety of classifiers have been employed for this task see mooney 1996 and ide and veronis 1998 for overviews the most popular being decision lists yarowsky 1994 1995 and naive bayesian classifiers pedersen 2000 ng 1997 pedersen and bruce 1998 mooney 1996 cucerzan and yarowsky 2002 ,1,0,0
the yarowsky 1995 algorithm was one of the first bootstrapping algorithms to become widely known in computational linguistics ,1,0,0
the algorithm we implemented is inspired by the work of yarowsky 1995 on word sense disambiguation ,0,1,0
1992 pereira and tishby 1992 and pereira tishby and lee 1993 propose methods that derive classes from the distributional properties of the corpus itself while other authors use external information sources to define classes resnik 1992 uses the taxonomy of wordnet yarowsky 1992 uses the categories of rogets thesaurus slator 1992 and liddy and paik 1993 use the subject codes in the ldoce luk 1995 uses conceptual sets built from the ldoce definitions ,0,1,0
aware of this problem resnik and yarowsky suggest creating the sense distance matrix based on results in experimental psychology such as miller and charles 1991 or resnik 1995b ,0,1,0
word senses sample size feedback size correct correct per sense total drug narcotic 65 100 923 905 medicine 83 65 891 sentence judgment 23 327 1000 925 grammar 4 42 500 suit court 212 1461 986 948 garment 21 81 550 player performer 48 230 875 923 participant 44 1552 977 the feedback sets consisted of a few dozen examples in comparison to thousands of examples needed in other corpusbased methods schitze 1992 yarowsky 1995 ,0,1,0
recently yarowsky 1995 combined an mrd and a corpus in a bootstrapping process ,0,1,0
rogets has been used as the sense division in two recent wsd works yarowsky 1992 luk 1995 more or less as is except for a small number of senses added to fill gaps ,0,1,0
wsd has received increasing attention in recent literature on computational linguistics lesk 1986 schiitze 1992 gale church and yarowsky 1992 yarowsky 1992 1995 bruce and wiebe 1995 luk 1995 ng and lee 1996 chang et al 1996 ,0,1,0
using thesaurus categories directly as a coarse sense division may seem to be a viable alternative yarowsky 1995 ,0,1,0
topsense is tested on 20 words extensively investigated in recent wsd literature schitze 1992 yarowsky 1992 luk 1995 ,0,1,0
the fact that the error rate more than doubles when the seeds in yarowskys 1995 experiments are reduced from a senses best collocations to just one word per sense suggests that the error rate would increase further if no seeds were provided ,0,1,0
yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process yarowsky 1995 ,1,0,0
at each trainingset size a new copy of the network is trained under each of the following conditions 1 using sulu 2 using sulu but supplying only the labeled training examples to synthesize 3 standard network training 4 using a reimplementation of an algorithm proposed by yarowsky 1995 and 5 using standard network training but with all training examples labeled to establish an upper bound ,0,1,0
yarowsky 1995 has proposed automatically augmenting a small set of experimentersupplied seed collocations eg manufacturing plant and plant life for two different senses of the noun plant into a much larger set of training materials ,0,1,0
various corpusbased approaches to word sense disambiguation have been proposed bruce and wiebe 1994 charniak 1993 dagan and itai 1994 fujii et al 1996 hearst 1991 karov and edelman 1996 kurohashi and nagao 1994 li szpakowicz and matwin 1995 ng and lee 1996 niwa and nitta 1994 schitze 1992 uramoto 1994b yarowsky 1995 ,0,1,0
iterating between these two 1 note that these problems are associated with corpusbased approaches in general and have been identified by a number of researchers engelson and dagan 1996 lewis and gale 1994 uramoto 1994a yarowsky 1995 ,0,1,0
our approach is closely related to previous cotraining methods yarowsky 1995 blum and mitchell 1998 goldman and zhou 2000 collins and singer 1999 ,0,1,0
yarowsky 1995 first introduced an iterative method for increasing a small set of seed data used to disambiguate dual word senses by exploiting the constraint that in a segment of discourse only one sense of a word is used ,0,1,0
cotraining has been used before in applications like wordsense disambiguation yarowsky 1995 webpage classification blum and mitchell 1998 and namedentity identification collins and singer 1999 ,0,1,0
the tag propagationelimination scheme is adopted from yarowsky 1995 ,0,1,0
the best example of such an approach is yarowsky 1995 who proposes a method that automatically identifies collocations that are indicative of the sense of a word and uses those to iteratively label more examples ,1,0,0
this iterative optimiser derived from a word disambiguation technique yarowsky 1995 finds the nearest local maximum in the lexical cooccurrence network from each concept seed ,0,1,0
two major research topics in this field are named entity recognition ner n wacholder and choi 1997 cucerzan and yarowsky 1999 and word sense disambiguation wsd yarowsky 1995 wilks and stevenson 1999 ,0,1,0
to alleviate this effort various semisupervised learning algorithms such as selftraining yarowsky 1995 cotraining blum and mitchell 1998 goldman and zhou 2000 transductive svm joachims 1999 and many others have been proposed and successfully applied under different assumptions and settings ,1,0,0
this method initially proposed by yarowsky 1995 was successfully evaluated in the context of the senseval framework mihalcea 2002 ,1,0,0
among the various knowledgebased lesk 1986 galley and mckeown 2003 navigli and velardi 2005 and datadriven yarowsky 1995 ng and lee 1996 pedersen 2001 word sense disambiguation methods that have been proposed to date supervised systems have been constantly observed as leading to the highest performance ,1,0,0
a variety of algorithms eg bootstrapping yarowsky 1995 cotraining blum and mitchell 1998 alternating structure optimization ando and zhang 2005 etc ,0,1,0
to overcome the knowledge acquisition bottleneck problem suffered by supervised methods these methods make use of a small annotated corpus as seed data in a bootstrapping process hearst 1991 yarowsky 1995 ,0,1,0
numerous approaches have been explored for exploiting situations where some amount of annotated data is available and a much larger amount of data exists unannotated eg marialdos hmm partofspeech tagger training 1994 charniaks parser retraining experiment 1996 yarowskys seeds for word sense disambiguation 1995 and nigam et als 1998 topic classifier learned in part from unlabelled documents ,0,1,0
the more recent set of techniques includes mult iplicative weightupdate algorithms golding and roth 1998 latent semantic analysis jones and martin 1997 transformationbased learning mangu and brill 1997 differential grammars powers 1997 decision lists yarowsky 1994 and a variety of bayesian classifiers gale et al 1993 golding 1995 golding and schabes 1996 ,0,1,0
this method of cotraining has been previously applied to a variety of natural language tasks such as word sense disambiguation yarowsky 1995 lexicon construction for information extraction riloff and jones 1999 and named entity classification collins and singer 1999 ,0,1,0
in addition since word senses are often associated with domains yarowsky 1995 word senses can be consequently distinguished by way of determining the domain of each description ,0,1,0
62 experiment 2 yarowskys words we also conducted translation on seven of the twelve english words studied in yarowsky 1995 ,0,1,0
note that the results of mbd here cannot be directly compared with those in yarowsky 1995 mainly because the data used are different ,0,1,0
yarowsky 1995 proposes a method for word sense disambiguation which is based on monolingual bootstrapping ,0,1,0
 yarowsky 1995 after using an ensemble of nbcs ,0,1,0
1 yarowsky 1995 proposes a method for word sense translation disambiguation that is based on a bootstrapping technique which we refer to here as monolingual bootstrapping mb ,0,1,0
this way of creating classified data is similar to that in yarowsky 1995 ,0,1,0
then the initial precision is 1yarowsky 1995 citing yarowsky 1994 actually uses a superficially different score that is however a monotone transform of precision hence equivalent to precision since it is used only for sorting ,0,1,0
current work has been spurred by two papers yarowsky 1995 and blum and mitchell 1998 ,0,1,0
in order to overcome this some unsupervised learning methods and minimallysupervised methods eg yarowsky 1995 yarowsky and wicentowski 2000 have been proposed ,1,0,0
however few papers in the field of computational linguistics have focused on this approach dagan and engelson 1995 thompson et al 1999 ngai and yarowsky 2000 hwa 2000 banko and brill 2001 ,0,1,0
all features encountered in the training data are ranked in the dl best evidence first according to the following loglikelihood ratio yarowsky 1995 log prreading i jfeature k p j6i prreading j jfeature k we estimated probabilities via maximum likelihood adopting a simple smoothing method martinez and agirre 2000 01 is added to both the denominator and numerator ,0,1,0
the word sense disambiguation method proposed in yarowsky 1995 can also be viewed as a kind of cotraining ,0,1,0
the tag propagationelimination scheme is adopted from yarowsky 1995 ,0,1,0
one example is the algorithm for word sense disambiguation in yarowsky 1995 ,0,1,0
supervised approaches which make use of a small handlabeled training set bruce and wiebe 1994 yarowsky 1993 typically outperform unsupervised approaches agirre et al 2000 litkowski 2000 lin 2000 resnik 1997 yarowsky 1992 yarowsky 1995 but tend to be tuned to a speci c corpus and are constrained by scarcity of labeled data ,0,0,1
two more recent investigations are by yarowsky yarowsky 1995 and later mihalcea mihalcea 2002 ,0,1,0
some tasks can thrive on a nearly pure diet of unlabeled data yarowsky 1995 collins and singer 1999 cucerzan and yarowsky 2003 ,1,0,0
starting from the list of 12 ambiguous words provided by yarowsky 1995 which is shown in table 2 we created a concordance for each word with the lines in the concordances each relating to a context window of 20 words ,0,1,0
in an attempt to provide a quantitative evaluation of our results for each of the 12 ambiguous words shown in table 1 we manually assigned the top 30 firstorder associations to one of the two senses provided by yarowsky 1995 ,0,1,0
a number of bootstrapping methods have been proposed for nlp tasks eg yarowsky 1995 collins and singer 1999 riloff and jones 1999 ,0,1,0
unlike wellknown bootstrapping approaches yarowsky 1995 em and ce have the possible advantage of maintaining posteriors over hidden labels or structure throughout learning bootstrapping either chooses for each example a single label or remains completely agnostic ,0,0,1
they roughly fall into three categories according to what is used for supervision in learning process 1 using external resources eg thesaurus or lexicons to disambiguate word senses or automatically generate sensetagged corpus lesk 1986 lin 1997 mccarthy et al 2004 seo et al 2004 yarowsky 1992 2 exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora eg parallel corpora or untagged monolingual corpora in two languages brown et al 1991 dagan and itai 1994 diab and resnik 2002 li and li 2004 ng et al 2003 3 bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sensetagged data hearst 1991 karov and edelman 1998 mihalcea 2004 park et al 2000 yarowsky 1995 ,0,1,0
it has been shown that one sense per discourse property can improve the performance of bootstrapping algorithm li and li 2004 yarowsky 1995 ,0,1,0
32 comparison between svm bootstrapping and lp for wsd svm is one of the state of the art supervised learning algorithms mihalcea et al 2004 while bootstrapping is one of the state of the art semisupervised learning algorithms li and li 2004 yarowsky 1995 ,1,0,0
many methods have been proposed to deal with this problem including supervised learning algorithms leacock et al 1998 semisupervised learning algorithms yarowsky 1995 and unsupervised learning algorithms schutze 1998 ,0,1,0
to compare the performance of different taggers learned by different mechanisms one can measure the precision recall and fmeasure given by precision correct predictions predicted gene mentions recall correct predictions true gene mentions fmeasure a96a15a14 precision a14 recallprecision a44 recall in our evaluation we compared the proposed semisupervised learning approach to the state of the art supervised crf of mcdonald and pereira 2005 and also to selftraining celeux and govaert 1992 yarowsky 1995 using the same feature set as mcdonald and pereira 2005 ,0,1,0
many approaches have been proposed for semisupervised learning in the past including generative models castelli and cover 1996 cohen and cozman 2006 nigam et al 2000 selflearning celeux and govaert 1992 yarowsky 1995 cotraining blum and mitchell 1998 informationtheoretic regularization corduneanu and jaakkola 2006 grandvalet and bengio 2004 and graphbased transductive methods zhou et al 2004 zhou et al 2005 zhu et al 2003 ,0,1,0
51 comparison to selftraining for completeness we also compared our results to the selflearning algorithm which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of yarowsky in word sense disambiguation abney 2004 yarowsky 1995 ,1,0,0
equation 3 reads if the target noun appears then it is distinguished by the majority the loglikelihood ratio yarowsky 1995 decides in which order rules are applied to the target noun in novel context ,0,1,0
determining the sense of an ambiguous word using bootstrapping and texts from a different language was done by yarowsky 1995 hearst 1991 diab 2002 and li and li 2004 ,0,1,0
unlike yarowsky 1995 we use automatic collection of seeds ,0,1,0
yarowsky 1994 and 1995 mihalcea and moldovan 2000 and mihalcea 2002 have made further research to obtain large corpus of higher quality from an initial seed corpus ,0,1,0
annealing resembles the popular bootstrapping technique yarowsky 1995 which starts out aiming for high precision and gradually improves coverage over time ,1,0,0
yarowsky 1995 studied a method for word sense disambiguation using unlabeled data ,0,1,0
yarowsky 1995 uses a conceptually similar technique for wsd that learns from a small set of seed examples and then increases recall by bootstrapping evaluated on 12 idiosyncratically polysemous words ,0,1,0
yarowsky 1995 demonstrated that semisupervised wsd could be successful ,1,0,0
in this method the decision list dl learning algorithm yarowsky 1995 is used ,0,1,0
this improvement is close to that of one sense per discourse yarowsky 1995 improvement ranging from 13 to 17 which seems to be a sensible upper bound of the proposed method ,0,1,0
these instances can be retagged with their countability by using the proposed method and some kind of bootstrapping yarowsky 1995 ,0,1,0
consequently semisupervised learning which combines both labeled and unlabeled data has been applied to some nlp tasks such as word sense disambiguation yarowsky 1995 pham et al 2005 classification blum and mitchell 1998 thorsten 1999 clustering basu et al 2004 named entity classification collins and singer 1999 and parsing sarkar 2001 ,0,1,0
3 the framework 31 the algorithm our transductive learning algorithm algorithm 1 is inspired by the yarowsky algorithm yarowsky 1995 abney 2004 ,0,1,0
2 related work wsd approaches can be classified as a knowledgebased approaches which make use of linguistic knowledge manually coded or extracted from lexical resources agirre and rigau 1996 lesk 1986 b corpusbased approaches which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models yarowsky 1995 schtze 1998 and c hybrid approaches which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge ng and lee 1996 stevenson and wilks 2001 ,0,1,0
in order to overcome this problem we look to the bootstrapping method outlined in yarowsky 1995 ,1,0,0
7 related work the trigger labeling task described in this paper is in part a task of word sense disambiguation wsd so we have used the idea of sense consistency introduced in yarowsky 1995 extending it to operate across related documents ,0,1,0
c2008 association for computational linguistics refining event extraction through crossdocument inference heng ji ralph grishman computer science department new york university new york ny 10003 usa hengji grishmancsnyuedu abstract we apply the hypothesis of one sense per discourse yarowsky 1995 to information extraction ie and extend the scope of discourse from one single document to a cluster of topicallyrelated documents ,0,1,0
selftraining is a commonly used technique for semisupervised learning that has been ap532 plied to several natural language processing tasks yarowsky 1995 charniak 1997 steedman et al 2003 ,0,1,0
one of the most notable examples is yarowskys 1995 bootstrapping algorithm for word sense disambiguation ,1,0,0
selftraining yarowsky 1995 is a form of semisupervised learning ,0,1,0
to reduce it we exploit the one sense per collocation property yarowsky 1995 ,0,1,0
yarowsky 1995 reports a success rate of 96 disambiguating twelve words with two clear sense distinctions each one ,0,1,0
furthermore it is not possible to apply the powerful one sense per discourse property yarowsky 1995 because there is no discourse in dictionaries ,0,1,0
in yarowskys experiment yarowsky 1995 an average of 3936 examples were used to disambiguate between two senses ,0,1,0
yarowsky yarowsky 1995 proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus thus avoided the need to handannotate any examples ,0,1,0
evidence have shown that by exploiting the constraint of socalled one sense per discourse gale church and yarowsky 1992b and the strategy of bootstrapping yarowsky 1995 it is possible to boost coverage while maintaining about the same level of precision ,0,1,0
the adaptive approach is somehow similar to their idea of incremental learning and to the bootstrap approach proposed by yarowsky 1995 ,0,1,0
this approach has also been used by dagan and itai 1994 gale et al 1992 shiitze 1992 gale et al 1993 yarowsky 1995 gale and church 1lunar is not an unknown word in english yeltsin finds its translation in the 4th candidate ,0,1,0
extracting semantic information from word cooccurrence statistics has been effective particularly for sense disambiguation schiitze 1992 gale et al 1992 yarowsky 1995 ,1,0,0
decision lists have already been successfully applied to lexical ambiguity resolution by yarowsky 1995 where they perfromed well ,1,0,0
first researchers are divided between a general method that attempts to apply wsd to all the content words of texts the option taken in this paper and one that is applied only to a small trial selection of texts words for example schiitze 1992 yarowsky 1995 ,0,1,0
wsd that use information gathered from raw corpora unsupervised training methods yarowsky 1995 resnik 1997 ,0,1,0
some of the best results were reported in yarowsky 1995 who uses a large training corpus ,1,0,0
the results are consistent with the idea in gale and church 1994 shfitze 1992 yarowsky 1995 ,0,1,0
cooccurrence statistics is collected from either bilingual parallel and 334 nonparallel corpora smadja et al 1996 kupiec 1993 wu 1995 tanaka and iwasaki 1996 fung and lo 1998 or monolingual corpora smadja 1993 fung and wu 1994 liu and li 1997 shiitze 1992 yarowsky 1995 ,0,1,0
finally we would like to investigate the incorporation of unsupervised methods for wsd such as the heuristicallybased methods of stetina and nagao 1997 and stetina et al 1998 and the theoretically purer bootstrapping method of yarowsky 1995 ,0,1,0
they have been successfully applied to accent restoration word sense disambiguation 209 and homograph disambiguation yarowsky 1994 1995 1996 ,1,0,0
in the last decade or so research on lexical semantics has focused more on subproblems like word sense disambiguation yarowsky 1995 stevenson and wilks 2001 named entity recognition collins and singer 1999 and vocabulary construction for information extraction riloff 1996 ,0,1,0
collocations have been widely used for tasks such as word sense disambiguation wsd yarowsky 1995 information extraction ie riloff 1996 and namedentity recognition collins and singer 1999 ,0,1,0
in another line of research yarowsky 1995 and blum and mitchell 1998 have shown that it is possible to reduce the need for supervision with the help of large amounts of unannotated data ,0,1,0
recent work emphasizes a corpusbased unsupervised approach dagon and itai 1994 yarowsky 1992 yarowsky 1995 that avoids the need for costly truthed training data ,0,1,0
bootstrapping methods similar to ours have been shown to be competitive in word sense disambiguation yarowsky and florian 2003 yarowsky 1995 ,0,1,0
to overcome this problem unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recentlyblum and mitchell 1998yarowsky 1995park et al 2000li and li 2002 ,1,0,0
yarowsky proposed the unsupervised learning method for wsdyarowsky 1995 ,0,1,0
1 introduction cotraining blum and mitchell 1998 and several variants of cotraining have been applied to a number of nlp problems including word sense disambiguation yarowsky 1995 named entity recognition collins and singer 1999 noun phrase bracketing pierce and cardie 2001 and statistical parsing sarkar 2001 steedman et al 2003 ,0,1,0
yarowsky 1995 presented an approach that significantly reduces the amount of labeled data needed for word sense disambiguation ,0,1,0
not unlike yarowsky 1995 we use confidence of our classifier on unannotated data to enrich itself that is by adding confidentlyclassified instances to the memory ,0,1,0
 1998 traupman and wilensky 2003 yarowsky 1995 ,0,1,0
see yarowsky 1995 for details ,0,1,0
in order to overcome this several methods are proposed including minimallysupervised learning methods eg yarowsky 1995 blum and mitchell 1998 and active learning methods eg thompson et al 1999 sassano 2002 ,0,1,0
yarowsky 1995 used the one sense per collocation property as an essential ingredient for an unsupervised wordsensedisambiguationalgorithm ,0,1,0
for the former we made use decision lists similar to yarowskys method for word sense disambiguation wsd yarowsky 1995 ,0,1,0
for this reason name classification has been studied in solving the named entity extraction task in the nlp and information extraction communities see for example collins and singer 1999 cucerzan and yarowsky 1999 and various approaches reported in the muc conferences muc6 1995 ,0,1,0
in the wsd work involving the use of context we can find two approaches one that uses few strong contextual evidences for disambiguation purposes as exemplified by yarowsky 1995 and the other that uses weaker evidences but considers a combination of a number of them as exemplified by gale et al 1992 ,0,1,0
to solve this problem we adopt an idea one sense per collocation which was introduced in word sense disambiguation research yarowsky 1995 ,1,0,0
the approach is very general and modular and can work in conjunction with a number of learning strategies for word sense disambiguation yarowsky 1995 li and li 2002 ,1,0,0
yarowsky 1995 showed that the learning strategy of bootstrapping from small tagged data led to results rivaling supervised training methods ,0,1,0
the decision list dl algorithm is described in yarowsky 1995b ,0,1,0
41 methods and parameters dl on senseval2 data we observed that dl improved significantly its performance with a smoothing technique based on yarowsky 1995a ,1,0,0
these include the bootstrapping approach yarowsky 1995 and the context clustering approach schutze 1998 ,0,1,0
yarowsky 1995 used both supervised and unsupervised wsd for correct phonetizitation of words in speech synthesis ,0,1,0
21 databased methods databased approaches extract their information directly from texts and are divided into supervised and unsupervised methods yarowsky 1995 stevenson 2003 ,0,1,0
we also note that there are a number of bootstrapping methods successfully applied to text eg word sense disambiguation yarowsky 1995 named entity instance classification collins and singer 1999 and the extraction of parts word given the whole word berland and charniak 1999 ,1,0,0
yarowsky 1995 used both supervised and unsupervised wsd for correct phonetizitation of words in speech synthesis ,0,1,0
databased methods databased approaches extract their information directly from texts and are divided into supervised and unsupervised methods yarowsky 1995 stevenson 2003 ,0,1,0
many of these tasks have been addressed in other fields for example hypothesis verification in the field of machine translation tran et al 1996 sense disambiguation in speech synthesis yarowsky 1995 and relation tagging in information retrieval marsh and perzanowski 1999 ,0,1,0
these include the bootstrapping approach yarowsky 1995 and the context clustering approach schtze 1998 ,0,1,0
for example yarowsky 1995 only requires sense number and a few seeds for each sense of an ambiguous word hereafter called keyword ,0,1,0
7 related work unannotated texts have been used successfully for a variety of nlp tasks including named entity recognition collins and singer 1999 subjectivity classification wiebe and riloff 2005 text classification nigam et al 2000 and word sense disambiguation yarowsky 1995 ,1,0,0
this task is closely related to both named entity recognition ner which traditionally assigns nouns to a small number of categories and word sense disambiguation agirre and 1httpclassinrialpesfr rigau 1996 yarowsky 1995 where the sense for a word is chosen from a much larger inventory of word senses ,0,1,0
the information for semisupervised sense disambiguation is usually obtained from bilingual corpora eg parallel corpora or untagged monolingual corpora in two languages brown et al 1991 dagan and itai 1994 or sensetagged seed examples yarowsky 1995 ,0,1,0
the principle of our approach is more similar to yarowsky 1995 ,0,1,0
several approaches have been proposed in the context of word sense disambiguation yarowsky 1995 named entity ne classification collins and singer 1999 patternacquisitionforieriloff1996 yangarber 2003 or dimensionality reduction for text categorization tc yang and pedersen 1997 ,0,1,0
similarlytocollins and singer 1999 yarowsky 1995 we define the strength of a pattern p in a category y as the precision of p in the set of documents labeled with category y estimated using laplace smoothing strengthpy countpy epsilon1countp kepsilon1 3 where countpy is the number of documents labeled y containing pattern p countp is the overall number of labeled documents containing p and k is the number of domains ,0,1,0
31 collocation features the collocation features were inspired by the onesensepercollocation heuristic proposed by yarowsky 1995 ,0,1,0
the notion that nouns have only one sense per discoursecollocation was also exploited by yarowsky 1995 in his seminal work on bootstrapping for word sense disambiguation ,1,0,0
we observe that the tagging method exploits the one sense per collocation property yarowsky 1995 which means that wsd based on collocations is probably finer than wsd based on simple words since ambiguity is reduced klapaftis and manandhar 2008 ,0,1,0
yarowsky 1995 used bootstrapping to train decision list classifiers to disambiguate between two senses of a word achieving impressive classification accuracy ,0,1,0
algorithms such as cotraining blum and mitchell 1998collins and singer 1999pierce and cardie 2001 and the yarowsky algorithm yarowsky 1995 make assumptions about the data that permit such an approach ,0,1,0
2 related work the yarowsky algorithm yarowsky 1995 originally proposed for word sense disambiguation makes the assumption that it is very unlikely for two occurrences of a word in the same discourse to have different senses ,0,1,0
collins et alcollins and singer 1999 proposed two algorithms for ner by modifying yarowskys method yarowsky 1995 and the framework suggested by blum and mitchell 1998 ,0,1,0
yarowsky 1995 successfully used this observation as an approximate annotation technique in an unsupervised wsd model ,1,0,0
1992b has proved to be a simple yet powerful observation and has been successfully used in word sense disambiguation wsd and related tasks eg yarowsky 1995 agirre and rigau the author was partially funded by gale darpa contract no ,1,0,0
the original training set before the addition of the feedback sets consisted of a few dozen examples in comparison to thousands of examples needed in other corpusbased methods schutze 1992 yarowsky 1995 ,0,1,0
in comparison yarowsky 1995 achieved 48 table 1 a summary of the experimental results on four polysemous words ,0,1,0
attempts to alleviate this tagbottleneck ilude tmotstrias te ot ill 1996 hearst 1991 and unsupervised algorith yarowsky 199s dictionarybased approaches rely on linguistic knowledge sources such as maliereadable dictionaries luk 1995 veronis and ide 1990 and wordnet agirre and rigau 1996 resnik 1995 and e0ploit these for word sense disaznbiguation ,0,1,0
unsupervised algoritm such as yarowsky 1995 have reported good accuracy that rivals that of supervised algorithms ,1,0,0
similarly yarowsky 1995 tested his wsd algorithm on a dozen words ,0,1,0
recently some kinds of learning techniques have been applied to cumulatively acquire exemplars form large corpora yarowsky 1994 1995 ,0,1,0
introduction word sense disambiguation has long been one of the major concerns in natural language processing area eg bruce et al 1994 choueka et al 1985 gale et al 1993 mcroy 1992 yarowsky 1992 1994 1995 whose aim is to identify the correct sense of a word in a particular context among all of its senses defined in a dictionary or a thesaurus ,0,1,0
in future work we will expand all of the above types of features and employ techniques to reduce dimensionality along the lines suggested in duda and hart 1973 and gale church and yarowsky 1995 ,0,1,0
a more recent bootstrapping approach is described in yarowsky 1995 ,0,1,0
while yarowsky 1995 does not discuss distinguishing more than 2 senses of a word there is no immediate reason to doubt that the one sense per collocation rule yarowsky 1993 would still hold for a larger number of senses ,1,0,0
yarowsky 1995 compares his method to schiitze 1992 and shows that for four words the former performs significantly better in distinguishing between two senses ,1,0,0
73 em algorithm the only other application of the em algorithm to wordsense disambiguation is described in gale church and yarowsky 1995 ,0,1,0
1992 yarowsky 1995 and karol edelman 1996 where strong reliance on statistical techniques for the calculation of word and context similarity commands large source corpora ,0,1,0
6 discourse context yarowsky 1995 pointed out that the sense of a target word is highly consistent within any given document one sense per discourse ,0,1,0
wsd that use information gathered from raw corpora unsupervised training methods yarowsky 1995 resnik 1997 ,0,1,0
however our system is the unsupervised learning with small postagged corpusand we do not restrict the words sense set within either binary sensesyarowsky1995 karov 1998 or dictionarys homograph levelwilks 1997 ,0,0,1
recently many works combined a mrd and a corpus for word sense disambiguationkarov 1998 luk 1995 ng 1996 yarowsky1995 ,0,1,0
in yarowsky1995 the definition words were used as initial sense indicators automatically tagging the target word examples containing them ,0,1,0
among them the unsupervised algorithm using decisiontrees yarowsky 1995 has achieved promising performance ,1,0,0
the preliminary labeling by keyword matching used in this paper is similar to the seed collocations used by yarowsky 1995 ,0,1,0
the parser induction algorithm used in all of the experiments in this paper was a distribution of collinss model 2 parser collins 1997 ,0,1,0
training on about 40000 sentences collins 1997 achieves a crossing brackets rate of 107 a better value than our 163 value for regular parsing or the 113 value assuming perfect segmentationtagging but even for similar text types comparisons across languages are of course problematic ,0,1,0
1 introduction since 1995 a few statistical parsing algorithms magerman 1995 collins 1996 and 1997 charniak 1997 rathnaparki 1997 demonstrated a breakthrough in parsing accuracy as measured against the university of pennsylvania treebank as a gold standard ,0,1,0
finally our newly constructed parser like that of collins 1997 was based on a generative statistical model ,0,1,0
7 model structure in our statistical model trees are generated according to a process similar to that described in collins 1996 1997 ,0,1,0
234 adv nonspecific adverbial bnf benefemtive clf itcleft clr closely related dir direction dtv dative ext extent hln headline lgs logical subject l0c location mni manner n0m nominal prd predicate prp purpose put locative complement of put sbj subject tmp temporal tpc topic ttl title v0c vocative grammatical dtv 048 lgs 30 prd 18 put 026 sbj 78 v0c 0025 figure 1 penn treebank function tags 53 formfunction 37 topicalisation 22 025 nom 68 25 tpc 100 22 15 adv 11 42 93 bnf 0072 0026 013 dir 83 30 41 ext 32 12 0013 loc 25 92 mnr 62 23 pi 52 19 33 12 miscellaneous 95 clr 94 88 clf 034 003 hln 26 025 ttl 31 029 figure 2 categories of function tags and their relative frequencies one project that used them at all collins 1997 defines certain constituents as complements based on a combination of label and function tag information ,0,1,0
bilexical contextfree grammars have been presented in eisner and satta 1999 as an abstraction of language models that have been adopted in several recent realworld parsers improving stateoftheart parsing accuracy a1shawl 1996 eisner 1996 charniak 1997 collins 1997 ,0,0,1
as in other work we collapsed aivp and pljl to the same label when calculating these scores see collins 1997 iatnaparkhi 1999 charniak 1997 ,0,1,0
these scores are higher than those of several other parsers eg collins 1997 99 charniak 1997 but remain behind tim scores of charniak 2000 who obtains 901 lp and 901 lr for sentences 40 words ,0,0,1
it also shows that dops frontier lexicalization is a viable alternative to constituent lexicalization as proposed in charniak 1997 collins 1997 99 eisner 1997 ,0,1,0
the inolel 1 3sielina 1998 was raimd tm semcor that was merged with a flfll senlential parse tree the determination of which is considered a difficult lrolflem of its own collins 1997 ,0,1,0
1998 directly estimate dps tbr given intut whereas other models suh as pcfotased teldown generation modls phs do not charnink 1997 collins 1997 shirfi et rl ,0,1,0
the state of a leftcorner parser does capture some linguistic generalizations mmming anl carpenter 1997 roark and johnson 1999 but one might still expect sparsedata problems ,0,1,0
as reported previously the standard leftcorner grmninar embeds sufficient nonlocal infornlation in its productions to significantly improve the labelled precision and recall of its mlps with respect to mlps of the pcfg estimated from the untransfornmd trees maiming and carpenter 1997 lloark and johnson 1999 ,0,1,0
ultinmtely however it seems that a more complex aitroach incorporating backoff and smoothing is necessary ill order to achieve the parsing accuracy achieved by charniak 1997 and collins 1997 ,0,1,0
recently in the area of parsers based oll a stochastic contextfiee grammar scfg some researchers have pointed out the importance of the lexicon and proposed lexiealized models charniak 1997 collins 1997 ,0,1,0
also in a stateoftheart english parser collins 1997 only the words tha t occur more than d times in training data ,1,0,0
this is the same separation of arguments and adjuncts as that employed by collins 1997 ,0,1,0
there have been a lot of prolosfls for statistical analysis in ninny languages in particular in english and japanese magerman 1995 sekine and grishman 1995 collins 1997 iatnalarkhi 1997 kshirai etal 1998 fujio and matsnlnoto 1998 itaruno ctal 1997ehara 1998 ,0,1,0
the progress in parsing technology are noteworthy and in particular various statistical dependency models have been proposedcollins 1997 ratnaparkhi 1997 charniak 2000 ,0,1,0
substantial improvements have been made to parse western language such as english and many powerful models have been proposed brill 1993 collins 1997 ,1,0,0
3thedata for our experiments we used a version of the british national corpus parsed with the statistical parser of collins 1997 ,0,1,0
in order to extract the linguistic features necessary for the model all sentences were first automatically partofspeechtagged using a maximum entropy tagger ratnaparkhi 1998 and parsed using the collins parser collins 1997 ,0,1,0
most probabilistic parsing research including for example work by by collins 1997 and charniak 1997 is based on branching process models harris 1963 ,0,1,0
abney 1997 notes important problems with the soundness of the approach when a unificationbased grammar is actually determining the derivations motivating the use of loglinear models agresti 1990 for parse ranking that johnson and colleagues further developed johnson geman canon chi riezler 1999 ,0,1,0
moreover the deterministic dependency parser of yamada and matsumoto 2003 when trained on the penn treebank gives a dependency accuracy that is almost as good as that of collins 1997 and charniak 2000 ,1,0,0
here we extract partofspeech tags from the collins parsers output collins 1997 for section 23 instead of reinventing a tagger ,0,1,0
as is common collins 1997 johnson 1998 klein and manning 2003 schmid 2006 the treebank is first transformed in various ways in order to give an accurate pcfg ,0,1,0
by habit most systems for automatic rolesemantic analysis have used pennstyle constituents marcus et al 1993 produced by collins 1997 or charniaks 2000 parsers ,0,1,0
typical approaches to conversion of constituent structures into dependencies are based on handconstructed head percolation rules an idea that has its roots in lexicalized constituent parsing magerman 1994 collins 1997 ,0,1,0
a constituentbased system using collins parser collins 1997 ,0,1,0
moreover as pdop is formulated as an enrichment of the treebank probabilistic contextfree grammar pcfg it allows for much easier comparison to alternative approaches to statistical parsing collins 1997 charniak 1997 johnson 1998 klein and manning 2003 petrov et al 2006 ,0,1,0
all stateoftheart widecoverage parsers relax this assumption in some way for instance by i changing the parser in step 3 such that the application of rules is conditioned on other steps in the derivation process collins 1997 charniak 1997 or by ii enriching the nonterminal labels in step 1 with contextinformation johnson 1998 klein and manning 2003 along with suitable backtransforms in step 4 ,1,0,0
presented in collins 1997 ,0,1,0
the parse trees on the english side of the bitexts were generated using a parser soricut 2004 implementing the collins parsing models collins 1997 ,0,1,0
even before the 2006 shared task the parsers of collins 1997 and charniak 2000 originally developed for english had been adapted for dependency parsing of czech and the parsing methodology proposed by kudo and matsumoto 2002 and yamada and matsumoto 2003 had been evaluated on both japanese and english ,0,1,0
our intuition is that we cannot apply our binarization to collins 1997 ,0,1,0
it is equipped with head binarization to help improve parsing accuracy following the traditional linguistic insight that phrases are organized around the head collins 1997 klein and manning 2003b ,0,1,0
this procedure uses the head finding rules of collins 1997 ,0,1,0
the stanford parser is representative of a large number of ptb parsers exemplified by collins 1997 and charniak 2000 ,0,1,0
while it was initially believed that lexicalization of pcfg parsers collins 1997 charniak 2000 is crucial for obtaining good parsing results gildea 2001 demonstrated that the lexicalized model1 parser of collins 1997 does not benefit from bilexical information when tested on a new text domain and only marginally benefits from such information when tested on the same text domain as the training corpora ,0,1,0
lexicalized pcfgs use the structural features on the lexical head of phrasal node in a tree and get significant improvements for parsing collins 1997 charniak 1997 collins 1999 charniak 2000 ,1,0,0
in general they can be divided into two major categories namely lexicalized models collins 1997 1999 charniak 1997 2000 and unlexicalized models klein and manning 2003 matsuzaki et al 2005 petrov et al 2006 petrov and klein 2007 ,0,1,0
1999 applied the parser of collins 1997 developed for english to czech and found thatthe performance wassubstantially lower when compared to the results for english ,0,0,1
note that since the framenet data does not include deep syntactic tree annotation we processed the framenet data with collins parser collins 1997 consequently the experiments on framenet relate to automatic syntactic parse trees ,0,1,0
i q det namedentity enterv q the roomn i q det femaleindividual havev q det roomn i q det femaleindividual sleepv i q det femaleindividual havev q det f plur clothen i q det f plur clothen washeda here the uppercase sentences are automatically generated verbalizations of the abstracted lfs shown beneath them1 the initial development of knext was based on the handconstructed parse trees in the penn treebank version of the brown corpus but subsequently schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers eg that of collins 1997 or charniak 2000 applied to larger corpora such as the british national corpus bnc a 100 millionword mixed genre collection along with web corpora of comparable size see work of van durme et al ,0,1,0
first for each verb occurrence subjects and objects were extracted from a parsed corpus collins 1997 ,0,1,0
for causativity the same counting scripts were used for both groups of verbs but the input to the counting programs was determined by manual inspection of the corpus for verbs belonging to group 1 while it was extracted automatically from a parsed corpus for group 2 wsj 1988 parsed with the parser from collins 1997 ,0,1,0
in our experiments we used a dependency parser only in english a version of the collins parser collins 1997 that has been adapted for building dependencies but not in the other language ,0,1,0
the modify features involve the dependency parse tree for the sentence obtained by first parsing the sentence collins 1997 and then converting the tree into its dependency representation xia and palmer 2001 ,0,1,0
in our experiments we used the full parse output from collins parser collins 1997 in which every nonterminal node is already annotated with head information ,0,1,0
the output of a contextfree parser such as that of collins 1997 or charniak 2000 can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser ,0,1,0
to compare the output of their shallow parser with the output of the wellknown collins 1997 parser li and roth applied the chunklink conversion script to extract the shallow constituents from the output of the collins parser on wsj section 00 ,1,0,0
perhaps the most widely accepted convention is that of ignoring punctuation for the purposes of assigning constituent span under the perspective that fun788 phrase evaluation scenario system type a b c modified all 9837 9972 9972 truth vp 9214 9870 9870 li and roth all 9464 2001 vp 9528 collins 1997 all 9216 9342 9428 vp 8815 9431 9442 charniak all 9388 9515 9532 2000 vp 8892 9511 9519 table 1 fmeasure shallow bracketing accuracy under three different evaluation scenarios a baseline used in li and roth 2001 with original chunklink script converting treebank trees and contextfree parser output b same as a except that empty subject nps are inserted into every unary svp production and c same as b except that punctuation is ignored for setting constituent span ,0,1,0
collins parser collins 1997 always predicts a flat np for such configurations ,0,1,0
for the constituentbased models constituent information was obtained from the output of collins parser 1997 for english and dubeys parser 2004 for german ,0,1,0
in batch mode opinionfinder parses the data again this time to obtain constituency parse trees collins 1997 which are then converted to dependency parse trees xia and palmer 2001 ,0,1,0
topdown parsing and language modeling statistically based heuristic bestfirst or beamsearch strategies caraballo and charniak 1998 charniak goldwater and johnson 1998 goodman 1997 have yielded an enormous improvement in the quality and speed of parsers even without any guarantee that the parse returned is in fact that with the maximum likelihood for the probability model ,0,1,0
the parsers with the highest published broadcoverage parsing accuracy which include charniak 1997 2000 collins 1997 1999 and ratnaparkhi 1997 all utilize simple and straightforward statistically based search heuristics pruning the searchspace quite dramatically ,1,0,0
1997 johnson 1998that conditioning the probabilities of structures on the context within which they appear for example on the lexical head of a constituent charniak 1997 collins 1997 on the label of its parent nonterrninal johnson 1998 or ideally on both and many other things besides leads to a much better parsing model and results in higher parsing accuracies ,0,1,0
examples of this are bilexical grammarssuch as eisner and satta 1999 charniak 1997 collins 1997where the lexical heads of each constituent are annotated on both the rightand lefthand sides of the contextfree rules under the constraint that every constituent inherits the lexical head from exactly one of its children and the lexical head of a pos is its terminal item ,0,1,0
the differences between a kbest and a beamsearch parser not to mention the use of dynamic programming make a running time difference unsur17 our score of 858 average labeled precision and recall for sentences less than or equal to 100 on section 23 compares to 867 in charniak 1997 869 in ratnaparkhi 1997 882 in collins 1999 896 in charniak 2000 and 8975 in collins 2000 ,0,1,0
this corpus of 29 million words was provided to us by michael collins and was automatically parsed with the parser described in collins 1997 ,0,1,0
we used collins 1997 statistical parser trained on examples from the penn treebank to generate parses of the same format for the sentences in our data ,0,1,0
recent lexicalized stochastic parsers such as collins 1999 charniak 1997 and others add additional features to each constituent the most important being the head word of the parse constituent ,0,1,0
the models were originally introduced in collins 1997 the current article 1 gives considerably more detail about the models and discusses them in greater depth ,0,1,0
previous workthe generative models described in collins 1996 and the earlier version of these models described in collins 1997conditioned on punctuation as surface features of the string treating it quite differently from lexical items ,0,1,0
in particular the model in collins 1997 failed to generate punctuation a deficiency of the model ,0,0,1
of particular relevance is other work on parsing the penn wsj treebank jelinek et al 1994 magerman 1995 eisner 1996a 1996b collins 1996 charniak 1997 goodman 1997 ratnaparkhi 1997 chelba and jelinek 1998 roark 2001 ,0,1,0
note that conditioning on the rules parent is needed to disallow the structure np np pp pp see johnson 1997 for further discussion ,0,1,0
moreover in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain we have performed the study on two parsing models one based on a contextfree variant of treeadjoining grammars joshi levy and takahashi 1975 the probabilistic lexicalized tree insertion grammar pltig formalism schabes and waters 1993 hwa 1998 and collinss model 2 parser 1997 ,0,1,0
in the second experiment the basic learning model is collinss 1997 model 2 parser which uses a historybased learning algorithm that takes statistics directly over the treebank ,0,1,0
another consequence of not generating posthead conjunctions and punctuation as firstclass words is that they 19 in fact if punctuation occurs before the head it is not generated at alla deficiency in the parsing model that appears to be a holdover from the deficient punctuation handling in the model of collins 1997 ,0,0,1
503 bikel intricacies of collins parsing model table 4 overall parsing results using only details found in collins 1997 1999 ,0,1,0
evaluation 81 effects of unpublished details in this section we present the results of effectively doing a cleanroom implementation of collins parsing model that is using only information available in collins 1997 1999 as shown in table 4 ,0,1,0
introduction michael collins 1996 1997 1999 parsing models have been quite influential in the field of natural language processing ,1,0,0
1998 we also introduce an approach related to the conditional loglinear models of ratnaparkhi roukos and ward 1994 papineni roukos and ward 1997 1998 johnson et al ,0,1,0
in particular previous work ratnaparkhi roukos and ward 1994 abney 1997 della pietra della pietra and lafferty 1997 johnson et al 1999 riezler et al 2002 has investigated the use of markov random fields mrfs or loglinear models as probabilistic models with global features for parsing and other nlp tasks ,0,1,0
first several of the bestperforming parsers on the wsj treebank eg ratnaparkhi 1997 charniak 1997 2000 collins 1997 1999 henderson 2003 are cases of historybased models ,1,0,0
the extraction procedure utilizes a head percolation table as introduced by magerman 1995 in combination with a variation of collinss 1997 approach to the differentiation between complement and adjunct ,0,1,0
the extraction procedure consists of three steps first the bracketing of the trees in the penn treebank is corrected and extended based on the approaches of magerman 1994 and collins 1997 ,0,1,0
manually defined heuristics are used to automatically annotate each tree in the treebank with partially specified hpsg derivation trees headargumentmodifier distinctions are made for each node in the tree based on magerman 1994 and collins 1997 336 odonovan et al largescale induction and evaluation of lexical resources the whole tree is then converted to a binary tree heuristics are applied to deal with phenomena such as ldds and coordination and to correct some errors in the treebank and finally an hpsg category is assigned to each node in the tree in accordance with its cfg category ,0,1,0
typically frequency information for rare words in the training data is used to estimate parameters for unknown words and when these rare or unknown words are encountered during parsing additional information may be obtained from a postagger collins 1997 ,0,1,0
the results so far mainly come from studies where a parser originally developed for englishsuch as the collins parser collins 19971999 is applied to a new languagewhich often leads to a signicant decrease in the measured accuracy collins et al 1999 bikel and chiang 2000 dubey and keller 2003 levy and manning 2003 corazza et al 2004 ,0,1,0
historybased models for predicting the next parser action black et al 1992 magerman 1995 ratnaparkhi 1997 collins 1999 3 ,0,1,0
recently specic probabilistic treebased models have been proposed not only for machine translation wu 1997 alshawi bangalore and douglas 2000 yamada and knight 2001 eisner 2003 gildea 2003 but also for summarization knight and marcu 2002 paraphrasing pang knight and marcu 2003 natural language generation langkilde and knight 1998 bangalore and rambow 2000 corstonoliver et al 2002 parsing and language modeling baker 1979 lari and young 1990 collins 1997 chelba and jelinek 2000 charniak 2001 klein information sciences institute 4676 admiralty way marina del rey ca 90292 ,0,1,0
more specically they used a parser collins 1997 to determine the constituent structure of the sentences from which the grammatical function for each np was derived ,0,1,0
statistical model in sifts statistical model augmented parse trees are generated according to a process similar to that described in collins 1996 1997 ,0,1,0
to identify these we use a wordaligned corpus annotated with parse trees generated by statistical syntactic parsers collins 1997 schmidt and schulte im walde 2000 ,0,1,0
model interpolation in this case persystem training heldout lr lp map brownt brownh 760 754 map brownt wsj24 769 771 gildea wsj221 861 866 map wsj221 wsj24 869 871 charniak 1997 wsj221 wsj24 867 866 ratnaparkhi 1999 wsj221 863 875 collins 1999 wsj221 881 883 charniak 2000 wsj221 wsj24 896 895 collins 2000 wsj221 896 899 table 4 parser performance on wsj23 baselines ,0,1,0
in the sequel we use collinss statistical parser collins 1997 as our canonical automated approximation of the treebank ,0,1,0
32 statistical learning model 321 nave bayes learning nave bayes learning has been widely used in natural language processing with good results such as statistical syntactic parsing collins 1997 charniak 1997 hidden language understanding miller et al 1994 ,1,0,0
summarization knight and marcu 2002 paraphrasing pang knight and marcu 2003 natural language generation langkilde and knight 1998 bangalore and rambow 2000 corstonoliver et al 2002 and language modeling baker 1979 lari and young 1990 collins 1997 chelba and jelinek 2000 charniak 2001 klein and manning 2003 ,0,1,0
due to advances in statistical syntactic parsing techniques collins 1997 charniak 2001 attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences ,0,1,0
2005 have implemented a dependency parser with good accuracy it is almost as good at dependency parsing as charniak 2000 and very impressive speed it is about ten times faster than collins 1997 and four times faster than charniak 2000 ,0,0,1
probability estimates of the rhs given the lhs are often smoothed by making a markov assumption regarding the conditional independence of a category on those more than k categories away collins 1997 charniak 2000 px y1yn py1x ny i2 pyixy1 yi1 py1x ny i2 pyixyik yi1 ,0,1,0
the sentences were processed with the collins parser collins 1997 to generate automatic parse trees ,0,1,0
parse parse score from model 2 of the statistical parser collins 1997 normalized by the number of words ,0,1,0
1 introduction over the past decade there has been tremendous progress on learning parsing models from treebank data magerman 1995 collins 1999 charniak 1997 ratnaparkhi 1999 charniak 2000 wang et al 2005 mcdonald et al 2005 ,0,1,0
most of the early work in this area was based on postulating generative probability models of language that included parse structures magerman 1995 collins 1997 charniak 1997 ,0,1,0
learning in this context consisted of estimating the parameters of the model with simple likelihood based techniques but incorporating various smoothing and backoff estimation tricks to cope with the sparse data problems collins 1997 bikel 2004 ,0,1,0
the syntaxbased system we ran a reimplementation of the collins parser collins 1997 on the english half of the bitext to produce parse trees then restructured and relabeled them as described in section 32 ,0,1,0
6 related work a pioneering antecedent for our work is miller et al 2000 who trained a collinsstyle generative parser collins 1997 over a syntactic structure augmented with the template entity and template relations annotations for the muc7 shared task ,0,1,0
since this transform takes a probabilistic grammar as input it can also easily accommodate horizontal and vertical markovisation annotating grammar symbols with parent and sibling categories as described by collins 1997 and subsequently ,0,1,0
many probabilistic evaluation models have been published inspired by one or more of these feature types black 1992 briscoe 1993 charniak 1997 collins 1996 collins 1997 magerman 1995 eisner 1996 but discrepancies between training sets algorithms and hardware environments make it difficult if not impossible to compare the models objectively ,0,1,0
such wordbased lexicalizations of probability models are used successfully in the statistical parsing models of eg collins 1997 charniak 1997 or ratnaparkhi 1997 ,1,0,0
headlexicalized stochastic grammars have recently become increasingly popular see collins 1997 1999 charniak 1997 2000 ,1,0,0
a major difference between our approach and most other models tested on the wsj is that the dop model uses frontier lexicalization while most other models use constituent lexicalization in that they associate each constituent non terminal with its lexical head see collins 1996 1999 charniak 1997 eisner 1997 ,0,1,0
contextfree rules charniak 1996 collins 1996 eisner 1996 contextfree rules headwords charniak 1997 contextfree rules headwords grandparent nodes collins 2000 contextfree rules headwords grandparent nodesrules bigrams twolevel rules twolevel bigrams nonheadwords bod 1992 all fragments within parse trees scope of statistical dependencies model figure 4 ,0,1,0
while early headlexicalized grammars restricted the fragments to the locality of headwords eg collins 1996 eisner 1996 later models showed the importance of including context from higher nodes in the tree charniak 1997 johnson 1998 ,0,1,0
the importance of including single nonheadwords is now also uncontroversial eg collins 1997 1999 charniak 2000 and the current paper has shown the importance of including two and more nonheadwords ,0,1,0
as in most other statistical parsing systems we therefore use the pruning technique described in goodman 1997 and collins 1999 263264 which assigns a score to each item in the chart equal to the product of the inside probability of the item and its prior probability ,0,1,0
41 the base line for our base line parse accuracy we used the now standard division of the wsj see collins 1997 1999 charniak 1997 2000 ratnaparkhi 1999 with sections 2 through 21 for training approx ,0,1,0
many stochastic parsing models use linguistic intuitions to find this minimal set for example by restricting the statistical dependencies to the locality of headwords of constituents collins 1997 1999 eisner 1997 leaving it as an open question whether there exist important statistical dependencies that go beyond linguistically motivated dependencies ,0,1,0
table 1 shows the lp and lr scores obtained with our base line subtree set and compares these scores with those of previous stochastic parsers tested on the wsj respectively charniak 1997 collins 1999 ratnaparkhi 1999 and charniak 2000 ,0,1,0
like the models of goodman 1997 the additional features in our model are generated probabilistically whereas in the parser of collins 1997 distance measures are assumed to be a function of the already generated structure and are not generated explicitly ,0,1,0
distance measures for ccg our distance measures are related to those proposed by goodman 1997 which are appropriate for binary trees unlike those of collins 1997 ,0,1,0
tactic parser collins 1997 ,0,1,0
at last the dependency parser presented in collins 1997 is used to generate the full parse ,0,1,0
the reader is referred to schmid 2000 and collins 1997 for details ,0,1,0
lexicalization can increase parsing performance dramatically for english carroll and rooth 1998 charniak 1997 2000 collins 1997 and the lexicalized model proposed by collins 1997 has been successfully applied to czech collins et al 1999 and chinese bikel and chiang 2000 ,1,0,0
sister head tag x table 4 linguistic features in the current model compared to the models of carroll and rooth 1998 collins 1997 and charniak 2000 negra based on collinss 1997 model for nonrecursive nps in the penn treebank which are also flat ,0,1,0
for nonrecursive nps collins 1997 does not use the probability function in 5 but instead substitutes p r and by analogy p l by p r r i tr i lr i jpr i1 tr i1 lr i1 di8 here the head h is substituted by the sister r i1 and l i1 ,0,1,0
the collins 1997 model does not use contextfree rules but generates the next category using zeroth order markov chains see section 33 hence no information about the previous sisters is included ,0,1,0
we first added sisterhead dependencies for nps following collinss 1997 original proposal and then for pps which are flat in negra and thus similar in structure to nps see section 22 ,0,1,0
the progression in the probabilistic parsing literature has been to start with lexical headhead dependencies collins 1997 and then add nonlexical sis2 this result generalizes to ss which are also flat in negra see section 22 ,0,1,0
section 5 presents an error analysis for collinss 1997 lexicalized model which shows that the headhead dependencies used in this model fail to cope well with the flat structures in negra ,0,0,1
1999 and bikel and chiang 2000 has demonstrated the applicability of the collins 1997 model for czech and chinese ,1,0,0
however the learning curve for negra see figure 1 indicates that the performance of the collins 1997 model is stable even for small training sets ,1,0,0
in experiment 1 we applied three standard parsing models from the literature to negra an unlexicalized pcfg model the baseline carroll and rooths 1998 headlexicalized model and collinss 1997 model based on headhead dependencies ,0,1,0
prominent among these properties is the semifree language size lr lp source english 40000 874 881 collins 1997 chinese 3484 690 748 bikel and chiang 2000 czech 19000 800 collins et al 1999 table 1 results for the collins 1997 model for various languages dependency precision for czech wordorder ie german wordorder is fixed in some respects but variable in others ,0,1,0
33 collinss headlexicalized model in contrast to carroll and rooths 1998 approach the model proposed by collins 1997 does not compute rule probabilities directly ,0,1,0
1 introduction treebankbased probabilistic parsing has been the subject of intensive research over the past few years resulting in parsing models that achieve both broad coverage and high parsing accuracy eg collins 1997 charniak 2000 ,1,0,0
the lexicalized model proposed by collins 1997 henceforth collins model was reimplemented by one of the authors ,0,1,0
first we extend the mechanism of adding gap variables for nodes dominating a site of discontinuity collins 1997 ,0,1,0
the idea of threading ees to their antecedents in a stochastic parser was proposed by collins 1997 following the gpsg tradition gazdar et al 1985 ,0,1,0
controlled nptraces npnp we follow the standard technique of marking nodes dominating the empty element up to but not including the parent of the antecedent as defective missing an argument with a gap feature gazdar et al 1985 collins 19971 furthermore to make antecedent coindexation possible with many types of ees we generalize collins approach by enriching the annotation of nonterminals with the type of the ee in question eg ,0,1,0
however such constructions prove to be difficult for stochastic parsers collins et al 1999 and they either avoid tackling the problem charniak 2000 bod 2003 or only deal with a subset of the problematic cases collins 1997 ,0,0,1
the sentences were processed using collins parser collins 1997 to generate parsetrees automatically ,0,1,0
both techniques implement variations on the approaches of magerman 1994 and collins 1997 for the purpose of differentiating between complement and adjunct ,0,1,0
one attempt to implement this idea is lexicalization increasing the information in the pos tag by adding the lemma to it collins 1997 simaan 2000 ,0,1,0
ngrams have been used extensively for this purpose collins 1996 1997 eisner 1996 ,0,1,0
as a side product we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques collins 1997 simaan 2000 and parent annotation techniques klein and manning 2003 is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora ,1,0,0
they are central to many parsing models charniak 1997 collins 1997 2000 eisner 1996 and despite their simplicity ngram models have been very successful ,1,0,0
previous approaches to the problem collins 1997 johnson 2002 dienes and dubey 2003ab higgins 2003 have all been learningbased the primary difference between the present algorithm and earlier ones is that it is not learned but explicitly incorporates principles of governmentbinding theory chomsky 1981 since that theory underlies the annotation ,0,1,0
1 empty categories however seem different in that for the most part their location and existence is determined not by observable data but by explicitly constructed linguistic principles which 1 both collins 1997 19 and higgins 2003 100 are explicit about this predisposition ,0,1,0
we employ a robust statistical parser collins 1997 to determine the constituent structure for each sentence from which subjects s objects o and relations other than subject or object x are identified ,1,0,0
the collins parser collins 1997 does use dynamic programming in its search ,0,1,0
we use a program to label syntactic arguments with the roles they are playing blaheta and charniak 2000 and the rules for complementadjunct distinction given by collins 1997 to never allow deletion of the complement ,0,1,0
the lexicalized parsing experiments were run using dan bikels probabilistic parsing engine bikel 2002 which in addition to replicating the models described by collins 1997 also provides a convenient interface to develop corresponding parsing models for other languages ,0,1,0
furthermore bikel 2004 provides evidence that lexical information in the form of bilexical dependencies only makes a small contribution to the performance of parsing models such as collinss 1997 ,0,1,0
previous work for english eg magerman 1995 collins 1997 has shown that lexicalization leads to a sizable improvement in parsing performance ,1,0,0
2 we used the collins parser 1997 to generate the constituency parse and a dependency converter hwa and lopez 2004 to obtain the dependency parse of english sentences ,0,1,0
this is well illustrated by the collins parser collins 1997 collins 1999 scrutinized by bikel 2004 where several transformations are applied in order to improve the analysis of noun phrases coordination and punctuation ,0,0,1
this was done for supervised parsing in different ways by collins 1997 klein and manning 2003 and mcdonald et al ,0,1,0
the supervised component is collins parser collins 1997 trained on the wall street journal ,0,1,0
6 related work other work combining supervised and unsupervised learning for parsing includes charniak 1997 johnson and riezler 2000 and schmid 2002 ,0,1,0
unlabeled r 100 2008199605081997 351 days 50 2008199617021997 182 days 10 2008199624091996 36 days labeled wsj 50 sections 0012 23412 sentences 25 lines 1 292960 11637 sentences 5 lines 1 58284 2304 sentences 1 lines 1 11720 500 sentences 005 lines 1 611 23 sentences table 1 corpora used for the experiments unlabeled reuters r corpus for attachment statistics labeled penn treebank wsj for training the collins parser ,0,1,0
scissor is implemented by augmenting collins 1997 headdriven parsing model ii to incorporate the generation of semantic labels on internal nodes ,0,1,0
historybased models for predicting the next parser action black et al 1992 magerman 1995 ratnaparkhi 1997 collins 1999 3 ,0,1,0
1 introduction mainstream approaches in statistical parsing are based on nondeterministic parsing techniques usually employing some kind of dynamic programming in combination with generative probabilistic models that provide an nbest ranking of the set of candidate analyses derived by the parser collins 1997 collins 1999 charniak 2000 ,0,1,0
in the probabilistic lr model probabilities are assigned to tree 696 precision recall fscore time min bestfirst classifierbased this paper 881 878 879 17 deterministic maxent this paper 854 848 851 1 charniak johnson 2005 913 906 910 unk bod 2003 908 907 907 145 charniak 2000 895 896 895 23 collins 1999 883 881 882 39 ratnaparkhi 1997 875 863 869 unk tsuruoka tsujii 2005 deterministic 865 812 838 1 tsuruoka tsujii 2005 search 868 850 859 2 sagae lavie 2005 860 861 860 11 table 1 summary of results on labeled precision and recall of constituents and time required to parse the test set ,0,1,0
some of the more popular and more accurate of these approaches to datadriven parsing charniak 2000 collins 1997 klein and manning 2002 have been based on generative models that are closely related to probabilistic contextfree grammars ,1,0,0
in order to objectively evaluate our representation we derived it from two different sources constituency parse trees generated with our implementation of collins 1997 and dependency parse trees created using minipar lin 19981 ,0,1,0
6 conclusion traditional approaches for devising parsing models smoothing techniques and evaluation metrics are not well suited for mh as they presuppose 13the lack of head marking for instance precludes the use of lexicalized models a la collins 1997 ,0,1,0
the data set is same as in section 51 except that we also parsed the englishside using a variant of the collins 1997 parser and then extracted 247m treetostring rules using the algorithm of galley et al 2006 ,0,1,0
these forest rescoring algorithms have potential applications to other computationally intensive tasks involving combinations of different models for example headlexicalized parsing collins 1997 joint parsing and semantic role labeling sutton and mccallum 2005 or tagging and parsing with nonlocal features ,0,1,0
in particular hockenmaier and steedman 2001 report a generative model for ccg parsing roughly akin to the collins parser collins 1997 specific to ccg ,0,1,0
we first determine lexical heads of nonterminal nodes by using bikels implementation of collins head detection algorithm9 bikel 2004 collins 1997 ,0,1,0
after parsing the corpus collins 1997 we artificially introduced verb form errors into these sentences and observed the resulting disturbances to the parse trees ,0,1,0
for example the sentence my father is work in the laboratory is parsed collins 1997 as s np my father vp is np work pp in the laboratory 2the abbreviations s is or has and d would or had compound the ambiguities ,0,1,0
for getting the syntax trees the latest version of collins parser collins 1997 was used ,0,1,0
3 we then run collins parser 1997 using just the sentence pairs where parsing succeeds with a negative log likelihood below 200 ,0,1,0
owenocogentexcom 1 introduction dependency grammar has a long tradition in syntactic theory dating back to at least tesnires work from the thirties3 recently it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words see eg collins 1997 which is what dependency grammars model explicitly do but contextfree phrasestructure grammars do not ,0,1,0
recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models jelinek et al 1994 collins 1997 ,0,1,0
1 introduction the probabilistic relation between verbs and their arguments plays an important role in modern statistical parsers and supertaggers charniak 1995 collins 19961997 joshi and srinivas 1994 kim srinivas and trueswell 1997 stolcke et al 1997 and in psychological theories of language processing clifton et al 1984 ferfeira mcclure 1997 gamsey et al 1997 jurafsky 1996 macdonald 1994 mitchell holmes 1985 tanenhaus et al 1990 trueswell et al 1993 ,0,1,0
several recent realworld parsers have improved stateoftheart parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars alshawi 1996 eisner 1996 charniak 1997 collins 1997 ,1,0,0
three models in collins 1997 are susceptible to the on 3 method cf ,0,0,1
introduction verb subcategorizafion probabilities play an important role in both computational linguistic applications eg carroll minnen and briscoe 1998 charniak 1997 collins 19961997 joshi and srinivas 1994 kim srinivas and tmeswell 1997 stolcke et al 1997 and psycholinguisfic models of language processing eg boland 1997 clifton et al 1984 ferreira mcclure 1997 fodor 1978 garnsey et al 1997 jurafsky 1996 macdonald 1994 mitchell holmes 1985 tanenhaus et al 1990 trueswell et al 1993 ,0,1,0
the lexicalized pcfg that sits behind model 2 of collins 1997 has rules of the form p lnlni lihrirnirn 1 swillmd npappinnp vpwilimd nnp i apple md vp buyvb vb prtoutrp npmicrosoftnnp i i buy rp nnp i i out microsoft figure 1 a sample sentence with parse tree ,0,1,0
in the bbn model as with model 2 of collins 1997 modifying nonterminals are generated conditioning both on the parent p and its head child h unlike model 2 of collins 1997 they are also generated conditioning on the previously generated modifying nonterminal l1 or pq1 and there is no subcat frame or distance feature ,0,1,0
while the bbn model does not perform at the level of model 2 of collins 1997 on wall street journal text it is also less languagedependent eschewing the distance metric which relied on specific features of the english treebank in favor of the bigrams on nonterminals model ,0,1,0
during training each example is broken into elementary trees using head rules and argumentadjunct rules similar to those of collins 1997 ,0,1,0
the success of recent highquality parsers charniak 1997 collins 1997 relies on the availability of such treebank corpora ,1,0,0
we choose those sections because several stateofthwart parsers collins 1997 ratnaparkhi 1998 charniak 1997 are trained on section 221 and tested on section 23 ,1,0,0
a head percolation table has previously been used in several statistical parsers magerman 1995 collins 1997 to find heads of phrases ,0,1,0
our strategy for choosing heads is similar to the one in collins 1997 ,0,1,0
related to this issue we note that the head rules which were nearly identical to those used in collins 1997 have not been tuned at all to this task ,0,1,0
 models 2 and 3 of collins 1997 ,0,1,0
this includes both the parsers that attach probabilities to parser moves magerman 1995 ratnaparkhi 1997 but also those of the lexicalized pcfg variety collins 1997 charniak 1997 ,0,1,0
however since work in this direction has started a significant progress has also been made in the research on statistical learning of full parsers both in terms of accuracy and processing time charniak 1997b charniak 1997a collins 1997 ratnaparkhi 1997 ,1,0,0
for the full parser we use the one developed by michael collins collins 1996 collins 1997 one of the most accurate full parsers around ,1,0,0
the reported results for the full parse tree on section 23 are recallprecision of 881875 collins 1997 ,0,1,0
thus over the past few years along with advances in the use of learning and statistical methods for acquisition of full parsers collins 1997 charniak 1997a charniak 1997b ratnaparkhi 1997 significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship church 1988 ramshaw and marcus 1995 argamon et al 1998 cardie and pierce 1998 munoz et al 1999 punyakanok and roth 2001 buchholz et al 1999 tjong kim sang and buchholz 2000 ,0,1,0
so the sequence with a fork which corresponds to only one nucleus is treated as a three word sequence in model c apart from this difference model c directly relies on a combination of equations 10 and 12 namely conditioning by a80a7a81a49a82a9a12 a74a61a8a65a75a57a12 and a74a61a8a65a75a57a14a61a86 both the probability of generating a74a61a8a65a75 a47 and the one of generating a80a7a81a49a82 a47 thus model c uses a reduced version of equation 12 and an extended version of 2other models as collins and brooks 1995 merlo et al 1998 for ppattachment resolution or collins 1997 samuelsson 2000 for probabilistic parsing are somewhat related but their supervised nature makes any direct comparison impossible ,0,1,0
a statistical language model a lexicalized pcfg similar to that of collins 1997 is derived from the analysis grammar by processing a corpus using the same grammar with no statistical model and recording frequencies of substructures built by each rule ,0,1,0
1997 and the english parser developed by collins 1997 ,0,1,0
we automatically converted the phrase structure output of the collins parser into the syntactic dependency representation used by our syntactic realizer realpro lavoie and rambow 1997 ,0,1,0
in order to extract the linguistic features necessary for the model all sentences were first automatically partofspeechtagged using a maximum entropy tagger ratnaparkhi 1998 and parsed using the collins parser collins 1997 ,0,1,0
this is similar to collins 1997s and charniak97s definition of a separate category for auxiliary verbs ,0,1,0
are the labeled parsing recall and precision respectively as defined in collins 1997 slightly different from black et al 1991 ,0,1,0
this model is very similar to the markovized rule models in collins 1997 ,0,1,0
21 lexicalized parse trees the first successful work on syntactic disambiguation was based on lexicalized probabilistic contextfree grammar lpcfg collins 1997 charniak 1997 ,1,0,0
however existing models of disambiguation with lexicalized grammars are a mere extension of lexicalized probabilistic contextfree grammars lpcfg collins 1996 collins 1997 charniak 1997 which are based on the decomposition of parsing results into the syntacticsemantic dependencies of two words in a sentence under the assumption of independence of the dependencies ,0,1,0
since an existing study incorporates these relations ad hoc collins 1997 they are apparently crucial in accurate disambiguation ,0,1,0
what does student want to write your figure 3 a derivation tree of lexicalized parse trees such as the distinction of argumentsmodifiers and unbounded dependencies collins 1997 are elegantly represented in derivation trees ,0,1,0
most statistical parsing research such as collins 1997 has centered on training probabilistic contextfree grammars using the penn treebank ,0,1,0
2 previous work other researchers have investigated the topic of automatic generation of abstracts but the focus has been different eg sentence extraction edmundson 1969 johnson et al 1993 kupiec et al 1995 mann et al 1992 teufel and moens 1997 zechner 1995 processing of structured templates paice and jones 1993 sentence compression hori et al 2002 knight and marcu 2001 grefenstette 1998 luhn 1958 and generation of abstracts from multiple sources radev and mckeown 1998 ,0,1,0
features for each frame element features are extracted from the surface text of the sentence and from an automatically generated syntactic parse tree collins 1997 ,0,1,0
bikel and chiang 2000 in fact contains two parsers one is a lexicalized probabilistic contextfree grammar pcfg similar to collins 1997 the other is based on statistical tag chiang 2000 ,0,1,0
to make the model more practical in parameter estimation we assume the features in feature set fs are independent from each other thus fsfi afipafsp 5 under this pcfgpf model the goal of a parser is to choose a parse that maximizes the following score maxarg 1 afs i i i n i t pstscore 6 our model is thus a simplification of more sophisticated models which integrate pcfgs with features such as those in magerman1995 collins1997 and goodman1997 ,0,1,0
the creation of the penn english treebank marcus et al 1993 a syntactically interpreted corpus played a crucial role in the advances in natural language parsing technology collins 1997 collins 2000 charniak 2000 for english ,1,0,0
parsers that attempt to disambiguate the input completely full parsing typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic topdown model in order to select the most probable analysis collins 1997 charniak 2000 ,0,1,0
these sentences were parsed with the collins parser collins 1997 ,0,1,0
head word and its partofspeech tag of the constituent after pos tagging a syntactic parser collins 1997 was then used to obtain the parse tree for the sentence ,0,1,0
recently it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words see eg collins 1997 which is what dependency grammars model explicitly but contextfree phrasestructure grammars do not ,0,1,0
of particular interest are lexicalized parsing models such as the ones developed by collins 1996 1997 and carroll and rooth 1998 ,1,0,0
these models include a standard unlexicalized pcfg parser a headlexicalized parser collins 1997 and a maximumentropy inspired parser charniak 2000 ,0,1,0
specifically the following information can be either automatically identified or manually annotated syntactic structures automatically identified from a parser collins 1997 semantic roles of entities in the question gildea and jurafsky 2002 gildea and palmer 2002 surdeanu et al 2003 discourse roles either manually annotated or identified by rules that map directly from semantic roles to discourse roles ,0,1,0
step 3 answer extraction we select the top 5 ranked sentences and return them as collins 1997 can be used to capture the binary dependencies between the head of each phrase ,0,1,0
23 collinss bikels parser collinss statistical parser cbp collins 1997 improved by bikel bikel 2004 is based on the probabilities between headwords in parse trees ,0,0,1
the parser implementation in bikel 2002 was used in this experiment and it was run in a mode which emulated the collins 1997 parser ,0,1,0
as in collins 1997 the parameter c8 d0 b4c4 cx b4d0d8 cx bnd0db cx b5cyc8bnc0bndbbnd8bna1bnc4bvb5 is further smoothed as follows c8 d0bd b4c4 cx cyc8bnc0bndbbnd8bna1bnc4bvb5 a2 c8 d0be b4d0d8 cx cyc8bnc0bndbbnd8bna1bnc4bvbnc4 cx b5a2 c8 d0bf b4d0db cx cyc8bnc0bndbbnd8bna1bnc4bvbnc4 cx b4d0d8 cx b5b5 note this smoothing is different from the syntactic counterpart ,0,1,0
the probabilities from these backoff levels are interpolated using the techniques in collins 1997 ,0,1,0
we augment collins headdriven model 2 collins 1997 to incorporate a semantic label on each internal node ,0,1,0
in the following section we follow the notation in collins 1997 ,0,1,0
in parsing the most relevant previous work is due to collins 1997 who considered three binary features of the intervening material did it contain a any word tokens at all b any verbs c any commas or colons ,0,1,0
5 related work as discussed in footnote 3 collins 1997 and mcdonald et al ,0,1,0
bilexical cfg is at the heart of most modern statistical parsers collins 1997 charniak 1997 because the statistics associated with wordspecific rules are more informative for disambiguation purposes ,0,1,0
2005 866 867 119 klein and manning 2003 869 857 863 309 110 charniak 1997 874 875 100 collins 1997 886 881 091 table 3 comparison with other parsers sentences of length 40 as head information ,0,1,0
this is in sharp contrast to the smoothed fixedword statistics in most lexicalized parsing models derived from sparse data magerman 1995 collins 1996 charniak 1997 etc ,0,1,0
with automatic refinement it is harder to guarantee improved performance than with manual refinements klein and manning 2003 or with refinements based on direct lexicalization magerman 1995 collins 1996 charniak 1997 etc ,0,1,0
2 head lexicalization as previously shown charniak 1997 collins 1997 carroll and rooth 1998 etc contextfree grammars cfgs can be transformed to lexicalized cfgs provided that a headmarking scheme for rules is given ,1,0,0
by contrast alternative approaches like collins 1997 apply an additional transformation to each tree in the treebank splitting each rule into small parts which finally results in a new grammar covering many more sentences than the explicit one ,0,1,0
the most important treebank transformation in the literature is lexicalization each node in a tree is labeled with its head word the most important word of the constituent under the node magerman 1995 collins 1996 charniak 1997 collins 1997 carroll and rooth 1998 etc ,0,1,0
table 1 shows a summary of the results of our experiments with svmpar and mblpar and also results obtained with the charniak 2000 parser the bikel 2003 implementation of the collins 1997 parser and the ratnaparkhi 1997 parser ,0,1,0
performance of alternative models 157 5 related work previous parsing models eg collins 1997 charniak 2000 maximize the joint probability ps t of a sentence s and its parse tree t we maximize the conditional probability pt s ,0,1,0
1 introduction there has been a great deal of progress in statistical parsing in the past decade collins 1996 collins 1997 chaniak 2000 ,1,0,0
instead researchers condition parsing decisions on many other features such as parent phrasemarker and famously the lexicalhead of the phrase magerman 1995 collins 1996 collins 1997 johnson 1998 charniak 2000 henderson 2003 klein and manning 2003 matsuzaki et al 2005 and others ,0,1,0
eisner 1996 charniak 1997 collins 1997 and many subsequent researchers1 annotated every node with lexical features passed up from its head child in order to more precisely reflect the nodes inside contents ,0,1,0
317 citation observed data hidden data collins 1997 treebank tree with head child annotated on each nonterminal no hidden data ,0,1,0
to avoid this problem generative models for nlp tasks have often been manually designed to achieve an appropriate representation of the joint distribution such as in the parsing models of collins 1997 charniak 2000 ,0,1,0
in addition to portability experiments with the parsing model of collins 1997 gildea 2001 provided a comprehensive analysis of parser portability ,0,1,0
for example smoothing methods have played a central role in probabilistic approaches collins 1997 wang et al 2005 and yet they are not being used in current large margin training algorithms ,0,1,0
1 introduction over the past decade there has been tremendous progress on learning parsing models from treebank data collins 1997 charniak 2000 wang et al 2005 mcdonald et al 2005 ,1,0,0
most of the early work in this area was based on postulating generative probability models of language that included parse structure collins 1997 ,0,1,0
it is interesting to note that while the study of how the granularity of contextfree grammars cfg affects the performance of a parser eg in the form 86 n1ip n2np subj n4nr gsc4es jiangzemin n3vp n5vv esdo interview n6np obj n7nr adjunct aic1 thai n8nn d3d2 president f1 pred esdo subj f2 pred gsc4esntype proper num sg obj f3 pred d3d2 ntype common num sg adjunct f4 pred aic1ntype proper num sg n f n1n3n5f1 n2n4f2 n6n8f3 n7f4 figure 1 cand fstructures with links for the sentence gsc4esesdoaic1d3d2 of grammar transforms johnson 1998 and lexicalisation collins 1997 has attracted substantial attention to our knowledge there has been a lot less research on this subject for surface realisation a process that is generally regarded as the reverse process of parsing ,1,0,0
methodologies such as lexicalisation collins 1997 charniak 2000 and tree transformations johnson 1998 weaken the independence assumptions and have been applied successfully to parsing and shown significant improvements over simple pcfgs ,1,0,0
31 a historybased model the historybased hb approach which incorporates more context information has worked well in parsing collins 1997 charniak 2000 ,1,0,0
as from a linguistic perspective it is the modifier 2we use a mechanism similar to collins 1997 but adapted to chinese data to find lexical heads in the treebank data ,0,1,0
to achieve step 1 we first apply a set of headfinding rules which are similar to those described in collins 1997 ,0,1,0
for test or development data we used the partofspeech tags generated by the parser of collins 1997 ,0,1,0
parse each sentence using a treebanktrained parser collins 1997 charniak 1999 ,0,1,0
24 germanenglish for germanenglish we additionally incorporated rulebased reordering we parse the input using the collins parser collins 1997 and apply a set of reordering rules to rearrange the german sentence so that it corresponds more closely english word order collins et al 2005 ,0,1,0
one can imagine the same techniques coupled with more informative probability distributions such as lexicalized pcfgs charniak 1997 or even grammars not based upon literal rules but probability distributions that describe how rules are built up from smaller components magerman 1995 collins 1997 ,0,1,0
nj 08903 usa suzanneruccs rutgers edu empiricallyinduced models that learn a linguistically meaningflll grammar collins 1997 seem to give tile best practical results in statistical natural language processing ,1,0,0
in agreement with recent results on parsing with lexicalised probabilistic grammars collins 1997 srinivas 1997 charniak 1997 our main result is that statistics over lexical features best correspond to independently established truman intuitive preferences and experimental findings ,1,0,0
in agreement with recent resuits on parsing with lexicalised probabilistic grammars collins 1997 srinivas 1997 we find that statistics over lexical as opposed to structural features best correspond to human intuitivejudgments and to experimental findings ,1,0,0
in addition many more sophisticated parsing models are elaborations of such pcfg models so understanding the properties of pcfgs is likely to be useful charniak 1997 collins 1997 ,0,1,0
also adding a constituent sizedistance effect as described by schubert 1986 and as used by some researchers in parsing eg lesmo and torasso 1985 and collins 1997 would almost certainly improve parsing ,0,1,0
while these approaches have had som e success to date collins 1997 charniak 1997a their usability as parsers in systems for natural language understanding is suspect ,0,0,1
the corpusbased statistical parsing community has many fast and accurate automated parsing systems including systems produced by collins 1997 charniak 1997 and ratnaparkhi 1997 ,1,0,0
collins 1997 ratnaparkhi 1997 use cascaded processing for full parsing with good results ,0,1,0
statistical model in sifts statistical model augmented parse trees are generated according to a process similar to that described in collins 1996 1997 ,0,1,0
one such technique is bootstrapping which was recently presented in riloff and jones 1999 jones et a11999 as an ideal framework for text learning tasks that have knowledge seeds ,0,1,0
riloff and jones 1999 note that the bootstrapping algorithm works well but its performance can deteriorate rapidly when noncoreferring data enter as candidate heuristics ,0,1,0
our system attempts to recognize these syntactic patterus in addition it considers as unfamiliar some definites occurring in 4this list was developed by hand more recently bean and riloff 1999 proposed methods for autolnatically extracting flom a corpus such special predicates ie heads that correlate well with discourse novelty ,0,1,0
nonanaphoric definite descriptions have been detected using heuristics eg vieira and poesio 2000 and unsupervised methods eg bean and riloff 1999 ,0,1,0
more recently the problem has been tackled using statisticsbased eg bean and riloff 1999 bergsma et al 2008 and learningbased eg evans 2001 ng and cardie 2002a ng 2004 yang et al 2005 denis and balbridge 2007 methods ,0,1,0
2 related work given its potential usefulness in coreference resolution anaphoricity determination has been studied fairly extensively in the literature and can be classified into three categories heuristic rulebased eg paice and husk 1987 lappin and leass 1994 kennedy and boguraev 1996 denber 1998 vieira and poesio 2000 statisticsbased eg bean and riloff 1999 cherry and bergsma 2005 bergsma et al 2008 and learningbased eg evans 2001 ng and cardie 2002a ng 2004 yang et al 2005 denis and balbridge 2007 ,0,1,0
for the statisticsbased approaches bean and riloff 1999 developed a statisticsbased method for automatically identifying existential definite nps which are nonanaphoric ,0,1,0
2005 for english but not identical to strictly anaphoric ones5 bean and riloff 1999 uryupina 2003 since a nonanaphoric np can corefer with a previous mention ,0,1,0
bean and riloff 1999 and uryupina 2003 have already employed a definite probability measure in a similar way although the way the ratio is computed is slightly different ,0,1,0
in contrast the latter computes four definite probabilities which are included as features within a machinelearning classifier from the web in an attempt to overcome bean and riloffs 1999 data sparseness problem ,0,1,0
a more finegrained distinction is made by bean and riloff 1999 and vieira and poesio 2000 to distinguish restrictive from nonrestrictive postmodification by ommitting those modifiers that occur between commas which should not be classified as chain starting ,0,1,0
we borrow the idea of classifying definites occurring in the first sentence as chain starting from bean and riloff 1999 ,0,1,0
18 more recently bean and riloff 1999 have proposed methods for automatically extracting from a corpus heads that correlate well with discourse novelty ,1,0,0
32 this problem is also a central concern in the work by bean and riloff 1999 ,0,1,0
first a nonanaphoric np classifier identifies definite noun phrases that are existential using both syntactic rules and our learned existential np recognizer bean and riloff 1999 and removes them from the resolution process ,0,1,0
in our future work we intend to adopt a looser filter together with an anaphoricity determination module bean and riloff 1999 ng and cardie 2002b ,0,1,0
the system described in bean and riloff 1999 also makes use of syntactic heuristics ,0,1,0
the task of classifying several different uses of definite descriptions vieira and poesio 2000 bean and riloff 1999 is somewhat analogous to that for bare nouns ,0,1,0
more recently the problem has been tackled using unsupervised eg bean and riloff 1999 and supervised eg evans 2001 ng and cardie 2002a approaches ,0,1,0
3bean and riloff 1999 and uryupina 2003 construct quite accurate classifiers to detect unique nps ,1,0,0
as resolving direct anaphoric descriptions the ones where anaphor and antecedent have the same head noun is a much simpler problem with high performance rates as shown in previous results vieira et al 2000 bean and riloff 1999 these heuristics should be applied first in a system that resolves definite descriptions ,0,1,0
version of the system p r f baseline 508 100 674 discoursenew detection only 69 72 70 handcoded dt partial 62 85 717 handcoded dt total 77 77 77 id3 75 75 75 table 1 overall results by vieira and poesio 22 bean and riloff bean and riloff 1999 developed a system for identifying discoursenew dds1 that incorporates in addition to syntaxbased heuristics aimed at recognizing predicative and established dds using postmodification heuristics similar to those used by vieira and poesio additional techniques for mining from corpora unfamiliar dds including proper names larger situation and semantically functional ,0,1,0
more recently other approaches have investigated the use of machine learning to nd patterns in documentsstrzalkowski et al 1998 and the utility of parameterized modules so as to deal with dierent genres or corporagoldstein et al 2000 ,0,1,0
alternatively we could have simply incorporated the diversity measure into the objective function or used an inference algorithm that specifically accounts for redundancy eg maximal marginal relevance goldstein et al 2000 ,0,1,0
the diversity function rewards summaries that cover many important aspects and plays the redundancy reducing role that is common in most extractive summarization frameworks goldstein et al 2000 ,0,1,0
this is in contrast to standard summarization models that look to promote sentence diversity in order to cover as many important topics as possible goldstein et al 2000 ,0,1,0
in 2004 conroy conroy 2004 tested maximal marginal relevance goldstein et al 2000 as well as qr decomposition ,1,0,0
while close attention has been paid to multidocument summarization technologies barzilay et al 2002 goldstein et al 2000 the inherent properties of humanwritten multidocument summaries have not yet been quantified ,0,1,0
for example extractive text summarization generates a summary by selecting a few good sentences from one or more articles on the same topic goldstein et al 2000 ,0,1,0
perhaps the most wellknown method is maximum marginal relevance mmr carbonell and goldstein 1998 as well as crosssentence informational subsumption radev 2000 mixture models zhang et al 2002 subtopic diversity zhai et al 2003 diversity penalty zhang et al 2005 and others ,0,1,0
methods like mcdonalds including the wellknown maximal marginal relevance mmr algorithm goldstein et al 2000 are subject to another problem summarylevel redundancy is not always well modeled by pairwise sentencelevel redundancy ,0,0,1
the algorithms were trained and tested using version 3 of the penn treebank using the training development and test split described in collins 2002 and also employed by toutanova et al ,0,1,0
we trained log linear models with theperceptronalgorithmcollins2002usingfea746 markov order classification task 0 1 2 s1 no multiword constituent start 967 969 969 e1 no multiword constituent end 973 973 973 table 2 classification accuracy on development set for binary classes s1 and e1 for various markov orders ,0,1,0
when we have a junction tree for each document we can efficiently perform belief propagation in order to compute argmax in equation 1 or the marginal probabilities of cliques and labels necessary for the parameter estimation of machine learning classifiers including perceptrons collins 2002 and maximum entropy models berger et al 1996 ,0,1,0
for bpm we run 100 averaged perceptrons collins 2002 with 10 iterations for each ,0,1,0
the model weights are trained using the standard ranking perceptron collins 2002 ,0,1,0
our hierarchical training method yields significant improvement when compared to a similar nonhierarchical model which instead uses the standard 2data and code used in this paper are available at httppeoplecsailmiteduedcemnlp07 perceptron update of collins 2002 ,0,1,0
we describe a new sequence alignment model based on the averaged perceptron collins 2002 which shares with the above approaches the ability to exploit arbitrary features of the input sequences but is distinguished from them by its relative simplicity and the incremental character of its training procedure ,0,1,0
22 a perceptronbased edit model in this section we present a generalpurpose extension of perceptron training for sequence labeling due to collins 2002 to the problem of sequence alignment ,0,1,0
reranking 1 uses the score of the rst model as a feature in addition to the nonlocal features as in collins 2002b ,0,1,0
to achieve robust training daume iii and marcu 2005 employed the averaged perceptron collins 2002a and alma gentile 2001 ,0,1,0
collins and roark 2004 used the averaged perceptron collins 2002a ,0,1,0
with nonlocal features we cannot use efcient procedures such as forwardbackward procedures and the viterbi algorithm that are required in training crfs lafferty et al 2001 and perceptrons collins 2002a ,1,0,0
in this paper we follow this line of research and try to solve the problem by extending collins perceptron algorithm collins 2002a ,0,1,0
2 perceptron algorithm for sequence labeling collins 2002a proposed an extension of the perceptron algorithm rosenblatt 1958 to sequence labeling ,0,1,0
the learning algorithm which is illustrated in collins 2002a proceeds as follows ,0,1,0
thus collins 2002a also proposed an averaged perceptron where the nal weight vector is 1collins2002aalsoprovidedproofthatguaranteedgood learning for the nonseparable case ,0,1,0
3 margin perceptron algorithm for sequence labeling weextendedaperceptronwithamarginkrauthand mezard 1987 to sequence labeling in this study as collins 2002a extended the perceptron algorithm to sequence labeling ,0,1,0
based on the proofs in collins 2002a and li et al ,0,1,0
2daume iii and marcu 2005 also presents the method using the averaged perceptron collins 2002a 3for reranking problems shen and joshi 2004 proposed a perceptron algorithm that also uses margins ,0,1,0
discriminative methods such as conditional random fields crfs lafferty et al 2001 semimarkov random fields sarawagi and cohen 2004 and perceptrons collins 2002a have been popular approaches for sequence labeling because of their excellent performance which is mainly due to their ability to incorporate many kinds of overlapping and nonindependent features ,1,0,0
this resembles the reranking approach collins and duffy 2002 collins 2002b ,0,1,0
however by examining the algorithm 42 perceptron with local and nonlocal features parameters n ca cl 0 until no more updates do for i 1 to l do8 yn nbestylxiy y argmaxyynaxiy y 2ndbestyynaxiy if y yi axiyiaxiy ca then axiyiaxiy a else if axiyiaxiy ca then axiyiaxiy a else b 8 if y1 yi then y1 represents the best in yn lxiyilxiy1 else if lxiyilxiy2 cl then lxiyilxiy2 proofs in collins 2002a we can see that the essential condition for convergence is that the weights are always updated using some y y that satises xiyixiy 0 c in the case of a perceptron with a margin ,0,1,0
as mentioned earlier both of these methods are based on collinss averagedperceptron algorithm for sequence labeling collins 2002 ,0,1,0
our learning method is an extension of collinss perceptronbased method for sequence labeling collins 2002 ,0,1,0
in step 3 a simple perceptron update collins 2002 is performed ,0,1,0
we trained a chinese treebankstyle tokenizer and partofspeech tagger both using a tagging model based on a perceptron learning algorithm collins 2002 ,0,1,0
the definition of 2x h m c is dir cposxh cposxm cposxc dir cposxh cposxc dir cposxm cposxc dir formxh formxc dir formxm formxc dir cposxh formxc dir cposxm formxc dir formxh cposxc dir formxm cposxc 3 experiments and results we report experiments with higherorder models for the ten languages in the multilingual track of the conll2007 shared task nivre et al 20071 in all experiments we trained our models using the averaged perceptron freund and schapire 1999 following the extension of collins 2002 for structured prediction problems ,0,1,0
the perceptron style for natural language processing problems as initially proposed by collins 2002 can provide state of the art results on various domains including text chunking syntactic parsing etc the main drawback of the perceptron style algorithm is that it does not have a mechanism for attaining the maximize margin of the training data ,1,0,0
averaging has been shown to help reduce overfitting mcdonald et al 2005a collins 2002 ,0,1,0
it is easy to see that the main difference between the pa algorithms and the perceptron algorithm pc collins 2002 as well as the mira algorithm mcdonald et al 2005a is in line 9 ,0,1,0
the learning methods using in discriminative parsing are perceptron collins 2002 and online largemargin learning mira crammer and singer 2003 ,0,1,0
23 online learning again following mcdonald et al 2005 we have used the single best mira crammer and singer 2003 which is a margin aware variant of perceptron collins 2002 collins and roark 2004 for structured prediction ,0,1,0
we discriminatively trained our parser in an online fashion using a variant of the voted perceptron collins 2002 collins and roark 2004 crammer and singer 2003 ,0,1,0
the technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training and has been shown to work well in practice freund and schapire 1999 collins 2002 ,1,0,0
furthermore we use averaged weights collins 2002 freund and schapire 1999 in algorithm 1 ,0,1,0
our learning algorithm stems from perceptron training in collins 2002 ,0,1,0
in section 3 we will present a perceptron like algorithm collins 2002 daume iii and marcu 2005 to obtain the parameters ,0,1,0
before parsing pos tags are assigned to the input sentence using our reimplementation of the postagger from collins 2002 ,0,1,0
we use the discriminative perceptron learning algorithm collins 2002 mcdonald et al 2005 to train the values of vectorw ,0,1,0
averaging parameters is a way to reduce overfitting for perceptron training collins 2002 and is applied to all our experiments ,1,0,0
as with the graphbased parser we use the discriminative perceptron collins 2002 to train the transitionbased model see figure 5 ,0,1,0
81 the averaged perceptron algorithm with separating plane the averaged perceptron algorithm collins 2002 has previously been applied to various nlp tasks collins 2002 collins 2001 for discriminative reranking ,0,1,0
the detailed algorithm can be found in collins 2002 ,0,1,0
1 introduction in this paper we show how discriminative training with averaged perceptron models collins 2002 can be used to substantially improve surface realization with combinatory categorial grammar steedman 2000 ccg ,0,1,0
3 perceptron reranking as collins 2002 observes perceptron training involves a simple online algorithm with few iterations typically required to achieve good performance ,1,0,0
online baselines include top1 perceptron collins 2002 top1 passiveaggressive pa and kbest pa crammer singer 2003 mcdonald et al 2004 ,0,1,0
2 problem setting in the multiclass setting instances from an input spacex take labels from a finite setyy k 496 we use a standard approach collins 2002 for generalizing binary classification and assume a feature function fxy rd mapping instances xx and labels yy into a common space ,0,1,0
we used the averaged perceptron algorithm freund and schapire 1998 collins 2002 to train the parameters of the model ,0,1,0
a structured perceptron collins 2002 learns weights for our transliteration features which are drawn from two broad classes indicator and hybrid generative features ,0,1,0
given an input training corpus of such derivations d d1 dn a vector feature function on derivations vectorfd and an initial weight vector vectorw the perceptron performs two steps for each training example di d decode d argmaxddsrcdi parenleftbig vectorw vectorfd parenrightbig update vectorw vectorw vectorfdi vectorfd where dsrcd enumerates all possible derivations with the same source side as d to improve generalization the final feature vector is the average of all vectors found during learning collins 2002 ,0,1,0
we evaluate its performance on the standard penn english treebank ptb dependency parsing task ie train on sections 0221 and test on section 23 with automatically assigned pos tags at 972 accuracy using a tagger similar to collins 2002 and using the headrules of yamada and matsumoto 2003 for conversion into dependency trees ,0,1,0
we use the popular online learning algorithm of structured perceptron with parameter averaging collins 2002 ,1,0,0
the averaged 1555 perceptron has a solid theoretical fundamental and was proved to be effective across a variety of nlp tasks collins 2002 ,1,0,0
our study also shows that the simulatedannealing algorithm kirkpatrick et al 1983 is more effective 1552 than the perceptron algorithm collins 2002 for feature weight tuning ,0,0,1
collins 2002 improves the f1 score from 882 to 897 while charniak and johnson 2005 improve from 903 to 914 ,1,0,0
an online learning algorithm considers a single training instance for each update to the weight vector w we use the common method of setting the final weight vector as the average of the weight vectors after each iteration collins 2002 which has been shown to alleviate overfitting ,1,0,0
this algorithm can thus be viewed as a largemargin version of the perceptron algorithm for structured outputs collins 2002 ,0,1,0
averaging has been shown to reduce overfitting collins 2002 as well as reliance on the order of the examples during training ,1,0,0
online votedperceptrons have been reported to work well in a number of nlp tasks collins 2002 liang et al 2006 ,1,0,0
the stateofthe art taggers are using feature sets discribed in the corresponding articles collins 2002 gimenez and marquez 2004 toutanova et al 2003 and shen et al 2007 morce supervised and morce semisupervised are using feature set desribed in section 4 ,1,0,0
to summarize we can describe our system as follows it is based on votrubec 2006s implementation of collins 2002 which has been fed at each iteration by a different dataset consisting of the supervised and unsupervised part precisely by a concatenation of the manually tagged training data wsj portion of the ptb 3 for english morphologically disambiguated data from pdt 20 for czech and a chunk of automatically tagged unsupervised data ,0,1,0
10our experiments have shown that using averaging helps tremendously confirming both the theoretical and practical results of collins 2002 ,0,1,0
for english after a relatively big jump achieved by collins 2002 we have seen two significant improvements toutanova et al 2003 and shen et al 2007 pushed the results by a significant amount each time1 1in our final comparison we have also included the results of gimenez and marquez 2004 because it has surpassed collins 2002 as well and we have used this tagger in the data preparation phase ,1,0,0
the supervised training described in collins 2002 uses manually annotated data for the estimation of the weight coefficients ,0,1,0
3 the data 31 the supervised data for english we use the same data division of penn treebank ptb parsed section marcus et al 1994 as all of collins 2002 toutanova et al 2003 gimenez and marquez 2004 and shen et al 2007 do for details see table 1 ,0,1,0
another way of doing the parameter estimation for this matching task would have been to use an averaged perceptron method as in collins 2002 ,0,1,0
at this point one can imagine estimating a linear matching model in multiple ways including using conditional likelihood estimation an averaged perceptron update see which matchings are proposed and adjust the weights according to the dierence between the guessed and target structures collins 2002 or in largemargin fashion ,0,1,0
4 parameter optimization we optimize the feature weights using a modified version of averaged perceptron learning as described by collins 2002 ,0,1,0
we also compared the msr algorithm to two of the stateoftheart discriminative training methods boosting in row 3 is an implementation of the improved algorithm for the boosting loss function proposed in collins 2000 and perceptron in row 4 is an implementation of the averaged perceptron algorithm described in collins 2002 ,0,1,0
5 related work discriminative models have recently been proved to be more effective than generative models in some nlp tasks eg parsing collins 2000 pos tagging collins 2002 and lm for speech recognition roark et al 2004 ,1,0,0
in the rest of the paper we use the following notation adapted from collins 2002 ,0,1,0
the four models we compare are a maximum a posteriori map method and three discriminative training methods namely the boosting algorithm collins 2000 the average perceptron collins 2002 and the minimum sample risk method gao et al 2005 ,0,1,0
for a detailed description of each algorithm readers are referred to collins 2000 for the boosting algorithm collins 2002 for perceptron learning and gao et al ,0,1,0
31 definition the following setup adapted from collins 2002 was used for all three discriminative training methods 266 training data is a set of inputoutput pairs ,0,1,0
we used the average perceptron algorithm of collins 2002 in our experiments a variation that has been proven to be more effective than the standard algorithm shown in figure 2 ,1,0,0
previous authors have used numerous hmmbased models banko and moore 2004 collins 2002 lee et al 2000 thede and harper 1999 and other types of networks including maximum entropy models ratnaparkhi 1996 conditional markov models klein and manning 2002 mccallum et al 2000 conditional random elds crf lafferty et al 2001 and cyclic dependency networks toutanova et al 2003 ,0,1,0
41 partofspeech tagging experiments we split the penn treebank corpus marcus et al 1994 into training development and test sets as in collins 2002 ,0,1,0
networks toutanova et al 2003 9724 perceptron collins 2002 9711 svm gimenez and marquez 2003 9705 hmm brants 2000 9648 easiestfirst 9710 full bidirectional 9715 table 3 pos tagging accuracy on the test set sections 2224 of the wsj 5462 sentences ,0,1,0
collins 2002 and used postrigrams as well ,0,1,0
this averaging effect has been shown to help overfitting collins 2002 ,1,0,0
this model is related to the averaged perceptron algorithm of collins 2002 ,0,1,0
we use the averaged perceptron algorithm as presented in collins 2002 to train the parser ,0,1,0
task originally introduced in ramshaw and marcus 1995 and also described in collins 2002 sha and pereira 2003 brackets just base np constituents5 ,0,1,0
prior to running the parsers we trained the pos tagger described in collins 2002 ,0,1,0
for instance work has been done in chinese using the penn chinese treebank levy and manning 2003 chiang and bikel 2002 in czech using the prague dependency treebank collins et al 1999 in french using the french treebank arun and keller 2005 in german using the negra treebank dubey 2005 dubey and keller 2003 and in spanish using the uam spanish treebank moreno et al 2000 ,0,1,0
aadjoin tattach cconjoin ggenerate in this paper we use the perceptronlike algorithm proposed in collins 2002 which does not suffer from the label bias problem and is fast in training ,1,0,0
we compare our methods with both the averaged perceptron collins 2002 and conditional random fields lafferty et al 2001 using identical predicate sets ,0,1,0
a pioneer work in online training is the perceptronlike algorithm used in training a hidden markov model hmm collins 2002 ,1,0,0
however due to the computational issues with the voted perceptron the averaged perceptron algorithm collins 2002a is used instead ,0,1,0
to reduce the time complexity we adapted the lazy update proposed in collins 2002b which was also used in zhang and clark 2007 ,1,0,0
however there has recently been much work drawing connections between the two methods friedman hastie and tibshirani 2000 lafferty 1999 duffy and helmbold 1999 mason bartlett and baxter 1999 lebanon and lafferty 2001 collins schapire and singer 2002 in this section we review this work ,0,1,0
the central question in learning is how to set the parameters a given the training examples b x 1 y 1 x 2 y 2 x n y n logistic regression and boosting involve different algorithms and criteria for training the parameters a but recent work friedman hastie and tibshirani 2000 lafferty 1999 duffy and helmbold 1999 mason bartlett and baxter 1999 lebanon and lafferty 2001 collins schapire and singer 2002 has shown that the methods have strong similarities ,0,1,0
in particular previous work ratnaparkhi roukos and ward 1994 abney 1997 della pietra della pietra and lafferty 1997 johnson et al 1999 riezler et al 2002 has investigated the use of markov random fields mrfs or loglinear models as probabilistic models with global features for parsing and other nlp tasks ,0,1,0
2002 do not use a feature selection technique employing instead an objective function which includes a table 4 values of savings a b for various values of a b ab savings a b 1100000 26927 110 486 11100 835 1011000 2800 100110000 12639 1000150000 29202 50001100000 42298 collins and koo discriminative reranking for nlp gaussian prior on the parameter values thereby penalizing parameter values which become too large a c3 arg min a loglossa x k0m a 2 k 7 2 k 28 closedform updates under iterative scaling are not possible with this objective function instead optimization algorithms such as gradient descent or conjugate gradient methods are used to estimate parameter values ,0,1,0
section 3 describes previous work friedman hastie and tibshirani 2000 duffy and helmbold 1999 mason bartlett and baxter 1999 lebanon and lafferty 2001 collins schapire and singer 2002 that derives connections between boosting and maximumentropy models for the simpler case of classification problems this work forms the basis for the reranking methods ,0,1,0
see collins 2002a 2002b and collins and duffy 2001 2002 for applications of the perceptron algorithm ,0,1,0
collins 2002b gives convergence proofs for the methods collins 2002a directly compares the boosting and perceptron approaches on a named entity task and collins and duffy 2001 2002 use a reranking approach with kernels which allow representations of parse trees or labeled sequences in veryhighdimensional spaces ,0,1,0
the boosting approach to ranking has been applied to named entity segmentation collins 2002a and natural language generation walker rambow and rogati 2001 ,0,1,0
results from collins schapire and singer 2002 show that under these definitions the following guarantee holds loglossupdak bestwtk a c20 bestlossk a so it can be seen that the update from a to upda k bestwtk a is guaranteed to decrease logloss by at least w k q c0 w c0 k qc16c17 2 from these results the algorithms in figures 3 and 4 could be altered to take the revised definitions of w k and w c0 k into account ,1,0,0
for a full derivation of the modified updates and for quite technical convergence proofs see collins schapire and singer 2002 ,1,0,0
the key difference is that instead of using the delta rule of equation 8 as shown in line 5 of figure 4 collins 2002 updates parameters using the rule t1 d t d f d w r i f d w i ,0,1,0
the published f score for voted perceptron is 9353 with a different feature set collins 2002 ,0,1,0
however work in that direction has so far addressed only parse reranking collins and duffy 2002 riezler et al 2002 ,0,0,1
full discriminative parser training faces signi cant algorithmic challenges in the relationship between parsing alternatives and feature values geman and johnson 2002 and in computing feature expectations ,0,1,0
the generalized perceptron proposed by collins 2002 is closely related to crfs but the best crf training methods seem to have a slight edge over the generalized perceptron ,0,0,1
we compare those algorithms to generalized iterative scaling gis darroch and ratcliff 1972 nonpreconditioned cg and voted perceptron training collins 2002 ,0,1,0
33 voted perceptron unlike other methods discussed so far voted perceptron training collins 2002 attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the bestscoring labeling of that instance according to the current model ,0,1,0
instead of taking just the nal weight vector the voted perceptron algorithm takes the average of the t collins 2002 reported and we con rmed that this averaging reduces overtting considerably ,0,1,0
at any rate regularized conditional loglinear models have not previously been applied to the problem of producing a high quality partofspeech tagger ratnaparkhi 1996 toutanova and manning 2000 and collins 2002 all present unregularized models ,0,0,1
indeed the result of collins 2002 that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized ,0,1,0
whereas ratnaparkhi 1996 used feature support cutoffs and early stopping to stop overfitting of the model and collins 2002 contends that including low support features harms a maximum entropy model our results show that low support features are useful in a regularized maximum entropy model ,0,0,1
table 6 contrasts our results with those from collins 2002 ,0,1,0
22 perceptron algorithm our discriminative ngram model training approach uses the perceptron algorithm as presented in roark et al 2004 which follows the general approach presented in collins 2002 ,0,1,0
as suggested in collins 2002 we use the averaged perceptron when applying the model to heldout or test data ,0,1,0
moreover the parameters of the model must be estimated using averaged perceptron training collins 2002 which can be unstable ,0,0,1
the averaged perceptron collins 2002 is a variant which averages the w across all iterations it has demonstrated good generalization especially with data that is not linearly separable as in many natural language processing problems ,1,0,0
the weights are then averaged across all iterations of the perceptron as in collins 2002 ,0,1,0
muc6 347 30 204071 68 enron 833 143 204423 30 mgmtgroups 631 128 104662 37 table 1 summary of the corpora used in the experiments we used an implementation of collins votedpercepton method for discriminatively training hmms henceforth vphmm collins 2002 as well as crf lafferty et al 2001 to learn a ner ,0,1,0
previous approaches for training crfs have either 1 opted for a training method that no longer maximizes the likelihood eg mccallum and wellner 2004 roth and yih 2005 1 or 2 opted for a 1 both mccallum and wellner 2004 and roth and yih 2005 used the voted perceptron algorithm collins 2002 to train intractable crfs ,0,1,0
for regularization purposes we adopt an average perceptron collins 2002 which returns for each y y 1t summationtexttt1 ty the average of all weight vectors ty posited during training ,0,1,0
one sees this clear trend in the supervised nlp literature examples include the perceptron algorithm for tagging collins 2002 mira for dependency parsing mcdonald et al 2005 exponentiated gradient algorithms collins et al 2008 stochastic gradient for constituency parsing finkel et al 2008 just to name a few ,0,1,0
the phoneme prediction and sequence modeling are considered as tagging problems and a perceptron hmm collins 2002 is used to model it ,0,1,0
we used a feature set which included the current next and previous word the previous two tags various capitalization and other features of the word being tagged the full feature set is described in collins 2002a ,0,1,0
from a theoretical point of view it is difficult to find motivation for the parameter estimation methods used by bod 1998 see johnson 2002 for discussion ,0,1,0
collins 2002a describes experiments on the same namedentity dataset as in this paper but using explicit features rather than kernels ,0,1,0
collins 2002b describes how the voted perceptron can be used to train maximumentropy style taggers and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks ,0,1,0
another attractive property of the voted perceptron is that it can be used with kernels for example the kernels over parse trees described in collins and duffy 2001 collins and duffy 2002 ,0,1,0
see collins 2002 for additional work using perceptron algorithms to train tagging models and a more thorough description of the theory underlying the perceptron algorithm applied to ranking problems ,0,1,0
collins 2002 proposed a new algorithm for parameter estimation as an alternate to crf ,0,1,0
many machine learning techniques have been successfully applied to chunking tasks such as regularized winnow zhang et al 2001 svms kudo and matsumoto 2001 crfs sha and pereira 2003 maximum entropy model collins 2002 memory based learning sang 2002 and snow munoz et al 1999 ,1,0,0
21 global linear models we follow the framework outlined in collins 2002 2004 ,0,1,0
following collins 2002 we used the averaged parameters from the training algorithm in decoding heldout and test examples in our experiments ,0,1,0
freund and schapire 1999 originally proposed the averaged parameter method it was shown to give substantial improvements in accuracy for tagging tasks in collins 2002 ,0,1,0
collins 2000 and collins and duffy 2002 rerank the top n parses from an existing generative parser but this kind of approach 1dynamic programming methods geman and johnson 2002 lafferty et al 2001 can sometimes be used for both training and decoding but this requires fairly strong restrictions on the features in the model ,0,0,1
this paper explores an alternative approach to parsing based on the perceptron training algorithm introduced in collins 2002 ,0,1,0
for this paper we used pos tags that were provided either by the treebank itself gold standard tags or by the perceptron pos tagger3 presented in collins 2002 ,0,1,0
21 linear models for nlp we follow the framework outlined in collins 2002 2004 ,0,1,0
we will briefly review the perceptron algorithm and its convergence properties see collins 2002 for a full description ,0,1,0
we can then state the following theorem see collins 2002 for a proof theorem 1 for any training sequence xiyi that is separable with margin for any value of t then for the perceptron algorithm in figure 1 ne r 2 2 where r is a constant such that 8i8z 2 genxi jj xiyi xizjj r this theorem implies that if there is a parameter vector u which makes zero errors on the training set then after a finite number of iterations the training algorithm will converge to parameter values with zero training error ,0,1,0
all of the convergence and generalization results in collins 2002 depend on notions of separability rather than the size of gen two questions come to mind ,0,1,0
freund and schapire 1999 discuss how the theory for classification problems can be extended to deal with both of these questions collins 2002 describes how these results apply to nlp problems ,0,1,0
examples of such techniques are markov random fields ratnaparkhi et al 1994 abney 1997 della pietra et al 1997 johnson et al 1999 and boosting or perceptron approaches to reranking freund et al 1998 collins 2000 collins and duffy 2002 ,0,1,0
discriminative learning methods such as maximum entropy markov models mccallum et al 2000 projection based markov models punyakanok and roth 2000 conditional random fields lafferty et al 2001 sequence adaboost altun et al 2003a sequence perceptron collins 2002 hidden markov support vector machines altun et al 2003b and maximummargin markov networks taskar et al 2004 overcome the limitations of hmms ,1,0,0
among these methods crfs is the most common technique used in nlp and has been successfully applied to partofspeech tagging lafferty et al 2001 namedentity recognition collins 2002 and shallow parsing sha and pereira 2003 mccallum 2003 ,0,1,0
we also implemented an averaged perceptron system collins 2002 another online learning algorithm for comparison ,0,1,0
discriminatively trained parsers that score entire trees for a given sentence have only recently been investigated riezler et al 2002 clark and curran 2004 collins and roark 2004 taskar et al 2004 ,0,1,0
following previous work on using global features of candidate structures to learn a ranking model collins 2002 the global ie partitionbased features we consider here are simple functions of the local features that capture the relationship between np pairs ,0,1,0
3 parse tree features we tagged each candidate transcription with 1 partofspeech tags using the tagger documented in collins 2002 and 2 a full parse tree using the parser documented in collins 1999 ,0,1,0
22 global linear models we follow the framework of collins 2002 2004 recently applied to language modeling in roark et al ,0,1,0
for a full description of the algorithm see collins 2004 2002 ,0,1,0
a related method is multicategory perceptron which explicitly finds a weight vector that separates correct labels from the incorrect ones in a mistake driven fashion collins 2002 ,0,1,0
section 4 describes the online training procedure and compares it to the well known perceptron training algorithm collins 2002 ,0,1,0
the principal training method is an adaptation of averaged perceptron learning as described by collins 2002 ,0,1,0
5 perceptron training we optimize feature weights using a modification of averaged perceptron learning as described by collins 2002 ,0,1,0
1 introduction stateoftheart part of speech pos tagging accuracy is now above 97 for newspaper text collins 2002 toutanova et al 2003 ,1,0,0
moreover under this view smt becomes quite similar to sequential natural language annotation problems such as partofspeech tagging and shallow parsing and the novel training algorithm presented in this paper is actually most similar to work on training algorithms presented for these task eg the online training algorithm presented in mcdonald et al 2005 and the perceptron training algorithm presented in collins 2002 ,0,1,0
averaged perceptron collins 2002a which has been successfully applied to several tagging and parsing reranking tasks collins 2002c collins 2002a was employed for training rerank267 clang geoquery p r f p r f scissor 895 737 808 985 744 848 scissor 870 780 823 955 772 854 table 2 the performance of the baseline model scissor compared with scissor with the best result in bold where p precision r recall and f fmeasure ,1,0,0
we also plan to explore other types of reranking features such as the features used in semantic role labeling srl gildea and jurafsky 2002 carreras and marquez 2005 like the path between a target predicate and its argument and kernel methods collins 2002b ,0,1,0
experimenting with other effective reranking algorithms such as svms joachims 2002 and maxent charniak and johnson 2005 is also a direction of our future research ,0,1,0
the model is composed of three parts collins 2002a a set of candidate sapts gen which is the top n sapts of a sentence from scissor a function that maps a sentence inputs a set of training examples xiyi i 1n where xi is a sentence and yi is a candidate sapt that has the highest similarity score with the goldstandard sapt initialization set w 0 algorithm for t 1ti 1n calculate yi argmaxygenxi xiy w if yi negationslash yi then w w xiyi xiyi output the parameter vector w figure 2 the perceptron training algorithm ,0,1,0
for a full description of the algorithm see collins 2002a ,0,1,0
in comparison with shallow semantic analysis tasks such as wordsense disambiguation ide and jeaneronis 1998 and semantic role labeling gildea and jurafsky 2002 carreras and marquez 2005 which only partially tackle this problem by identifying the meanings of target words or finding semantic roles of predicates semantic parsing kate et al 2005 ge and mooney 2005 zettlemoyer and collins 2005 pursues a more ambitious goal mapping natural language sentences to complete formal meaning representations mrs where the meaning of each part of a sentence is analyzed including noun phrases verb phrases negation quantifiers and so on ,0,1,0
c0cq 1q xq xq1 xq1 xq xr xr1 table 6 lexicalized features for joint models aging of the weights suggested by collins 2002 ,0,1,0
the final model v uses the weight vector w summationtextk j1cjwj tn collins 2002 ,0,1,0
33 crfs and perceptron learning perceptron training for conditional models collins 2002 is an approximation to the sgd algorithm using feature counts from the viterbi label sequence in lieu of expected feature counts ,0,1,0
in many cases improving semisupervised models was done by seeding these models with domain information taken from dictionaries or ontology cohen and sarawagi 2004 collins and singer 1999 haghighi and klein 2006 thelen and riloff 2002 ,0,1,0
this was used for example by thelen and riloff 2002 collins and singer 1999 in information extraction and by smith and eisner 2005 in pos tagging ,0,1,0
this decomposition applies both to discriminative linear models and to generative models such as hmms and crfs in which case the linear sum corresponds to log likelihood assigned to the inputoutput pair by the model for details see roth 1999 for the classi cation case and collins 2002 for the structured case ,0,1,0
weight averaging was also employed collins 2002 which helped improve performance ,1,0,0
in this work we will use structured linear classifiers collins 2002 ,0,1,0
in our experiments we have used averaged perceptron collins 2002 freund and schapire 1999 and perceptron with margin krauth and mezard 1987 to improve performance ,1,0,0
following ratnaparkhi 1996 collins 2002 toutanova et al 2003 tsuruoka and tsujii 2005 765 feature sets templates error a ratnaparkhis 305 b a t0t1t0t1t1t0t1t2 292 c b t0t2t0t2t0t2w0t0t1w0t0t1w0 t0t2w0 t0t2t1w0t0t1t1w0t0t1t2w0 284 d c t0w1w0t0w1w0 278 e d t0x prefix or suffix of w04 x 9 272 table 2 experiments on the development data with beam width of 3 we cut the ptb into the training development and test sets as shown in table 1 ,0,1,0
following collins 2002 we do not distinguish rare words ,0,1,0
collins 2002 proposed a perceptron like learning algorithm to solve sequence classification in the traditional lefttoright order ,0,1,0
766 system beam error ratnaparkhi 1996 5 337 tsuruoka and tsujii 2005 1 290 collins 2002 289 guided learning feature b 3 285 tsuruoka and tsujii 2005 all 285 gimenez and marquez 2004 284 toutanova et al 2003 276 guided learning feature e 1 273 guided learning feature e 3 267 table 4 comparison with the previous works according to the experiments shown above we build our best system by using feature set e with beam width b 3 ,0,1,0
we use a bidirectional search strategy woods 1976 satta and stock 1994 and our algorithm is based on perceptron learning collins 2002 ,0,1,0
in our experiments we used the averaged perceptron algorithm of freund and schapire 1999 a variation that has been shown to be more effective than the standard algorithm collins 2002 ,1,0,0
in addition the perceptron algorithm and its variants eg the voted or averaged perceptron is becoming increasingly popular due to their competitive performance simplicity in implementation and low computational cost in training eg collins 2002 ,1,0,0
liang 2005 uses the discriminative perceptron algorithm collins 2002 to score whole character tag sequences finding the best candidate by the global score ,0,1,0
collins 2002 proposed the perceptron as an alternative to the crf method for hmmstyle taggers ,0,1,0
given an input sentence x the correct output segmentation fx satisfies fx argmax ygenx scorey where genx denotes the set of possible segmentations for an input sentence x consistent with notation from collins 2002 ,0,1,0
the term global feature vector is used by collins 2002 to distinguish between feature count vectors for whole sequences and the local feature vectors in me tagging models which are boolean valued vectors containing the indicator features for one element in the sequence ,0,1,0
denote the global feature vector for segmented sentence y with y rd where d is the total number of features in the model then scorey is computed by the dot product of vector y and a parameter vector rd where i is the weight for the ith feature scorey y 841 inputs training examples xiyi initialization set 0 algorithm for t 1t i 1n calculate zi argmaxygenxi y if zi negationslash yi yizi outputs figure 1 the perceptron learning algorithm adapted from collins 2002 the perceptron training algorithm is used to determine the weight values ,0,1,0
despite the above differences since the theorems of convergence and their proof collins 2002 are only dependent on the feature vectors and not on the source of the feature definitions the perceptron algorithm is applicable to the training of our cws model ,0,1,0
4 evaluation the purpose of our evaluation is to contrast our proposed feature based approach with a stateoftheart sequential learning technique collins 2002 ,1,0,0
for a sequential learning algorithm we make use of the collins perceptron learner collins 2002 ,0,1,0
in this work we use the averaged perceptron algorithm collins 2002 since it is an online algorithm much simpler and orders of magnitude faster than boosting and maxent methods ,1,0,0
to facilitate comparisons with previous work mcdonald et al 2005b mcdonald and pereira 2006 we used the trainingdevelopmenttest partition defined in the corpus and we also used the automaticallyassigned part of speech tags provided in the corpus10 czech word clusters were derived from the raw text section of the pdt 10 which contains about 39 million words of newswire text11 we trained the parsers using the averaged perceptron freund and schapire 1999 collins 2002 which represents a balance between strong performance and fast training times ,1,0,0
the training is performed by a single generalized perceptron collins 2002 ,0,1,0
1 word w 2 word bigram w1w2 3 singlecharacter word w 4 a word of length l with starting character c 5 a word of length l with ending character c 6 spaceseparated characters c1 and c2 7 character bigram c1c2 in any word 8 the first last characters c1 c2 of any word 9 word w immediately before character c 10 character c immediately before word w 11 the starting characters c1 and c2 of two consecutive words 12 the ending characters c1 and c2 of two consecutive words 13 a word of length l with previous word w 14 a word of length l with next word w table 1 feature templates for the baseline segmentor 2 the baseline system we built a twostage baseline system using the perceptron segmentation model from our previous work zhang and clark 2007 and the perceptron pos tagging model from collins 2002 ,0,1,0
the features used by the pos tagger some of which are different to those from collins 2002 and are specific to chinese are shown in table 2 ,0,1,0
each element in vectorw gives a weight to its corresponding element in y which is the count of a particular feature over the whole sentence y we calculate the vectorw value by supervised learning using the averaged perceptron algorithm collins 2002 given in figure 1 ,0,1,0
another widely used discriminative method is the perceptron algorithm collins 2002 which achieves comparable performance to crfs with much faster training so we base this work on the perceptron ,1,0,0
the perceptron has been used in many nlp tasks such as pos tagging collins 2002 chinese word segmentation ng and low 2004 zhang and clark 2007 and so on ,0,1,0
32 training algorithm we adopt the perceptron training algorithm of collins 2002 to learn a discriminative model mapping from inputs xx to outputs yy where x is the set of sentences in the training corpus and y is the set of corresponding labelled results ,0,1,0
899 to alleviate overfitting on the training examples we use the refinement strategy called averaged parameters collins 2002 to the algorithm in algorithm 1 ,1,0,0
perceptron learning a discriminative structure prediction model with a perceptron update was first proposed by collins 2002 ,0,1,0
we view l2p as a tagging task that can be performed with a discriminative learning method such as the perceptron hmm collins 2002 ,0,1,0
this algorithm and its many variants are widely used in the computational linguistics community collins 2002a collins and duffy 2002 collins 2002b collins and roark 2004 henderson and titov 2005 viola and narasimhan 2005 cohen et al 2004 carreras et al 2005 shen and joshi 2005 ciaramita and johnson 2003 ,0,1,0
following collins 2002 we used sections 018 of the wall street journal wsj corpus for training sections 1921 for development and sections 2224 for final evaluation ,0,1,0
linear weights are assigned to each of the transducers features using an averaged perceptron for structure prediction collins 2002 ,0,1,0
as reported in collins 2002 mcdonald et al 2005 parameter averaging can effectively avoid overfitting ,0,1,0
several classification models can be adopted here however we choose the averaged perceptron algorithm collins 2002 because of its simplicity and high accuracy ,1,0,0
it is an online training algorithm and has been successfully used in many nlp tasks such as pos tagging collins 2002 parsing collins and roark 2004 chinese word segmentation zhang and clark 2007 jiang et al 2008 and so on ,0,1,0
on the hansards data the simple averaging technique described by collins 2002 yields a reasonable model ,1,0,0
33 perceptron learning of feature weights as we saw above our model is a linear model with the global weight vector w acting as the coefficient vector and hence various existing techniques can be exploited to optimize w in this paper we use the averaged perceptron learning collins 2002 freund and schapire 1999 to optimize w on a training corpus so that the system assigns the highest score to the correct coordination tree among all possible trees for each training sentence ,0,1,0
learning we model the problem of selecting the best derivation as a structured prediction problem johnson et al 1999 lafferty et al 2001 collins 2002 taskar et al 2004 ,0,1,0
the averaged version of the perceptron collins 2002 like the voted perceptron freund and schapire 1999 reduces the effect of overtraining ,1,0,0
the classifier consists of two components based on the averaged multiclass perceptron collins 2002 crammer and singer 2003 ,0,1,0
the learning algorithm follows the global strategy introduced in collins 2002 and adapted in carreras and marquez 2004b for partial parsing tasks ,0,1,0
the algorithm is essentially the same as the one introduced in collins 2002 ,0,1,0
12 as such we resort to an approximation voted perceptron training collins 2002 ,0,1,0
to set the weight vector w we train twenty averaged perceptrons collins 2002 on different shuffles of data drawn from sections 0221 of the penn treebank ,0,1,0
to train the model we use the averaged perceptron algorithm described by collins 2002 ,0,1,0
this combination of the perceptron algorithm with beamsearch is similar to that described by collins and roark 20045 the perceptron algorithm is a convenient choice because it converges quickly usually taking only a few iterations over the training set collins 2002 collins and roark 2004 ,1,0,0
the limitations of the generative approach to sequence tagging i e hidden markov models have been overcome by discriminative approaches proposed in recent years mccallum et al 2000 lafferty et al 2001 collins 2002 altun et al 2003 ,1,0,0
in this paper we apply perceptron trained hmms originally proposed in collins 2002 ,0,1,0
we use the perceptron algorithm for sequence tagging collins 2002 ,0,1,0
to this extent we cast the supersense tagging problem as a sequence labeling task and train a discriminative hidden markov model hmm based on that of collins 2002 on the manually annotated semcor corpus miller et al 1993 ,0,1,0
collins 2002 showed how to use the voted perceptron algorithm for learning w and we use it for learning the global transliteration model ,0,1,0
using a variant of the voted perceptron collins 2002 collins and roark 2004 crammer and singer 2003 we discriminatively trained our parser in an online fashion ,0,1,0
3 online learning again following mcdonald et al 2005 we have used the single best mira crammer and singer 2003 which is a variant of the voted perceptron collins 2002 collins and roark 2004 for structured prediction ,0,1,0
collins 2002 introduced the averaged perceptron as a way of reducing overfitting and it has been shown to perform better than the nonaveraged version on a number of tasks ,1,0,0
in the tagging domain collins 2002 compared loglinear and perceptron training for hmmstyle tagging based on dynamic programming ,0,1,0
we chose the perceptron for the training algorithm because it has shown good performance on other nlp tasks in particular collins 2002 reported good performance for a perceptron tagger compared to a maximum entropy tagger ,1,0,0
like collins 2002 the decoder is the same for both the perceptron and the loglinear parsing models the only change is the method for setting the weights ,0,1,0
3 perceptron training the parsing problem is to find a mapping from a set of sentences x x to a set of parses y y we assume that the mapping f is represented through a feature vector xy rd and a parameter vector rd in the following way collins 2002 fx argmax ygenx xy 1 where genx denotes the set of possible parses for sentence x and xy summationtexti iixy is the inner product ,0,1,0
1 introduction a recent development in datadriven parsing is the use of discriminative training methods riezler et al 2002 taskar et al 2004 collins and roark 2004 turian and melamed 2006 ,0,1,0
one popular approach is to use a loglinear parsing model and maximise the conditional likelihood function johnson et al 1999 riezler et al 2002 clark and curran 2004b malouf and van noord 2004 miyao and tsujii 2005 ,0,1,0
we use the adaptation of this algorithm to structure prediction first proposed by collins 2002 ,0,1,0
these include the perceptron collins 2002 and its largemargin variants crammer and singer 2003 mcdonald et al 2005a ,0,1,0
to regularize the model we take as the final model the average of all weight vectors posited during training collins 2002 ,0,1,0
we delete all links in the set a an the df gi from ainitial as a preprocessing step7 24 perceptron training we set the feature weights using a modified version of averaged perceptron learning with structured outputs collins 2002 ,0,1,0
1 introduction in global linear models glms for structured prediction eg johnson et al 1999 lafferty et al 2001 collins 2002 altun et al 2003 taskar et al 2004 the optimal label y for an input x is y arg max yyx w fxy 1 where yx is the set of possible labels for the input x fxy rd is a feature vector that represents the pair xy and w is a parameter vector ,0,1,0
22 table 5 comparison with previous best results top pos tagging bottom text chunking pos tagging f1 perceptron collins 2002 9711 dep ,0,1,0
we split the treebank into training sections 018 development sections 1921 and test sections 2224 as in collins 2002 ,0,1,0
therefore other machine learning techniques such as perceptron collins 2002 could also be applied for this problem ,0,1,0
it combines online peceptron learning collins 2002 with a parsing model based on the eisner algorithm eisner 1996 extended so as to jointly assign syntactic and semantic labels ,0,1,0
our proposal is a first order linear model that relies on an online averaged perceptron for learning collins 2002 and an extended eisner algorithm for the joint parsing inference ,0,1,0
later taggers have managed to improve brills figures a little bit to just above 97 on the wall street journal corpus using hidden markov models hmm and conditional random fields crf eg collins 2002 and toutanova et al ,0,1,0
the learning algorithm used for each stage of the classification task is a regularized variant of the structured perceptron collins 2002 ,0,1,0
ner is typically viewed as a sequential prediction problem the typical models include hmm rabiner 1989 crf lafferty et al 2001 and sequential application of perceptron or winnow collins 2002 ,0,1,0
the parser is coupled with an online averaged perceptron collins 2002 as the learning method ,0,1,0
1999 pedersen 2001 yarowsky and florian 2002 as well as maximum entropy models eg dang and palmer 2002 klein and manning 2002 in particular have shown a large degree of success for wsd and have established challenging stateoftheart benchmarks ,1,0,0
the second model is a maximum entropy model jaynes 1978 since klein and manning klein and manning 2002 found that this model yielded higher accuracy than nave bayes in a subsequent comparison of wsd performance ,0,1,0
previous authors have used numerous hmmbased models banko and moore 2004 collins 2002 lee et al 2000 thede and harper 1999 and other types of networks including maximum entropy models ratnaparkhi 1996 conditional markov models klein and manning 2002 mccallum et al 2000 conditional random elds crf lafferty et al 2001 and cyclic dependency networks toutanova et al 2003 ,0,1,0
however klein and manning 2002 showed that for natural language and text processing tasks conditional models are usually better than joint likelihood models ,0,1,0
1994 and magerman 1995 can suffer from very similar problems to the label bias or observation bias problem observed in tagging models as described in lafferty mccallum and pereira 2001 and klein and manning 2002 ,0,1,0
that some model structures work better than others at real nlp tasks was discussed by johnson 2001 and klein and manning 2002 ,0,1,0
in the context of partofspeech tagging klein and manning 2002 argue for the same distinctions made here between discriminative models and discriminative training criteria and come to the same conclusions ,0,1,0
while both johnson 2001 and klein and manning 2002 propose models which use the parameters of the generative model but train to optimize a discriminative criteria neither proposes training algorithms which are computationally tractable enough to be used for broad coverage parsing ,0,0,1
in this form the distinction between our two models is sometimes referred to as joint versus conditional johnson 2001 klein and manning 2002 rather than generative versus discriminative ng and jordan 2002 ,0,1,0
klein and manning 2002 argue that these results show a pattern where discriminative probability models are inferior to generative probability models but that improvements can be achieved by keeping a generative probability model and training according to a discriminative optimization criteria ,0,1,0
1999 pedersen 2001 yarowsky and florian 2002 as well as maximum entropy models eg dang and palmer 2002 klein and manning 2002 ,0,1,0
however the maximum entropy jaynes 1978 was found to yield higher accuracy than nave bayes in a subsequent comparison by klein and manning 2002 who used a different subset of either senseval1 or senseval2 english lexical sample data ,0,1,0
the models in the comparative study by klein and manning 2002 did not include such features and so again for consistency of comparison we experimentally verified that our maximum entropy model a consistently yielded higher scores than when the features were not used and b consistently yielded higher scores than nave bayes using the same features in agreement with klein and manning 2002 ,0,1,0
klein and manning 2002 argue for cl on grounds of accuracy but see also johnson 2001 ,0,1,0
the second voting model is a maximum entropy model jaynes 1978 since klein and manning 2002 found that this model yielded higher accuracy than naive bayes in a subsequent comparison of wsd performance ,0,1,0
generative and discriminative models have been comparedanddiscussedagreatdealngandjordan 2002 including for nlp models johnson 2001 klein and manning 2002 ,0,1,0
a22 a14 is the sufficient statistic of a16 a14 then we can rewrite a2a24a3 a10a27 a42a7 a25 as a5a7a6a9a8a11a10 a23 a3 a10 a7 a15 a27 a25a18a17a26a25 a12a28a27 a5a7a6a29a8a30a10 a23 a3 a10 a7 a15 a27 a25a18a17 3 loss functions for label sequences given the theoretical advantages of discriminative models over generative models and the empirical support by klein and manning 2002 and that crfs are the stateoftheart among discriminative models for label sequences we chose crfs as our model and trained by optimizing various objective functions a31 a3 a10a36 a25 with respect to the corpus a36 the application of these models to the label sequence problems vary widely ,0,1,0
discriminative models do not only have theoretical advantages over generative models as we discuss in section 2 but they are also shown to be empirically favorable over generative models when features and objective functions are fixed klein and manning 2002 ,0,1,0
the second voting model a maximum entropy model jaynes 1978 was built as klein and manning 2002 found that it yielded higher accuracy than nave bayes in a subsequent comparison of wsd performance ,0,1,0
4 comparison to related work previous work has compared generative and discriminative models having the same structure such as the naive bayes and logistic regression models ng and jordan 2002 klein and manning 2002 and other models klein and manning 2002 johnson 2001 ,0,1,0
the superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure klein and manning 2002 ,0,1,0
it is believed that improvement can be achieved by training the generative model based on a discriminative optimization criteria klein and manning 2002 in which the training procedure is designed to maximize the conditional probability of the parses given the sentences in the training corpus ,0,1,0
2 previous work on sentiment analysis some prior studies on sentiment analysis focused on the documentlevel classification of sentiment turney 2002 pang et al 2002 where a document is assumed to have only a single sentiment thus these studies are not applicable to our goal ,0,1,0
recent computational work either focuses on sentence subjectivity wiebe et al 2002 riloff et al 2003 concentrates just on explicit statements of evaluation such as of films turney 2002 pang et al 2002 or focuses on just one aspect of opinion eg hatzivassiloglou and mckeown 1997 on adjectives ,0,1,0
one major focus is sentiment classification and opinion mining eg pang et al 2002 turney 2002 hu and liu 2004 wilson et al 2004 kim and hovy 2004 popescu and etzioni 2005 2008 ,0,1,0
sentiment classification at the document level investigates ways to classify each evaluative document eg product review as positive or negative pang et al 2002 turney 2002 ,0,1,0
2 related work there has been extensive research in opinion mining at the document level for example on product and movie reviews pang et al 2002 pang and lee 2004 dave et al 2003 popescu and etzioni 2005 ,0,1,0
80 80 positive child education positive cost negative subject increase figure 3 an example of a wordpolarity lattice various methods have already been proposed for sentiment polarity classification ranging from the use of cooccurrence with typical positive and negative words turney 2002 to bag of words pang et al 2002 and dependency structure kudo and matsumoto 2004 ,0,1,0
2 related work recently several studies have reported about dialog systems that are capable of classifying emotions in a humancomputer dialog batliner et al 2004 ang et al 2002 litman and forbesriley 2004 rotaru et al 2005 ,0,1,0
1 introduction in the past few years there has been an increasing interest in mining opinions from product reviews pang et al 2002 liu et al 2004 popescu and etzioni 2005 ,0,1,0
pang et al 2002 considered the same problem and presented a set of supervised machine learning approaches to it ,0,1,0
most of researchers focus on how to extract useful textual features lexical syntactic punctuation etc for determining the semantic orientation of the sentences using machine learning algorithm bo et al 2002 kim and hovy 2004 bo et al 2005 hu et al 2004 alina et al 2008 alistair et al 2006 ,0,1,0
2002 various classification models and linguistic features have been proposed to improve the classification performance pang and lee 2004 mullen and collier 2004 wilson et al 2005a read 2005 ,0,1,0
one of the main directions is sentiment classification which classifies the whole opinion document eg a product review as positive or negative eg pang et al 2002 turney 2002 dave et al 2003 ng et al 2006 mcdonald et al 2007 ,0,1,0
amount of works have been done on sentimental classification in different levels zhang et al 2009 somasundaran et al 2008 pang et al 2002 dave et al 2003 kim and hovy 2004 takamura et al 2005 ,0,1,0
such a lexicon can be used eg to classify individual sentences or phrases as subjective or not and as bearing positive or negative sentiments pang et al 2002 kim and hovy 2004 wilson et al 2005a ,0,1,0
other systems morinaga et al 2002 kushal et al 2003 also look at web product reviews but they do not extract 345 opinions about particular product features ,0,1,0
subjective phrases are used by turney 2002 pang and vaithyanathan 2002 kushal et al 2003 kim and hovy 2004 and others in order to classify reviews or sentences as positive or negative ,0,1,0
partofspeech features based on the lexical categories produced by gate cunningham et al 2002 each token xi is classified into one of a set of coarse partofspeech tags noun verb adverb whword determiner punctuation etc we do the same for neighboring words in a 2 2 window in order to assist noun phrase segmentation ,0,1,0
2 related work supervised machine learning methods including support vector machines svm are often used in sentiment analysis and shown to be very promising pang et al 2002 matsumoto et al 2005 kudo and matsumoto 2004 mullen and collier 2004 gamon 2004 ,1,0,0
one of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated matsumoto et al 2005 kudo and matsumoto 2004 pang et al 2002 ,0,1,0
turney 2002 pang et al 2002 dave at al 2003 ,0,1,0
2 relatedwork 21 sentiment classification most previous work on the problem of categorizing opinionated texts has focused on the binary classification of positive and negative sentiment turney 2002 pang et al 2002 dave at al 2003 ,0,1,0
svm has been shown to be useful for text classification tasks joachims 1998 and has previously given good performance in sentiment classification experiments kennedy and inkpen 2006 mullen and collier 2004 pang and lee 2004 pang et al 2002 ,1,0,0
unigram models have been previously shown to give good results in sentiment classification tasks kennedy and inkpen 2006 pang et al 2002 unigram representations can capture a variety of lexical combinations and distributions including those of emotion words ,1,0,0
automatic subjectivity analysis would also be useful to perform flame recognition spertus 1997 kaufer 2000 email classification aone ramossantacruze and niehaus 2000 intellectual attribution in text teufel and moens 2000 recognition of speaker role in radio broadcasts barzialy et al 2000 review mining terveen et al 1997 review classification turney 2002 pang lee and vaithyanathan 2002 style in generation hovy 1987 and clustering documents by ideological point of view sack 1995 ,0,1,0
workshop towards genreenabled search enginesbooktitle pages1320pages editorin g rehm and m santini editorseditor contexts contextork on an intradocument or page segment level because a single document can contain instances of multiple genres eg contact information list of publications cv see rehm 2002 rehm 2007 mehler et al 2007 ,0,1,0
hicss35booktitle contexts contextdocuments genres also work on an intradocument or page segment level because a single document can contain instances of multiple genres eg contact information list of publications cv see rehm 2002 rehm 2007 mehler et al 2007 ,0,1,0
so far research in automatic opinion recognition has primarily addressed learning subjective language wiebe et al 2004 riloff et al 2003 riloff and wiebe 2003 identifying opinionated documents yu and hatzivassiloglou 2003 and sentences yu and hatzivassiloglou 2003 riloff et al 2003 riloff and wiebe 2003 and discriminating between positive and negative language yu and hatzivassiloglou 2003 turney and littman 2003 pang et al 2002 dave et al 2003 nasukawa and yi 2003 morinaga et al 2002 ,0,1,0
1 introduction previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text pang et al 2002 turney 2002 yu and hatzivassiloglou 2003 ,0,1,0
sentiment analysis includes a variety of different problems including sentiment classification techniques to classify reviews as positive or negative based on bag of words pang et al 2002 or positive and negative words turney 2002 mullen and collier 2004 classifying sentences in a document as either subjective or objective riloff and wiebe 2003 pang and lee 2004 identifying or classifying appraisal targets nigam and hurst 2004 identifying the source of an opinion in a text choi et al 2005 whether the author is expressing the opinion or whether he is attributing the opinion to someone else and developing interactive and visual opinion mining methods gamon et al 2005 popescu and etzioni 2005 ,0,1,0
automatic identification of subjective content often relies on word indicators such as unigrams pang et al 2002 or predetermined sentiment lexica wilson et al 2005 ,0,1,0
2 related work there has been a large and diverse body of research in opinion mining with most research at the text pang et al 2002 pang and lee 2004 popescu and etzioni 2005 ounis et al 2006 sentence kim and hovy 2005 kudo and matsumoto 2004 riloff et al 2003 yu and hatzivassiloglou 2003 or word hatzivassiloglou and mckeown 1997 turney and littman 2003 kim and hovy 2004 takamura et al 2005 andreevskaia and bergler 2006 kaji and kitsuregawa 2007 level ,0,1,0
wst summationdisplay usvt wuv globally optimal minimum cuts can be found in polynomial time and nearlinear running time in practice using the maximum flow algorithm pang and lee 2004 cormen et al 2002 ,0,1,0
many researchers have focused the related problem of predicting sentiment and opinion in text pang et al 2002 wiebe and riloff 2005 sometimes connected to extrinsic values like prediction markets lerman et al 2008 ,0,1,0
applications have included the categorization of documents by topic joachims 1998 language cavnar and trenkle 1994 genre karlgren and cutting 1994 author bosch and smith 1998 sentiment pang et al 2002 and desirability sahami et al 1998 ,0,1,0
the research of opinion mining began in 1997 the early research results mainly focused on the polarity of opinion words hatzivassiloglou et al 1997 and treated the textlevel opinion mining as a classification of either positive or negative on the number of positive or negative opinion words in one text turney et al 2003 pang et al 2002 zagibalov et al 2008 ,0,1,0
examples of such early work include turney 2002 pang et al 2002 dave et al 2003 hu and liu 2004 popescu and etzioni 2005 ,0,1,0
1 introduction in the community of sentiment analysis turney 2002 pang et al 2002 tang et al 2009 transferring a sentiment classifier from one source domain to another target domain is still far from a trivial work because sentiment expression often behaves with strong domainspecific nature ,0,1,0
although such approaches have been employed effectively pang et al 2002 there appears to remain considerable room for improvement ,0,0,1
there are studies on learning subjective language wiebe et al 2004 identifying opinionated documents yu and hatzivassiloglou 2003 and sentences riloff et al 2003 riloff and wiebe 2003 and discriminating between positive and negative language turney and littman 2003 pang et al 2002 dave et al 2003 nasukawa and yi 2003 morinaga et al 2002 ,0,1,0
so far this approach has been taken by a lot of researchers pang et al 2002 dave et al 2003 wilson et al 2005 ,0,1,0
negation was processed in a similar way as previous works pang et al 2002 ,0,1,0
consider the following example pang et al 2002 this film should be brilliant ,0,1,0
document level sentiment classification is mostly applied to reviews where systems assign a positive or negative sentiment for a whole review document pang et al 2002 turney 2002 ,0,1,0
a richer set of features besides ngrams should be checked and we should not ignore the potential effectiveness of unigrams in this task pang et al 2002 ,0,1,0
work focusses on analysing subjective features of text or speech such as sentiment opinion emotion or point of view pang et al 2002 turney 2002 dave et al 2003 liu et al 2003 pang and lee 2005 shanahan et al 2005 ,0,1,0
finally other approaches rely on reviews with numeric ratings from websites pang and lee 2002 dave et al 2003 pang and lee 2004 cui et al 2006 and train semisupervised learning algorithms to classify reviews as positive or negative or in more finegrained scales pang and lee 2005 wilson et al 2006 ,0,1,0
previous workonsentimentanalysishascoveredawiderange of tasks including polarity classification pang et al 2002 turney 2002 opinion extraction pang and lee 2004 and opinion source assignment choi et al 2005 choi et al 2006 ,0,1,0
furthermore these systems have tackled the problem at different levels of granularity from the document level pang et al 2002 sentence level pang and lee 2004 mao and lebanon 2006 phrase level turney 2002 choi et al 2005 as well as the speaker level in debates thomas et al 2006 ,0,1,0
after this conversion we had 1000 positive and 1000 negative examples for each domain the same balanced composition as the polarity dataset pang et al 2002 ,0,1,0
1 introduction sentiment detection and classification has received considerable attention recently pang et al 2002 turney 2002 goldberg and zhu 2004 ,0,1,0
2 motivation automatic subjectivity analysis methods have been used in a wide variety of text processing applications such as tracking sentiment timelines in online forums and news lloyd et al 2005 balog et al 2006 review classification turney 2002 pang et al 2002 mining opinions from product reviews hu and liu 2004 automatic expressive texttospeech synthesis alm et al 2005 text semantic analysis wiebe and mihalcea 2006 esuli and sebastiani 2006 and question answering yu and hatzivassiloglou 2003 ,0,1,0
researchers extracted opinions from words sentences and documents and both rulebased and statistical models are investigated wiebe et al 2002 pang et al 2002 ,0,1,0
subjective so far none of the studies in sentiment detection eg wilson et al 2005 pang et al 2002 or opinion extraction eg hu and liu 2004 popescu and etzioni 2005 have specifically looked at the role of superlatives in these areas ,0,1,0
291 31 level of analysis research on sentiment annotation is usually conducted at the text aue and gamon 2005 pang et al 2002 pang and lee 2004 riloff et al 2006 turney 2002 turney and littman 2003 or at the sentence levels gamon and aue 2005 hu and liu 2004 kim and hovy 2005 riloff et al 2006 ,0,1,0
1 introduction sentiment classification is a special task of text categorization that aims to classify documents according to their opinion of or sentiment toward a given subject eg if an opinion is supported or not pang et al 2002 ,0,1,0
experiment implementation we apply svm algorithm to construct our classifiers which has been shown to perform better than many other classification algorithms pang et al 2002 ,0,1,0
2002 various classification models and linguistic features have been proposed to improve the classification performance pang and lee 2004 mullen and collier 2004 wilson et al 2005 read 2005 ,0,1,0
movies reviews this is a popular dataset in sentiment analysis literature pang et al 2002 ,1,0,0
in particular the use of svms in pang et al 2002 initially sparked interest in using machine learning methods for sentiment classi cation ,1,0,0
in their seminal work pang et al 2002 demonstrated that supervised learning signi cantly outperformed a competing body of work where handcrafted dictionaries are used to assign sentiment labels based on relative frequencies of positive and negative terms ,1,0,0
most semiautomated approaches have met with limited success ng et al 2006 and supervised learning models have tended to outperform dictionarybased classi cation schemes pang et al 2002 ,1,0,0
most work in machine learning literature on utilizing labeled features has focused on using them to generate weakly labeled examples that are then used for standard supervised learning schapire et al 2002 propose one such framework for boosting logistic regression wu and srihari 2004 build a modi ed svm and liu et al 2004 use a combination of clustering and em based methods to instantiate similar frameworks ,0,1,0
specifically we explore the statistical term weighting features of the word generation model with support vector machine svm faithfully reproducing previous work as closely as possible pang et al 2002 ,0,1,0
moviereview dataset consists of positive and negative reviews from the internet movie database imdb archive pang et al 2002 ,0,1,0
to closely reproduce the experiment with the best performance carried out in pang et al 2002 using svm we use unigram with the presence feature ,0,1,0
among these methods svm is shown to perform better than other methods yang and pedersen 1997 pang et al 1 httppeoplecsailmitedujrennie20newsgroups 2 httpwwwcscornelledupeoplepabomoviereviewdata 3 httpwwwseasupennedumdredzedatasetssentiment 2002 ,0,1,0
4 evaluation 41 experimental setup for evaluation we use five sentiment classification datasets including the widelyused movie review dataset mov pang et al 2002 as well as four datasets that contain reviews of four different types of product from amazon books boo dvds dvd electronics ele and kitchen appliances kit blitzer et al 2007 ,1,0,0
while the nasa researchers have applied a heuristic method for labeling a report with shapers posse 1httpkddicsuciedudatabases20newsgroups 2of course the fact that sentiment classification requires a deeper understanding of a text also makes it more difficult than topicbased text classification pang et al 2002 ,0,1,0
1 introduction sentiment analysis have been widely conducted in several domains such as movie reviews product reviews news and blog reviews pang et al 2002 turney 2002 ,0,1,0
in most cases supervised learning methods can perform well pang et al 2002 ,0,1,0
some work identifies inflammatory texts eg spertus 1997 or classifies reviews as positive or negative turney 2002 pang et al 2002 ,0,1,0
for example spertus 1997 developed a system to identify inflammatory texts and turney 2002 pang et al 2002 developed methods for classifying reviews as positive or negative ,0,1,0
for example pang et al 2002 collected reviews from a movie database and rated them as positive negative or neutral based on the rating eg number of stars given by the reviewer ,0,1,0
our focus is on the sentence level unlike pang et al 2002 and turney 2002 we employ a significantly larger set of seed words and we explore as indicators of orientation words from syntactic classes other than adjectives nouns verbs and adverbs ,0,1,0
gildea and jurafsky 2002 used a supervised learning method to learn both the identifier of the semantic roles defined in framenet such as theme target goal and the boundaries of the roles baker et al 2003 ,0,1,0
most of the annotation approaches tackling these issues however are aimed at performing classifications at either the document level pang et al 2002 turney 2002 or the sentence or word level wiebe et al 2004 yu and hatzivassiloglou 2003 ,0,1,0
in particular since we treat each individual speech within a debate as a single document we are considering a version of documentlevel sentimentpolarity classification namely automatically distinguishing between positive and negative documents das and chen 2001 pang et al 2002 turney 2002 dave et al 2003 ,0,1,0
32 classifying speech segments in isolation in our experiments we employed the wellknown classifier svmlight to obtain individualdocument classification scores treating y as the positive class and using plain unigrams as features5 following standard practice in sentiment analysis pang et al 2002 the input to svmlight consisted of normalized presenceoffeature rather than frequencyoffeature vectors ,0,1,0
this negation handling is similar to that used in das and chen 2001 pang et al 2002 ,0,1,0
a number of studies have investigated sentiment classification at document level eg pang et al 2002 dave et al 2003 and at sentence level eg hu and liu 2004 kim and hovy 2004 nigam and hurst 2005 however the accuracy is still less than desirable ,0,0,1
lexical cues of differing complexities have been used including single words and ngrams eg mullen and collier 2004 pang et al 2002 turney 2002 yu and hatzivassiloglou 2003 wiebe et al 2004 as well as phrases and lexicosyntactic patterns eg kim and hovy 2004 hu and liu 2004 popescu and etzioni 2005 riloff and wiebe 2003 whitelaw et al 2005 ,0,1,0
so far research in automatic opinion recognition has primarily addressed learning subjective language wiebe et al 2004 riloff et al 2003 identifying opinionated documents yu and hatzivassiloglou 2003 and sentences yu and hatzivassiloglou 2003 riloff et al 2003 and discriminating between positive and negative language pang et al 2002 morinaga et al 2002 yu and hatzivassiloglou 2003 turney and littman 2003 dave et al 2003 nasukawa and yi 2003 popescu and etzioni 2005 wilson et al 2005 ,0,1,0
tomokiyo and hurst 2003 use pointwise kldivergence between multiple language models for scoring both phraseness and informativeness of phrases ,0,1,0
responsiveness differs from other measures of summary content such as see coverage lin and hovy 2002 and pyramid scores nenkova and passonneau 2004 in that it does not compare a peer summary against a set of known human summaries ,0,1,0
1 introduction rouge lin 2004 and its linguisticallymotivated descendent basic elements be hovy et al 2005 evaluate a summary by computing its overlap with a set of model human summaries rouge considers lexical ngrams as the unit for comparing the overlap between summaries while basic elements uses larger units of comparison based on the output of syntactic parsers ,0,1,0
to evaluate the quality of our generated summaries we choose to use the rouge3 lin 2004 evaluation toolkit that has been found to be highly correlated with human judgments ,1,0,0
we evaluate the summaries using the automatic evaluation tool rouge lin 2004 described in section 6 and the rouge value works as the feedback to our learning loop ,0,1,0
some improvements on bow are given by the use of dependency trees and syntactic parse trees hirao et al 2004 punyakanok et al 2004 zhang and lee 2003 but these too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs ,0,1,0
we carried out automatic evaluation of our summaries using rouge lin 2004 toolkit which has been widely adopted by duc for automatic summarization evaluation ,1,0,0
the basic lcs has a problem that it does not differentiate lcses of different spatial relations within their embedding sequences lin 2004 ,0,1,0
given two sentences x and y the wlcs score of x and y can be computed using the similar dynamic programming procedure as stated in lin 2004 ,0,1,0
we computed the lcs and wlcsbased fmeasure following lin 2004 using both the query pool and the sentence pool as in the previous section ,0,1,0
2004 use an information extraction engine to extract linguistic features from documents relevant to the target term ,0,1,0
wlcswd summationtextmi0 fki we then compute the following quantities where is word length and f1 is the inverse of f pwd f1wlcswdfw rwd f1wlcswdfd fwd 12rwdpwdrwd2pwd in effect pwd examines how close the longest common substring is to w and rwd how close it is to d following lin 2004 we use 8 assigninggreaterimportancetorwd ,0,1,0
rouge lin 2004 a recalloriented evaluation package for automatic summarization ,0,1,0
in addition to tfidf scores hulth 2004 uses partofspeech tags and np chunks and complements this with machine learning the latter has been used to good results in similar cases turney 2000 neto et al 2002 ,0,1,0
a pipage approach ageev and sviridenko 2004 has been proposed for mckp but we do not use this algorithm since it requires costly partial enumeration and solutions to many linear relaxation problems ,0,1,0
46 weaklyconstrained algorithms in evaluation with rouge lin 2004 summaries are truncated to a target length k yih et al2007usedastackdecodingwithaslightmodication which allows the last sentence in a summary to be truncated to a target length ,0,1,0
rouge version 155 lin 2004 was used for evaluation2 among others we focus on rouge1 in the discussion of the result because rouge1 has proved to have strong correlation with human annotation lin 2004 lin and hovy 2003 ,1,0,0
we use only the words that are content words nouns verbs or adjectives and not in the stopword list used in rouge lin 2004 ,0,1,0
rouges rouges is an extension of rouge2 defined as follows lin 2004b rougesa59a61a146a31a62a98a147a49a65a68a67 a59a68a101a161a128a104a162 a2 a65a161a163 a157 a134a61a135a93a245a246 a2 a59a61a146a31a62a98a147a49a65a161a163 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a164a147a49a65 a157 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a51a128a104a162 a2 a145 a134a61a135a89a245a246 a2 a59a61a146a31a62a98a147a49a65 11 where a166a168a169a78a170a248a247a250a249 a26 and a171a138a169a90a170a158a247a250a249 a26 are defined as follows a251 a134a61a135a89a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a248a253a85a254a255 a1 a59a61a146a31a62a90a147a49a65 of skip bigram a2a23a147 12 a3 a134a136a135a93a245a246 a2 a59a61a146a31a62a90a147a49a65a68a67 a252a83a253a118a254a255 a1 a59a61a146a31a62a90a147a49a65 of skip bigram a2 a146 13 here function skip2 returns the number of skipbigrams that are common to a141 and a139 rougesu rougesu is an extension of rouges which includes unigrams as a feature defined as follows lin 2004b rougesua59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a117a163 a157 a134a5a4 a59a61a146a31a62a98a147a49a65a71a163 a145 a134a6a4 a59a61a146a31a62a98a147a49a65 a157 a134a5a4 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145 a134a5a4 a59a61a146a31a62a164a147a49a65 14 where a166 a169a8a7 and a171 a169a8a7 are defined as follows a251 a134a5a4 a59a61a146a31a62a98a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 of skip bigrams of unigrams a2 a147 15 a3 a134a5a4 a59a61a146a31a62a90a147a49a65a68a67 a252 a9 a59a61a146a31a62a90a147a49a65 of skip bigrams of unigrams a2 a146 16 here function su returns the number of skipbigrams and unigrams that are common to a141 and a139 rougel rougel is an lcsbased evaluation measure defined as follows lin 2004b rougela59a61a146a31a62a90a147a49a65a68a67 a59a68a101a161a128a49a162 a2 a65a161a163 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a161a163 a145a12a10 a225a90a134 a59a61a146a31a62a98a147a49a65 a157a11a10 a225a90a134 a59a61a146a31a62a90a147a49a65a47a128a49a162 a2 a145a12a10 a225a98a134 a59a61a146a31a62a90a147a49a65 17 where a166a14a13a250a241a132a169 and a171a15a13a250a241a130a169 are defined as follows a157a11a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a91 a16 a75 a77a29a216 lcsa17a244a59a61a156 a88 a62a90a146a21a65 18 a145a18a10 a225a98a134 a59a61a146a31a62a98a147a49a65a68a67 a101 a95 a16 a75a78a77a83a216 lcsa17 a59a61a156a34a88a78a62a98a146a21a65 19 here lcsa19a244a28a78a144a183a114a93a32a93a139a102a36 is the lcs score of the union longest common subsequence between reference sentences a144a25a114 and a139 a115 and a122 are the number of words contained in a141 and a139 respectively ,0,1,0
for rouges and rougesu we use three variations following lin 2004b the maximum skip distances are 4 9 and infinity 7 ,0,1,0
our method is based on the extended string subsequence kernel esk hirao et al 2004b which is a kind of convolution kernel collins and duffy 2001 ,0,1,0
the results of the comparison with rougen lin and hovy 2003 lin 2004a lin 2004b rougesu lin 2004b lin and och 2004 and rougel lin 2004a lin 2004b show that our method correlates more closely with human evaluations and is more robust ,0,1,0
one is the longest common subsequence lcs based approach hori et al 2003 lin 2004a lin 2004b lin and och 2004 ,0,1,0
a0 subsequence s1 s2 a0 subsequence s1 s2 a0 subsequence s1 s2 becoming 1 1 becomingis a1 a2 a1 a2 astronautdream 0 a1 a2 dream 1 1 becomingmy a1a4a3a5a1a4a3 astronautambition 0 a1 a2 spaceman 1 1 spacemandream a1a4a3a5a1 a2 astronautis 0 1 a 1 0 spacemanambition 0 a1 a2 astronautmy 0 a1 ambition 0 1 spacemandream a1 a3 0 cosmonautdream a1 a3 0 1 an 0 1 spacemangreat a1 a2 0 cosmonautdream a1 a3 0 astronaut 0 1 spacemanis 1 1 cosmonautgreat a1 a2 0 cosmonaut 1 0 spacemanmy a1a6a1 cosmonautis 1 0 dream 1 0 adream a1 a7 0 cosmonautmy a1 0great 1 0 aspaceman 1 0 greatdream 1 0 is 1 1 2 acosmonaut 1 0 2 greatdream 1 0 my 1 1 adream a1 a7 0 isdream a1 a2 a1 becomingdream a1a4a8a5a1 a7 agreat a1 a3 0 isambition 0 a1 becomingspaceman a1a6a1 ais a1 0 isdream a1 a2 0 becominga 1 0 amy a1 a2 0 isgreat a1 0 becomingambition 0 a1 a7 andream 0 a1 a3 ismy 1 1 2 becomingan 0 1 anspaceman 0 1 mydream a1 1 becomingastronaut 0 a1 anambition 0 a1 a3 myambition 0 1 becomingcosmonaut a1 0 anastronaut 0 1 mydream a1 0 becomingdream a1a4a8 0 anis 0 a1 mygreat 1 0 becominggreat a1 a7 0 anmy 0 a1 a2 2002 lin and hovy 2003 lin 2004a lin 2004b soricut and brill 2004 ,0,1,0
lin 2004a 2004b and lin and och 2004 proposed an lcsbased automatic evaluation measure called rougel ,0,1,0
therefore lin and och 2004 introduced skipbigram statistics for the evaluation of machine translation ,0,1,0
since the duc 2004 evaluation lin 2004 has concluded that certain rouge metrics correlate better with human judgments than others depending on the summarisation task being evaluated ie single document headline or multidocument summarisation ,0,1,0
in this paper we compare the performance of this system hybridtrim with the topiary system and a number of other baseline gisting systems on a collection of news documents from the duc 2004 corpus duc 2003 ,0,1,0
these linguisticallymotivated trimming rules dorr et al 2003 zajic et al 2004 iteratively remove constituents until a desired sentence compression rate is reached ,0,1,0
it was also included in the duc 2004 evaluation plan where summary quality was automatically judged using a set of ngram word overlap metrics called rouge lin and hovy 2003 ,0,1,0
42 a rouge based approach rouge lin 2004 is the standard automatic evaluation metric in the summarization community ,0,1,0
we have also used rouge evaluation approach lin 2004 which is based on ngram cooccurrences between machine summaries and ideal human summaries ,0,1,0
acm transactions on computerhuman interaction tochi 113rawstring citation citation validtrue authors authorm e pollackauthor authors titleintelligent technology for an aging population the use of ai to assist elders with cognitive impairmenttitle date2005date journalai magazinejournal pages262pages contexts contextrch on developing sds for homecare and telecare applications examples include scheduling appointments over the phone zajicek et al 2004 wolters et al submitted interactive reminder systems pollack 2005 symptom management systems black et al 2005 or environmental control systems clarke et al 2005 ,0,1,0
oxford uk oxford university pressrawstring citation citation validtrue authors authorl a blackauthor authorc mcmeelauthor authorm mctearauthor authorn blackauthor authorr harperauthor authorm lemonauthor authors titleimplementing autonomy in a diabetes management systemtitle date2005date journalj telemed telecarejournal volume11volume contexts contextcare applications examples include scheduling appointments over the phone zajicek et al 2004 wolters et al submitted interactive reminder systems pollack 2005 symptom management systems black et al 2005 or environmental control systems clarke et al 2005 ,0,1,0
32 rouge version 155 of the rouge scoring algorithm lin 2004 is also used for evaluating results ,0,1,0
many methods for calculating the similarity have been proposed niessen et al 2000 akiba et al 2001 papineni et al 2002 nist 2002 leusch et al 2003 turian et al 2003 babych and hartley 2004 lin and och 2004 banerjee and lavie 2005 gimenez et al 2005 ,0,1,0
in our research 23 scores namely bleu papineni et al 2002 with maximum ngram lengths of 1 2 3 and 4 nist nist 2002 with maximum ngram lengths of 1 2 3 4 and 5 gtm turian et al 2003 with exponents of 10 20 and 30 meteor exact banerjee and lavie 2005 wer niessen et al 2000 per leusch et al 2003 and rouge lin 2004 with ngram lengths of 1 2 3 and 4 and 4 variants lcs ssu w12 were used to calculate each similarity s i therefore the value of m in eq ,0,1,0
in recent years many researchers have tried to automatically evaluate the quality of mt and improve the performance of automatic mt evaluations niessen et al 2000 akiba et al 2001 papineni et al 2002 nist 2002 leusch et al 2003 turian et al 2003 babych and hartley 2004 lin and och 2004 banerjee and lavie 2005 gimenez et al 2005 because improving the performance of automatic mt evaluation is expected to enable us to use and improve mt systems efficiently ,0,1,0
2note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent there are several approaches to modeling document content simple word frequencybased methods luhn 1958 nenkova and vanderwende 2005 graphbased approaches radev 2004 wan and yang 2006 as well as more linguistically motivated techniques mckeown et al 1999 leskovec et al 2005 harabagiu et al 2007 ,0,1,0
all topic models utilize gibbs sampling for inference griffiths 2002 blei et al 2004 ,0,1,0
automated evaluation will utilize the standard duc evaluation metric rouge lin 2004 which representsrecallovervariousngramsstatisticsfrom asystemgeneratedsummaryagainstasetofhumangenerated peer summaries5 we compute rouge scores with and without stop words removed from peer and proposed summaries ,0,1,0
official duc scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling lin 2004 ,0,1,0
8this result is presented as 0053 with the official rouge scorer lin 2004 ,0,1,0
612 rouge evaluation table 4 presents rouge scores lin 2004 of each of humangenerated 250word surveys against each other ,0,1,0
our aim is not only to determine the utility of citation texts for survey creation but also to examine the quality distinctions between this form of input and others such as abstracts and full textscomparing the results to humangenerated surveys using both automatic and nuggetbased pyramid evaluation lin and demnerfushman 2006 nenkova and passonneau 2004 lin 2004 ,0,1,0
rougel rougew and rouges have also been applied in automatic evaluation of summarization and achieved very promising results lin 2004 ,1,0,0
this evaluation shows that our widlbased approach to generation is capable of obtaining headlines that compare favorably in both content and fluency with extractive stateoftheart results zajic et al 2004 while it outperforms a previouslyproposed abstractive system by a wide margin zhou and hovy 2003 ,0,1,0
we automatically measure performance by comparing the produced headlines against one reference headline produced by a human using rougea129 lin 2004 ,0,1,0
using spearmans rank correlation coefficient and pearsons rank correlation coefficient lin et al 2003 lin 2004 hirao et al 2005 ,0,1,0
donaway et al 2000 hirao et al 2005 lin et al 2003 lin 2004 hori et al 2003 and manual methods ,0,1,0
we tested several measures such as rouge lin 2004 and the cosine distance ,0,1,0
rougen lin 2004 this measure compares ngrams of two summaries and counts the number of matches ,0,1,0
rougel lin 2004 this measure evaluates summaries by longest common subsequence lcs defined by equation 4 ,0,1,0
in the following rougesn denotes rouges with maximum skip distance n rougesu lin 2004 this measure is an extension of rouges it adds a unigram as a counting unit ,0,1,0
rouge lin 2004 is a set of recallbased criteria that is mainly used for evaluating summarization tasks ,0,1,0
rougel and rouge1 are supposed to be appropriate for the headline gener853 ation task lin 2004 ,0,1,0
for evaluation we use rouge lin 2004 su4 recall metric1 which was among the official automatic evaluation metrics for duc ,0,1,0
the dif1the routinely used tool for automatic evaluation rouge was adopted exactly because it was demonstrated it is highly correlated with the manual duc coverage scores lin and hovy 2003a lin 2004 ,1,0,0
we carried out automatic evaluation of our summaries using rouge lin 2004 toolkit which has been widely adopted by duc for automatic summarization evaluation ,1,0,0
we use rouge lin 2004 to assess summary quality using common ngram counts and longest common subsequence lcs measures ,0,1,0
we report on rouge1 unigrams rouge2 bigrams rouge w12 weighted lcs and rouges skip bigrams as they have been shown to correlate well with human judgments for longer multidocument summaries lin 2004 ,1,0,0
different approaches have been proposed to measure matches using words or more meaningful semantic units for example rouge lin 2004 factoid analysis teufel and halteren 2004 pyramid method nenkova and passonneau 2004 and basic element be hovy et al 2006 ,0,1,0
32 automatic rouge evaluation rougelin 2004measuresthengrammatchbetween system generated summaries and human summaries ,0,1,0
in the news article domain rouge scores have been shown to be generally highly correlated with human evaluation in content match lin 2004 ,1,0,0
with our best performing features we get rouge2 lin 2004 scores of 011 and 00925 on 2007 and 2006 5this threshold was derived experimentally with previous data ,0,1,0
2004 applied to the output of the reranking parser of charniak and johnson 2005 whereas in be in the version presented here dependencies are generated by the minipar parser lin 1995 ,0,1,0
in tac 2008 summarization track all submitted runs were scored with the rouge lin 2004 and basic elements be metrics hovy et al 2005 ,0,1,0
these domains have been commonly used in prior work on summarization weischedel et al 2004 zhou et al 2004 filatova and prager 2005 demnerfushman and lin 2007 biadsy et al 2008 ,0,1,0
we use the publicly available rouge toolkit lin 2004tocomputerecall precision andfscorefor rouge1 ,0,1,0
there has been a sizable amount of research on structure induction ranging fromlinearsegmentationhearst 1994tocontent modeling barzilay and lee 2004 ,0,1,0
as such we quantify success based on rouge lin 2004 scores ,0,1,0
automated evaluation metrics that rate system behaviour based on automatically computable properties have been developed in a number of other fields widely used measures include bleu papineni et al 2002 for machine translation and rouge lin 2004 for summarisation for example ,0,1,0
rouge lin 2004 is an evaluation metric designed to evaluate automatically generated summaries ,0,1,0
the rouge lin 2004 suite of metrics are ngram overlap based metrics that have been shown to highly correlate with human evaluations on content responsiveness ,1,0,0
for a comparison we also include the rouge1 fscores lin 2004 of each system output against the human compressed sentences ,0,1,0
2 automatic annotation schemes using rouge similarity measures rouge recalloriented understudy for gisting evaluation is an automatic tool to determine the quality of a summary using a collection of measures rougen n1234 rougel rougew and rouges which count the number of overlapping units such as ngram wordsequences and wordpairs between the extract and the abstract summaries lin 2004 ,0,1,0
we evaluate the system generated summaries using the automatic evaluation toolkit rouge lin 2004 ,0,1,0
5 related work the methodology which is closest to our framework is orange lin 2004a which evaluates a similarity metric using the average ranks obtained by reference items within a baseline set ,0,1,0
lin 2004b ,0,1,0
additionally automatic evaluation of content coverage using rouge lin 2004 was explored in 2004 ,0,1,0
two metrics have become quite popular in multidocument summarization namely the pyramid method nenkova and passonneau 2004b and rouge lin 2004 ,1,0,0
empirical evaluations using two standard summarization metricsthe pyramid method nenkova and passonneau 2004b and rouge lin 2004show that the best performing system is a crf incorporating both order2 markov dependencies and skipchain dependencies which achieves 913 of human performance in pyramid score and outperforms our bestperforming nonsequential model by 39 ,0,1,0
to find these pairs automatically wetrainedanonsequentialloglinearmodel that achieves a 902 accuracy galley et al 2004 ,0,1,0
most previous work with crfs containing nonlocal dependencies used approximate probabilistic inference techniques including trp sutton and mccallum 2004 and gibbs sampling finkel et al 2005 ,0,1,0
we have implemented them as defined in lin 2004 ,0,1,0
eg bleu papineni et al 2001 for machine translation rouge lin 2004 for summarization ,0,1,0
there are also automatic methods for summary evaluation such as rouge lin 2004 which gives a score based on the similarity in the sequences of words between a humanwritten model summary and the machine summary ,0,1,0
such metrics have been introduced in other fields including paradise walker et al 1997 for spoken dialogue systems bleu papineni et al 2002 for machine translation1 and rouge lin 2004 for summarisation ,0,1,0
in what concerns the evaluation process although rouge lin 2004 is the most common evaluation metric for the automatic evaluation of summarization since our approach might introduce in the summary information that it is not present in the original input source we found that a human evaluation was more adequate to assess the relevance of that additional information ,0,0,1
5 results the model summaries were compared against 24 summaries generated automatically using summa by calculating rouge1 to rouge4 rougel and rougew12 recall metrics lin 2004 ,0,1,0
we use summa saggion and gaizauskas 2005 to generate generic and querybased multidocument summaries and evaluate them using rouge evaluation metrics lin 2004 relative to human generated summaries ,0,1,0
we considered a variety of tools like rouge lin 2004 and meteor lavie and agarwal 2007 but decided they were unsuitable for this task ,0,0,1
finally in order to formally evaluate the method and the different heuristics a largescale evaluation on the biomed corpus is under way based on computing the rouge measures lin 2004 ,0,1,0
the summaries from the above algorithm for the qfmds were evaluated based on rouge metrics lin 2004 ,0,1,0
in particular rouge2 is the recall in bigrams with a set of humanwritten abstractive summaries lin 2004 ,0,1,0
42 building a human performance model we adopt the evaluation approach that a good content selection strategy should perform similarly to humans which is the view taken by existing summarization evaluation schemes such as rouge lin 2004 and the pyramid method nenkova et al 2007 ,0,1,0
this view is supported by lin 2004a who concludes that correlations to human judgments were increased by using multiple references but using single reference summary with enough number of samples was a valid alternative ,0,1,0
interestingly similar conclusions were also reached in the area of machine translation evaluation in their experiments zhang and vogel 2004 show that adding an additional reference translation compensates the effects of removing 1015 of the testing data and state that therefore it seems more cost effective to have more test sentences but fewer reference translations ,0,1,0
all submitted runs were evaluated with the automatic metrics rouge lin 2004b which calculates the proportion of ngrams shared between the candidate summary and the reference summaries and basic elements hovy et al 2005 which compares the candidate to the models in terms of headmodifier pairs ,0,1,0
22 automatic metrics similarly to the pyramid method rouge lin 2004b and basic elements hovy et al 2005 require multiple topics and model summaries to produce optimal results ,0,1,0
our question here is not only what this relation looks like as it was examined on the basis of document understanding conference data in lin 2004a but also how it compares to the reliability of other metrics ,0,1,0
the 45 stochastic word mapping is trained on a frenchenglish parallel corpus containing 700000 sentence pairs and following liu and gildea 2005 we only keep the top 100 most similar words for each english word ,0,1,0
 metrics based on syntactic similarities such as the headword chain metric hwcm liu and gildea 2005 ,0,1,0
secondly we explore the possibility of designing complementary similarity metrics that exploit linguistic information at levels further than lexical inspired in the work by liu and gildea 2005 who introduced a series of metrics based on constituentdependency syntactic matching we have designed three subgroups of syntactic similarity metrics ,0,1,0
this con rms liu and gildea 2005s nding that in sentence level evaluation long ngrams in bleu are not bene cial ,0,1,0
also relevant is previous work that applied machine learning approaches to mt evaluation both with human references corstonoliver et al 2001 kulesza and shieber 2004 albrecht and hwa 2007 liu and gildea 2007 and without gamon et al 2005 ,0,1,0
the hwc metrics compare dependency and constituency trees for both reference and machine translations liu and gildea 2005 ,0,1,0
in addition to adapting the idea of head word chains liu and gildea 2005 we also compared the input sentences argument structures against the treebank for certain syntactic categories ,0,1,0
metrics in the rouge family allow for skip ngrams lin and och 2004a kauchak and barzilay 2006 take paraphrasing into account metrics such as meteor banerjee and lavie 2005 and gtm melamed et al 2003 calculate both recall and precision meteor is also similar to sia liu and gildea 2006 in that word class information is used ,0,1,0
for example liu and gildea 2005 developed the subtree metric stm over constituent parse trees and the headword chain metric hwcm over dependency parse trees ,0,1,0
in comparison we introduce 28 several metrics coefficients reported in albrecht and hwa 2007 including smoothed bleu lin and och 2004 meteor banerjee and lavie 2005 hwcm liu and gildea 2005 and the metric proposed in albrecht and hwa 2007 using the full feature set ,0,1,0
then we compute the same ratio of machine translation sentence to source sentence and take the output of pnorm function as a feature s csrcoflengthtoflenght ptf norm 7 features based on parse score the usual practice to model the wellformedness of a sentence is to employ the ngram language model or compute the syntactic structure similarity liu and gildea 2005 ,0,1,0
2008 to lfg parses and by liu and gildea 2005 to features derived from phrasestructure tress ,0,1,0
but there is also extensive research focused on including linguistic knowledge in metrics owczarzak et al 2006 reeder et al 2001 liu and gildea 2005 amigo et al 2006 mehay and brew 2007 gimenez and marquez 2007 owczarzak et al 2007 popovic and ney 2007 gimenez and marquez 2008b among others ,0,1,0
this finding has been previously reported among others in liu and gildea 2005 ,0,1,0
while liu and gildea 2005 calculate ngram matches on nonlabelled headmodifier sequences derived by headextraction rules from syntactic trees we automatically evaluate the quality of translation by calculating an fscore on labelled dependency structures produced by a lexicalfunctional grammar lfg parser ,0,1,0
these dependencies differ from those used by liu and gildea 2005 in that they are extracted according to the rules of the lfg grammar and they are labelled with a type of grammatical relation that connects the head and the modifier such as subject determiner etc the presence of grammatical relation labels adds another layer of important linguistic information into the comparison and allows us to account for partial matches for example when a lexical item finds itself in a correct relation but with an incorrect partner ,0,1,0
the use of dependencies in mt evaluation has not been extensively researched before one exception here would be liu and gildea 2005 and requires more research to improve it but the method shows potential to become an accurate evaluation metric ,1,0,0
although evaluated on a different test set our method also outperforms the correlation with human scores reported in liu and gildea 2005 ,0,0,1
by contrast liu and gildea 2005 present three metrics that use syntactic and unlabelled dependency information ,0,1,0
our method extending this line of research with the use of labelled lfg dependencies partial matching and nbest parses allows us to considerably outperform liu and gildeas 2005 highest correlations with human judgement they report 0144 for the correlation with human fluency judgement 0202 for the correlation with human overall judgement although it has to be kept in mind that such comparison is only tentative as their correlation is calculated on a different test set ,0,0,1
our method follows and substantially extends the earlier work of liu and gildea 2005 who use syntactic features and unlabelled dependencies to evaluate mt quality outperforming bleu on segmentlevel correlation with human judgement ,0,1,0
24 syntactic similarity we have incorporated with minor modifications some of the syntactic metrics described by liu and gildea 2005 and amigo et al ,0,1,0
similarities are captured from different viewpoints dphwcil this metric corresponds to the hwc metric presented by liu and gildea 2005 ,0,1,0
for instance bleu and rouge lin and och 2004 are based on ngram precisions meteor banerjee and lavie 2005 and stm liu and gildea 2005 use wordclass or structural information kauchak 2006 leverages on paraphrases and ter snover et al 2006 uses editdistances ,0,1,0
we use three different kinds of metrics drstm semantic tree matching a la liu and gildea 2005 but over drs instead of over constituency trees ,0,1,0
three kinds of metrics have been defined 1httpwwwlsiupcedunlpiqmt 2httpsvnaskitusydeduautrac candc drstml semantic tree matching these metrics are similar to the syntactic tree matching metric defined by liu and gildea 2005 in this case applied to drss instead of constituency trees ,0,1,0
for instance we may find metrics which compute similarities over shallow syntactic structuressequences gimenez and marquez 2007 popovic and ney 2007 constituency trees liu and gildea 2005 and dependency trees liu and gildea 2005 amigo et al 2006 mehay and brew 2007 owczarzak et al 2007 ,0,1,0
banerjee and lavie 2005 calculated the scores by matching the unigrams on the surface forms stemmed forms and senses ,0,1,0
in addition to the widely used bleu papineni et al 2002 and nist doddington 2002 scores we also evaluate translation quality with the recently proposed meteor banerjee and lavie 2005 and four editdistance style metrics word error rate wer positionindependent word error rate per tillmann et al 1997 cder which allows block reordering leusch et al 2006 and translation edit rate ter snover et al 2006 ,0,1,0
there exists a variety of different metrics eg word error rate positionindependent word error rate bleu score papineni et al 2002 nist score doddington 2002 meteor banerjee and lavie 2005 gtm turian et al 2003 ,0,1,0
experimental results were only reported for the meteor metric banerjee and lavie 2005 ,0,1,0
table 1 shows theresultsalongwithb andthethreemetricsthat achieved higher correlations than b semantic role overlap gimenez and marquez 2007 paraeval recall zhou et al 2006 and meteor banerjee and lavie 2005 ,0,1,0
1 introduction b papineni et al 2002 was one of the first automatic evaluation metrics for machine translation mt and despite being challenged by a number of alternative metrics melamed et al 2003 banerjee and lavie 2005 snover et al 2006 chan and ng 2008 it remains the standard in the statistical mtliteraturecallisonburchetal2006havesubjected b to a searching criticism with two realworld case studies of significant failures of correlation between b and human adequacyfluency judgmentsbothcasesinvolvecomparisonsbetween statistical mt systems and other translation methods human postediting and a rulebased mt system and they recommend that the use of b be restrictedtocomparisonsbetweenrelatedsystemsor different versions of the same systems ,0,1,0
in none of these cases did we repeat minimumerrorrate training all these systems were trained using maxb the metrics we tested were meteor banerjee and lavie 2005 version 06usingtheexactporterstemmerandwordnet synonmy stages and the optimized parameters 081 083 028 as reported in lavie and agarwal 2007 ,0,1,0
therefore we also carried out evaluations using the nist doddington 2002 meteor banerjee and lavie 2005 wer hunt 1989 per tillmann et al 1997 and ter snover et al 2005 machine translation evaluation techniques ,0,1,0
examples of such methods are the introduction of information weights as in the nist measure or the comparison of stems or synonyms as in meteor banerjee and lavie 2005 ,0,1,0
banerjee and lavie 2005 introduce the meteor metric which also incorporates recall on the unigram level and further provides facilities incorporating stemming and wordnet synonyms as a more flexible match ,0,1,0
the quality of the translation output is mainly evaluated using bleu with nist doddington 2002 and meteor banerjee and lavie 2005 as complementary metrics ,0,1,0
other wellknown metrics are wer nieen et al 2000 nist doddington 2002 gtm melamed et al 2003 rouge lin and och 2004a meteor banerjee and lavie 2005 and ter snover et al 2006 just to name a few ,1,0,0
examples of such knowledge sources include stemming and tfidf weighting babych and hartley 2004 banerjee and lavie 2005 ,0,1,0
in recent years many researchers have tried to automatically evaluate the quality of mt and improve the performance of automatic mt evaluations niessen et al 2000 akiba et al 2001 papineni et al 2002 nist 2002 leusch et al 2003 turian et al 2003 babych and hartley 2004 lin and och 2004 banerjee and lavie 2005 gimenez et al 2005 because improving the performance of automatic mt evaluation is expected to enable us to use and improve mt systems efficiently ,0,1,0
 metrics based on word alignment between mt outputs and the references banerjee and lavie 2005 ,0,1,0
the final smt system performance is evaluated on a uncased test set of 3071 sentences using the bleu papineni et al 2002 nist doddington 2002 and meteor banerjee and lavie 2005 scores ,0,1,0
other metrics assess the impact of alignments externally eg different alignments are tested by comparing the corresponding mt outputs using automated evaluation metrics eg bleu papineni et al 2002 or meteor banerjee and lavie 2005 ,0,1,0
in order to improve sentencelevel evaluation performance several metrics have been proposed including rougew rouges lin and och 2004 and meteor banerjee and lavie 2005 ,0,1,0
meteor uses the porter stemmer and synonymmatching via wordnet to calculate recall and precision more accurately banerjee and lavie 2005 ,0,1,0
it has been argued that meteor correlates better with human judgment due to higher weight on recall than precision banerjee and lavie 2005 ,1,0,0
metrics in the rouge family allow for skip ngrams lin and och 2004a kauchak and barzilay 2006 take paraphrasing into account metrics such as meteor banerjee and lavie 2005 and gtm melamed et al 2003 calculate both recall and precision meteor is also similar to sia liu and gildea 2006 in that word class information is used ,0,1,0
the results show that as compared to bleu several recently proposed metrics such as semanticrole overlap gimenez and marquez 2007 paraevalrecall zhou et al 2006 and meteor banerjee and lavie 2005 achieve higher correlation ,1,0,0
for meteor when used with its originally proposed parameter values of 09 30 05 which the meteor researchers mentioned were based on some early experimental work banerjee and lavie 2005 we obtain an average correlation value of 0915 as shown in the row meteor ,0,1,0
24 meteor given a pair of strings to compare a system translation and a reference translation meteor banerjee and lavie 2005 first creates a word alignment between the two strings ,0,1,0
to address this standard measures like precision and recall could be used as in some previous research banerjee and lavie 2005 melamed et al 2003 ,0,1,0
we measure translation performance by the bleu papineni et al 2002 and meteor banerjee and lavie 2005 scores with multiple translation references ,0,1,0
in comparison we introduce 28 several metrics coefficients reported in albrecht and hwa 2007 including smoothed bleu lin and och 2004 meteor banerjee and lavie 2005 hwcm liu and gildea 2005 and the metric proposed in albrecht and hwa 2007 using the full feature set ,0,1,0
in owczarzak 2008 the method achieves equal or higher correlations with human judgments than meteor banerjee and lavie 2005 one of the bestperformingautomaticmtevaluationmetrics ,0,0,1
banerjee and lavie 2005 and chan and ng 2008 use wordnet and zhou et al ,0,1,0
in a different work banerjee and lavie 2005 argued that the measured reliability of metrics can be due to averaging effects but might not be robust across translations ,0,1,0
an automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 proposed in banerjee and lavie 2005 ,0,1,0
others try to accommodate both syntactic and lexical differences between the candidate translation and the reference like cder leusch et al 2006 which employs a version of edit distance for word substitution and reordering meteor banerjee and lavie 2005 which uses stemming and wordnet synonymy and a linear regression model developed by russolassner et al 2005 which makes use of stemming wordnet synonymy verb class synonymy matching noun phrase heads and proper name matching ,0,1,0
for evaluation we have selected a set of 8 metric variants corresponding to seven different families bleu n 4 papineni et al 2001 nist n 5 lin and hovy 2002 gtm f1measure e 12 melamed et al 2003 1wer nieen et al 2000 1per leusch et al 2003 rouge rouges lin and och 2004 and meteor3 banerjee and lavie 2005 ,0,1,0
comparing the lfgbased evaluation method with other popular metrics bleu nist general text matcher gtm turian et al 2003 translation error rate ter snover et al 20061 and meteor banerjee and lavie 2005 we show that combining dependency representations with paraphrases leads to a more accurate evaluation that correlates better with human judgment ,0,1,0
others try to accommodate both syntactic and lexical differences between the candidate translation and the reference like cder leusch et al 2006 which employs a version of edit distance for word substitution and reordering or meteor banerjee and lavie 2005 which uses stemming and wordnet synonymy ,0,1,0
note that using stems and their synonyms as used in meteor banerjee and lavie 2005 could also be considered for word similarity ,0,1,0
a new automatic metric meteor banerjee and lavie 2005 uses stems and synonyms of the words ,0,1,0
in an experiment on 16800 sentences of chineseenglish newswire text with segmentlevel human evaluation from the linguistic data consortiums ldc multiple translation project we compare the lfgbased evaluation method with other popular metrics like bleu nist general text matcher gtm turian et al 2003 translation error rate ter snover et al 20061 and meteor banerjee and lavie 2005 and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment ,1,0,0
others try to accommodate both syntactic and lexical differences between the candidate translation and the reference like cder leusch et al 2006 which employs a version of edit distance for word substitution and reordering or meteor banerjee and lavie 2005 which uses stemming and wordnet synonymy ,0,1,0
och showed thatsystemperformanceisbestwhenparametersare optimizedusingthesameobjectivefunctionthatwill be used for evaluation bleu papineni et al 2002 remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used eg banerjee and lavie 2005 snover et al 2006 ,0,1,0
2006 propose a new metric that extends ngram matching to include synonyms and paraphrases and lavies meteor metric banerjee and lavie 2005 can be used with additionalknowledgesuchaswordnetinordertosupport inexact lexical matches ,0,1,0
they are meteor banerjee and lavie 2005meteor measures precision and recall of unigrams when comparing a hypothesis translation 142 language pair test set adequacy fluency rank constituent englishgerman europarl 1416 1418 1419 2626 news commentary 1412 1413 1412 2755 germanenglish europarl 1525 1521 1514 2999 news commentary 1626 1620 1601 3084 englishspanish europarl 1000 1003 1064 1001 news commentary 1272 1272 1238 1595 spanishenglish europarl 1174 1175 1224 1898 news commentary 947 949 922 1339 englishfrench europarl 773 772 769 1456 news commentary 729 735 728 1313 frenchenglish europarl 834 833 830 1641 news commentary 1041 1045 1035 2036 englishczech news commentary 2303 2304 2331 3968 czechenglish news commentary 1711 1711 1733 0 totals 17763 17771 17820 27711 table 2 the number of items that were judged for each task during the manual evaluation against a reference ,0,1,0
while these are based on a relatively few number of items and while we have not performed any tests to determine whether the differences in are statistically significant the results 7the czechenglish conditions were excluded since there were so few systems 146 are nevertheless interesting since three metrics have higher correlation than bleu semantic role overlap gimenez and marquez 2007 which makes its debut in the proceedings of this workshop paraeval measuring recall zhou et al 2006 which has a model of allowable variation in translation that uses automatically generated paraphrases callisonburch 2007 meteor banerjee and lavie 2005 which also allows variation by introducing synonyms and by flexibly matches words using stemming ,1,0,0
we might find better suited metrics such as meteor banerjee and lavie 2005 which is oriented towards word selection8 ,0,1,0
previous publications on meteor lavie et al 2004 banerjee and lavie 2005 have described the details underlying the metric and have extensively compared its performance with bleu and several other mt evaluation metrics ,0,1,0
4 optimizing metric parameters the original version of meteor banerjee and lavie 2005 has instantiated values for three parameters in the metric one for controlling the relative weight of precision and recall in computing the fmean score one governing the shape of the penalty as a function of fragmentation and one for the relative weight assigned to the fragmentation penalty ,0,1,0
al 2006 we are interested in applying alternative metrics such a meteor banerjee and lavie 2005 ,0,1,0
it is dubious whether swd is useful regarding recalloriented metrics like meteor banerjee and lavie 2005 since swd removes information in source sentences ,0,1,0
evaluation we evaluate translation output using three automatic evaluation measures bleu papineni et al 2002 nist doddington 2002 and meteor banerjee and lavie 2005 version 065 all measures used were the casesensitive corpuslevel versions ,0,1,0
moses provides bleu kpapineni et al 2001 and nist doddington 2002 but meteor banerjee and lavie 2005 lavie and agarwal 2007 and ter snover et al 2006 can easily be used instead ,0,1,0
previous publications on meteor lavie et al 2004 banerjee and lavie 2005 lavie and agarwal 2007 have described the details underlying the metric and have extensively compared its performance with bleu and several other mt evaluation metrics ,0,1,0
many researchers banerjee and lavie 2005 liu and gildea 2006 have observed consistent gains by using more flexible matching criteria ,0,1,0
furthermore the bleu score performance suggests that our model is not very powerful but some interesting hints can be found in table 3 when we compare our method with a 5gram language model to a stateoftheart system moses koehn and hoang 2007 based on various evaluation metrics including bleu score nist score doddington 2002 meteor banerjee and lavie 2005 ter snover et al 2006 wer and per ,0,1,0
for instance bleu and rouge lin and och 2004 are based on ngram precisions meteor banerjee and lavie 2005 and stm liu and gildea 2005 use wordclass or structural information kauchak 2006 leverages on paraphrases and ter snover et al 2006 uses editdistances ,0,1,0
these were bleu papineni 2001 nist doddington 2002 wer word error rate per positionindependent wer gtm general text matcher and meteor banerjee and lavie 2005 ,0,1,0
109 machine translation evaluation eg banerjee and lavie 2005 lin and och 2004paraphraserecognition eg brockett and dolan 2005 hatzivassiloglou et al 1999 and automatic grading eg leacock2004marn 2004 ,0,1,0
banerjee and lavie 2005 ,0,1,0
however there is little agreement on what types of knowledge are helpful some suggestions concentrate on lexical information eg by the integration of word similarity information as in meteor banerjee and lavie 2005 or maxsim chan and ng 2008 ,0,1,0
33 system evaluation since both the system translations and the reference translations are available for the tuning 43 set we first compare each output to the reference translation using bleu papineni et al 2001 and meteor banerjee and lavie 2005 and a combined scoring scheme provided by the ulc toolkit gimenez and marquez 2008 ,0,1,0
21 alignment sentences from different systems are aligned in pairs using a modified version of the meteor banerjee and lavie 2005 matcher ,0,1,0
in this paper translation quality is evaluated according to 1 the bleu metrics which calculates the geometric mean of ngram precision by the system output with respect to reference translations papineni et al 2002 and 2 the meteor metrics that calculates unigram overlaps between translations banerjee and lavie 2005 ,0,1,0
experiments are presented in table 1 using bleu papineni et al 2001 and meteor5 banerjee and lavie 2005 and we also show the length ratio ratio of hypothesized tokens to reference tokens ,0,1,0
apart from bleu a standard automatic measure meteor banerjee and lavie 2005 was used for evaluation ,0,1,0
52 impact on translation quality as reported in table 3 small increases in meteor banerjee and lavie 2005 bleu papineni et al 2002 and nist scores doddington 2002 suggest that smt output matches the references better after postprocessing or decoding with the suggested lemma translations ,0,1,0
since the lexical translations and dependency paths are typically not labeled in the english corpus a given pair must be counted fractionally according to its posterior probability of satisfying these conditions given models of contextual translation and english parsing3 3similarly jansche 2005 imputes missing trees by using comparable corpora ,0,1,0
the output by each approach will be evaluated using benchmark data sets of bakeoff32 levow 2006 ,0,1,0
4 evaluation the evaluation is conducted with all four corpora from bakeoff3 levow 2006 as summarized in table 1 with corpus size in number of characters ,0,1,0
1 introduction chinese word segmentation cws has been witnessed a prominent progress in the last three bakeoffs sproat and emerson 2003 emerson 2005 levow 2006 ,1,0,0
sighan the special interest group for chinese language processing of the association for computational linguistics conducted three prior word segmentation bakeoffs in 2003 2005 and 2006sproat and emerson 2003 emerson 2005 levow 2006 which established benchmarks for word segmentation and named entity recognition ,0,1,0
taking sighan bakeoff 2006 levow 2006 as an example the recall is lower about 5 than the precision for each submitted system on msra and cityu closed track ,0,1,0
the flow using nonlocal features in twostage architecture 24 results we employ bioe1 label scheme for the ner task because we found it performs better than iob2 on bakeoff 2006 levow 2006 ner msra and cityu corpora ,0,1,0
thus as a powerful sequence tagging model crf became the dominant method in the bakeoff 2006 levow 2006 ,0,1,0
we tested the techniques described above with the previous bakeoffs data5 sproat and emerson 2003 emerson 2005 levow 2006 ,0,1,0
since the word support model and triple context matching model have been proposed in our previous work tsai 2005 2006a and 2006b at the sighan bakeoff 2005 thomas 2005 and 2006 levow 2006 the major descriptions of this paper is on the wbt model ,0,1,0
in this paper we employed the chinese word segmentation tool wu et al 2006 that achieved about 093096 recallprecision rates in the sighan3 word segmentation task levow 2006 ,0,1,0
detail of the bakeoff data sets is in levow 2006 ,0,1,0
2005 kim and hovy 2006 source extraction eg bethard et al ,0,1,0
a notable exception is the work of kim and hovy 2006 ,1,0,0
opendomain opinion extraction is another trend of research on opinion extraction which aims to extract a wider range of opinions from such texts as newspaper articles yu and hatzivassiloglou 2003 kim and hovy 2004 wiebe et al 2005 choi et al 2006 ,0,1,0
kim and hovy 2006 integrated verb information from framenet and incorporated it into semantic role labeling ,0,1,0
other research has been conducted in analysing sentiment at a sentence level using bootstrapping techniques riloff and wiebe 2003 finding strength of opinions wilson wiebe and hwa 2004 summing up orientations of opinion words in a sentence kim and hovy 2004 and identifying opinion holders stoyanov and cardie 2006 ,0,1,0
more details on the different parameter settings and instance selection algorithms as well as trends in the performance of different settings can be found in stoyanov and cardie 2006 ,0,1,0
more details about why heuristics are needed and the process used to map sources to nps can be found in stoyanov and cardie 2006 ,0,1,0
fujii and ishikawa 2006 also work with arguments ,0,1,0
reported and direct speech are certainly important in discourse prasad et al 2006 we do not believe however that they enter discourse relations of the type that rst attempts to capture ,0,1,0
135 considering the discourse relation annotations in the pdtb prasad et al 2006 there can be alignment between discourse relations like contrast and our opinion frames when the frames represent dominant relations between two clauses ,0,1,0
table 2 corpora and modalities corpus modality ace asserted or other timeml must may should would or could prasad et al 2006 assertion belief facts or eventualities saur et al 2007 certain probable possible or other inui et al 2008 affirm infer doubt hear intend ask recommend hypothesize or other this study so necessity hope possible recommend intend table 3 markup scheme tags and definitions tag definition examples r remedy medical operation eg radiotherapy t medical test medical examination eg ct mri d deasese symptom eg endometrial cancer headache m medication administration of a drug eg levofloxacin flexeril a patient action eg admitted to a hospital v other verb eg cancer spread to 2 related works 21 previous markup schemes in the nlp field fact identification has not been studied well to date ,0,1,0
however another approach is to train a separate outofdomain parser and use this to generate additional features on the supervised and unsupervised indomain data blitzer et al 2006 ,0,1,0
2006 and blitzer et al ,0,1,0
in this paper we investigate the effectiveness of structural correspondence learning scl blitzer et al 2006 in the domain adaptation task given by the conll 2007 ,0,1,0
c2007 association for computational linguistics structural correspondence learning for dependency parsing nobuyuki shimizu information technology center university of tokyo tokyo japan shimizurdlitcutokyoacjp hiroshi nakagawa information technology center university of tokyo tokyo japan nakagawadlitcutokyoacjp abstract following blitzer et al 2006 we present an application of structural correspondence learning to nonprojective dependency parsing mcdonald et al 2005 ,0,1,0
3 domain adaptation following blitzer et al 2006 we present an application of structural correspondence learning scl to nonprojective dependency parsing mcdonald et al 2005 ,0,1,0
note that there are some similarities between our twostage semisupervised learning approach and the semisupervised learning method introduced by blitzer et al 2006 which is an extension of the method described by ando and zhang 558 2005 ,0,1,0
blitzer et al 2006 jiang and zhai 2007 daume iii 2007 finkel and manning 2009 or st where no labeled target domain data is available eg ,0,1,0
blitzer et al 2006 jiang and zhai 2007 ,0,1,0
due to the positive results in ando 2006 blitzer et al ,0,1,0
2 motivation and prior work while several authors have looked at the supervised adaptation case there are less and especially less successful studies on semisupervised domain adaptation mcclosky et al 2006 blitzer et al 2006 dredze et al 2007 ,0,0,1
c2009 association for computational linguistics structural correspondence learning for parse disambiguation barbara plank alfainformatica university of groningen the netherlands bplankrugnl abstract the paper presents an application of structural correspondence learning scl blitzer et al 2006 for domain adaptation of a stochastic attributevalue grammar savg ,0,1,0
while scl has been successfully applied to pos tagging and sentiment analysis blitzer et al 2006 blitzer et al 2007 its effectiveness for parsing was rather unexplored ,0,0,1
parse selection constitutes an important part of many parsing systems johnson et al 1999 hara et al 2005 van noord and malouf 2005 mcclosky et al 2006 ,0,1,0
so far scl has been applied successfully in nlp for partofspeech tagging and sentiment analysis blitzer et al 2006 blitzer et al 2007 ,1,0,0
we examine the effectiveness of structural correspondence learning scl blitzer et al 2006 for this task a recently proposed adaptation technique shown to be effective for pos tagging and sentiment analysis ,1,0,0
intuitively if we are able to find good correspondences among features then the augmented labeled source domain data should transfer better to a target domain where no labeled data is available blitzer et al 2006 ,0,1,0
figure 1 scl algorithm blitzer et al 2006 ,0,1,0
applying the projection wtx where x is a training instance would give us m new features however for both computational and statistical reasons blitzer et al 2006 ando and zhang 2005 a lowdimensional approximation of the original feature space is computed by applying singular value decomposition svd on w step 4 ,0,1,0
furthermore i plan to apply my parsers in other domains eg biomedical data blitzer et al 2006 besides treebank data to investigate the effectiveness and generality of my approaches ,0,1,0
for each pivot feature k we use a loss function l k 2 1 wxwxpl i i t ikk 1 where the function p k x i indicates whether the pivot feature k occurs in the instance x i otherwise xif xp ik ik 0 1 1 where the weight vector w encodes the correspondence of the nonpivot features with the pivot feature k blitzer et al 2006 ,0,1,0
for transferlearning baseline we implement traditional scl model tscl blitzer et al 2006 ,0,1,0
among these techniques scl structural correspondence learning blitzer et al 2006 is regarded as a promising method to tackle transferlearning problem ,1,0,0
the pos data set and the cts data set have previously been used for testing other adaptation methods daume iii and marcu 2006 blitzer et al 2006 though the setup there is different from ours ,0,1,0
we augment each labeled target instance xj with the label assigned by the source domain classifier florian et al 2004 blitzer et al 2006 ,0,1,0
as we noted in section 5 we are able to significantly outperform basic structural correspondence learning blitzer et al 2006 ,0,1,0
440 respondence learning scl domain adaptation algorithm blitzer et al 2006 for use in sentiment classification ,0,1,0
then it models the correlations between the pivot features and all other features by training linear pivot predictors to predict occurrences of each pivot in the unlabeled data from both domains ando and zhang 2005 blitzer et al 2006 ,0,1,0
most of this prior work deals with supervised transfer learning and thus requires labeled source domain data though there are examples of unsupervised arnold et al 2007 semisupervised grandvalet and bengio 2005 blitzer et al 2006 and transductive approaches taskar et al 2003 ,0,1,0
various machine learning strategies have been proposed to address this problem including semisupervised learning zhu 2007 domain adaptation wu and dietterich 2004 blitzer et al 2006 blitzer et al 2007 arnold et al 2007 chan and ng 2007 daume 2007 jiang and zhai 2007 reichart and rappoport 2007 andreevskaia and bergler 2008 multitask learning caruana 1997 reichart et al 2008 arnold et al 2008 selftaught learning raina et al 2007 etc a commonality among these methods is that they all require the training data and test data to be in the same feature space ,0,1,0
for our pos tagging experiments we use 561 medline sentences 9576 words from the penn bioie project pennbioie 2005 a test set previously used by blitzer et al2006 ,0,1,0
performance also degrades when the domain of the test set differs from the domain of the training set in part because the test set includes more oov words and words that appear only a few times in the training set henceforth rare words blitzer et al 2006 daume iii and marcu 2006 chelba and acero 2004 ,0,1,0
while transfer learning was proposed more than a decade ago thrun 1996 caruana 1997 its application in natural language processing is still a relatively new territory blitzer et al 2006 daume iii 2007 jiang and zhai 2007a arnold et al 2008 dredze and crammer 2008 and its application in relation extraction is still unexplored ,0,1,0
therefore domain adaptation methods have recently been proposed in several nlp areas eg word sense disambiguation chan and ng 2006 statistical parsing lease and charniak 2005 mcclosky et al 2006 and lexicalizedgrammar parsing johnson and riezler 2000 hara et al 2005 ,0,1,0
5 conclusions and future work the paper compares structural correspondence learning blitzer et al 2006 with various instances of selftraining abney 2007 mcclosky et al 2006 for the adaptation of a parse selection model to wikipedia domains ,0,1,0
we examine structural correspondence learning scl blitzer et al 2006 for this task and compare it to several variants of selftraining abney 2007 mcclosky et al 2006 ,0,1,0
pivots are features occurring frequently and behaving similarly in both domains blitzer et al 2006 ,0,1,0
intuitively if we are able to find good correspondences through linking pivots then the augmented source data should transfer better to a target domain blitzer et al 2006 ,0,1,0
algorithm 1 scl blitzer et al 2006 1 select m pivot features ,0,1,0
scl for discriminative parse selection so far pivot features on the word level were used blitzer et al 2006 blitzer et al 2007 ,0,1,0
6 related work evidence from the surrounding context has been used previously to determine if the current sentence should be subjectiveobjective riloff et al 2003 pang and lee 2004 and adjacency pair information has been used to predict congressional votes thomas et al 2006 ,0,1,0
in thomas et al 2006 the authors use the transcripts of debates from the us congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers ,0,1,0
2 data 21 the us congressional speech corpus the text used in the experiments is from the united states congressional speech corpus monroe et al 2006 which is an xml formatted version of the electronic united states congressional record from the library of congress1 ,0,1,0
the capitalization and punctuation is then removed from the text as in monroe et al 2006 and then the 1httpthomaslocgov 659 text stemmed using porters snowball ii stemmer2 ,0,1,0
others use sentence cohesion pang and lee 2004 agreementdisagreement between speakers thomas et al 2006 bansal et al 2008 or structural adjacency ,0,1,0
furthermore these systems have tackled the problem at different levels of granularity from the document level pang et al 2002 sentence level pang and lee 2004 mao and lebanon 2006 phrase level turney 2002 choi et al 2005 as well as the speaker level in debates thomas et al 2006 ,0,1,0
5 related work evidence from the surrounding context has been used previously to determine if the current sentence should be subjectiveobjective riloff et al 2003 pang and lee 2004 and adjacency pair information has been used to predict congressional votes thomas et al 2006 ,0,1,0
for the muc6 data set we extract noun phrases mentions automatically but for mpqa we assume mentions for coreference resolution are given as in stoyanov and cardie 2006 ,0,1,0
as a followup to the work described in this paper we developed a method that utilizes the unlabeled nps in the corpus using a structured rule learner stoyanov and cardie 2006 ,0,1,0
the latter problem of developing methods that can work with incomplete supervisory information is addressed in a subsequent effort stoyanov and cardie 2006 ,0,1,0
polarity orientation identification has many useful applications including opinion summarization ku et al 2006 and sentiment retrieval eguchi and lavrenko 2006 ,0,1,0
several sentiment information retrieval models were proposed in the framework of probabilistic language models by eguchi and lavrenko 2006 ,0,1,0
discovering orientations of context dependent opinion comparative words is related to identifying domain opinion words hatzivassiloglou and mckeown 1997 kanayama and nasukawa 2006 ,0,1,0
the acquisition of clues is a key technology in these research efforts as seen in learning methods for documentlevel sa hatzivassiloglou and mckeown 1997 turney 2002 and for phraselevel sa wilson et al 2005 kanayama and nasukawa 2006 ,0,1,0
kanayama and nasukawa 2006 reported that it was appropriate in 722 of cases ,0,1,0
typically a small set of seed polar phrases are prepared and new polar phrases are detected based on the strength of cooccurrence with the seeds hatzivassiloglous and mckeown 1997 turney 2002 kanayama and nasukawa 2006 ,0,1,0
kanayama and nasukawa used both intraand intersentential cooccurrence to learn polarity of words and phrases kanayama and nasukawa 2006 ,0,1,0
in kanayamas method the cooccurrence is considered as the appearance in intraor intersentential context kanayama and nasukawa 2006 ,0,1,0
see table 4 in kanayama and nasukawa 2006 for the detail ,0,1,0
much of the work in subjectivity analysis has been applied to english data though work on other languages is growing eg japanese data are used in kobayashi et al 2004 suzuki et al 2006 takamura et al 2006 kanayama and nasukawa 2006 chinese data are used in hu et al 2005 and german data are used in kim and hovy 2006 ,0,1,0
it is possible that there is a better automated method for finding such phrases such as the methods in kanayama and nasukawa 2006 breck choi and cardie 2007 ,0,1,0
these words and phrases are usually compiled using different approaches hatzivassiloglou and mckeown 1997 kaji and kitsuregawa 2006 kanayama and nasukawa 2006 esuli and sebastiani 2006 breck et al 2007 ding liu and yu ,0,1,0
2005 while exploring wordtoexpression interexpression relations has connections to techniques that employ more of a globalview of corpus statistics eg kanayama and nasukawa 20061 while most previousresearch exploits only one or the other type of relation we propose a unified method that can exploit both types of semantic relation while adapting a general purpose polarity lexicon into a domain specific one ,0,1,0
hyperparameter is automatically selected from 2although kanayama and nasukawa 2006 that for their dataset similar to ours was 083 this value cannot be directly compared with our value because their dataset includes both individual words and pairs of words ,0,1,0
in addition to individual seed words kanayama and nasukawa 2006 used more complicated syntactic patterns that were manually created ,0,1,0
while work on subjectivity analysis in other languages is growing eg japanese data are used in takamura et al 2006 kanayama and nasukawa 2006 chinese data are used in hu et al 2005 and german data are used in kim and hovy 2006 much of the work in subjectivity analysis has been applied to english data ,0,1,0
maskey and hirschberg 2005 murray et al 2005a galley 2006 ,0,1,0
galley 2006 used skipchain conditional random fields to model pragmatic dependencies between paired meeting utterances eg questionanswer relations and used a combination of lexical prosodic structural and discourse features to rank utterances by importance ,0,1,0
the skipchain crfs sutton and mccallum 2004 galley 2006 model the long distance dependency between context and answer sentences and the 2d crfs zhu et al 2005 model the dependency between contiguous questions ,0,1,0
skipchain crf model is applied for entity extraction and meeting summarization sutton and mccallum 2006 galley 2006 ,0,1,0
rouge has been used in meeting summarization evaluation murray et al 2005 galley 2006 yet the question remained whether rouge is a good metric for the meeting domain ,0,1,0
for this study we used the same 6 test meetings as in murray et al 2005 galley 2006 ,0,1,0
we used four different system summaries for each of the 6 meetings one based on the mmr method in mead carbonell and goldstein 1998 et al 2003 the other three are the system output from galley 2006 murray et al 2005 xie and liu 2008 ,0,1,0
galley 2006 considered some location constrains in meeting summarization evaluation which utilizes speaker information to some extent ,0,1,0
supervised methods include hidden markov model hmm maximum entropy conditional random fields crf and support vector machines svm galley 2006 buist et al 2005 xie et al 2008 maskey and hirschberg 2006 ,0,1,0
a variety of approaches have been investigated for speech summarization for example maximum entropy conditional random fields latent semantic analysis support vector machines maximum marginal relevance maskey and hirschberg 2003 hori et al 2003 buist et al 2005 galley 2006 murray et al 2005 zhang et al 2007 xie and liu 2008 ,0,1,0
wick et al 2006 report extracting database records by learning record field compatibility ,0,1,0
however much recent work in machine learning and statistics has turned away from maximumlikelihood in favor of bayesian methods and there is increasing interest in bayesian methods in computational linguistics as well finkel et al 2006 ,0,1,0
this algorithm appears fairly widely known it was described by goodman 1998 and finkel et al 2006 and used by ding et al 2005 and is very similar to other dynamic programming algorithms for cfgs so we only summarize it here ,1,0,0
previous work on linguistic annotation pipelines finkel et al 2006 hollingshead and roark 2007 has enforced consistency from one stage to the next ,0,1,0
finkel et al 2006 and in some cases to factor the translation problem so that the baseline mt system can take advantage of the reduction in sparsity by being able to work on word stems ,0,1,0
different methods have been proposed to reduce error propagation between pipelined tasks both in general sutton et al 2004 daume iii and marcu 2005 finkel et al 2006 and for specific problems such as language modeling and utterance classification saraclar and roark 2005 and labeling and chunking shimizu and haas 2006 ,0,1,0
doing joint inference instead of taking a pipeline approach has also been shown useful for other problems eg finkel et al 2006 cohen and smith 2007 ,0,1,0
our use of gibbs sampling follows from its increasing use in bayesian inference problems in nlp finkel et al 2006 johnson et al 2007b ,0,1,0
this model is very similar to smith and eisner 2006 ,0,1,0
our story makes use of a weighted formalism known as quasisynchronous grammar hereafter qg originally developed by d smith and eisner 2006 for machine translation ,0,1,0
2 related work to model the syntactic transformation process researchers in these fieldsespecially in machine translationhave developed powerful grammatical formalisms and statistical models for representing and learning these treetotree relations wu and wong 1998 eisner 2003 gildea 2003 melamed 2004 ding and palmer 2005 quirk et al 2005 galley et al 2006 smith and eisner 2006 inter alia ,1,0,0
we propose smith and eisners 2006 quasisynchronous grammar section 3 as a general solution and the jeopardy model section 4 as a specific instance ,0,1,0
3 quasisynchronous grammar for a formal description of qg we recommend smith and eisner 2006 ,1,0,0
here following smith and eisner 2006 we use a weighted quasisynchronous dependency grammar apart from the obvious difference in application task there are a few important differences with their model ,0,1,0
a similar soft projection of dependencies was used in supervised machine translation by smith and eisner 2006 who used a source sentences dependency paths to bias the generation of its translation ,0,1,0
thus our generative model is a quasisynchronous grammar exactly as in smith and eisner 2006a3 when training on target sentences w therefore we tune the model parameters to maximize notsummationtextt ptw as in ordinary em but rather 3our task here is new they used it for alignment ,0,1,0
bilingual configurations that condition on tprimewprime 2 are incorporated into the generative process as in smith and eisner 2006a ,0,1,0
much previous work on unsupervised grammar induction has used goldstandard partofspeech tags smith and eisner 2006b klein and manning 2004 klein and manning 2002 ,0,1,0
one option would be to leverage unannotated text mcclosky et al 2006 smith and eisner 2007 ,0,1,0
ourmodelisthusa form of quasisynchronous grammar qg smith and eisner 2006a ,0,1,0
2007 explored the use a formalism called quasisynchronous grammar smith and eisner 2006 in order to find a more explicit model for matching the set of dependencies and yet still allow for looseness in the matching ,0,1,0
following smith and eisner 2006 we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be inspired by the structure of s because dependency syntax is still only a crude approximation to semantic structure we augment the model with a lexical semantics component based on wordnet miller 1995 that models how words are probabilistically altered in generating a paraphrase ,0,1,0
the model cleanly incorporates both syntax and lexical semantics using quasisynchronous dependency grammars smith and eisner 2006 ,0,1,0
31 background smith and eisner 2006 introduced the quasisynchronous grammar formalism ,0,1,0
since it loosely links the two sentences syntactic structures qg is well suited for problems like word alignment for mt smith and eisner 2006 and question answering wang et al 2007 ,0,1,0
smith and eisner 2006 used a quasisynchronous grammar to discover the correspondence between words implied by the correspondence between the trees ,0,1,0
these are identical to prior work smith and eisner 2006 wang et al 2007 except that we add a root configuration that aligns the target parentchild pair to null and the head word of the source sentence respectively ,0,1,0
we estimate loss gradients equation 13 using a sample of the inference set which gives a 100fold increase in training speed turian melamed 2006 ,0,1,0
more details about the reranking algorithm are presented in ji et al 2006 ,0,1,0
but there is also extensive research focused on including linguistic knowledge in metrics owczarzak et al 2006 reeder et al 2001 liu and gildea 2005 amigo et al 2006 mehay and brew 2007 gimenez and marquez 2007 owczarzak et al 2007 popovic and ney 2007 gimenez and marquez 2008b among others ,0,1,0
the query tions the syntax semantics and abstract knowledge representation have type declarations crouch and king 2008 which help to detect malformed representations ,0,1,0
the bridge system uses the xle crouch et al 2008 parser to produce syntactic structures and then the xle ordered rewrite system to produce linguistic semantics crouch and king 2006 and abstract knowledge representations ,0,1,0
when we run our classifiers on resourcetight environments such as cellphones we can use a random feature mixing technique ganchev and dredze 2008 or a memoryefficient trie implementation based on a succinct data structure jacobson 1989 delpratt et al 2006 to reduce required memory usage ,1,0,0
1 a please move your car her sadness moves him b john enjoys the book john enjoys reading the book e the two alibis do not accord they accorded him a warm welcome d john swam for hours john swam across the channel although the precise nrechanisms which govern lexical knowledge are still largely unknown there is strong evidence that word sense extensibiity is not arbitrary atkins levin 1991 pustejovsky 1991 1994 ostler atkius 1991 ,0,1,0
53 systematic sense shift ostler and atkins 1991 contend that there is strong evidence to suggest that a large part of word sense ambiguity is not arbitrary but follows regular patterns ,0,1,0
or cooking which agrees with the knowledge presented in previous work ostler and atkins 1991 ,0,1,0
it has been lately incorporated into computational lexicography in atkins 1991 ostler and atkins 1992 briscoe and copestake 1991 copestake and briscoe 1992 briscoe et al 1993 ,0,1,0
within this class would fall the lexical implication rules lirs of ostler and atkins 1991 the lexical rules of copestake and briscoe 1991 the generative lexicon of pustejovsky 1995 and the ellipsis recovery procedures of viegas and nirenburg 1995 ,0,1,0
let us now compare our results to those obtained using shallow parsing as previously done by grefenstette 1993 ,0,1,0
its previous applications eg grefenstette 1993 hearst and schuetze 1993 takunaga et al 1997 lin 1998 caraballo 1999 demonstrated that cooccurrence statistics on a target word is often sufficient for its automatical classification into one of numerous classes such as synsets of wordnet ,0,1,0
it can also be considered as an extension from the monolingual to the bilingual case of the wellestablished methods for semantic or syntactic word clustering as proposed by schtitze 1993 grefenstette 1994 ruge 1995 rapp 1996 lin 1998 and others ,0,1,0
the typical practice of preprocessing distributional data is to remove rare word cooccurrences thus aiming to reduce noise from idiosyncratic word uses and linguistic processing errors and at the same time form more compact word representations eg grefenstette 1993 ciaramita 2002 ,0,1,0
first it recognizes nonrecursive base noun phrase bnp our specifications for bnp resemble those in ramshaw and marcus 1995 ,0,1,0
section 20 majority voting mufioz et al 1999 tjong kim sang and veenstra 1999 ramshaw and marcus 1995 argarnon et al 1998 accuracy precision o9810 c9829 9363 o981 c982 931 9758 9250 9737 9180 916 recall fzi 9289 9326 924 928 9225 9237 9227 9203 916 916 section 00 accuracy precision majority voting 09859 c9865 9504 r tjong kim sang and veenstra 1999 9804 9371 ramshaw and marcus 1995 978 931 recall fbi 9475 9490 9390 9381 935 933 table 3 the results of majority voting of different data representations applied to the two standard data sets put forward by ramshaw and marcus 1995 compared with earlier work ,0,1,0
we have applied it to the two data sets mentioned in ramshaw and marcus 1995 ,0,1,0
ramshaw and marcus 1995 have build a chunker by applying transformationbased learning to sections of the penn treebank ,0,1,0
two basenp data sets have been put forward by ramshaw and marcus 1995 ,0,1,0
the noun phrases in this data set are the same as in the treebank and therefore the basenps in this data set are slightly different from the ones in the ramshaw and marcus 1995 data sets ,0,1,0
an alternative representation for basenps has been put forward by ramshaw and marcus 1995 ,0,1,0
like the data used by ramshaw and marcus 1995 this data was retagged by the brill tagger in order to obtain realistic partofspeech pos tags 3 ,0,1,0
we an tinl 111 sam lylolopy in other works anlshaw rod marcus 1995 ca rdi and pierc 1998 ,0,1,0
levelopment of cor1ora with morlhosyntati and syntacti mmotation marcus et al 1993 sampson 1995 ,0,1,0
amufioz et al 1999 showed that this representation tends to provide better results than the representation used in ramshaw and marcus 1995 where each word is tagged with a tag iinside ooutskte or bbreaker ,0,1,0
l lhmsetsu ideniillcation is a lnoblem similar to ohmking llamshaw and marcus 1995 sang and hellslra 1999 in other lmguages ,0,1,0
this means that the 1roblem of recognizing named entities in those cases can be solved by incorporating techniques of base noun phrase chunking ramshaw and marcus 1995 ,1,0,0
321 insideoutside encoding the insideoutside scheme of encoding chunking states of base noun phrases was studied in ibmlshaw and marcus 1995 ,0,1,0
ramshaw and marcus ramshaw and marcus 1995 successflflly applied eric brills transformationbased learning method to the chunking problem ,1,0,0
like the data used by ramshaw and marcus 1995 this data was retagged by the brill tagger in order to obtain realistic partof speech pos tags 5 ,0,1,0
the data was seglnented into basenp parts and nonlmsenp tarts ill a similar fitshion as the data used 1y ramshaw and marcus 1995 ,0,1,0
1999 916 916 f31 9386 9326 928 9203 916 table 3 the overall pertbrmance of the majority voting combination of our best five systems selected on tinting data perfbrnmnce applied to the standard data set pnt tbrward by ramshaw and marcus 1995 together with an overview of earlier work ,0,1,0
tile data put tbrward by llamshaw and marcus 1995 ,0,1,0
been put forward by ramshaw and marcus 1995 ,0,1,0
an alternative representation for basenps has been put tbrward by ramshaw and marcus 1995 ,0,1,0
he used the ramshaw and marcus 1995 representation as well iob1 ,0,1,0
this approach is also used in basenp chunking ramshaw and marcus 1995 and named entity recognition sekine et al 1998 as well as word segmentation ,0,1,0
the noun phrase chunking np chunking module uses the basic np chunker software from 483 ramshaw and marcus 1995 to recognize the noun phrases in the question ,0,1,0
data set sang buchholz 2000 ramshow marcus 1995 ,0,1,0
this is confirmed by a comparison between our baseline result f1554 and some baseline results of english basenp chunking task eg precision819 recall782 f1800 ramshaw and marcus 1995 ,0,1,0
we adopted iob iob2 labeling ramshaw and marcus 1995 where the rst word of an entity of class c is labeled bc the words in the entity are labeled ic and other words are labeled o ,0,1,0
training and testing were performed using the noun phrase chunking corpus described in ramshaw marcus 1995 ramshaw and marcus 1995 ,0,1,0
similarly to classical nlp tasks such as text chunking ramshaw and marcus 1995 and named entity recognition tjong kim sang 2002 we formulate mention detection as a sequence classification problem by assigning a label to each token in the text indicating whether it starts a specific mention is inside a specific mention or is outside any mentions ,0,1,0
cotraining yarowsky 1995 blum and mitchell 1998 is related to selftraining in that an algorithm is trained on its own predictions ,0,1,0
we use 3500 sentences from conll tjong kim sang and de meulder 2003 as the ner data and section 2023 of the wsj marcus et al 1993 ramshaw and marcus 1995 as the poschunk data 8936 sentences ,0,1,0
our conception of the task is inspired by ramshaw and marcus representation of text chunking as a tagging problem ramshaw and marcus 1995 the information that can be used to train the system appears in columns 1 to 8 of table 1 ,0,1,0
all corpora are formatted in the iob sequence representation ramshaw and marcus 1995 ,0,1,0
we have used the optimal experiment configurations that we had obtained from the fourth experiment series for processing the complete ramshaw and marcus 1995 data set ,0,1,0
again the best result was obtained with iob1 fi 9237 which is an imirovement of the best reported f1 rate for this data set ramshaw and marcus 1995 9203 ,0,0,1
we would like to apply our learning approach to the large data set mentioned in ramshaw and marcus 1995 wall street journal corpus sections 221 as training material and section 0 as test material ,0,1,0
this time the chunker achieved a fl score of 9381 which is half a point better than the results obtained by ramshaw and marcus 1995 933 other chunker rates for this data accuracy 9804 precision 9371 recalh 9390 ,0,0,1
ramshaw and marcus used transformationbased learning tbl for developing two chunkers ramshaw and marcus 1995 ,0,1,0
ramshaw and marcus 1995 shows that basenp recognition fzi 920 is easier than finding both np and vp chunks fz1881 and that increasing the size of the training data increases the performance on the test set ,1,0,0
it performed slightly worse on basenp recognition than the ramshaw and marcus 1995 experiments fz1916 ,1,0,0
with all but two formats ibiig achieves better fzl rates than the best published result in ramshaw and marcus 1995 ,0,0,1
however they use the ramshaw and marcus 1995 data set in a different trainingtest division 10fold cross validation which makes it tifficult to compare their results with others ,0,1,0
21 data representation we have compared four complete and three partial data representation formats for the basenp recognition task presented in ramshaw and marcus 1995 ,0,1,0
in their treatment of chunkinitial and chunkfinal words iob1 iob2 ioe1 ioe2 the first word inside a basenp immediately following another basenp receives a b tag ramshaw and marcus 1995 ,0,1,0
in ramshaw and marcus 1995 a set of transformational rules is used for modifying the classification of words ,0,1,0
lrll is a part of the timbl software package which is available from httpilkkubnl 3 results we have used the basenp data presented in ramshaw and marcus 1995 2 ,0,1,0
ramshaw and marcus 1995 describe an errordriven transformationbased learning tbl method for finding np chunks in texts ,0,1,0
the chunking classification was made by ramshaw and marcus 1995 based on the parsing information in the wsj corpus ,0,1,0
all formats 2the data described in ramshaw and marcus 1995 is available from ftpftpcisupennedupubchunker 175 proceedings of eacl 99 wordpos context chunk tag context iob1 l2ri iob2 l2ri ioe1 lir2 ioe2 lir2 l2ri l0r2 io l2r0 liri io liril0r2 fi 12 9012 10 8930 12 8955 01 8973 00 00 8932 00 ii 8978 11 00 8986 table 3 results second experiment series the best fi scores for different left l and right r chunk tag context sizes for the seven representation formats using 5fold crossvalidation on section 15 of the wsj corpus ,0,1,0
we adopted the chunk representation proposed by ramshaw and marcus 1995 and used four different tags bnuc and bsat for nucleus and satelliteinitial tokens and inuc and isat for noninitial tokens ie tokens inside a nucleus and satellite span ,0,1,0
if one reduces the problem of entity mention detection to the detection of its head the nature of the problem changes and the annotation of data becomes at the gpe jordanian org military per spokesman this allows us to consider the problem as a taggingchunking problem and describe each word as beginning b an entity mention inside i an entity mention or outside o an entity mention ramhsaw and marcus 1995 sang and veenstra 1999 ,0,1,0
2 evaluating heterogeneous parser output two commonly reported shallow parsing tasks are nounphrase np chunking ramshaw and marcus 1995 and the conll2000 chunking task sang and buchholz 2000 which extends the npchunking task to recognition of 11 phrase types1 annotated in the penn treebank ,0,1,0
1 introduction finitestate parsing also called chunking or shallow parsing has typically been motivated as a fast firstpass for or approximation to more expensive contextfree parsing abney 1991 ramshaw and marcus 1995 abney 1996 ,0,1,0
they mention that the resulting shallow parse tags are somewhat different than those used by ramshaw and marcus 1995 but that they found no significant accuracy differences in training on either set ,0,1,0
ramshaw and marcus 1995 used transformation based learning using a large annotated corpus for english ,0,1,0
many statistical taggers and parsers have been trained on it eg ramshaw and marcus 1995 srinivas 1997 and alshawi and carter 1994 ,0,1,0
meanwhile it is common for np chunking tasks to represent a chunk eg np with two labels the begin eg bnp and inside eg inp of a chunk ramshaw and marcus 1995 ,0,1,0
on the base of the chunk scheme proposed by abney 1991 and the bio tagging system proposed in ramshaw and marcus1995 many machine learning techniques are used to deal with the problem ,0,1,0
31 word sequence classification similar to english text chunking ramshaw and marcus 1995 lee and wu 2007 the word sequence classification model aims to classify each word via encoding its context features ,0,1,0
this tagging scheme is the iob scheme originally put forward by ramshaw and marcus ramshaw and marcus 1995 ,0,1,0
various machine learning approaches have been proposed for chunking ramshaw and marcus 1995 tjong kim sang 2000a tjong kim sang et al 2000 tjong kim sang 2000b sassano and utsuro 2000 van halteren 2000 ,0,1,0
a176 base np standard data set basenps this data set was first introduced by ramshaw and marcus 1995 and taken as the standard data set for basenp identification task2 ,0,1,0
insideoutside this representation was first introduced in ramshaw and marcus 1995 and has been applied for base np chunking ,0,1,0
the pioneering work of ramshaw and marcus 1995 introduced np chunking as a machinelearning problem with standard datasets and evaluation metrics ,1,0,0
in contrast generative models are trained to maximize the joint probability of the training data which is 1ramshaw and marcus 1995 used transformationbased learning brill 1995 which for the present purposes can be tought of as a classi cationbased method ,0,1,0
following ramshaw and marcus 1995 the input to the np chunker consists of the words in a sentence annotated automatically with partofspeech pos tags ,0,1,0
41 data preparation np chunking results have been reported on two slightly different data sets the original rm data set of ramshaw and marcus 1995 and the modi ed conll2000 version of tjong kim sang and buchholz 2000 ,0,1,0
toward a taskbased gold standard for evaluation of np chunks and technical terms nina wacholder rutgers university ninascilsrutgersedu peng song rutgers university psongpaulrutgersedu abstract we propose a gold standard for evaluating two types of information extraction output noun phrase np chunks abney 1991 ramshaw and marcus 1995 and technical terms justeson and katz 1995 daille 2000 jacquemin 2002 ,0,1,0
np chunks abney 1991 ramshaw and marcus 1995 evans and zhai 1996 frantzi and ananiadou 1996 and technical terms dagan and church 1994 justeson and katz 1995 daille 1996 jacquemin 2001 bourigault et al 2002 fall into this difficulttoassess category ,0,1,0
to evaluate the performance of a parser np chunks can usefully be evaluated by a gold standard many systems eg ramshaw and marcus 1995 and cardie and pierce 1988 use the penn treebank for this type of evaluation ,0,1,0
similarly to classical nlp tasks such as base noun phrase chunking ramshaw and marcus 1994 text chunking ramshaw and marcus 1995 or named entity recognition tjong kim sang 2002 we formulate the mention detection problem as a classification problem by assigning to each token in the text a label indicating whether it starts a specific mention is inside a specific mention or is outside any mentions ,0,1,0
we then apply brills rulebased tagger brill 1995 and basenp noun phrase chunker ramshaw and marcus 1995 to extract noun phrases from these sentences ,0,1,0
this second expression is similar to that used in marcus 1995 ,0,1,0
the simplest one is the bio representation scheme ramshaw and marcus 1995 where a b denotes the first item of an element and an i any noninitial item and a syllable with tag o is not a part of any element ,1,0,0
ramshaw and marcus 1995 approached chucking by using transformation based learningtbl ,0,1,0
we repeat ramshaw and marcus transformation based np chunking ramshaw and marcus 1995 algorithm by substituting supertags for pos tags in the dataset ,0,1,0
the class labeling system in our experiment is iob2 sang 2000 which is a variation of iob ramshaw and marcus 1995 ,0,1,0
this segmentation task can be achieved by assigning words in a sentence to one of three tokens b for beginnp i for insidenp or o for outsidenp ramshaw and marcus 1995 ,0,1,0
these tasks are generally treated as sequential labeling problems incorporating the iob tagging scheme ramshaw and marcus 1995 ,0,1,0
the results were evaluated using the conll shared task evaluation tools 5 the approaches tested were error driven pruning edp cardie and pierce 1998 and transformational based learning of iob tagging tbl ramshaw and marcus 1995 ,0,1,0
conjunctions are a major source of errors for english chunking as well ramshaw and marcus 1995 cardie and pierce 19989 and we plan to address them in future work ,0,1,0
the np chunks in the shared task data are basenp chunks which are nonrecursive nps a definition first proposed by ramshaw and marcus 1995 ,0,1,0
3 hebrew simple np chunks the standard definition of english basenps is any noun phrase that does not contain another noun phrase with possessives treated as a special case viewing the possessive marker as the first word of a new basenp ramshaw and marcus 1995 ,0,1,0
ramshaw and marcusramshaw and marcus 1995 first represented base noun phrase recognition as a machine learning problem ,0,1,0
we only describe these models briefly since full details are presented elsewherekudo and matsumoto 2001 sha and pereira 2003 ramshaw and marcus 1995 sang 2002 ,0,1,0
following ramshaw and marcus 1995 the slot labels are drawn from a set of classes constructed by extending each label by three additional symbols beginninginsideoutside bio ,0,1,0
ramshaw and marcus 1995 to reduce the inference time following mccallum et al 2003 we collapsed the 45 different pos labels contained in the original data ,0,1,0
following ramshaw and marcus 1995 the current dominant approach is formulating chunking as a classification task in which each word is classified as the beginning inside or ooutside of a chunk ,1,0,0
np chunks in the shared task data are basenps which are nonrecursive nps a definition first proposed by ramshaw and marcus 1995 ,0,1,0
for the english experiments we use the nowstandard training and test sets that were introduced in marcus and ramshaw 19952 ,0,1,0
240 2 motivation many approaches to identifying base noun phrases have been explored as part of chunking ramshaw and marcus 1995 but determining subnp structure is rarely addressed ,0,1,0
transformationbased learning has also been successfully applied to text chunking ramshaw and marcus 1995 morphological disambiguation oflazer and tur 1996 and phrase parsing vilain and day 1996 ,0,1,0
we used the np data prepared by ramshaw and marcus 1995 hereafter rm95 ,0,1,0
the last line shows the results of ramshaw and marcus 1995 recognizing nps with the same traintest data ,0,1,0
vilain and day 1996 identify and classify name phrases such as company names locations etc ramshaw and marcus 1995 detect noun phrases by classifying each word as being inside a phrase outside or on the boundary between phrases ,0,1,0
more recently ramshaw marcus in press apply transformationbased learning brill 1995 to the problem ,0,1,0
much previous work has been done on this problem and many different methods have been used churchs parts 1988 program uses a markov model bourigault 1992 uses heuristics along with a grammar voutilainens nptool 1993 uses a lexicon combined with a constraint grammar juteson and katz 1995 use repeated phrases veenstra 1998 argamon dagan krymolowski1998 and daelemaus van den bosch zavrel 1999 use memorybased systems ramshaw marcus in press and cardie pierce 1998 use rulebased systems ,0,1,0
1 to train their system rm used a 200kword chunk of the penn treebank parsed wall street journal marcus et al 1993 tagged using a transformationbased tagger brill 1995 and extracted base noun phrases from its parses by selecting noun phrases that contained no nested noun phrases and further processing the data with some heuristics like treating the possessive marker as the first word of a new base noun phrase to flatten the recursive structure of the parse ,0,1,0
among the machine learning algorithms studied rule based systems have proven effective on many natural language processing tasks including partofspeech tagging brill 1995 ramshaw and marcus 1994 spelling correction mangu and brill 1997 wordsense disambiguation gale et al 1992 message understanding day et al 1997 discourse tagging samuel et al 1998 accent restoration yarowsky 1994 prepositionalphrase attachment brill and resnik 1994 and base noun phrase identification ramshaw and marcus in press cardie and pierce 1998 veenstra 1998 argamon et al 1998 ,1,0,0
veenstra 1998 used the basenp tag set as presented in ramshaw and marcus 1995 i for inside a basenp o for outside a basenp and b for the first word in a basenp following another basenp ,0,1,0
our goal is to come up with a mechanism that given an input string identifies the phrases in this string this is a fundamental task with applications in natural language church 1988 ramshaw and marcus 1995 mufioz et al 1999 cardie and pierce 1998 ,0,1,0
the data sets used are the standard data sets for this problem ramshaw and maxcus 1995 argamon et al 1999 mufioz et al 1999 tjong kim sang and veenstra 1999 taken from the wall street journal corpus in the penn treebank marcus et al 1993 ,0,1,0
ramshaw and marcus 1995 approached chunking by using a machine learning method ,0,1,0
31 np our np chunks are very similar to the ones of ramshaw and marcus 1995 ,0,1,0
adverbsadverbial phrases becorae part of the vp chunk as long as they are in front of the main verb vp could advp very well vp show ve could very well show in contrast to ramshaw and marcus 1995 predicative adjectives of the verb are not part of the vp chunk eg in np they vp are adjp unhappy ,0,1,0
the first solution might also introduce errors elsewhere as ramshaw and marcus 1995 already noted while this automatic derivation process introduced a small percentage of errors on its own it was the only practical way both to provide the amount of training data required and to allow for fullyautomatic testing ,0,1,0
4 data and evaluation for the conll shared task we have chosen to work with the same sections of the penn treebank as the widely used data set for base noun phrase recognition ramshaw and marcus 1995 wsj sections 1518 of the penn treebank as training material and section 20 as test material 3 ,1,0,0
bx ix 0 first word of a chunk of type x noninitial word in an x chunk word outside of any chunk this representation type is based on a representation proposed by ramshaw and marcus 1995 for noun phrase chunks ,0,1,0
1 introduction shallow parsing has received a reasonable amount of attention in the last few years for example ramshaw and marcus 1995 ,0,1,0
chunks can be represented with bracket structures but alternatively one can use a tagging representation which classifies words as being inside a chunk i outside a chunk o or at a chunk boundary b ramshaw and marcus 1995 ,0,1,0
the first is a baseline of sorts our own version of the chunking as tagging approach introduced by ramshaw and marcus ramshaw and marcus 1995 ,0,1,0
 00 the current input token and the previous one have the same parent 90 one ancestor of the current input token and the previous input token have the same parent 09 the current input token and one ancestor of the previous input token have the same parent 99 one ancestor of the current input token and one ancestor of the previous input token have the same parent compared with the bchunk and ichunk used in ramshaw and marcus1995 structural relations 99 and 90 correspond to bchunk which represents the first word of the chunk and structural relations 00 and 09 correspond to ichunk which represents each other in the chunk while 90 also means the beginning of the sentence and 09 means the end of the sentence ,0,1,0
ramshaw and marcus ramshaw and marcus 1995 views chunking as a tagging problem ,0,1,0
ramshaw and marcus1995 used transformationbased learning an errordriven learning technique introduced by eric bn111993 to locate chunks in the tagged corpus ,0,1,0
null compared with the bchunk and ichunk used in ramshaw and marcus1995 structural relations 99 and 90 correspond to bchunk which represents the first word of the chunk and structural relations 00 and 09 correspond to ichunk which represnts each other in the chunk while 90 also means the beginning of the sentence and 09 means the end of the sentence ,0,1,0
5 the task base np chunking the task is base np chunking on section 20 of the wall street journal corpus using sections 15 to 18 of the corpus as training data as in ramshaw and marcus 1995 ,0,1,0
thus over the past few years along with advances in the use of learning and statistical methods for acquisition of full parsers collins 1997 charniak 1997a charniak 1997b ratnaparkhi 1997 significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship church 1988 ramshaw and marcus 1995 argamon et al 1998 cardie and pierce 1998 munoz et al 1999 punyakanok and roth 2001 buchholz et al 1999 tjong kim sang and buchholz 2000 ,1,0,0
the original ramshaw and marcus 1995 publication evaluated their np chunker on two data sets the second holding a larger amount of training data penn treebank sections 0221 while using 00 as test data ,0,1,0
the data set that has become standard for evaluation machine learning approaches is the one first used by ramshaw and marcus 1995 ,0,1,0
for extracting simple noun phrases we first used ramshaw and marcuss base np chunker ramshaw and marcus 1995 ,0,1,0
wallstreet journal wsj sections 1518 and 20 were used by ramshaw and marcus 1995 as training and test data respectively for evaluating their basenp chunker ,0,1,0
the noun phrase extraction module uses brills pos tagger brill 1992and a base np chunker ramshaw and marcus 1995 ,0,1,0
this tagging scheme is the iob scheme originally put forward by ramshaw and marcus 1995 ,0,1,0
2 system description 21 data representation in this paper we change the representation of the original data as follows bracketed representation of roles is converted into iob2 representation ramhsaw and marcus 1995 sang and veenstra 1995 word tokens are collapsed into base phrase bp tokens ,0,1,0
the examples represent sevenword windows of words and their respective predicted partofspeech tags and each example is labeled with a class using the iob type of segmentation coding as introduced by ramshaw and marcus 1995 marking whether the middle word is inside i outside o or at the beginning b of a chunk ,0,1,0
1httpchasenorg takusoftwareyamcha 2httpchasenorg takusoftwaretinysvm 197 a0 bracketed representation of roles was converted into iob2 representation ramhsaw and marcus 1995 sang and veenstra 1999 ,0,1,0
this is referred to as an iob representation ramshaw and marcus 1995 ,0,1,0
sang used the iob tagging method proposed by ramshowramshaw and marcus 1995 and memorybased learning for each level of chunking and achieved an fscore of 8049 on the penn treebank corpus ,0,1,0
ramshaw and marcus 1995 introduced a transformationbased learning method which considered chunking as a kind of tagging problem ,0,1,0
ramshaw and marcus 1995 first introduced the machine learning techniques to chunking problem ,0,1,0
6 related works after the work of ramshaw and marcus 1995 many machine learning techniques have been applied to the basic chunking task such as support vector machines kudo and matsumoto 2001 hidden markov modelmolina and pla 2002 memory based learning sang 2002 conditional random fields sha and pereira 2003 and so on ,0,1,0
21 word sequence classification similar to english text chunking ramshaw and marcus 1995 wu et al 2006a the word sequence classification model aims to classify each word via encoding its context features ,0,1,0
the sentences in the training and testing sets were already perfectly postagged and noun chunked and that in a reallife situation additional preprocessing by a postagger such as the ltpostagger4 and noun chunker such as described in ramshaw and marcus 1995 which will introduce additional errors ,0,1,0
each token is labelled with a class using the iob type of segmentation coding as introduced by ramshaw and marcus 1995 marking whether the middle word is inside i outside o or at the beginning b of a chunk or named entity ,0,1,0
it was first cast as a classification problem by ramshaw and marcus 1995 as a problem of np chunking ,0,1,0
a la ramshaw and marcus 1995 they represent the words as a sequence of labeled words with iob annotations where the b marks a word at the beginning of a chunk i marks a word inside a chunk and o marks those words and punctuation that are outside chunks ,0,1,0
a la ramshaw and marcus 1995 and kudo and matsumato 2000 we use the iob tagging style for modeling and classification ,0,1,0
the difficulty of this task is that the standard method for converting ner to a sequence tagging problem with bioencoding ramshaw and marcus 1995 where each 1httpwwwnistgovspeechtestsace indexhtm token is assigned a tag to indicate whether it is at the beginning b inside i or outside o of an entity is not directly applicable when tokens belong to more than one entity ,0,1,0
the concept of basenp has undergone a number of revisions ramshaw and marcus 1995 tjong kim sang and buchholz 2000 but has previously always been tied to extraction from a more completely annotated treebank whose annotations are subject to other pressures than just initial material up to the head to our knowledge our gures for interannotator agreement on the basenp task itself 169 ie not derived from a larger annotation task are the rst to be reported ,0,1,0
ramshaw and marcus 1995 state that a basenp aims to identify essentially the initial portions of nonrecursive noun phrases up to the head including determiners but not including postmodifying prepositional phrases or clauses however work on basenps has essentially always proceeded via algorithmic extraction from fully parsed corpora such as the penn treebank ,0,1,0
given a weight vector w the score wfxy ranks possible labelings of x and we denote by ykwx the set of k top scoring labelings for x we use the standard bio encoding for named entities ramshaw and marcus 1995 ,0,1,0
we hence chose transformationbased learning to create this shallow segmentation grammar converting the segmentation task into a tagging task as is done in 85 ramshaw and marcus 1995 inter alia ,0,1,0
44 text chunking next a rulebased text chunker ramshaw and marcus 1995 is applied on the tagged sentences to further identify phrasal units such as base noun phrases np and verbal units vb ,0,1,0
all our experiments used the standard bio encoding ramshaw and marcus 1995 with different feature sets and learning procedures ,0,1,0
rm95 lance a ramshaw mitchell p marcus 1995 ,0,1,0
we do not consider mixed features between words and pos tags as in ltamshaw and marcus 1995 that is a single feature consists of either words or tags ,0,1,0
instead of using the np bracketing information present in the tagged treebank data ramshaw and marcus modified the data so as to include bracketing information related only to the nonrecursive base nps present in each sentence while the subject verb phrases were taken as is the data sets include pos tag information generated by ramshaw and marcus using brills transformational partofspeech tagger brill 1995 ,0,1,0
the results are comparable to other results reported using the insideoutside method ramshaw and marcus 1995 see table 7 ,0,1,0
this is similar to results in the literature ramshaw and marcus 1995 ,0,1,0
perhaps this was not observed earlier since ramshaw and marcus 1995 studied only base nps most of which are short ,0,1,0
of the several slightly different definitions of a base np in the literature we use for the purposes of this work the definition presented in ramshaw and marcus 1995 and used also by argamon et al 1998and others ,0,1,0
for example the sentence i went to california last may would be marked for base nps as i went to california last may i 0 0 i b i indicating that the nps are i california and last may this approach has been studied in ramshaw and marcus 1995 ,0,1,0
the observation that shallow syntactic information can be extracted using local information by examining the pattern itself its nearby context and the local partofspeech information has motivated the use of learning methods to recognize these patterns church 1988 ramshaw and marcus 1995 argamon et al 1998 cardie and pierce 1998 ,0,1,0
ramshaw and marcus 1995 first assigned a chunk tag to each word in the sentence i for inside a chunk o for outside a chunk and 240 type precision b tbr inside a chunk but tile preceding word is in another chunk ,0,1,0
introduction since eric brill first introduced the method of transformationbased learning tbl it has been used to learn rules for many natural language processing tasks such as partofspeech tagging brill 1995 ppattachment disambiguation brill and resnik 1994 text chunking ramshaw and marcus 1995 spelling correction mangu and brill 1997 dialogue act tagging samuel et al 1998 and ellipsis resolution hardt 1998 ,0,1,0
we also present the results of argamon et al 1998 ramshaw and marcus 1995 and cardie and pierce 1998 in table 4 ,0,1,0
introduction recently there has been an increased interest in approaches to automatically learning to recognize shallow linguistic patterns in text ramshaw and marcus 1995 vilain and day 1996 argamon et al 1998 buchholz 1998 cardie and pierce 1998 veenstra 1998 daelemans et ai ,0,1,0
recent comparisons of approaches that can be trained on corpora van halteren et al 1998 volk and schneider 1998 have shown that in most cases statistical aproaches cutting et al 1992 schmid 1995 ratnaparkhi 1996 yield better results than finitestate rulebased or memorybased taggers brill 1993 daelemans et al 1996 ,1,0,0
for the penn treebank ratnaparkhi 1996 reports an accuracy of 966 using the maximum entropy approach our much simpler and therefore faster hmm approach delivers 967 ,0,0,1
according to current tagger comparisons van halteren et al 1998 zavrel and daelemans 1999 and according to a comparsion of the results presented here with those in ratnaparkhi 1996 the maximum entropy framework seems to be the only other approach yielding comparable results to the one presented here ,0,1,0
the penn treebank results reported here for the markov model approach are at least equivalent to those reported for the maximum entropy approach in ratnaparkhi 1996 ,0,1,0
this is the way the maximum entropy tagger ratnaparkhi 1996 runs if one uses the binary version from the website see the comparison in section 5 ,0,1,0
we have chosen the maximum entropy tagger ratnaparkhi 1996 for a comparison with our universal tagger since it achieved by a small margin the best overall result on slovene as reported there 86360 on all tokens of taggers available to us mbt the best overall was not freely available to us at the time of writing ,1,0,0
adwait ratnaparkhi 1996 estimates a probability distribution for tagging using a maximum entropy approach ,0,1,0
regarding error detection in corpora ratnaparkhi 1996 discusses inconsistencies in the penn treebank and relates them to interannotator differences in tagging style ,0,1,0
4 maxilnum entropy the model used here for sentenceboundary detection is based on the maximum entropy model used for pos tagging in ratnaparkhi 1996 ,0,1,0
a2 maximumentropy method the maximumentropy method is useful with sparse data conditions and has been used by many researchers berger et al 1996 ratnaparkhi 1996 ratnaparkhi 1997 borthwick el al 1998 uchimoto et al 1999 ,1,0,0
we can find some other machinelearning approaches that use more sophisticated lms such as decision trees mhrquez and rodrfguez 1998magerman 1996 memorybased approaclms to learn special decision trees daelemans et al 1996 maximmn entropy approaches that combine statistical information from different sources ratnaparkhi 1996 finite state autonmt2 inferred using grammatical inference pla and prieto 1998 etc the comparison among different altroaches is dif ficult due to the nmltiple factors that can be eonsid614 ered tile languagk tile mmfler and tyte of the tags the size of tilt vocabulary thk ambiguity the diiticulty of the test ski kte ,0,1,0
many studies and improvements have been conducted for presently with service media laboratory corporate researchanddevelopmentcenter okielectricindustry co ltd pos tagging and major methods of pos tagging achieve an accuracy of 9697 on the penn treebank wsj corpus but obtaining higher accuracies is difficult ratnaparkhi 1996 ,0,1,0
it is mentioned that the limitation is largely caused by inconsistencies in the corpus ratnaparkhi 1996 padro and marquez 1998 van halteren et al 2001 ,0,1,0
our pos tagger is essentially the maximum entropy tagger by ratnaparkhi 1996 retrained on the ctbi data ,0,1,0
according to the document it is the output of ratnaparkhis tagger ratnaparkhi 1996 ,0,1,0
 pos tagger the maximum entropy pos tagger developed by ratnaparkhi ratnaparkhi 1996 and the rulebased pos tagger developed by brill brill 1994 are trained with 1200 abstracts extracted from the genia corpus which achieve accuracies of 9797 and 9806 respectively when testing on the rest 800 abstract of the genia corpus ,0,1,0
we use two stateoftheart pos taggersa maximum entropy based english pos tagger ratnaparkhi 1996 and an hmm based chinese pos tagger ,0,1,0
second several tagging experiments on newspaper language whether statistical ratnaparkhi 1996 brants 2000 or rulebased brill 1995 report that the tagging accuracy for unknown words is much lower than the overall accuracy2 thus the lower percentage of unknown words in medical texts seems to be a sublanguage feature beneficial to pos taggers whereas the higher proportion of unknown words in newspaper language seems to be a prominent source of tagging errors ,0,1,0
to accommodate multiple overlapping features on observations some other approaches view the sequence labeling problem as a sequence of classification problems including support vector machines svms kudo matsumoto 2001 and a variety of other classifiers punyakanok roth 2001 abney et al 1999 ratnaparkhi 1996 ,0,1,0
we tokenized sentences using the standard treebank tokenization script and then we performed partofspeech tagging using mxpost tagger ratnaparkhi 1996 ,0,1,0
in contrast the cc tagger which is based on that of ratnaparkhi 1996 utilizes a wide range of features and a larger contextual window including the previous two tags and the two previous and two following words ,0,1,0
we then tagged the search queries using a maximum entropy partofspeech tagger ratnaparkhi 1996 ,0,1,0
we use mxpost tagger adwait 1996 for pos tagging charniak parser charniak 2000 for extracting syntactic relations svmlight1 for svm classifier and david bleis version of lda2 for lda training and inference ,0,1,0
hidden markov models are simple and effective but unlike discriminative models such as maximum entropy models ratnaparkhi 1996 and conditional random fields john lafferty 2001 they have more difficulty utilizing a rich set of conditionally dependent features ,1,0,0
the crf tagger was implemented in mallet mccallum 2002 using the original feature templates from ratnaparkhi 1996 ,0,1,0
some examples of pos taggers that perform reasonably well on monolingual text of each language can be found in brants 2000 brill 1992 carreras and padro 2002 charniak 1993 ratnaparkhi 1996 schmid 1994 ,0,1,0
this model can be seen as an extension of the standard maximum entropy markov model memm see ratnaparkhi 1996 with an extra dependency on the predicate label we will henceforth refer to this model as memmpred ,0,1,0
english pos tags were assigned by mxpost ratnaparkhi 1996 which was trained on the training data described in section 41 ,0,1,0
2008 we used the mxpost ratnaparkhi 1996 tagger trained on training data to provide partofspeech tags for the development and the test set and we used 10way jackknifing to generate tags for the training set ,0,1,0
the target set is built using the 8889 wall street journal corpus wsj tagged using the ratnaparkhi 1996 tagger and the bangalore joshi 1999 supertagger the feedback sets are built using wsj sentences con330 algorithm 1 ketrain karov edelman 1998 algorithm adapted to literalnonliteral classification require s the set of sentences containing the target word require l the set of literal seed sentences require n the set of nonliteral seed sentences require w the set of wordsfeatures w s means w is in sentence s s owner w means s contains w require epsilon1 threshold that determines the stopping condition 1 wsim0wxwy 1 if wx wy0 otherwise 2 ssimi0sxsy 1 for all sxsy s s where sx sy 0 otherwise 3 i 0 4 while true do 5 ssimli1sxsy summationtextwxsx pwxsxmaxwysy wsimiwxwy for all sxsy s l 6 ssimni1sxsy summationtextwxsx pwxsxmaxwysy wsimiwxwy for all sxsy s n 7 for wxwy w w do 8 wsimi1wxwy braceleftbigg i 0 summationtextsxownerwx pwxsxmaxsyownerwy ssimiisxsy else summationtextsxownerwx pwxsxmaxsyownerwyssimli sxsyssimni sxsy 9 end for 10 if wxmaxwywsimi1wxwywsimiwxwy epsilon1 then 11 break algorithm converges in 1epsilon1 steps ,0,1,0
we then proceed to split the data into smaller sentences and tag them using ratnaparkhis maximum entropy tagger ratnaparkhi 1996 ,0,1,0
thus we used the five taggers mbl daelemans et al 1996 mxpost ratnaparkhi 1996 fntbl ngai and florian 2001 tnt and icetagger3 in the same manner as described in loftsson 2006 but with the following minor changes ,0,1,0
16in fact we have experimented with other tagger combinations and configurations as wellwith the tnt brants 2000 maxent ratnaparkhi 1996 and treetagger schmid 1994 with or without the morce tagger in the pack see below for the winning combination ,0,1,0
we generate pos tags using the mxpost tagger ratnaparkhi 1996 for english and chinese and connexor for spanish ,0,1,0
previous work used all possible pre xes and suf xes ranging in length from 1 to k characters with k 4 ratnaparkhi 1996 and k 10 toutanova et al 2003 ,0,1,0
this is based on the idea from ratnaparkhi 1996 that rare words in the training set are similar to unknown words in the test set and can be used to learn how to tag the unknown words that will be encountered during testing ,0,1,0
both systems rely on the opennlp maximumentropy partofspeech tagger and chunker ratnaparkhi 1996 but knowitall applies them to pages downloaded from the web based on the results of google queries whereas knowitnow applies them once to crawled and indexed pages6 overall each of the above elements of knowitall and knowitnow are the same to allow for controlled experiments ,0,1,0
models of that form include hidden markov models rabiner 1989 bikel et al 1999 as well as discriminative tagging models based on maximum entropy classification ratnaparkhi 1996 mccallum et al 2000 conditional random fields lafferty et al 2001 sha and pereira 2003 and largemargin techniques kudo and matsumoto 2001 taskar et al 2003 ,0,1,0
mxpost ratnaparkhi 1996 and in order to discover more general patterns we map the tag set down after tagging eg nn nnp nnps and nns all map to nn ,0,1,0
the unknown word tokens are with respect to training i data set sectns token unknown training i 26270 600931 213986 training ii 600931 500527 10011039 204701 training iii 001270 301527 590593 6001039 10431151 485321 devset 23839 2849 xh 001025 7844 381 hksar 500527 8202 1168 sm 590593 10011002 7793 1300 test set 23522 2957 xh 271300 8008 358 hksar 528554 7153 1020 sm 594596 10401042 8361 1579 52 the model our model builds on research into loglinear models by ng and low 2004 toutanova et al 2003 and ratnaparkhi 1996 ,0,1,0
ng and low 2004 toutanova et al 2003 brants 2000 ratnaparkhi 1996 samuelsson 1993 ,0,1,0
previous work on pos tagging of unknown words has proposed a number of features based on prefixes and suffixes and spelling cues like capitalization toutanova et al 2003 brants 2000 ratnaparkhi 1996 ,0,1,0
v j della pietra 1996 ratnaparkhi 1996 was proposed in the original work to solve the lmr tagging problem ,0,1,0
the models are based on a maximum entropy framework ratnaparkhi 1996 xue and shen 2003 ,0,1,0
2 maximum entropy in this bakeoff our basic model is based on the framework described in the work of ratnaparkhi 1996 which was applied for english pos tagging ,0,1,0
4 pos tagger and named entity recognizer for the pos tagging task the tagger is built based on the work of ratnaparkhi 1996 which was applied for english pos tagging ,0,1,0
7 for a more detailed discussion see berger della pietra and della pietra 1996 and ratnaparkhi 1996 ,0,1,0
tagging can also be done using maximum entropy modeling see section 24 a maximum entropy tagger called mxpost was developed by ratnaparkhi 1996 we will refer to this tagger as mxp below ,0,1,0
our method was applied to 23 million words of the wsj that were automatically tagged with ratnaparkhis maximum entropy tagger ratnaparkhi 1996 and chunked with the partial parser cass abney 1996 ,0,1,0
we see no good reason however why such text spans should necessarily be sentences since the majority of tagging paradigms eg hidden markov model hmm kupiec 1992 brills brill 1995a and maxent ratnaparkhi 1996 do not attempt to parse an entire sentence and operate only in the local window of two to three tokens ,0,1,0
words in test data that have not been seen in training are deterministically assigned the pos tag that is assigned by the tagger described in ratnaparkhi 1996 ,0,1,0
32 73 unknown words and parts of speech when the parser encounters an unknown word the firstbest tag delivered by ratnaparkhis 1996 tagger is used ,0,1,0
ratnaparkhi 1996 estimates a pos tagging error rate of 3 in the treebank ,0,1,0
clark and curran 2004a describe the supertagger which uses loglinear models to define a distribution over the lexical category set for each local fiveword context containing the target word ratnaparkhi 1996 ,0,1,0
related work the first application of loglinear models to parsing is the work of ratnaparkhi and colleagues ratnaparkhi roukos and ward 1994 ratnaparkhi 1996 1999 ,0,1,0
also we used adwait ratnaparkhis partofspeech tagger ratnaparkhi 1996 to tag unknown words in the test data ,0,1,0
the perstate models in this paper are loglinear models building upon the models in ratnaparkhi 1996 and toutanova and manning 2000 though some models are in fact strictly simpler ,0,1,0
highperformance taggers typically also include joint threetag counts in some way either as tag trigrams brants 2000 or tagtriple features ratnaparkhi 1996 toutanova and manning 2000 ,0,1,0
words surrounding the current word have been occasionally used in taggers such as ratnaparkhi 1996 brills transformation based tagger brill 1995 and the hmm model of lee et al ,0,1,0
33 unknown word features most of the models presented here use a set of unknown word features basically inherited from ratnaparkhi 1996 which include using character ngram prefixes and suffixes for n up to 4 and detectors for a few other prominent features of words such as capitalization hyphens and numbers ,0,1,0
at any rate regularized conditional loglinear models have not previously been applied to the problem of producing a high quality partofspeech tagger ratnaparkhi 1996 toutanova and manning 2000 and collins 2002 all present unregularized models ,0,1,0
2 bidirectional dependency networks when building probabilistic models for tag sequences we often decompose the global probability of sequences using a directed graphical model eg an hmm brants 2000 or a conditional markov model cmm ratnaparkhi 1996 ,0,1,0
we then piped the text through a maximum entropy sentence boundary detector ratnaparkhi 1996 and performed text normalization using nsw tools sproat et al 2001 ,0,1,0
the best result known to us is achieved by toutanova2002 by enriching the feature representation of the maxent approach ratnaparkhi 1996 ,0,1,0
for example since the collins parser depends on a prior partofspeech tagger ratnaparkhi 1996 we included the time for pos tagging in our collins measurements ,0,1,0
the pos tag features were produced by rst predicting the tags with ratnaparkhis maximum entropy tagger ratnaparkhi 1996 and then clustered by hand into a smaller number of groups based on their syntactic role ,0,1,0
maximum entropy maxent principle has been successfully applied in many classification and tagging tasks ratnaparkhi 1996 k nigam and amccallum 1999 a mccallum and pereira 2000 ,1,0,0
2we use a pos tagger ratnaparkhi 1996 trained on switchboard data with the additional tags of fp filled pause and frag word fragment ,0,1,0
examples of statistical and machine learning approaches that have been used for tagging include transformation based learning brill 1995 memory based learning daelemans et al 1996 and maximum entropy models ratnaparkhi 1996 ,0,1,0
the chinese text was tagged using the mxpost maximumentropy part of speech tagging tool ratnaparkhi 1996 trained on the penn chinese treebank 51 the english text was tagged using the tnt part of speech tagger brants 2000 trained on the wall street journal portion of the english penn treebank ,0,1,0
hakkanitur et al 2000 and basque ezeiza et al 1998 which pose quite different and in the end less severe problems there have been attempts at solving this problem for some of the highly inflectional european languages such as daelemans et al 1996 erjavec et al 1999 slovenian hajic and hladka 1997 hajic and hladka 1998 czech and hajic 2000 five central and eastern european languages but so far no system has reached in the absolute terms a performance comparable to english tagging such as ratnaparkhi 1996 which stands around or above 97 ,1,0,0
we use the beam search technique of ratnaparkhi 1996 to search the space of all hypotheses ,0,1,0
berger 1996 ratnaparkhi 1996 1998 mikheev 1998 2000 ,0,1,0
31 maximum entropy this section presents a brief description of me a more detailed and informative description can be found in berger 1996 4 ratnaparkhi 1998 manning and shutze 2000 to name just a few ,0,1,0
as a baseline model we used a maximum entropy tagger very similar to the one described in ratnaparkhi 1996 ,0,1,0
maximum entropy taggers have been shown to be highly competitive on a number of tagging tasks such as partofspeech tagging ratnaparkhi 1996 and namedentity recognition borthwick et ,1,0,0
we have explained elsewhere clark 2002 how suitable features can be defined in terms of the a18 word postag a20 pairs in the context and how maximum entropy techniques can be used to estimate the probabilities following ratnaparkhi 1996 ,0,1,0
we therefore ran the dependency model on a test corpus tagged with the postagger of ratnaparkhi 1996 which is trained on the original penn treebank see hwdep tagger in table 3 ,0,1,0
in modern lexicalized parsers pos tagging is often interleaved with parsing proper instead of being a separate preprocessing module collins 1996 ratnaparkhi 1997 ,0,1,0
task church 1988 brill 1993 ratnaparkhi 1996 daelemans et al 1996 and reported errors in the range of 26 are common ,0,1,0
chunks as a separate level have also been used in collins 1996 and ratnaparkhi 1997 ,0,1,0
maxent taggers have been shown to be highly competitive on a number of tagging tasks such as partofspeech tagging ratnaparkhi 1996 namedentity recognition borthwick et ,1,0,0
for example animal would be mapped to aa gm would again be mapped to aa the tagger was applied and trained in the same way as described in ratnaparkhi 1996 ,0,1,0
following ratnaparkhi 1996 we only include features which occur 5 times or more in training data ,0,1,0
templates for local features are similar to the ones employed by ratnaparkhi 1996 for postagging table 3 though as our input already includes postags we can make use of partofspeech information as well ,0,1,0
in each case the input to the network is a sequence of tagword pairs5 5we used a publicly available tagger ratnaparkhi 1996 to provide the tags ,0,1,0
however the fact that the dgssn uses a largevocabulary tagger ratnaparkhi 1996 as a preprocessing stage may compensate for its smaller vocabulary ,0,1,0
collins 1999 falls back to the pos tagging of ratnaparkhi 1996 for words seen fewer than 5 times in the training corpus ,0,1,0
as the tagger of ratnaparkhi 1996 cannot tag a word lattice we cannot back off to this tagging ,0,0,1
the algorithm is exactly the same as the one described in ratnaparkhi 1996 to find the most probable partofspeech sequence ,0,1,0
our system assumes pos tags as input and uses the tagger of ratnaparkhi 1996 to provide tags for the development and evaluation sets ,0,1,0
in each case the input to the network is a sequence of tagword pairs2 we report results for two different vocabulary sizes varying in the frequency with which tagword pairs must 2we used a publicly available tagger ratnaparkhi 1996 to provide the tags ,0,1,0
motivated by our goal of representing syntax we used partofspeech pos tags as labeled by a maximum entropy tagger ratnaparkhi 1996 ,0,1,0
it will also be relevant to apply advanced statistical models that can incorporate various useful information to this task eg the maximum entropy model ratnaparkhi 1996 ,0,1,0
the most notable of these include the trigram hmm tagger brants 2000 maximum entropy tagger ratnaparkhi 1996 transformationbased tagger brill 1995 and cyclic dependency networks toutanova et al 2003 ,1,0,0
this test set was tagged using mxpost ratnaparkhi 1996 which was itself trained on switchboard ,0,1,0
the pos tagger uses the same contextual predicates as ratnaparkhi 1996 the supertagger adds contextual predicates corresponding to pos tags and bigram combinations of pos tags curran and clark 2003 ,0,1,0
pos tag the text using the tagger of ratnaparkhi 1996 ,0,1,0
the initial state contains terminal items whose labels are the pos tags given by the tagger of ratnaparkhi 1996 ,0,1,0
5 external knowledge sources 51 lexical dependencies features derived from ngrams of words and tags in the immediate vicinity of the word being tagged have underpinned the world of pos tagging for many years kupiec 1992 merialdo 1994 ratnaparkhi 1996 and have proven to be useful features in wsd yarowsky 1993 ,0,1,0
in these experiments we used the mxpost tagger ratnaparkhi 1996 combined withcollinsparsercollins1996toassignparse trees to the corpus ,0,1,0
tag test data using the postagger described in ratnaparkhi 1996 ,0,1,0
for english there are many pos taggers employing machine learning techniques like transformationbased errordriven learning brill 1995 decision trees black et al 1992 markov model cutting et al 1992 maximum entropy methods ratnaparkhi 1996 etc there are also taggers which are hybrid using both stochastic and rulebased approaches such as claws garside and smith 1997 ,0,1,0
tag test data using the postagger described in ratnaparkhi 1996 ,0,1,0
verbs and possible senses in our corpus both corpora were lemmatized and partofspeech pos tagged using minipar lin 1993 and mxpost ratnaparkhi 1996 respectivelly ,0,1,0
the standard split of the corpus into training sections 222 9753 sentences validation section 24 321 sentences and testing section 23 603 sentences was performed2 as in henderson 2003 turian and melamed 2006 we used a publicly available tagger ratnaparkhi 1996 to provide the partofspeech tag for each word in the sentence ,0,1,0
following previous work ratnaparkhi 1996 we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words ie 2 in the equation above ,0,1,0
3 maximum entropy taggers the taggers are based on maximum entropy tagging methods ratnaparkhi 1996 and can all be trained on new annotated data using either gis or bfgs training code ,0,1,0
57 given a pair of english sentences to be compared a system translation against a reference translation we perform tokenization2 lemmatization using wordnet3 and partofspeech pos tagging with the mxpost tagger ratnaparkhi 1996 ,0,1,0
during training the baseline pos tagger stores special wordtag pairs into a tag dictionary ratnaparkhi 1996 ,0,1,0
this method led to improvement in the decoding speed as well as the output accuracy for english pos tagging ratnaparkhi 1996 ,1,0,0
several models were introduced for these problems for example the hidden markov model hmm rabiner 1989 maximum entropy model me ratnaparkhi and adwait 1996 and conditional random fields crfs lafferty et al 2001 ,0,1,0
in our experiments these were obtained automatically using mxpost ratnaparkhi 1996 and bbns identifinder bikel et al 1999 ,0,1,0
the applications range from simple classification tasks such as text classification and historybased tagging ratnaparkhi 1996 to more complex structured prediction tasks such as partofspeech pos tagging lafferty et al 2001 syntactic parsing clark and curran 2004 and semantic role labeling toutanova et al 2005 ,0,1,0
there is a large number of potentially informative features that could play a role in correctly predicting the tag of an unknown word ratnaparkhi 1996 weischedel et al 1993 daelemans et al 1996 ,0,1,0
more recently the integration of information sources and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods collins z brooks 1995 ratnaparkhi 1996 magerman 1994 ng lee 1996 collins 1996 ,0,1,0
in ratnaparkhi 1996 a maximum entropy tagger is presented ,0,1,0
since the advent of manually tagged corpora such as the brown corpus and the penn treebank francis1982 marcus1993 the efficacy of machine learning for training a tagger has been demonstrated using a wide array of techniques including markov models decision trees connectionist machines transformations nearestneighbor algorithms and maximum entropy weischedel1993 black1992 schmid1994 brill1995daelemans1995ratnaparkhi1996 ,0,1,0
the tagger from ratnaparkhi 1996 first annotates sentences of raw text with a sequence of partofspeech tags ,0,1,0
entropy used in some partofspeech tagging systems ratnaparkhi 1996 is a measure of how much information is necessary to separate data ,0,1,0
much research has been done to improve tagging accuracy using several different models and methods including hidden markov models hmms kupiec 1992 charniak et al 1993 rulebased systems brill 1994 brill 1995 memorybased systems daelemans et al 1996 maximumentropy systems ratnaparkhi 1996 path voting constraint systems tiir and oflazer 1998 linear separator systems roth and zelenko 1998 and majority voting systems van halteren et al 1998 ,0,1,0
some statistical model to estimate the part of speech of unknown words from the case of the first letter and the prefix and suffix is proposed weischedel et al 1993 brill 1995 ratnaparkhi 1996 mikheev 1997 ,0,1,0
to improve the unknown word model featurebased approach such as the maximum entropy method ratnaparkhi 1996 might be useful because we dont have to divide the training data into several disjoint sets like we did by part of speech and word type and we can incorporate more linguistic and morphological knowledge into the same probabilistic framework ,1,0,0
our named entity recognizer used a maximum entropy model built with adwait ratnaparkhis tools ratnaparkhi 1996 to label word sequences as either person place company or none of the above based on local cues including the surrounding words and whether honorifics eg mrs or gen ,0,1,0
2 the tagger we used ratnaparkhis maximum entropybased pos tagger ratnaparkhi 1996 ,0,1,0
for our experiments we used the binaryonly distribution of the tagger ratnaparkhi 1996 ,0,1,0
we use a tagger based on adwait ratnaparkhis method ratnaparkhi 1996 ,0,1,0
a maximum entropy approach has been applied to partofspeech tagging before ratnaparkhi 1996 but the approachs ability to incorporate nonlocal and nonhmmtaggertype evidence has not been fully explored ,0,0,1
1 the baseline maximum entropy model we started with a maximum entropy based tagger that uses features very similar to the ones proposed in ratnaparkhi 1996 ,0,1,0
the features that define the constraints on the model are obtained by instantiation of feature templates as in ratnaparkhi 1996 ,0,1,0
they are a subset of the features used in ratnaparkhi 1996 ,0,1,0
among recent top performing methods are hidden markov models brants 2000 maximum entropy approaches ratnaparkhi 1996 and transformationbased learning brill 1994 ,1,0,0
the feature templates in ratnaparkhi 1996 that were left out were the ones that look at the previous word the word two positions before the current and the word two positions after the current ,0,1,0
model overall unknown word accuracy accuracy baseline 9672 845 j ratnaparkhi 9663 8556 1996 table 3 baseline model performance this table also shows the results reported in ratnaparkhi 1996 142for convenience ,0,1,0
this may stem from the differences between the two models feature templates thresholds and approximations of the expected values for the features as discussed in the beginning of the section or may just reflect differences in the choice of training and test sets which are not precisely specified in ratnaparkhi 1996 ,0,1,0
one conclusion that we can draw is that at present the additional word features used in ratnaparkhi 1996 looking at words more than one position away from the current do not appear to be helping the overall performance of the models ,0,0,1
some are the result of inconsistency in labeling in the training data ratnaparkhi 1996 which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context ,0,0,1
a possible solution to his problem might be the use of more general morphological rules like those used in partofspeech tagging models eg 1 2 3 4 530 40 50 60 70 80 90 100 level error rand base boosts nntfidf nb boostm figure 6 comparison of all models for a129 a48a51a95a66a97a98a97a180a222 ratnaparkhi 1996 where all suffixes up to a certain length are included ,0,1,0
for these words we first used a pos tagger ratnaparkhi 1996 to determine the correct pos ,0,1,0
to obtain these distances ratnaparkhis partofspeech pos tagger ratnaparkhi 1996 and collins parser collins 1999 were used to obtain parse trees for the english side of the test corpus ,0,1,0
if pos denotes the pos of the english word we can define the wordtoword distance measure equation 4 as pos pos 15 ratnaparkhis pos tagger ratnaparkhi 1996 was used to obtain pos tags for each word in the english sentence ,0,1,0
for this work an offtheshelf maximum entropy tagger 10 ratnaparkhi 1996 was used ,0,1,0
2 combining classifiers for chinesewordsegmentation thetwomachinelearningmodelsweuseinthis work are the maximum entropy model ratnaparkhi 1996 and the errordriven transformationbased learning model brill 1994weusetheformerasthemainworkhorse and the latter to correct some of the errors producedbytheformer ,0,1,0
22 themaximumentropytagger the maximum entropy model used in postagging is described in detail in ratnaparkhi 1996andthepoctaggerhereusesthesame probability model ,0,1,0
the leader of the pack is the mxpost tagger ratnaparkhi 1996 ,0,1,0
2 the me tagger the me tagger is based on ratnaparkhi 1996s pos tagger and is described in curran and clark 2003 ,0,1,0
finally in section 4 we add additional features to the maxent model and chain these models into a conditional markov model cmm as used for tagging ratnaparkhi 1996 or earlier ner work borthwick 1999 ,0,1,0
1 introduction the maximum entropy model berger et al 1996 pietra et al 1997 has attained great popularity in the nlp field due to its power robustness and successful performance in various nlp tasks ratnaparkhi 1996 nigam et al 1999 borthwick 1999 ,1,0,0
we used mxpost ratnaparkhi 1996 a maximum entropy based pos tagger ,0,1,0
the maximum entropy markov model used in postagging is described in detail in ratnaparkhi 1996 and the lmr tagger here uses the same probability model ,0,1,0
this approach allows to combine strengths of generality of context attributes as in ngram models brants 2000 megyesi 2001 with their specificity as for binary features in maxent taggers ratnaparkhi 1996 hajic and hladk 1998 ,1,0,0
we determined appropriate training parameters and network size based on intermediate validation 1we used a publicly available tagger ratnaparkhi 1996 to provide the tags ,0,1,0
every sentence was partofspeech tagged using a maximum entropy tagger ratnaparkhi 1996 and parsed using a stateoftheart wide coverage phrase structure parser collins 1999 ,0,1,0
31 partofspeech pos of neighboring words we use 7 features to encode this knowledge source a0a2a1a4a3a6a5a7a0a8a1a10a9a11a5a7a0a8a1a13a12a14a5a15a0a17a16a6a5a15a0a2a12a18a5a7a0a19a9a20a5a15a0a17a3 where a0a8a1 a21 a0 a21 is the pos of thea6 th token to the left right ofa0 and a0a17a16 is the pos of a0 a token can be a word or a punctuation symbol and each of these neighboring tokens must be in the same sentence asa0 we use a sentence segmentation program reynar and ratnaparkhi 1997 and a pos tagger ratnaparkhi 1996 to segment the tokens surroundinga0 into sentences and assign pos tags to these tokens ,0,1,0
we assign tags of partofspeech pos to the words with mxpost that adopts the penn treebank tag set ratnaparkhi 1996 ,0,1,0
direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machinelearning architectures such as the the maximumentropy tagger described by ratnaparkhi 1996 and the memorybased tagger mbt proposed by daelemans et al ,0,1,0
output sequence optimization rather than basing classifications only on model parameters estimated from cooccurrences between input and output symbols employed for maximizing the likelihood of pointwise singlelabel predictions at the output level classifier output may be augmented by an optimization over the output sequence as a whole using optimization techniques such as beam searching in the space of a conditional markov models output ratnaparkhi 1996 or hidden markov models skut and brants 1998 ,0,1,0
therefore the base forms have been introduced manually and the pos tags have been provided partly manually and partly automatically using a statistical maximumentropy based pos tagger similar to the one described in ratnaparkhi 1996 ,0,1,0
4 filtering with the cfg rule dictionary we use an idea that is similar to the method proposed by ratnaparkhi ratnaparkhi 1996 for partofspeech tagging ,0,1,0
pos tag the text using ratnaparkhi 1996 ,0,1,0
both charniak 2000 and bikel 2004 were trained using the goldstandard tags as this produced higher accuracy on the development set than using ratnaparkhi 1996s tags ,0,0,1
maxarg stpt t 1 then we assume that the tagging of one character is independent of each other and modify formula 1 as n i ii tttt nn tttt ctp ccctttpt n n 1 2121 maxarg maxarg 21 21 2 beam search n3 ratnaparkhi1996 is applied for tag sequence searching but we only search the valid sequences to ensure the validity of searching result ,0,1,0
we used the same 58 feature types as ratnaparkhi 1996 ,0,1,0
b medline dt jj vbn nns in dt nn nns vbp the oncogenic mutated forms of the ras proteins are rb jj cc vbp in jj nn nn constitutively active and interfere with normal signal transduction figure 1 part of speechtagged sentences from both corpora we investigate its use in part of speech pos tagging ratnaparkhi 1996 toutanova et al 2003 ,0,1,0
discriminative taggers and chunkers have been the stateoftheart for more than a decade ratnaparkhi 1996 sha and pereira 2003 ,1,0,0
partofspeech tags are assigned by the mxpost maximumentropy based partofspeech tagger ratnaparkhi 1996 ,0,1,0
2 method maximum entropy markov models memms ratnaparkhi 1996 and their extensions tutanova et al 2003 tsuruoka et al 2005 have been successfully applied to english pos tagging ,1,0,0
we use the same preprocessing steps as turian and melamed 2005 during both training and testing the parser is given text postagged by the tagger of ratnaparkhi 1996 with capitalization stripped and outermost punctuation removed ,0,1,0
step description mean stddev 15 sample 15s 007s 07 16 extraction 382s 013s 186 17 build tree 1276s 2760s 623 18 percolation 314s 491s 153 1911 leaf updates 62s 175s 30 1511 total 2049s 326s 1000 200410 the only one that we were able to train and test under exactly the same experimental conditions including the use of pos tags from ratnaparkhi 1996 ,0,1,0
the initial state contains terminal items whose labels are the pos tags given by ratnaparkhi 1996 ,0,1,0
so we pretagged the input to the bikel parser using the mxpost tagger ratnaparkhi 1996 ,0,1,0
more recent work has achieved stateoftheart results with maxi101 mum entropy conditional markov models maxent cmms or memms for short ratnaparkhi 1996 toutanova manning 2000 toutanova et al 2003 ,1,0,0
the supertagger uses a loglinear model to define a distribution over the lexical category set for each word and the previous two categories ratnaparkhi 1996 and the forward backward algorithm efficiently sums over all histories to give a distribution for each word ,0,1,0
for hw6 students compared their pos tagging results with the ones reported in ratnaparkhi 1996 ,0,1,0
for instance for maximum entropy i picked berger et al 1996 ratnaparkhi 1997 for the basic theory ratnaparkhi 1996 for an application pos tagging in this case and klein and manning 2003 for more advanced topics such as optimization and smoothing ,0,1,0
we tagged all the sentences in the training and devset3 using a maximum entropybased pos tagger mxpost ratnaparkhi 1996 trained on the penn english and chinese treebanks ,0,1,0
the features we used are as follows direct and inverse ibm model 3 4gram target language model 3 4 5gram pos language model ratnaparkhi 1996 schmid 1994 96 sentence length posterior probability zens and ney 2006 ngram posterior probabilities within the nbest list zens and ney 2006 minimum bayes risk probability length ratio between source and target sentence the weights are optimized via mert algorithm ,0,1,0
using an maximum entropy approach to pos tagging ratnaparkhi 1996 reports a tagging accuracy of 966 on the wall street journal ,0,1,0
ratnaparkhi 1996 a single inconsistency in a test set tree will very likely yield a zero percent parse accuracy for the particular test set sentence ,0,1,0
the maximum entropy models used here are similar in form to those in ratnaparkhi 1996 berger della pietra and della pietra 1996 lau rosenfeld and roukos 1993 ,0,1,0
the training samples are respectively used to create the models ptg pchunk pbuild and pcmeck all of which have the form k pa b ii ijob j 1 j1 where a is some action b is some context is a nor4 model categories description templates used tag see ratnaparkhi 1996 chunk chunkandpostagn build check chunkandpostagm n consn consre n consm np t punctuation checkconsn checkconsmn production surroundn the word pos tag and chunk tag of nth leaf ,0,1,0
models that can handle nonindependent lexical features have given very good results both for partofspeech and structural disambiguation ratnaparkhi 1996 ratnaparkhi 1997 ratnaparkhi 1998 ,1,0,0
its applications range from sentence boundary disambiguation reynar and ratnaparkhi 1997 to partofspeech tagging ratnaparkhi 1996 parsing ratnaparkhi 1997 and machine translation berger et al 1996 ,0,1,0
he has achieved stateofthe art results by applying me to parsing ratnaparkhi 1997a partofspeech tagging ratnaparkhi 1996 and sentenceboundary detection reynar and ratnaparkhi 1997 ,1,0,0
b brill and wu 1998 m magerman 1995 o our data r ratnaparkhi 1996 w weischedel and others 1993 ,0,1,0
the model we use is similar to that of ratnaparkhi 1996 ,0,1,0
in that table tbl stands for brills transformationbased errordriven tagget brill 1995 me stands for a tagger based on the maimum entropy modelling ratnaparkhi 1996 spatter stands for a statistical parser based on decision trees magerman 1996 igtree stands for the memorybased tagger by daelemans et al ,0,1,0
