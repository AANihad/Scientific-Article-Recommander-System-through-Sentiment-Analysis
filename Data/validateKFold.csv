CITATION,Positive,Objective,Negative
automatic text summarization approaches have offered reasonably wellperforming approximations for identifiying important sentences lin and hovy 2002 schiffman et al 2002 erkan and radev 2004 mihalcea and tarau 2004 daume iii and marcu 2006 but not surprisingly text regeneration has been a major challange despite some work on subsentential modification jing and mckeown 2000 knight and marcu 2000 barzilay and mckeown 2005 ,0,1,0
additionally some research has explored cutting and pasting segments of text from the full document to generate a summary jing and mckeown 2000 ,0,1,0
but in fact the issue of editing in text summarization has usually been neglected notable exceptions being the works by jing and mckeown 2000 and mani gates and bloedorn 1999 ,1,0,0
jing and mckeown 2000 and jing 2000 propose a cutandpaste strategy as a computational process of automatic abstracting and a sentence reduction strategy to produce concise sentences ,0,1,0
our work in sentence reformulation is different from cutandpaste summarization jing and mckeown 2000 in many ways ,0,1,0
jing and mckeown 2000 have proposed a rulebased algorithm for sentence combination but no results have been reported ,0,0,1
as previously observed in the literature mani gates and bloedorn 1999 jing and mckeown 2000 such components include a clause in the clause conjunction relative clauses and some elements within a clause such as adverbs and prepositions ,0,1,0
to contrast jing mckeown 2000 concentrated on analyzing humanwritten summaries in order to determine how professionals construct summaries ,0,1,0
first splitting and merging of sentences jing and mckeown 2000 which seems related to content planning and aggregation ,0,1,0
one of the applications is in automatic summarization in order to compress sentences extracted for the summary lin 2003 jing and mckeown 2000 ,0,1,0
in cutandpaste summarization jing and mckeown 2000 sentence combination operations were implemented manually following the study of a set of professionally written abstracts however the particular pasting operation presented here was not implemented ,0,1,0
close to the problem studied here is jing and mckeowns jing and mckeown 2000 cutandpaste method founded on endresniggemeyers observations ,0,1,0
jing and mckeown 1999 2000 found that human summarization can be traced back to six cutandpaste operations of a text and proposed a revision method consisting of sentence reduction and combination modules with a sentence extraction part ,0,1,0
the specialist minimal commitment parser relies on the specialist lexicon as well as the xerox stochastic tagger cutting et al 1992 ,0,1,0
recent comparisons of approaches that can be trained on corpora van halteren et al 1998 volk and schneider 1998 have shown that in most cases statistical aproaches cutting et al 1992 schmid 1995 ratnaparkhi 1996 yield better results than finitestate rulebased or memorybased taggers brill 1993 daelemans et al 1996 ,1,0,0
the two systems we use are engcg karlsson et al 1994 and the xerox tagger cutting et al 1992 ,0,1,0
22 xerox tagger the xerox tagger 1 xt cutting et al 1992 is a statistical tagger made by doug cutting julian kupiec jan pedersen and penelope sibun in xerox parc ,0,1,0
the xerox experiments cutting et al 1992 correspond to something between d1 and d2 and between to and t1 in that there is some initial biasing of the probabilities ,0,1,0
all 8907 articles were tagged by the xerox partofspeech tagger cutting et al 1992 4 ,0,1,0
for czech we created a prototype of the first step of this process the partofspeech pos tagger using rank xerox tools tapanainen 1995 cutting et al 1992 ,0,1,0
5 related work cutting introduced grouping of words into equivalence classes based on the set of possible tags to reduce the number of the parameters cutting et al 1992 schmid used tile equivaleuce classes for smoothing ,0,1,0
in this paper a new partofspeech tagging method hased on neural networks nettagger is presented and its performance is compared to that of a llmmtagger cutting et al 1992 and a trigrambased tagger kempe 1993 ,0,1,0
the performance of tle presented tagger is measured and compared to that of two other taggers cutting et al 1992 kempe 1993 ,0,1,0
no documentation of tile construction algorithm of the sulix lexicon in cutting et al 1992 was available ,0,1,0
it is a natural extension of the viterii algorithm church 188 cutting et al 1992 for those languages that do not have delimiters between words and it can generate nbest morphological analysis hypotheses like tree trellis search soong and luang 1991 ,0,1,0
derose 1988 cutting et al 1992 merialdo 1994 ,0,1,0
52 assigning complex ambiguity tags in the tagging literature eg cutting et al 1992 an ambiguity class is often composed of the set of every possible tag for a word ,0,1,0
the corpus lines retained are partofspeech tagged cutting et al 1992 ,0,1,0
no pretagged text is necessary for hidden markov models jelinek 1985 cutting et al 1991 kupiec 1992 ,0,1,0
we obtained 47025 50dimensional reduced vectors from the svd and clustered them into 200 classes using the fast clustering algorithm buckshot cutting et al 1992 group average agglomeration applied to a sample ,0,1,0
3 the statistical model we use the xerox partofspeech tagger cutting et al 1992 a statistical tagger made at the xerox palo alto research center ,0,1,0
this corpusbased information typically concerns sequences of 13 tags or words with some wellknown exceptions eg cutting et al 1992 ,0,1,0
157 ena or the linguists abstraction capabilities eg knowledge about what is relevant in the context they tend to reach a 9597 accuracy in the analysis of several languages in particular english marshall 1983 black et al 1992 church 1988 cutting et al 1992 de marcken 1990 derose 1988 hindle 1989 merialdo 1994 weischedel et al 1993 brill 1992 samuelsson 1994 eineborg and gambick 1994 etc ,0,1,0
on the other hand according to the datadriven approach a frequencybased language model is acquired from corpora and has the forms of ngrams church 1988 cutting et al 1992 rules hindle 1989 brill 1995 decision trees cardie 1994 daelemans et al 1996 or neural networks schmid 1994 ,0,1,0
pos disambiguation has usually been performed by statistical approaches mainly using the hidden markov model hmm in english research communities cutting et al 1992 kupiec 1992 weischedel et al 1993 ,0,1,0
the main application of these techniques to written input has been in the robust lexical tagging of corpora with partofspeech labels eg garside leech and sampson 1987 de rose 1988 meteer schwartz and weischedel 1991 cutting et al 1992 ,0,1,0
almost all recent work in developing automatically trained partofspeech taggers has been on further exploring markovmodel based tagging jelinek 1985 church 1988 derose 1988 demarcken 1990 merialdo 1994 cutting et al 1992 kupiec 1992 charniak et al 1993 weischedel et al 1993 schutze and singer 1994 ,0,1,0
as the baseline standard we took the endingguessing rule set supplied with the xerox tagger cutting et al 1992 ,0,1,0
the xerox tagger cutting et al 1992 comes with a set of rules that assign an unknown word a set of possible postags ie posclass on the basis of its ending segment ,0,1,0
in such cases additional information may be coded into the hmm model to achieve higher accuracy cutting et al 1992 ,0,1,0
stochastic models cutting et al 1992 dermatas et al 1995 brants 2000 have been widely used in pos tagging for simplicity and language independence of the models ,1,0,0
1 motivation statistical partofspeech disambiguation can be efficiently done with ngram models church 1988 cutting et al 1992 ,1,0,0
2000 that draws on a stochastic tagger see cutting et al 1992 for details as well as the specialist lexicon5 a large syntactic lexicon of both general and medical english that is distributed with the umls ,0,1,0
the prime public domain examples of such implementations include the trigramsntags tagger brandts 2000 xerox tagger cutting et al 1992 and lt pos tagger mikheev 1997 ,0,1,0
the initial phase relies on a parser that draws on the specialist lexicon mccray et al 1994 and the xerox partofspeech tagger cutting et al 1992 to produce an underspecified categorial analysis ,0,1,0
brills results demonstrate that this approach can outperform the hidden markov model approaches that are frequently used for partofspeech tagging jelinek 1985 church 1988 derose 1988 cutting et al 1992 weischedel et al 1993 as well as showing promise for other applications ,0,0,1
it is possible to use unsupervised learning to train stochastic taggers without the need for a manually annotated corpus by using the baumwelch algorithm baum 1972 jelinek 1985 cutting et al 1992 kupiec 1992 elworthy 1994 merialdo 1995 ,0,1,0
this method is employed in kupiec 1992 cutting et al 1992 ,0,1,0
almost all of the work in the area of automatically trained taggers has explored markovmodel based part of speech tagging jelinek 1985 church 1988 derose 1988 demarcken 1990 cutting et al 1992 kupiec 1992 charniak et al 1993 weischedel et al 1993 schutze and singer 1994 lin et al 1994 elworthy 1994 merialdo 1995 ,0,1,0
1 introduction in the partofspeech hterature whether taggers are based on a rulebased approach klein and simmons 1963 brill 1992 voutilainen 1993 or on a statistical one bahl and mercer 1976 leech et al 1983 merialdo 1994 derose 1988 church 1989 cutting et al 1992 there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones ,0,1,0
chanod and tapanainen 1995 compare two tagging frameworks for tagging french one that is statistical built upon the xerox tagger cutting et al 1992 and another based on linguistic constraints only ,0,1,0
generalized forward backward reestimation generalization of the forward and viterbi algorithm in english part of speech taggers the maximization of equation 1 to get the most likely tag sequence is accomplished by the viterbi algorithm church 1988 and the maximum likelihood estimates of the parameters of equation 2 are obtained from untagged corpus by the forwardbackward algorithm cutting et al 1992 ,0,1,0
cutting et al 1992 feldweg 1995 the tagger for grammatical functions works with lexical and contextual probability measures pq ,0,1,0
in our experiments we used the hidden markov model hmm tagging method described in cutting et al 1992 ,0,1,0
the pos disambiguation has usually been performed by statistical approaches mainly using hidden markov model hmm cutting et al 1992 kupiec ,0,1,0
22 stt a statistical treebased tagger the aim of statistical or probabilistic tagging church 1988 cutting et al 1992 is to assign the most likely sequence of tags given the observed sequence of words ,0,1,0
roughly in keeping with rapp 2002 we hereby regard paradigmatic assocations as those based largely on word similarity ie including those typically classed as synonyms antonyms hypernyms hyponyms etc whereas syntagmatic associations are all those words which strongly invoke one another yet which cannot readily be said to be similar ,0,1,0
while choosing an optimum window size for an application is often subject to trial and error there are some generally recognized tradeoffs between small versus large windows such as the impact of datasparseness and the nature of the associations retrieved church and hanks 1989 church and hanks 1991 rapp 2002 measures based on distance between words in the text ,0,1,0
at the present time given the key role of window size in determining the selection and apparent strength of associations under the conventional cooccurrence model highlighted here and in the works of church et al 1991 rapp 2002 wang 2005 and schulte im walde melinger 2008 we would urge that this is an issue which windowdriven studies continue to conscientiously address at the very least scale is a parameter which findings dependent on distributional phenomena must be qualified in light of ,0,1,0
rapp 2002 calls this tradeoff specificity equivalent observations were made by church hanks 1989 and church et al 1991 who refer to the tendency for large windows to wash out smear or defocus those associations exhibited at smaller scales ,0,1,0
21 scaledependence it has been shown that varying the size of the context considered for a word can impact upon the performance of applications rapp 2002 yarowsky florian 2002 there being no ideal window size for all applications ,0,1,0
22 data sparseness another facet of the general tradeoff identified by rapp 2002 pertains to how limitations in862 herent in the combination of data and cooccurrence retrieval method are manifest ,0,1,0
whereas until recently the focus of research had been on sense disambiguation papers like pantel lin 2002 neill 2002 and rapp 2003 give evidence that sense induction now also attracts attention ,0,1,0
3 algorithm as in previous work rapp 2002 our computations are based on a partially lemmatized version of the british national corpus bnc which has the function words removed ,0,1,0
for example it has been used to measure centrality in hyperlinked web pages networks brin and page 1998 kleinberg 1998 lexical networks erkan and radev 2004 mihalcea and tarau 2004 kurland and lee 2005 kurland and lee 2006 and semantic networks mihalcea et al 2004 ,0,1,0
our method is based on the ones described in erkan and radev 2004 mihalcea and tarau 2004 fader et al 2007 the objective of this paper is to dynamically rank speakers or participants in a discussion ,0,1,0
eigenvector centrality in particular has been successfully applied to many different types of networks including hyperlinked web pages brin and page 1998 kleinberg 1998 lexical networks erkan and radev 2004 mihalcea and tarau 2004 kurland and lee 2005 kurland and lee 2006 and semantic networks mihalcea et al 2004 ,1,0,0
still it is in our next plans and part of our future work to embed in our model some of the interesting wsd approaches like knowledgebased sinha and mihalcea 2007 brody et al 2006 corpusbased mihalcea and csomai 2005 mccarthy et al 2004 or combinations with very high accuracy montoyo et al 2005 ,0,1,0
417 structure of semantic networks was proposed in mihalcea et al 2004 with a disambiguation accuracy of 509 measured on all the words in the senseval2 data set ,0,1,0
previous approaches include supervised learning hirao et al 2002 teufel and moens 1997 vectorial similarity computed between an initial abstract and sentences in the given document intradocument similarities salton et al 1997 or graph algorithms mihalcea and tarau 2004 erkan and radev 2004 wolf and gibson 2004 ,0,1,0
inspired by the idea of graph based algorithms to collectively rank and select the best candidate research efforts in the natural language community have applied graphbased approaches on keyword selection mihalcea and tarau 2004 text summarization erkan and radev 2004 mihalcea 2004 word sense disambiguation mihalcea et al 2004 mihalcea 2005 sentiment analysis pang and lee 2004 and sentence retrieval for question answering otterbacher et al 2005 ,0,1,0
such textoriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases to extractive summarization and word sense disambiguation mihalcea et al 2004 ,0,1,0
using dictionaries as network of lexical items or senses has been quite popular for word sense disambiguation veronis and ide 1990 hkozima and furugori 1993 niwa and nitta 1994 before losing ground to statistical approaches even though gaume et al 2004 mihalcea et al 2004 tried a revival of such methods ,0,1,0
he et al 2008 ,0,1,0
due to the lack of a good arabic parser compatible with the sakhr tokenization that we used on the source side we did not test the source dependency lm for arabictoenglish mt when extracting rules with source dependency structures we applied the same wellformedness constraint on the source side as we did on the target side using a procedure described by shen et al 2008 ,0,1,0
in post and gildea 2008 shen et al 2008 target trees were employed to improve the scoring of translation theories ,0,1,0
a few studies carpuat and wu 2007 ittycheriah and roukos 2007 he et al 2008 hasan et al 2008 addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence ,0,1,0
the other approach is to estimate a single score or likelihood of a translation with rich features for example with the maximum entropy maxent method as in carpuat and wu 2007 ittycheriah and roukos 2007 he et al 2008 ,0,1,0
in he et al 2008 lexical 72 features were limited on each single side due to the feature space problem ,0,1,0
similar ideas were explored in he et al 2008 ,0,1,0
2 linguistic and context features 21 nonterminal labels in the original stringtodependency model shen et al 2008 a translation rule is composed of a string of words and nonterminals on the source side and a wellformed dependency structure on the target side ,0,1,0
lopez 2008 explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrasebased systems ,0,1,0
first such a system makes use of lexical information when modeling reordering lopez 2008 which has previously been shown to be useful in germantoenglish translation koehn et al 2008 ,1,0,0
lopez 2008b gives indirect experimental evidence that this difference affects performance ,0,1,0
our hierarchical system is hiero chiang 2007 modified to construct rules from a small sample of occurrences of each source phrase in training as described by lopez 2008b ,0,1,0
macken et al 2008 showed that the results for frenchenglish were competitive to stateoftheart alignment systems ,1,0,0
then the same system weights are applied to both inchmm and joint decoding based approaches and the feature weights of them are trained using the maxbleu training method proposed by och 2003 and refined by moore and quirk 2008 ,1,0,0
previouswork eg moore and quirk 2008 cer et al 2008 has focusedonimprovingtheperformanceofpowells algorithm ,0,1,0
moore and quirk 2008 share the goal underlying our own research improving rather than replacing ochs mert procedure ,0,1,0
turney 2008 has recently proposed a simpler svmbased algorithm for analogical classification called pairclass ,0,1,0
in table 6 we report our results together with the stateoftheart from the acl wiki5 and the scores of turney 2008 pairclass and from amac herdagdelens pairspace system that was trained on ukwac ,0,1,0
turney 2008 is the first to the best of our knowledge to raise the issue of a unified approach ,0,1,0
the algorithm proposed by turney 2008 is labeled as turneypairclass ,0,1,0
building on a recent proposal in this direction by turney 2008 we propose a generic method of this sort and we test it on a set of unrelated tasks reporting good performance across the board with very little taskspecific tweaking ,0,1,0
this unfortunately significantly jeopardizes performance koehn et al 2003 xiong et al 2008 because by integrating syntactic constraint into decoding as a hard constraint it simply prohibits any other useful nonsyntactic translations which violate constituent boundaries ,0,1,0
the machine translation literature is littered with various attempts to learn a phrasebased string transducer directly from aligned sentence pairs doing away with the separate word alignment step marcu and wong 2002 cherry and lin 2007 zhang et al 2008b blunsom et al 2008 ,0,1,0
the sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions eg constraints drawn from a wordalignment cherry and lin 2007 zhang et al 2008b or a grammar fixed a priori blunsom et al 1f and e are the input and output sentences respectively ,0,1,0
other linear time algorithms for rank reduction are found in the literature zhang et al 2008 but they are restricted to the case of synchronous contextfree grammars a strict subclass of the lcfrs with f 2 ,0,1,0
in the smt research community the second step has been well studied and many methods have been proposed to speed up the decoding process such as nodebased or spanbased beam search with different pruning strategies liu et al 2006 zhang et al 2008a 2008b and cube pruning huang and chiang 2007 mi et al 2008 ,0,1,0
31 exhaustive search by tree fragments this method generates all possible tree fragments rooted by each node in the source parse tree or forest and then matches all the generated tree fragments against the source parts left hand side of translation rules to extract the useful rules zhang et al 2008a ,0,1,0
1 introduction recently linguisticallymotivated syntaxbased translation method has achieved great success in statistical machine translation smt galley et al 2004 liu et al 2006 2007 zhang et al 2007 2008a mi et al 2008 mi and huang 2008 zhang et al 2009 ,1,0,0
motivated by the fact that nonsyntactic phrases make nontrivial contribution to phrasebased smt the tree sequencebased translation model is proposed liu et al 2007 zhang et al 2008a that uses tree sequence as the basic translation unit rather than using single subtree as in the stsg ,0,1,0
2008a propose a tree sequencebased tree to tree translation model and zhang et al ,0,1,0
a tree sequence to string rule 174 a treesequence to string translation rule in a forest is a triple l r a where l is the tree sequence in source language r is the string containing words and variables in target language and a is the alignment between the leaf nodes of l and r this definition is similar to that of liu et al 2007 zhang et al 2008a except our treesequence is defined in forest ,0,1,0
zollmann et al 2008 ,0,1,0
this is in direct contrast to recent reported results in which other filtering strategies lead to degraded performance shen et al 2008 zollmann et al 2008 ,0,1,0
extensions to hiero several authors describe extensions to hiero to incorporate additional syntactic information zollmann and venugopal 2006 zhang and gildea 2006 shen et al 2008 marton and resnik 2008 or to combine it with discriminative latent models blunsom et al 2008 ,0,1,0
the fluency models hold promise for actual improvements in machine translation output quality zwarts and dras 2008 ,1,0,0
dolan 1994 and krovetz and croft 1992 claim that finegrained semantic distinctions are unlikely to be of practical value for many applications ,0,1,0
much work has gone into methods for measuring synset similarity early work in this direction includes dolan 1994 which attempted to discover sense similarities between dictionary senses ,0,1,0
recognizing this dolan 1994 proposes a method for ambiguating dictionary senses by combining them to create grosser sense distinctions ,0,1,0
82 chen and chang topical clustering dolan 1994 maintains the position that intersense relations are mostly idiosyncratical thereby making it difficult to characterize them in a general way so as to identify them ,0,1,0
as noted in dolan 1994 it is possible to run a senseclustering algorithm on several mrds to build an integrated lexical database with more complete coverage of word senses ,0,1,0
these relations are then used for various tasks ranging from the interpretation of a noun sequence vanderwende 1994 or a prepositional phrase ravin 1990 to resolving structural ambiguity jenson and binot 1987 to merging dictionary senses for wsd dolan 1994 ,0,1,0
5 related work dolan 1994 describes a method for clustering word senses with the use of information provided in the electronic version of ldoce textual definitions semantic relations domain labels etc ,0,1,0
this approach took inspiration from the pioneering work by dolan 1994 but it is also fundamentally different because instead of grouping similar senses together the corelex approach groups together words according to all of their senses ,1,0,0
there is also work on grouping senses of other inventories using information in the inventory dolan 1994 along with information retrieval techniques chen and chang 1998 ,0,1,0
zero derivation dolan 1994 pointed out that it is helpful to identify zeroderived nounverb pairs for such tasks as normalization of the semantics of expressions that are only superficially different ,0,1,0
dolan 1994 observed that sense division in mrd is frequently too free for the purpose of wsd ,0,1,0
towards a meaningfull comparison of lexieal resources kenneth c lltkowska cl research 9208 gue road damascus md 20872 kenclres corn httpwww tires tom abstract the mapping from wordnet to hector senses m senseval provides a gold standard against wluch to judge our ability to compare lexlcal resources the gold standard is provided through a word overlap analysis with and without a stop list for flus mapping achieving at most a 36 percent correct mapping inflated by 9 percent from empty assignments an alternauve componenttal analysis of the defimtaons using syntacuc collocatmnal and semantac component and relation identification through the use ofdefimng patterns integrated seamlessly mto the parsing thclaonary provides an almost 41 percent correct mapping with an additaonal 4 percent by recogmzmg semantic components not used in the senseval mapping defimtion sets of the senseval words from three pubhshed thclaonanes and dorrs lextcal knowledge base were added to wordnet and the hector database to exanune the nature of the mapping process between defimtton sets of more and less scoe the tecbauques described here consutute only an maaal implementation of the componenual analysis approach and suggests that considerable further improvements can be aclueved introduction the difficulty of companng lemcal resources long a sgnfficant challenge in computauonal hnguistlcs atlans 1991 came to the fore in the recent senseval competatton iolgarnff 1998 when some systems that relied heavily on the wordnet miller et al 1990 sense inventory were faced with the necessity of using another sense inventory hecto0 a hasty solutaon to the problem was the development of a map between the two inventories but some partcipants expressed concerns that use of flus map may have degraded their performance to an unknown degree although there were disclaimers about the wordnethector map it nonetheless stands as a usable gold standard for efforts to compare lexical resources moreover we have a usable baseline a word overlap method suggested m lesk 1986 against which to compare whether we are able to make improvements m the mapping since flus method has been shown to perform not as well as expected krovetz 1992 we first describe the lextcal resources used m the study hector wordnet other dicuonanes and a lexcal knowledge base first characterizing them in terms ofpolysemy and the types of leracal mformauon each contmns syntacuc properties and features semantac components and relauons and collocauonal properties we then present results of perfornung the word overlap analysis of the 18 verbs used m senseval analyzing the definitions m wordnet and hector we then expand our analysis to include other dictionaries we describe our methods of analysis particularly the methods of parsing defimtaons and identffqng semantic relations semrels based on defimng patterns essentially takang first steps m implementing the program described by atkms and focusmg on the use ofmeamng full mformataon rather than statistical mformauon we identify the results that have been achieved thus far and outline further steps that may add more meanmg to the analysis iall analyses described m this paper were performed automatically using functlonahty incorporated m dimap dictionary maintenance programs available for immediate download at cl research 1999a this includes automatac extracuon of wordnet reformation for the selected words mtegrated m dimap hector defimtlons were uploaded into dimap dicuonanes after use of a conversmn program defimtlons for other 30 the lexical resources tlus analysis focuses on the mmn verb senses used in senseval not ichoms and phrases specifically the followmg amaze band bet bother bury calculate consume derive float hurdle invade promise sack sanction scrap seize shake slight the hector database used in senseval consists of a tree of senses each of which contains defimttons syntactic properties example usages and clues collocational information about the syntactic and semantic enwronment in wluch a word appears in the spectfic sense the wordnet database contmns synonyms synsets perhaps a defimtton or example usages gloss some syntactic mformauon verb frames hypernyms hyponyms and some other semrels entails causes to extend our analysis in order to look at other issues of lexacal resource comparison we have included the defirauons or leracal information from the following additional sources websters 3 ra new international dictionary w3 oxford advanced learners dctlonary oald american hentage dlcuonary aiid dorrs lexacal knowledge base dorr we used only the defimuons from w3 oald and ahd which also contmn sample usages and some collocattonal information m the form of usage notes not used at the present tame dorrs database contains thematic grids wluch characterize the thematic roles of obligatory and optional semanuc components frequently identifying accompanying preposmons olsen et al 1998 the following table identities the number of senses and average overall polysemy for each of these resources dictionaries were entered by hand word amaze band bet bother bury calculate consume denve float hurdle invade pronuse sack sanction scrap seize shake shght average polysemy o o o 1 2 4 2 3 1 ii 4 4 2 5 5 7 6 9 7 12 6 14 5 5 5 10 9 6 6 8 8 6 5 15 5 16 4 41 14 2 1 4 3 6 2 10 5 5 4 7 4 4 4 6 3 2 2 5 2 3 1 3 3 11 6 21 13 8 8 37 17 1 1 6 3 o 1 2 2 4 1 3 4 4 8 1 3 1 3 1 3 2 10 5 1 0 3 1 3 2 2 0 1 1 1 0 7 1 7 12 i 0 57 37 120 62 34 22 word overlap analysis we first estabhsh a baseline for automatic replication of the lexicographers mappmg from wordnet 1 6 to hector using a smple word overlap analysis smular to lesk 1986 the lextcographer mapped the 66 wordnet senses each synset m which a test occurred into 102 hector senses a total of 86 assignments were made 9 wordnet senses were gwen no assignments 40 recewed exactly one and 17 senses received 2 or 3 asssgnments the wordnet senses contained 348 words about half of wluch were common words appeanng on our stop list which contained 165 words mostly preposmons pronouns and conjunctions the hector senses selected m the word overlap analysis contained about 960 words all hector senses contained 1878 words we performed a strict word overlap analysts with and wsthout a stop hst between tile definluons in wordnet and the hector senses that is we did not attempt to ldenttfy root forms of inflected words we took each word m a wordnet sense and determined whether t appeared in a hector sense we selected a hector sense based on the highest percentage of words over all hector senses an 31 empty selection was made ff all the words in the wordnet sense did not appear in any hector sense only content words were considered when the stop hst was used for example for bet wordnet sense 2 stake money on the outcome of an issue mapped into hector sense 4 of a person to risk a sum of money or property m thts way in this case there was an overlap on two words money 039 in the hector defimtlon 0 13 of its 15 words without the stop list when the stop list was invoked there was an overlap of only one word money 0 07 of the hector defimtion in this case the lexicographer had made three assignments hector senses 2 3 and 4 our scoring method treated flus as only 1 out of 3 correct not using the relaxed method employed in senseval of treating flus as completely correct without the stop hst our selections matched the lexicographers in 28 of 86 cases 32 6 using the stop list we were successful in 31 of 86 cases 36 1 the improvement arising when the stop list was used is deceptive where 8 cases were due to empty assignments so that only 23 cases 26 7 were due to matching content words overall only 41 content words were involved in these 23 successes when the stop list was used an average of i 8 content words to summanze the word overlap analysis 1 despite a ncher set of defimtions in hector 9 of 66 wordnet senses 13 6 could not be assigned 2 despite the greater detail in hector senses compared to wordnet senses 2 8 times as many words only 1 8 content words participated in the assignments and 3 therefore the defimng vocabulary between these two definition sets seems to be somewhat divergent although it might appear as if the word overlap analysis does not perform well this is not the case the analysis provides a broad overview of the defimuon companson process between two definmon sets and frames a deeper analysis of the differences moreover it appears that the accuracy of a gold standard mapping is not crucially important the quality of the mapping may help frame the subsequent analysis more precisely but it seems sufficient that any reasonable mapping will suffice this will be discussed further after presenting the results of the componentlal analysis of the defimtlons 32 meaningfull analysis of definitions the deeper analysis of the mapping between two defimtion sets relies primarily on two major steps 1 parsing definitions and using defimng patterns to identify semrels present m the definitions and 2 relaxing values to these relations by allowing synonymic substitution using wordnet thus for example ffwe identify hypernyms or instruments from parsing a defimtion we would say that the defimtions are equal not just ffthe hypernym or instrument is the same word but also lf the hypernyms or instruments are members of the same synset this approach is based on the finding litkowski 1978 that a dictionary induces a semantic network where nodes represent concepts that may be lexicahzed and verbalized in more than one way this finding implies in general the absence of true synonyms and instead the kind of concept embodied in wordnet synsets with several lexical items and phraseologles a slmdar approach parsing defimtlons and relaxing semrel values was followed in dolan 1994 for clnstenng related senses wthin a single dictionary the ideal toward which this approach strives is a complete identification of the meamng components included in a defimtion the meaning components can include syntactic features and charactenstlcs including subcategonzation patterns semantm components realized through identification of semrels selectional restrictions and couocational specifications the first stage of the analysis parses the definitions cl research 1999b litkowski to appear and uses the parse results to extract via defining patterns semrels since definitions have many idiosyncrasies that do not follow ordinary text an important first step in this stage is preprocessmg the definition text to put it into a sentence frame that facilitates the extraction of semrels 2 2note that the stop hst is not applicable to the definition parsing the parser is a fullscale sentence parser where prepositmns and other words on the stop list are necessary for successful parsing moreover inclusion of the prepositions is cmcml to the method since they are the bearers of much semrel information the extractmn of semrels examines the parse results a e a tree whose mtermedaate nodes represent nonternunals and whose leaves represent the lextcal atems that compnse the defimuons where any node may also include annotations such as characterizations of number and tense for all noun or verb defimttons flus includes identification of the head noun with recogmtton ofempty heads or verb for verbs we signal whether the defimtaon contmned any selecttonal restnctmus that as pamcular parenthesazed expressaons for the subject and object we then exanune preposattonal phrases in the defimuon and deterrmne whether we have a defining pattern for the preposauon whach we can use as mdacauve of a partacular semrel we also identify adverbs m the parse tree and look these up in wordnet to adentffy an adjecuve synset from wluch they are derived if one is gwen the defimng pattems are actually part of the dictionary used by the parser that is we do not have to develop specafic routines to look for speclfic patterns a defimng pattern s a regular expressaon that arlaculates a syntactac pattern to be matched thus to recograze a manner semrel we have the fouowmg entry for m mdpat rep0 ldet0 adj manner0 stmanner this allows us to recognize m as possibly gwmg rise to a manner component where we recogmze m the tdde which allows us to specify partacular elements before the m as well vath a noun phrase that consasts of 0 or 1 determiner an adjectwe and the lateral manner the 0 after the detenmner and the hteral mdacate that these words are not copied into the value for a manner role so that the value to the manner semrel becomes only the adjectwe that as recogmzed the second stage of the analysis uses the populated lexacal database to compare senses and make the selectaons this process follows the general methodology used m senseval lltkowska to appear specifically m the defimtaon comparison we first exanune exclusaon cntena to rule out specific mappings these criteria include syntacuc properues e g a verb sense that is only transluve cannot map into one that is only mtransrave and collocataonal propertaes e g a sense that is used with a parucle cannot map into one that uses a different particle at the present tune these are used only rmmmally 33 we next score each viable sense based on rots semrels we increment the score ff the senses have a common hypernym or if a senses hypernyms belong to the same synset as the other senses hypernyms if a parucular sense conns a large number of synonyms that as no differentiae on the hypernym and they overlap consaderably m the synsets they evoke the score can be increased substanually currently we add 5 points for each match 3 we increment the score based on common semrels in tins amtml tmplementauon we have defimng patterns usually qmte nummal for recogmzmg instrument means location purpose source manner hasconstituents hasmembers ispartof locale and goal 4 we increment the score by 2 points when we have a common semrel and then by another 5 points when the value is dentacal or m the same synset after all possable increments to the scores have been made we then select the senses wth the lughest score finally we compare our selecuon with that of the gold standard to assess our mapping over all senses another way an wluch our methodology follows the senseval process as that at proceeds incrementally thus t ms not necessary to have a final perfect parse and mapping rouune we can make conunual refinements at any stage of the process and exarmne the overall effect as m senseval we may make changes to deal wath a particular phenomenon with the result that overall performance dechnes but wth a sounder basis for making subsequent amprovements results of componential analysis the gold standard analysis involves mapping 66 wordnet senses with 348 words into 102 hector senses with 1878 words using the method described above we obtained 35 out of 86 correct 3at the present tame we use wordnet to adentffy semreis we envaslon usmg the full semanlac network created by parsing all a dlcuonarys defimtaons thas would include a richer set of semrels than currently included m wordnet 4the defimng patterns are developed by hand we have onlyjust begun this effort so the current set ms somewhat impoverished mappmgs 407 a shght improvement over the 31 correct assignments usmg the stoplast word overlap techmque however as mentioned above the stophst techmque had aclueved 8 of its successes by matclung null assignments consadered on tlus basins t seems that the componentaal analysis techmque provides substantial mprovement in addition our technique erred on 4 cases by malang assagnments where none were made by the leracographer we suggest that these cases do conn some common elements of meaning and may conceivably not be construed as errors the mapping from wordnet to hector had relatavely few empty mappings senses for wtuch it was not possable to make an assignment these are the cases where at appears that the chetmnanes do not overlap and thus prowde a tentative mdacataon of where two dictionaries may have different coverage the cases of multiple assignments mchcate the degree ofamblgmty m the mapping the average m both darecuons between hector and wordnet were donunated by the mabdaty to obtain good dascnnunatton for the word semze thus tlus method identifies individual words where the scnnunatwe ablhty needs to be further refined perhaps more importantly the componentml analysis method exploits consaderably more wordnet hector mformauon than the word overlap methods whereas the stophst word overlap mapping was based on only 41 content words the componenual approach in the selected mappings had 228 hits in developing ats scores with only a small number of defining patterns comparison of dictionaries tel o 3 03 we next exanuned the nature of the mterrelalaons between parrs of chctaonanes wthout use of a gold standard to assess the process of mapping for tus purpose we mapped m both recttons between the paars wordnet hector w3 oald and w3 ahd we exanune dorrs lexacal knowledge base for the amphcatlons it may have m the mapping process neither wordnet nor hector are properly vewed as chcuonanes since there was no mtenuon to pubhsh them as such wordnet glosses are generally smaller 53 words per sense compared to hector 184 words per sense whach contains many words specff3nng selectmnal restnctons on the subject and object of the verbs hector was used primarily for a largescale sense tagging project the three formal dctmnanes were subject to rigorous pubhslung and style standards the average number of words per sense were 87 oald 7 1 ahd and 9 9 w3 wth an average of 3 4 62 and 120 senses per word each table shows the average number of senses being mapped the average number of assignments m the target dlctmnary the average number of senses for which no assagnment could be made the average number of muluple assignments per word and the average score of the assignments that were made wnhector 37 47 06 17 119 hectorwn 57 64 14 22 113 these points are further emphasized m the mapping between w3 and oald where the disparity between the empty and muluple assagnments indicate that we are mapping between dictionaries qmte disparate this tends to be the case not only for the enure set of words but also is evident for individual words where there is a considerable dspanty m the number of senses wtuch then dominate the overall dlspanty thus for example w3 has 41 defimuons for float while oald has 10 we tend to be unable to find the specific sense m going from w3 to oald because at is likely that we have many more specific defimtlons that are not present in the other direction we are hkely to have considerable ambiguity and multiple assignments w3oald oaldw3 w3 oald 120 78 60 18 99 34 60 07 32 86 34 a between w3 and ahd there ss less overall daspanty between the defimtaon sets although since w3 is tmabndged we stall have a relatavely lugh number of senses m w3 that do not appear to be present m ahd finally it should be noted that the scores for the published dictaonanes tend to be a little lower than for wordnet and hector tlus reflects the hkehhood that we have not extracted as much mformataon as we dad m parsing and analyzmg the defimtaon sets used m senseval w3 ahd oj q o w3ahd 120 115 40 36 90 ahdw3 6 2 9 1 1 2 4 1 9 1 we next considered dorrs lexacal database we first transformed her theta grids to syntactic spectflcataons transttave or lntransmttve and identtficataon of semreis e g where she identified an instr component we added such a semrel to the dimap sense we were able to identify a mappmg from wordnet to her senses for two words float and shake for wluch dorr has several entries however smce she has considerably more semanuc components than we are currently able to recogmze we dad not pursue this avenue any further at flus time more important than just mappmg between two words dorrs data mdacates the posstbday of further exploitation of a richer set of semanuc components spectfically as reported m olsen et al 1998 m descnbmg procedures for automatically acqumng thematic grids for mandann chinese t was noted that verbs that incorporate themauc elements m their meamng would not allow that element to appear m the complement structure thus by usmg dorrs thematic grids when verb are parsed m defimtaons it s possible to dentffy where partacular semantac components are lexicahzed and which others are transnutted through to the themauc grid complement or subcategonzataon pattern for the defimendum the transmisson of semantic components to the thematic gnd s also reflected overtly m many defimtlons for example shake has one definition to bnng to a specified condatton by or as ffby repeated qmck jerky movements we would thus expect that the thematac grid for this defimtaon should include a goal and deed dorrs database has two senses whch reqmre a goal as part of their thematic grid smularly for many defimtaons m the sample set we dentlfied a source defimng pattern based on the word from frequently the object of the preposmon was the word source ttseff mdacatmg that the subcategonzauon properties of the defimendum should elude a source component discussion wlule the improvement m mapping by using the componentaal analysis techmque over the word overlap methods is modest we consider these results qmte slgmficant m wew of the very small number of defimng patterns we have implemented most of the improvement stems from the word substatuuon pnnclple described earlier as ewdenced by the preponderance of 5 point scores this techmque also provides a mechamsm for bnngmg back the stop words wz the preposmons wluch are the careers of mformatmn about semrels the 2 point scores the more general conclusion from the word subsutuuon is that the success arises from no longer considenng a defimtmn m solation the proper context for a word and its defimtions consists not lust of the words that make up the definition but also the total semantac network represented by the dictaonary we have aclueved our results by explomng only a small part of that network we have moved only a few steps to that network beyond the mdawdual words and their definitions we would expect that further expansmn first by the addon of further and mproved semrel defining patterns and second through the identaficataon of more pnmmve semanuc components will add considerably to our abflay to map between lexacal resources we also expect mprovements from consideration of other techniques such as attempts at ontology ahgnment hovy 1998 although tile definition analysis provlded here was performed on definmons with a stogie language the vanous meamng components m m m m m m m m 35 correspond to those used in an interhngua the use of the exuncuon method developed m order to charactenze verbs m another language clunese can frmtfully be applied here as well two further observauons about tlus process can be made the first is that rchance on a wellestablished semantic network such as wordnets not necessary the componenual analysis method rehes on the local neighborhood of words m the defimuons not on the completeness of the network indeed the network tsel can be bootstrapped based on the parsing results the method can work vath any semanuc network or ontology and may be used to refine or flesh out the network or ontology the second observation is that it is not necessary to have a wellestabhshed gold standard any mapping vail do all that is necessary is for any mvesugator lemcographer or not to create a judgmental mappmg the methods employed here can then quanufy ttus mapping based on a word overlap analysis and then further examine tt based on the componenaal analysis the componenual analysis method can then be used to exanune underlying subtleues and nuances tn the defimuous wluch a lemcographer or analyst can then examine m further detail to assess the mapping future work tlus work has marked the first ume that all the necessary mfrastructure has been combmed tn a rudimentary form because of its rudimentary status the opportumues for improvement are quite extensive in addluon there are many opportumues for using the techmques descnbed here m further nlp apphcatlons first the techmques described here have immediate apphcabtllty as part of a lexicographers workstauon when defimuons are parsed and semrels are zdenttfied the resulung data structures can be apphed against a corpus of instances for parucular words as m senseval for improving wordsense disamblguauon the techmques will also permit comparing an entry vath itself to deternune the mterrelattonshtps among ts defimuons and of companng the defimuons of two synonyms to deternune the amount of overlap between them on a defimtlon by defimuon bass although the analyss here has focused on the parsing of defimuous the development of defimng patterns clearly extends to generalized text parsing since the defimng patterns have been incorporated mto the same chcttonary used for parsing free text the patterns can be used threctly to identify the presence of parucular semrels among sentenual consutuents we are working to integrate ths funcuonahty into our wordsense sambiguauon techruques both the defimng patterns and the semrels even further mt seems that matclung defimng patterns in free text can be used for lextcal acquisition textual matenal that contains these patterns could concewably be flagged as providing defimuonal matenal which can then be compared to emstmg defimuons to assess whether their use ts cousstent vath these defimuons and ff not at least to flag the inconsistency the tecluuques descnbed here can be apphed directly to the fields of ontology development and analysis of ternunologlcal databases for ontoiogles vath or wthout defimuons the methods employed can be used to compare entries m daierent ontologles based pnmanly on the relattous m the ontology both luerarclucal and other for ternunologlcal databases the methods descnbed here can be used to exanune the set of conceptual relauons lmphed by the defimtmus the defimuon parsing wall facdtate the development of the termmologca i network tn the pamcular field covered by the database the componenual analysts methods result m a richer semantic network that can be used m other apphcattous thus for example t ts possible to extend the leracal chatmng methods described m green 1997 which are based on the semrels used m wordnet the semrels developed with the componenttal analysis method would provide additional detad available for apphcauon of lexlcal cohesion methods in particular addtuonal relattous would penmt some structunng wmthm the individual leracal chams rather than just consldenng each cham as an amorphous set green 1999 finally we are currently investigating the use of the componenual analysts techmque for mformauon extracuon the techmque identifies from defimtlous slots that can be used as slots or fields m template generataon once these slots are identified we wall be attemptmg to extract slot values from items m large catalog databases mdhons of items 36 in conclusion it would seem that instead of a paucity of tnformation allovang us to compare lexmal resources by bnngmg m the full semantic network of the lexicon we are overwhelmed with a plethora of data acknowledgments i would like to thank bonnie dorr chnstiane fellbaum steve green ed hovy ramesh knshnamurthy bob krovetz thomas potter lucy vanderwende and an anonymous reviewer for their comments on an earlier draft of this paper references atlans b t s 1991 bmldmga lexicon the contribution of lexicography lnternattonal journal of lextcography 43 167204 cl research 1999a cl research demos httpwww clres comdemo html cl research 1999b dmtlonary parsing project httpwww clres comdpp html dolan w b 1994 59 aug word sense amblguation chistenng related senses coling94 the 15th international conference on computational linguistics kyoto japan green s j 1997 automatically generating hypertext by computing semantic smulanty dlss toronto canada umverstty of toronto green s j sjgreenmn mq edu au 1999 1 june rich semantic networks hovy e 1998 may combining and standardizing largescale practical ontologms for machine translation and other uses language resources and evaluation conference granada spam kalgarnff a 1998 senseval home page httpwww itn bton ac ukeventssenseval krovetz r 1992 june senselinking m a machine readable dictionary 30th annual meeting of the association for computational lmgustics newark delaware association for computational lmgtustics lesk m 1986 automatic sense dlsamblguation using machine readable dmttonanes how to tell a pine cone from an ice cream cone proceechngs of sigdoc lttkowski k c 1978 models of the semantic structure of dictionaries american journal of computattonal lmgutsttcs atf 81 2574 lttkowskl k c to appear senseval the cl research expenence computers and the humamttes mtller g a beckwlth r fellbaum c gross d miller k j 1990 introduction to wordnet an onhne lexical database lnternatwnal journal of lexicography 34 235244 olsen m b dorr b j thomas s c 1998 2831 october enhancmg automatic acqulsmon of thematic structure in a largescale lexacon for mandann chinese tlurd conference of the association for machine translation m the americas amta98 langhorne pa ,0,1,0
as described in section 3 we retrieved neighbors using lins 1998 similarity measure on a rasp parsed briscoe and carroll 2002 version of the bnc ,0,1,0
the best accuracies are observed when the labelsarecreatedfromdistributionallysimilarwords using lins 1998 dependencybased similarity measure depend ,0,1,0
lins 1998 informationtheoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised wsd mccarthy et al 2004 ,1,0,0
pointwise mutual information lin 1998 and relative feature focus geffet and dagan 2004 are wellknown examples ,1,0,0
lins measure lin 1998 proposed a symmetrical measure par lin s t summationtext ff s f t wsfwtf summationtext ff s wsf summationtext ff t wtf where f s and f t denote sets of features with positive weights for words s and t respectively ,0,1,0
 three kmeans algorithms using different distributional similarity or dissimilarity measures cosine skew divergence lee 1999 4 and lins similarity lin 1998 ,0,1,0
405 prf 1 proposed 383 437 408 multinomial mixture 360 374 367 newman 2004 318 353 334 cosine 603 114 192 skew divergence lee 1999 730 155 255 lins similarity lin 1998 691 096 169 cbc lin and pantel 2002 981 060 114 table 3 precision recall and fmeasure ,0,1,0
applications of word clustering include language modeling brown et al 1992 text classification baker and mccallum 1998 thesaurus construction lin 1998 and so on ,0,1,0
2005 applied the distributional similarity proposed by lin 1998 to coordination disambiguation ,0,1,0
following lin 1998 we use syntactic dependencies between words to model their semantic properties ,0,1,0
for each word in the ldv we consulted three existing thesauri rogets thesaurus roget 1995 collins cobuild thesaurus collins 2002 and wordnet fellbaum 1998 ,0,1,0
various methods hindle 1990 lin 1998 of automatically acquiring synonyms have been proposed ,0,1,0
41 features we used a dependency structure as the context for words because it is the most widely used and one of the best performing contextual information in the past studies ruge 1997 lin 1998 ,1,0,0
texts are represented by dependency parse trees using the minipar parser lin 1998b and templates by parse subtrees ,0,1,0
one of the most important is lins 1998 ,1,0,0
lin 1998as similar word list for eat misses these but includes sleep ranked 6 and sit ranked 14 because these have similar subjects to eat ,0,1,0
discriminative contextspecific training seems to yield a better set of similar predicates eg the highestranked contexts for dspcooc on the verb join3 lead 142 rejoin 139 form 134 belong to 131 found 131 quit 129 guide 119 induct 119 launch subj 118 work at 114 give a better simsjoin for equation 1 than the top similarities returned by lin 1998a participate 0164 lead 0150 return to 0148 say 0143 rejoin 0142 sign 0142 meet 0142 include 0141 leave 0140 work 0137 other features are also weighted intuitively ,0,1,0
we also test an mi model inspired by erk 2007 misimnv log summationdisplay nsimsn simnn prvn prvprn we gather similar words using lin 1998a mining similar verbs from a comparablesized parsed corpus and collecting similar nouns from a broader 10 gb corpus of english text4 we also use keller and lapata 2003s approach to obtaining webcounts ,0,1,0
erk 2007 compared a number of techniques for creating similarword sets and found that both the jaccard coefficient and lin 1998as informationtheoretic metric work best ,1,0,0
two lus close in the space are likely to be in a paradigmatic relation ie to be close in a isa hierarchy budanitsky and hirst 2006 lin 1998 pado 2007 ,0,1,0
this similarity score is computed as a max over a number of component scoring functions some based on external lexical resources including various string similarity functions of which most are applied to word lemmas measures of synonymy hypernymy antonymy and semantic relatedness including a widelyused measure due to jiang and conrath 1997 based on manually constructed lexical resources such as wordnet and nombank a function based on the wellknown distributional similarity metric of lin 1998 which automatically infers similarity of words and phrases from their distributions in a very large corpus of english text the ability to leverage external lexical resources both manually and automatically constructedis critical to the success of manli ,1,0,0
for each word pair from the antonym set we calculated the distributional distance between each of their senses using mohammad and hirsts 2006 method of concept distance along with the modified form of lins 1998 distributional measure equation 2 ,0,1,0
again we used mohammad and hirsts 2006 method along with lins 1998 distributional measure to determine the distributional closeness of two thesaurus concepts ,0,1,0
curran 2002 and lin 1998 use syntactic features in the vector definition ,0,1,0
method correlation edgecounting 0664 jiang conrath 1998 0848 lin 1998a 0822 resnik 1995 0745 li et al ,0,1,0
lin 1998b defined the similarity between two concepts as the information that is in common to both concepts and the information contained in each individual concept ,0,1,0
pereira et al1993 curran and moens 2002 and lin 1998 use syntactic features in the vector definition ,0,1,0
3httpwwwopenofficeorg another corpora based method due to turney and littman 2003 tries to measure the semantic orientation ot for a term t by ot summationdisplay tis pmitti summationdisplay tjs pmittj where s and s are minimal sets of polar terms that contain prototypical positive and negative terms respectively and pmitti is the pointwise mutual information lin 1998b between the terms t and ti ,0,1,0
for each word in ldv three existing thesauri are consulted rogets thesaurus roget 1995 collins cobuild thesaurus collins 2002 and wordnet fellbaum 1998 ,0,1,0
we propose using distributional similarity using lin 1998 as an approximation of semantic distancebetweenthewordsinthetwoglossesrather than requiring an exact match ,0,1,0
we adopt the similarity score proposed by lin 1998 as the distributional similarity score and use 50 nearest neighbours in line with mccarthy et al for the random baseline we select one word sense at random for each word token and average the precision over 100 trials ,0,1,0
the thesaurus was produced using the metric described by lin 1998 with input from the grammatical relation data extracted using the 90 million words of written english from the british national corpus bnc leech 1992 using the rasp parser briscoe and carroll 2002 ,0,1,0
syntactic context information is used hindle 1990 ruge 1992 lin 1998 to compute term similarities based on which similar words to a particular word can directly be returned ,0,1,0
semantic dsn the construction of this network is inspired by lin 1998 ,0,1,0
corpora and corpus query tools has been particularly significant in the area of compiling and developing lexicographic materials kilgarriff and rundell 2002 and in the area of creating various kinds of lexical resources such as wordnet fellbaum 1998 and framenet atkins et al 2003 fillmore et al 2003 ,0,1,0
this approach is similar to conventional techniques for automatic thesaurus construction lin 1998 ,0,1,0
our next steps will be to take a closer look at the following work clustering of similar words lin 1998 topic signatures lin and hovy 2000 and kilgariffs sketch engine kilgarriff et al 2004 ,0,1,0
2004 we use k 50 and obtain our thesaurus using the distributional similarity metric described by lin 1998 ,0,1,0
thus we rank each sense wsi wsw using prevalence score wsi 11 njnw dssnj wnsswsinj wsiwsw wnsswsinj where the wordnet similarity score wnss is defined as wnsswsinj max nsxnsnj wnsswsinsx 22 building the thesaurus the thesaurus was acquired using the method described by lin 1998 ,0,1,0
concept similarity is often measured by vectors of cooccurrence with context words that are typed with dependency information lin 1998 curran and moens 2002 ,0,1,0
whereas dependency based semantic spaces have been shown to surpass other word space models for a number of problems pad and lapata 2007 lin 1998 for the task of categorisation simple pattern based spaces have been shown to perform equally good if not better poesio and almuhareb 2005b almuhareb and poesio 2005b ,1,0,0
in particular we work with dependency paths that can reach beyond direct dependencies as opposed to lin 1998 but in the line of pado and lapata 2007 ,0,1,0
example of such algorithms are pereira et al 1993 and lin 1998 that use syntactic features in the vector definition ,0,1,0
pereira 1993 curran 2002 and lin 1998 use syntactic features in the vector definition ,0,1,0
more recently clarke and lapata 2007 use centering theory grosz et al 1995 and lexical chains morris and hirst 1991 to identify which information to prune ,0,1,0
clarke and lapata 2007 included discourse level features in their framework to leverage context for enhancing coherence ,0,1,0
in the first set of experiments we compare two settings of our ualign system with other aligners giza union och and ney 2003 and leaf with 2 iterations fraser and marcu 2007 ,0,1,0
1 introduction word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research for example brown et al 1993 ittycheriah and roukos 2005 fraser and marcu 2007 including work leveraging syntactic parse trees eg cherry and lin 2006 denero and klein 2007 fossum et al 2008 ,0,1,0
however except for fraser and marcu 2007b none of these advances in alignment quality has improved translation quality of a stateoftheart system ,1,0,0
they propose two modifications to fmeasure varying the precisionrecall tradeoff and fullyconnecting the alignment links before computing fmeasure11 weighted fullyconnected fmeasure given a hypothesized set of alignment links h and a goldstandard set of alignment links g we define h fullyconnecth and g fullyconnectg and then compute fmeasureh 1 precisionh 1 recallh for phrasebased chineseenglish and arabicenglish translation tasks fraser and marcu 2007a obtain the closest correlation between weighted fullyconnected alignment fmeasure and bleu score using 05 and 01 respectively ,0,1,0
carpuat and wu 2007b and chan et al ,0,1,0
another wsd approach incorporating contextdependent phrasal translation lexicons is given in carpuat and wu 2007 and has been evaluated on several translation tasks ,0,1,0
the senses are 1 material from cellulose 2 report 3 publication 4 medium for writing 5 scientific 6 publishing firm 7 physical object inventory is suitable for which application other than crosslingual applications where the inventory can be determined from parallel data carpuat and wu 2007 chan et al 2007 ,0,1,0
maximum entropy estimation for translation of individual words dates back to berger et al 1996 and the idea of using multiclass classifiers to sharpen predictions normally made through relative frequency estimates has been recently reintroducedundertherubricofwordsensedisambiguation and generalized to substrings chan et al 2007 carpuat and wu 2007a carpuat and wu 2007b ,0,1,0
4 are equivalent to a maximum entropy variant of the phrase sense disambiguation approach studied by carpuat wu 2007b ,0,1,0
in statistical machine translation smt recent work shows that wsd helps translation quality when the wsd system directly uses translation candidates as sense inventories carpuat and wu 2007 chan et al 2007 gimenez and marquez 2007 ,1,0,0
even the recent generation of smt models that explicitly use wsd modeling to perform lexical choice rely on sentence context rather than wider document context and translate sentences in isolation carpuat and wu 2007 chan et al 2007 gimenez and marquez 2007 stroppa et al 2007 specia et al 2008 ,0,1,0
the corresponding unlabeled figures are 733 and 3343 this confirms the results of previous studies showing that the pseudoprojective parsing technique used by maltparser tends to give high precision given that nonprojective dependencies are among the most difficult to parse correctly but rather low recall mcdonald and nivre 2007 ,0,1,0
as shown by mcdonald and nivre 2007 the single malt parser tends to suffer from two problems error propagation due to the deterministic parsing strategy typicallyaffectinglongdependenciesmorethan short ones and low precision on dependencies originating in the artificial root node due to fragmented parses9 the question is which of these problems is alleviatedbythemultipleviewsgivenbythecomponent parsers in the blended system ,0,1,0
however they make different types of errors which can be seen as a reflection of their theoretical differences mcdonald and nivre 2007 ,0,1,0
f 1 2precisionrecallprecision recall the figure we find that f 1 score decreases when dependency length increases as mcdonald and nivre 2007 found ,0,1,0
the reason may be that shorter dependencies are often modifier of nouns such as determiners or adjectives or pronouns modifying their direct neighbors while longer dependencies typically represent modifiers of the root or the main verb in a sentencemcdonald and nivre 2007 ,0,1,0
however current statistical dependency parsers provide worse results if the dependency length becomes longer mcdonald and nivre 2007 ,0,1,0
sentence length the longer the sentence is the poorer the parser performs mcdonald and nivre 2007 ,0,1,0
looking rst at learning times it is obvious that learning time depends primarily on the number of training instances which is why we can observe a difference of several orders of magnitude in learning time between the biggest training set czech and the smallest training set slovene 14 this is shown by nivre and scholz 2004 in comparison to the iterative arcstandard algorithm of yamada and matsumoto 2003 and by mcdonald and nivre 2007 in comparison to the spanning tree algorithm of mcdonald lerman and pereira 2006 ,0,1,0
practically all datadriven models that have been proposed for dependency parsing in recent years can be described as either graphbased or transitionbased mcdonald and nivre 2007 ,0,1,0
as expected malt and mst have very similar accuracy for short sentences but malt degrades more rapidly with increasing sentence length because of error propagation mcdonald and nivre 2007 ,0,1,0
again we find the clearest patterns in the graphs for precision where malt has very low precision near the root but improves with increasing depth while mst shows the opposite trend mcdonald and nivre 2007 ,0,1,0
the experimental results in mcdonald and nivre 2007 show a negative impact on the parsing accuracy from too long dependency relation ,0,1,0
the majority of these systems used models belonging to one of the twodominantapproachesindatadrivendependency parsinginrecentyearsmcdonaldandnivre2007 in graphbased models every possible dependency graph for a given input sentence is given a score that decomposes into scores for the arcs of the graph ,0,1,0
two other groups of authors have independently and simultaneously proposed adaptations of the matrixtree theorem for structured inference on directed spanning trees mcdonald and satta 2007 smithandsmith2007 ,0,1,0
second mcdonald and satta 2007 propose an on5 algorithm for computing the marginals as opposed to the on3 matrixinversion approach used by smith and smith 2007 and ourselves ,0,1,0
similar adaptations of the matrixtree theorem have been developed independently and simultaneouslybysmithandsmith2007andmcdonaldand satta 2007 see section 5 for more discussion ,0,1,0
for nonprojective parsing the analogy to the inside algorithm is the on3 matrixtree algorithm which is dominated asymptotically by a matrix determinant smith and smith 2007 koo et al 2007 mcdonald and satta 2007 ,0,1,0
2007 and smith and smith 2007 show how to employ the matrixtree theorem ,0,1,0
we used a nonprojective model trained using an application of the matrixtree theorem koo et al 2007 smith and smith 2007 mcdonald and satta 2007 for the firstorder czech models and projective parsers for all other models ,0,1,0
note that it is straightforward to calculate these expected counts using a variant of the insideoutside algorithm baker 1979 applied to the eisner 1996 dependencyparsing data structures paskin 2001 for projective dependency structures or the matrixtree theorem koo et al 2007 smith and smith 2007 mcdonald and satta 2007 for nonprojective dependency structures ,0,1,0
unfortunately there is no straightforward generalization of the method of smith and smith 2007 to the two edge marginal problem ,0,0,1
smith and smith 2007 ,0,1,0
in this paper we use a nonprojective dependency tree crf smith and smith 2007 ,0,1,0
this generates tens of millions features so we prune those features that occur fewer than 10 total times as in smith and eisner 2007 ,0,1,0
followingtheworkofkooetal2007andsmith and smith 2007 it is possible to compute all expectations in on3 ln2 through matrix inversion ,0,1,0
it is often straightforward to obtain large amounts of unlabeled data making semisupervised approaches appealing previous work on semisupervised methods for dependency parsing includes smith and eisner 2007 koo et al 2008 wang et al 2008 ,0,1,0
note that it is straightforward to calculate these expected counts using a variant of the insideoutside algorithm baker 1979 applied to the eisner 1996 dependencyparsing data structures paskin 2001 for projective dependency structures or the matrixtree theorem koo et al 2007 smith and smith 2007 mcdonald and satta 2007 for nonprojective dependency structures ,0,1,0
mann and yarowsky 2003 chen and martin 2007 baron and freedman 2008 ,0,1,0
we base our work partly on previous work done by bagga and baldwin bagga and baldwin 1998 which has also been used in later work chen and martin 2007 ,0,1,0
chen and martin 2007 explored the use of a range of syntactic and semantic features in unsupervised clustering of documents ,0,1,0
chen martin 2007 introduced one of those similarity schemes twolevel softtfidf ,0,1,0
standard sequence prediction models are highly effective for supertagging including hidden markov models bangalore and joshi 1999 nielsen 2002 maximum entropy markov models clark 2002 hockenmaier et al 2004 clark and curran 2007 and conditional random fields blunsom and baldwin 2006 ,0,1,0
recentworkconsidersadamagedtagdictionary by assuming that tags are known only for words that occur more than once or twice toutanova and johnson 2007 ,0,1,0
other work aims to do truly unsupervised learning of taggers such as goldwater and griffiths 2007 and johnson 2007 ,0,1,0
dirichlet priors can be used to bias hmms toward more skewed distributions goldwater and griffiths 2007 johnson 2007 which is especially useful in the weakly supervised setting consideredhere ,0,1,0
there is evidence that this leads to better performance on some partofspeech induction metrics johnson 2007 goldwater and griffiths 2007 ,0,1,0
following the setup in johnson 2007 we initialize the transition and emission distributions to be uniform with a small amount of noise and run em and vb for 1000 iterations ,0,1,0
in our vb experiments we set i j 01i 1tj 1v which yielded the best performance on most reported metrics in johnson 2007 ,0,1,0
we use maximum marginal decoding which johnson 2007 reports performs better than viterbi decoding ,0,1,0
one option is what johnson 2007 calls manytoone mto1 accuracy in which each induced tag is labeled with its most frequent gold tag ,0,1,0
in cases where the number of gold tags is different than the number of induced tags some must necessarily remain unassigned johnson 2007 ,0,1,0
the studies presented by goldwater and griffiths 2007 and johnson 2007 differed in the number of states that they used ,0,1,0
the largest corpus that goldwater and griffiths 2007 studied contained 96000 words while johnson 2007 used all of the 1173766 words in the full penn wsj treebank ,0,1,0
 prime 1 1 1 05 05 1 05 05 01 01 01 00001 00001 01 00001 00001 further we ran each setting of each estimator at least 10 times from randomly jittered initial starting points for at least 1000 iterations as johnson 2007 showed that some estimators require many iterations to converge ,0,1,0
expectation maximization does surprisingly well on larger data sets and is competitive with the bayesian estimators at least in terms of crossvalidation accuracy confirming the results reported by johnson 2007 ,0,1,0
monte carlo sampling methods and variational bayes are two kinds of approximate inference methods that have been applied to bayesian inference of unsupervised hmm pos taggers goldwater and griffiths 2007 johnson 2007 ,0,1,0
johnson 2007 compared two bayesian inference algorithms variational bayes and what we call here a pointwise collapsed gibbs sampler and found that variational bayes produced the best solution and that the gibbs sampler was extremely slow to converge and produced a worse solution than em ,0,1,0
recent work johnson 2007 goldwater and griffiths 2007 gao and johnson 2008 explored the task of partofspeech tagging pos using unsupervised hidden markov models hmms with encouraging results ,0,1,0
johnson 2007 and gao johnson 2008 assume that words are generated by a hidden markov model and find that the resulting states strongly correlate with pos tags ,0,1,0
the fact that different authors use different versions of the same gold standard to evaluate similar experiments eg goldwater griffiths 2007 versus johnson 2007 supports this claim ,0,1,0
johnson 2007 reports results for different numbers of hidden states but it is unclear how to make this choice a priori while goldwater griffiths 2007 leave this question as future work ,0,1,0
given the parameterspi0pikof the hmm the joint distribution over hidden states s and observationsy can be written with s0 0 psypi0pik tproductdisplay t1 pstst1pytst as johnson 2007 clearly explained training the hmm with em leads to poor results in pos tagging ,1,0,0
41 variational bayes beal 2003 and johnson 2007 describe variational bayes for hidden markov model in detail which can be directly applied to our bilingual model ,0,1,0
importantly this bayesian approach facilitates the incorporation of sparse priors that result in a more practical distribution of tokens to lexical categories johnson 2007 ,0,1,0
there has been an increased interest recently in employing bayesian modeling for probabilistic grammars in different settings ranging from putting priors over grammar probabilities johnson et al 2007 to putting nonparametric priors over derivations johnson et al 2006 to learning the set of states in a grammar finkel et al 2007 liang et al 2007 ,0,1,0
mostcommonlyvariational johnson 2007 kurihara and sato 2006 or sampling techniques are applied johnson et al 2006 ,0,1,0
unlike johnson 2007 who found optimal performance when was approximately 104 we observed monotonic increases in performance as dropped ,0,0,1
however in experiments in unsupervised pos tag learning using hmm structured models johnson 2007 shows that vb is more effective than gibbs sampling in approaching distributions that agree with the zipfs law which is prominent in natural languages ,0,1,0
6 smaller tagset and incomplete dictionaries previously researchers working on this task have also reported results for unsupervised tagging with a smaller tagset smith and eisner 2005 goldwater and griffiths 2007 toutanova and johnson 2008 goldberg et al 2008 ,0,1,0
this is also the main reason why most summarization systems applied to news articles do not outperform a simple baseline that just uses the first 100 words of an article svore et al 2007 nenkova 2005 ,1,0,0
they are not used in ln but they are known to be useful for wsd tanaka et al 2007 magnini et al 2002 ,0,1,0
although to a lesser extent measures of word relatedness have also been applied on other languages including german zesch et al 2007 zesch et al 2008 mohammad et al 2007 chinese wang et al 2008 dutch heylen et al 2008 and others ,0,1,0
0 500 1000 1500 2000 5000 10000 15000 20000 25000 30000 number of interlanguage links vector length aren ares arro enes enro esro figure 5 number of interlanguage links vs vector length for the millercharles data set 0 500 1000 1500 2000 2500 3000 3500 4000 5000 10000 15000 20000 25000 30000 number of interlanguage links vector length aren ares arro enes enro esro figure 6 number of interlanguage links vs vector length for the wordsimilarity353 data set edge bases lesk 1986 wu and palmer 1994 resnik 1995 jiang and conrath 1997 hughes and ramage 2007 or on large corpora salton et al 1997 landauer et al 1998 turney 2001 gabrilovich and markovitch 2007 ,0,1,0
the dataset is available only in english and has been widely used in previous semantic relatedness evaluations eg resnik 1995 hughes and ramage 2007 zesch et al 2008 ,0,1,0
method source spearman strube and ponzetto 2006 wikipedia 019048 jarmasz 2003 wordnet 033035 jarmasz 2003 rogets 055 hughes and ramage 2007 wordnet 055 finkelstein et al 2002 web corpus wn 056 gabrilovich and markovitch 2007 odp 065 gabrilovich and markovitch 2007 wikipedia 075 svm web corpus wn 078 table 9 comparison with previous work for wordsim353 ,0,1,0
the techniques used to solve this problem can be roughly classified into two main categories those relying on preexisting knowledge resources thesauri semantic networks taxonomies or encyclopedias alvarez and lim 2007 yang and powers 2005 hughes and ramage 2007 and those inducing distributional properties of words from corpora sahami and heilman 2006 chen et al 2006 bollegala et al 2007 ,0,1,0
we further note that our results are different from that of hughes and ramage 2007 as they use extensive feature engineering and weight tuning during the graph generation process that we have not been able to reproduce ,0,1,0
hughes and ramage 2007 described the use of a biased pagerank over the wordnet graph to compute word pair semantic relatedness using the divergence of the probability values over the graph created by each word ,0,1,0
it is often straightforward to obtain large amounts of unlabeled data making semisupervised approaches appealing previous work on semisupervised methods for dependency parsing includes smith and eisner 2007 koo et al 2008 wang et al 2008 ,0,1,0
smith and eisner 2007 apply entropy regularization to dependency parsing ,0,1,0
recently a number of machine learning approaches have been proposed zettlemoyer and collins 2005 mooney 2007 ,0,1,0
there has thus been a trend recently towards robust widecoverage semantic construction eg bos et al 2004 zettlemoyer and collins 2007 ,0,1,0
1 introduction recent work in learning semantics has focused on mapping sentences to meaning representations eg some logical form given aligned sentencemeaning pairs as training data ge and mooney 2005 zettlemoyer and collins 2005 zettlemoyer and collins 2007 lu et al 2008 ,0,1,0
a number of systems for automatically learning semantic parsers have been proposed ge and mooney 2005 zettlemoyer and collins 2005 wong and mooney 2007 lu et al 2008 ,0,1,0
our starting point is the work done by zettlemoyer and collins on parsing using relaxed ccg grammars zettlemoyer and collins 2007 zc07 ,0,1,0
practically the grammar relaxation is done via the introduction of nonstandard ccg rules zettlemoyer and collins 2007 ,0,1,0
general grammars with infinite numbers of nonterminals were studied by liang et al 2007b ,0,1,0
our work differs from these previous approaches in that we explicitly model a prior over grammars within a bayesian framework4 models of grammar refinement petrov et al 2006 liang et al 2007 finkel et al 2007 also aim to automatically learn latent structure underlying treebanked data ,0,1,0
first we can construct an infinite number of more specialized pcfgs by splitting or refining the pcfgs nonterminals into increasingly finer states this leads to the ipcfg or infinite pcfg liang et al 2007 ,0,1,0
first we can let the number of nonterminals grow unboundedly as in the infinite pcfg where the nonterminals of the grammar can be indefinitely refined versions of a base pcfg liang et al 2007 ,0,1,0
wikipedia first sentence wikifs kazama and torisawa 2007 used wikipedia as an external knowledge to improve named entity recognition ,1,0,0
recently wikipedia is emerging as a source for extracting semantic relationships suchanek et al 2007 kazama and torisawa 2007 ,0,1,0
41 extraction from definition sentences definition sentences in the wikipedia article were used for acquiring hyponymy relations by kazama and torisawa 2007 for named entity recognition ,0,1,0
hyponymy relations were extracted from definition sentences herbelot and copestake 2006 kazama and torisawa 2007 ,0,1,0
although this wikipedia gazetteer is much smaller than the english version used by kazama and torisawa 2007 that has over 2000000 entries it is the largest gazetteer that can be freely used for japanese ner ,0,0,1
we follow the method used by kazama and torisawa 2007 which encodes the matching with a gazetteer entity using iob tags with the modication for japanese ,0,1,0
first the wikipedia gazetteer improved the accuracy as expected ie it reproduced the result of kazama and torisawa 2007 for japanese ner ,0,1,0
the previous studies with the exception of kazama and torisawa 2007 used smaller gazetteers than ours ,0,1,0
we also compared the cluster gazetteers with the wikipedia gazetteer constructed by following the method of kazama and torisawa 2007 ,0,1,0
some regarded wikipedia as the corpora and applied handcrafted or machinelearned rules to acquire semantic relations herbelot and copestake 2006 kazama and torisawa 2007 ruizcasado et al 2005 nastase and strube 2008 sumida et al 2008 suchanek et al 2007 ,0,1,0
kazama and torisawa 2007 explores the first sentence of an article and identifies the first noun phrase following the verb be as a label for the article title ,0,1,0
as the most concise definition we take the first sentence of each article following kazama and torisawa 2007 ,0,1,0
it turns out that while problems of coverage and ambiguity prevent straightforward lookup injection of gazetteer matches as features in machinelearning based approaches is critical for good performance cohen 2004 kazama and torisawa 2007a toral and munoz 2006 florian et al 2003 ,0,1,0
recently toral and munoz 2006 kazama and torisawa 2007a have successfully constructed high quality and high coverage gazetteers from wikipedia ,1,0,0
ner proves to be a knowledgeintensive task and it was reassuring to observe that system resources used f1 lbjner wikipedia nonlocal features wordclass model 9080 suzuki and isozaki 2008 semisupervised on 1gword unlabeled data 8992 ando and zhang 2005 semisupervised on 27mword unlabeled data 8931 kazama and torisawa 2007a wikipedia 8802 krishnan and manning 2006 nonlocal features 8724 kazama and torisawa 2007b nonlocal features 8717 finkel et al 2005 nonlocal features 8686 table 7 results for conll03 data reported in the literature ,0,1,0
4 semantic class induction from wikipedia wikipedia has recently been used as a knowledge source for various language processing tasks including taxonomy construction ponzetto and strube 2007a coreference resolution ponzetto and strube 2007b and english ner eg bunescu and pasca 2006 cucerzan 2007 kazama and torisawa 2007 watanabe et al ,0,1,0
some of these have been previously employed for various tasks by gabrilovich and markovitch 2006 overell and ruger 2006 cucerzan 2007 and suchanek et al ,0,1,0
wu and weld 2007 and cucerzan 2007 calculate the overlap between contexts of named entities and candidate articles from wikipedia using overlap ratios or similarity scores in a vector space model respectively ,0,1,0
test additional resources jesscm crfhmm 9448 8992 1gword unlabeled data 9366 8936 37mword unlabeled data ando and zhang 2005 9315 8931 27mword unlabeled data florian et al 2003 9387 8876 own large gazetteers 2mword labeled data suzuki et al 2007 na 8841 27mword unlabeled data sup ,0,1,0
24 comparison with hybrid model ssl based on a hybrid generativediscriminative approach proposed in suzuki et al 2007 has been defined as a loglinear model that discriminatively combines several discriminative models pdi and generative models pgj such that ryx producttext i p di yxii producttext j p gj xjyjj summationtext y producttext i p di yxii producttext j p gj xjyjj where iii1 and iii1jijji1 ,0,1,0
as a solution a given amount of labeled training data is divided into two distinct sets ie 45 for estimating and the 667 remaining 15 for estimating suzuki et al 2007 ,0,1,0
networks toutanova et al 2003 9724 svm gimenez and marquez 2003 9705 me based a bidirectional inference tsuruoka and tsujii 2005 9715 guided learning for bidirectional sequence classification shen et al 2007 9733 adaboostsdf with candidate features 21100 wdist 9732 adaboostsdf with candidate features 21010 fdist 9732 svm with candidate features c01 d2 9732 text chunking f1 regularized winnow full parser output zhang et al 2001 9417 svmvoting kudo and matsumoto 2001 9391 aso unlabeled data ando and zhang 2005 9439 crfrerankingkudo et al 2005 9412 me based a bidirectional inference tsuruoka and tsujii 2005 9370 laso approximate large margin update daume iii and marcu 2005 944 hysol suzuki et al 2007 9436 adaboostsdf with candidate featuers 21 wdist 9432 adaboostsdf with candidate featuers 21010wdist 9430 svm with candidate features c1 d2 9431 one of the reasons that boostingbased classifiers realize faster classification speed is sparseness of rules ,0,1,0
in the first a separate language model is trained on each column of the database and these models are then used to segment and label a given text sequence agichtein and ganti 2004 canisius and sporleder 2007 ,0,1,0
in the second pass 5gram and 6gram zerocutoff stupidbackoff brants et al 2007 language models estimated using 47 billion words of english newswire text are used to generate lattices for phrasal segmentation model rescoring ,0,1,0
5gram word language models in english are trained on a variety of monolingual corpora brants et al 2007 ,0,1,0
we conclude by noting that english language models currently used in speech recognition chelba and jelinek 1999 and automated language translation brants et al 2007 are much more powerful employing for example 7gram word models not letter models trained on trillions of words ,1,0,0
this was expected as it has been observed before that very simple smoothing techniques can perform well on large data sets such as web data brants et al 2007 ,0,1,0
in nlp community it has been shown that having more data results in better performance ravichandran et al 2005 brants et al 2007 turney 2008 ,0,1,0
to scale lms to larger corpora with higherorder dependencies researchers work completed while this author was at google inc have considered alternative parameterizations such as classbased models brown et al 1992 model reduction techniques such as entropybased pruning stolcke 1998 novel represention schemes such as suffix arrays emami et al 2007 golomb coding church et al 2007 and distributed language models that scale more readily brants et al 2007 ,1,0,0
previous work brants et al 2007 has shown it to be appropriate to largescale language modeling ,0,1,0
emami et al 2007 brants et al 2007 church et al 2007 ,0,1,0
we use the distributed training and application infrastructure described in brants et al 2007 with modifications to allow the training of predictive classbased models and their application in the decoder of the machine translation system ,0,1,0
759 for all models used in our experiments both wordand classbased the smoothing method used was stupid backoff brants et al 2007 ,0,1,0
classbased ngram models have also been shown to benefit from their reduced number of parameters when scaling to higherorder ngrams goodman and gao 2000 and even despite the increasing size and decreasing sparsity of language model training corpora brants et al 2007 classbased ngram models might lead to improvements when increasing the ngram order ,0,1,0
either pruning stolcke 1998 church et al 2007 or lossy randomizing approaches talbot and brants 2008 may result in a compact representation for the application runtime ,0,1,0
to support distributed computation brants et al 2007 we further split the ngram data into shards by hash values of the first bigram ,0,1,0
phrasebased mt systems are straightforward to train from parallel corpora koehn et al 2003 and like the original ibm models brown et al 1990 benefit from standard language models built on large monolingual targetlanguage corpora brants et al 2007 ,0,1,0
for our contrast submission we rescore the firstpass translation lattices with a large zerocutoff stupidbackoff brants et al 2007 language model estimated over approximately five billion words of newswire text ,0,1,0
decoding is carriedout using the moses decoder koehn and hoang 2007 ,0,1,0
5 smt experiments 51 experimental setup we used publicly available resources for all our tests for decoding we used moses koehn and hoang 2007 and our parallel data was taken from the spanishenglish section of europarl ,0,1,0
koehn and hoang 2007 propose factored translation models which extend phrasebased statistical machine translation by allowing the integration of additional morphological features at the word level ,0,1,0
24 factor model decomposition factored translation models koehn and hoang 2007 extend the phrasebased model by integrating word level factors into the decoding process ,0,1,0
41 training the training procedure is identical to the factored phrasebased training described in koehn and hoang 2007 ,0,1,0
the reader is referred to koehn and hoang 2007 koehn et al 2007 for detailed information about phrasebased statistical machine translation ,0,1,0
decoding is carriedout using the moses decoder koehn and hoang 2007 ,0,1,0
initial results show the potential benefit of factors for statistical machine translation koehn et al 2006 and koehn and hoang 2007 ,0,1,0
in their presentation of the factored smt models koehn and hoang 2007 describe experiments for translating from english to german spanish and czech using morphology tags added on the morphologically rich side along with pos tags ,0,1,0
the model is defined mathematically koehn and hoang 2007 as following pfe 1zexp nsummationdisplay i1 ihife 1 where i is a vector of weights determined during a tuning process and hi is the feature function ,0,1,0
chiang 2005 distinguishes statistical mt approaches that are syntactic in a formal sense going beyond the nitestate underpinnings of phrasebased models from approaches that are syntactic in a linguistic sense ie taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking1 the two forms of syntactic modeling are doubly dissociable current research frameworks include systems that are nite state but informed by linguistic annotation prior to training eg koehn and hoang 2007 birch et al 2007 hassan et al 2007 and also include systems employing contextfree models trained on parallel text without bene t of any prior linguistic analysis eg ,0,1,0
we also report on applying factored translation models koehn and hoang 2007 for englishtoarabic translation ,0,1,0
any way to enforce linguistic constraints will result in a reduced need for data and ultimately in more complete models given the same amount of data koehn and hoang 2007 ,0,1,0
therefore including a model based on surface forms as suggested koehn and hoang 2007 is also necessary ,0,1,0
 truecase 207 04 278 02 table 2 impact of truecasing on casesensitive bleu in a more integrated approach factored translation models koehn and hoang 2007 allow us to consider grammatical coherence in form of partofspeech language models ,0,1,0
321 factored treelet translation labels of nodes at the tlayer are not atomic but consist of more than 20 attributes representing various linguistic features3 we can consider the attributes as individual factors koehn and hoang 2007 ,0,1,0
in order to generate a value for each targetside factor we use a sequence of mapping steps similar to koehn and hoang 2007 ,0,1,0
furthermore the bleu score performance suggests that our model is not very powerful but some interesting hints can be found in table 3 when we compare our method with a 5gram language model to a stateoftheart system moses koehn and hoang 2007 based on various evaluation metrics including bleu score nist score doddington 2002 meteor banerjee and lavie 2005 ter snover et al 2006 wer and per ,1,0,0
some research into factored machine translation has been published by koehn and hoang 2007 ,0,1,0
we believe that other kinds of translationunit such as ngram jos et al 2006factoredphrasaltranslationkoehn and hoang 2007 or treelet quirk et al 2005 can be used in this method ,0,1,0
a tight integration of morphosyntactic information into the translation model was proposed by koehn and hoang 2007 where lemma and morphological information are translated separately and this information is combined on the output side to generate the translation ,0,1,0
unlike with factored models koehn and hoang 2007 or additional translation lexicons schwenk et al 2008 we do not generate the surface form back from the lemma translation which means that tense gender and number information are 151 newsdev2009a representation oov meteor bleu nist baseline surface form only 224 4905 2045 6135 decoding lemma backoff 213 4912 2044 6143 word alignment lemmapos for all 224 4887 2036 6145 lemmapos for adj 225 4894 2046 6131 lemmapos for verbs 221 4905 2047 6137 decoding alignment backoff all 210 4897 2036 6147 backoff adj 212 4905 2048 6140 backoff verbs 208 4915 2050 6148 newsdev2009b representation oov meteor bleu nist baseline surface form only 252 4960 2110 6211 decoding lemma backoff 243 4966 2102 6210 word alignment lemmapos for all 253 4956 2103 6199 lemmapos for adj 252 4974 2100 6213 lemmapos for verbs 247 4973 2110 6217 decodingalignment backoff all 244 4959 2092 6194 backoff adj 243 4980 2103 6217 backoff verbs 239 4980 2103 6217 table 2 evaluation of the decoding backoff strategy the modified word alignment strategy and their combination input meme sil demissionnait la situation ne changerait pas ,0,1,0
many strategies have been proposed to integrate morphology information in smt including factored translation models koehn and hoang 2007 adding a translation dictionary containing inflected forms to the training data schwenk et al 2008 entirely replacing surface forms by representations built on lemmas and pos tags popovic and ney 2004 morphemes learned in an unsupervised manner virpojia et al 2007 and using porter stems and even 4letter prefixes for word alignment watanabe et al 2006 ,0,1,0
partofspeech language model we use factored translation models koehn and hoang 2007 to also output partofspeech tags with each word in a single phrase mapping and run a second ngram model over them ,0,1,0
another technique used was to filter sentences of the outofdomain corpus based on their similarity to the target domain as predicted by a classifier dredze et al 2007 ,0,1,0
as with many domain adaptation problems it is quite helpful to have some annotated target data especially when annotation styles vary dredze et al 2007 ,0,1,0
the problem itself has started to get attention only recently roark and bacchiani 2003 hara et al 2005 daume iii and marcu 2006 daume iii 2007 blitzer et al 2006 mcclosky et al 2006 dredze et al 2007 ,0,1,0
2 motivation and prior work while several authors have looked at the supervised adaptation case there are less and especially less successful studies on semisupervised domain adaptation mcclosky et al 2006 blitzer et al 2006 dredze et al 2007 ,0,1,0
dredze et al yielded the second highest score1 in the domain adaptation track dredze et al 2007 ,0,1,0
dredze et al also indicated that unlabeled dependency parsing is not robust to domain adaptation dredze et al 2007 ,0,1,0
it is important to realize that the output of all mentioned processing steps is noisy and contains plenty of mistakes since the data has huge variability in terms of quality style genres domains etc and domain adaptation for the nlp tasks involved is still an open problem dredze et al 2007 ,0,1,0
based on annotation differences in the datasets dredze et al 2007 and a bug in their system shimizu and nakagawa 2007 their results are inconclusive ,0,1,0
opinion forecasting differs from that of opinion analysis such as extracting opinions evaluating sentiment and extracting predictions kim and hovy 2007 ,0,1,0
kim and hovy 2007 make a similar assumption ,0,1,0
with the success of collaborative sites like amazons mechanical turk 1 one 1httpwwwmturkcom 59 can provide the task of annotation to multiple oracles on the internet snow et al 2008 ,0,1,0
a few studies carpuat and wu 2007 ittycheriah and roukos 2007 he et al 2008 hasan et al 2008 addressed this defect by selecting the appropriate translation rules for an input span based on its context in the input sentence ,0,1,0
we chose this inverse direction since it can be integrated directly into the decoder and thus does not rely on a twopass approach using reranking as it is the case for hasan et al 2008 ,0,1,0
the triggerbased lexicon model used in this work follows the training procedure introduced in hasan et al 2008 and is integrated directly in the decoder instead of being applied in nbest list reranking ,0,1,0
while early machine learning approaches for the task relied on local discriminative classifiers soon et al 2001 ng and cardie 2002b morton 2000 kehler et al 2004 more recent approaches use joint andor global models mccallum and wellner 2004 ng 2004 daume iii and marcu 2005 denis and baldridge 2007a ,0,1,0
the most similar work to ours is daume iii and marcu 2005 in which two most common synsets from wordnet for all words in an np and their hypernyms are extracted as features ,0,1,0
see luo and zitouni 2005 and daume iii and marcu 2005 ,0,1,0
we implement this algorithm using the perceptron framework as it can be easily modified for structured prediction while preserving convergence guarantees daume iii and marcu 2005 snyder and barzilay 2007 ,0,1,0
training procedure our algorithm is a modification of the perceptron ranking algorithm collins 2002 which allows for joint learning across several ranking problems daume iii and marcu 2005 snyder and barzilay 2007 ,0,1,0
daumeiii and marcu 2005a use the learning as search optimization framework to take into account the nonlocality behavior of the coreference features ,0,1,0
we have already shown in section 3 how to solve a here we avoid b by maximizing conditional likelihood marginalizing out the hidden variable denotedz max vector summationdisplay xy pxylog summationdisplay z pvectoryz x 17 this sort of conditional training with hidden variables was carried out by koo and collins 2005 for example in reranking it is related to the information bottleneck method tishby et al 1999 and contrastive estimation smith and eisner 2005 ,0,1,0
our method is based on the ones described in erkan and radev 2004 mihalcea and tarau 2004 fader et al 2007 the objective of this paper is to dynamically rank speakers or participants in a discussion ,0,1,0
discriminative parsing has been investigated before such as in johnson 2001 clark and curran 2004 henderson 2004 koo and collins 2005 turian et al ,0,1,0
1113 recursive dp equations for summing over t and a alignments are treated as a hidden variable to be marginalized out10 optimization problems of this form are by now widely known in nlp koo and collins 2005 and have recently been used for machinetranslationaswellblunsometal2008 ,0,1,0
discriminative training with hidden variables has been handled in this probabilistic framework quattoni et al 2004 koo and collins 2005 but we choose equation 3 for efficiency ,0,1,0
a reranking parser see also koo and collins 2005 is a layered model the base layer is a generative statistical pcfg parser that creates a ranked list of k parses say 50 and the second layer is a reranker that reorders these parses using more detailed features ,0,1,0
5 related work there has not been much previous work on graphical models for full parsing although recently several latent variable models for parsing have been proposed koo and collins 2005 matsuzaki et al 2005 riezler et al 2002 ,0,1,0
in koo and collins 2005 an undirected graphical model is used for parse reranking ,0,1,0
1 introduction the reranking approach is widely used in parsing collins and koo 2005 koo and collins 2005 henderson and titov 2005 shen and joshi 2003 as well as in other structured classification problems ,0,1,0
in syntactic parse reranking supersenses have been used to build useful latent semantic features koo and collins 2005 ,1,0,0
collins and koo collins koo 2005 introduced an improved reranking model for parsing which includes a hidden layer of semantic features ,0,1,0
in koo and collins 2005 an undirected graphical model for constituent parse reranking uses dependency relations to define the edges ,0,1,0
some researchers lappin and leass 1994 kennedy and boguraev 1996 use manually designed rules to take into account the grammatical role of the antecedent candidates as well as the governing relations between the candidate and the pronoun while others use features determined over the parse tree in a machinelearning approach aone and bennett 1995 yang et al 2004 luo and zitouni 2005 ,0,1,0
2007 and luo and zitouni 2005 ,0,1,0
see luo and zitouni 2005 and daume iii and marcu 2005 ,0,1,0
we also tested the flat syntactic feature set proposed in luo and zitouni 2005s work ,0,1,0
in line with the reports in luo and zitouni 2005 we do observe the performance improvement against the baseline norm for all the domains ,0,1,0
the training methods of lrmf and svmf were useful to improve the f m scores of lrm and svm as reported in jansche 2005 joachims 2005 ,0,1,0
recently methods for training binary classifiers to maximize the f 1 score have been proposed for svm joachims 2005 and lrm jansche 2005 ,0,1,0
to estimate combination weights we extend the f 1 score maximization training algorithm for lrm described in jansche 2005 ,0,1,0
by contrast in the training method proposed by jansche 2005 the discriminative function fxw is estimated to maximize the f 1 score of training dataset d this training method employs an approximate form of the f 1 score obtained by using a logistic function ,0,1,0
c a and b are computed for training dataset d as c summationtext m m1 y m y m a summationtext m m1 y m and b summationtext m m1 y m in jansche 2005 y m was approximated by using the discriminative and logistic functions shown in eqs ,0,1,0
moreover an fscore optimization method for logistic regression has also been proposed jansche 2005 ,0,1,0
in his analysis of yarowsky 1995 abney 2004 formulates several variants of bootstrapping ,0,1,0
drawing on abneys 2004 analysis of the yarowsky algorithm we perform bootstrapping by entropy regularization we maximize a linear combination of conditional likelihood on labeled data and confidence negative renyi entropy on unlabeled data ,0,1,0
this approach however does not have a theoretical guarantee on optimality unless certain nontrivial conditions are satisfied abney 2004 ,0,1,0
51 comparison to selftraining for completeness we also compared our results to the selflearning algorithm which has commonly been referred to as bootstrapping in natural language processing and originally popularized by the work of yarowsky in word sense disambiguation abney 2004 yarowsky 1995 ,0,1,0
although a rich literature covers bootstrapping methods applied to natural language problems yarowsky 1995 riloff 1996 collins and singer 1999 yangarber et al 2000 yangarber 2003 abney 2004 several questions remain unanswered when these methods are applied to syntactic or semantic pattern acquisition ,0,1,0
unsupervised methods have been developed for wsd but despite modest success have not always been well understood statistically abney 2004 ,0,0,1
many 649 similarity measures and weighting functions have been proposed for distributional vectors comparative studies include lee 1999 curran 2003 and weeds and weir 2005 ,0,1,0
for example weeds 2003 weeds and weir 2005 see below took verbs as contexts for nouns in object position so they regarded two nouns to be similar to the extent that they occur as direct objects of the same set of verbs ,0,1,0
if distributional similarity is conceived of as substitutability as weeds and weir 2005 and lee 1999 emphasize then asymmetries arise when one word appears in a subset of the contexts in which the other appears for example the adjectives that typically modify apple are a subset of those that modify fruitsofruit substitutes for apple better than apple substitutes for fruit ,0,1,0
however the study of weeds and weir 2005 provides interesting insights into what makes a good distributional similarity measure in the contexts of semantic similarity prediction and language modeling ,1,0,0
this is important when large cutoff 0 5 100 naive 541721 184493 35617 sash 10599 8796 6231 index 5844 13187 32663 table 4 average number of comparisons per term considering that different tasks may require different weights and measures weeds and weir 2005 ,0,1,0
following initial work by sparck jones 1964 and grefenstette 1994 an early online distributional thesaurus presented in lin 1998 has been widely used and cited and numerous authors since have explored thesaurus properties and parameters see survey component of weeds and weir 2005 ,0,1,0
it is explored extensively in curran 2004 weeds and weir 2005 ,1,0,0
the earliest work in this direction are those of hindle 1990 lin 1998 dagan et al 1999 chen and chen 2000 geffet and dagan 2004 and weeds and weir 2005 ,0,1,0
they generally perform less well on lowfrequency words weeds and weir 2005 van der plas 2008 ,0,1,0
this upper bound is consistent with the upper limit of 50 found by daume iii and marcu 2005 which takes into account stemming differences ,0,1,0
daume iii and marcu 2005 propose a model that encodes how likely it is that different sized spans of text are skipped to reach words and phrases to recycle ,0,1,0
32 compound noun interpretation the task of interpreting the semantics of noun compounds is one which has recently received considerable attention lauer 1995 girju et al 2005 turney 2006 ,0,1,0
pairclass is most similar to the algorithm of turney 2006 but it differs in the following ways pairclass does not use a lexicon to find synonyms for the input word pairs ,0,1,0
 pairclass generates probability estimates whereas turney 2006 uses a cosine measure of similarity ,0,1,0
 the automatically generated patterns in pairclass are slightly more general than the patterns of turney 2006 ,0,0,1
 the morphological processing in pairclass minnen et al 2001 is more sophisticated than in turney 2006 ,0,0,1
veale 2004 used wordnet to answer 374 multiplechoice sat analogy questions achieving an accuracy of 43 but the best corpusbased approach attains an accuracy of 56 turney 2006 ,1,0,0
the template we use here is similar to turney 2006 but we have added extra context words before the x and after the y our morphological processing also differs from turney 2006 ,0,1,0
turney 2006 also selects patterns based on the number of pairs that generate them but the number of selected patterns is a constant 8000 independent of the number of input word pairs ,0,1,0
turney 2006 used a corpusbased algorithm ,0,1,0
one such relational reasoning task is the problem of compound noun interpretation which has received a great deal of attention in recent years girju et al 2005 turney 2006 butnariu and veale 2008 ,0,1,0
turney 2006 describes a method latent relational analysis that extracts subsequence patterns for noun pairs from a large corpus using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space ,0,1,0
in at with use teacher school 11894470201 289 00 teacher handbook 25 00 32 101 soldier gun 28 103 1059 410 table 5 a fragment of the ccxl space we use this space to measure relational similarity turney 2006 of concept pairs eg finding that the relation between teachers and handbooks is more similar to the one between soldiers and guns than to the one between teachers and schools ,0,1,0
we solve sat analogies with a simplified version of the method of turney 2006 ,0,1,0
1 introduction corpusderived distributional semantic spaces have proved valuable in tackling a variety of tasks ranging from concept categorization to relation extraction to many others sahlgren 2006 turney 2006 pado and lapata 2007 ,0,1,0
the literature on relational similarity on the other hand has focused on pairs of words devising various methods to compare how similar the contexts in which target pairs appear are to the contexts of other pairs that instantiate a relation of interest turney 2006 pantel and pennacchiotti 2006 ,0,1,0
1 introduction cooccurrence statistics extracted from corpora lead to good performance on a wide range of tasks that involve the identification of the semantic relation between two words or concepts sahlgren 2006 turney 2006 ,1,0,0
congress of the italian association for artificial intelligence palermo 1991 b boguraev building a lexicon the contribution of computers ibm report tj watson research center 1991 m brent automatic aquisition of subcategorization frames from untagged texts in acl 1991 n calzolari r bindi acquisition of lexical information from corpus in coling 1990 k w church p hanks word association norms mutual information and lexicography computational linguistics vol ,0,1,0
the results of these studies have important applications in lexicography to detect lexicosyntactic regularities church and hanks 19901 calzolari and bindi1990 such as for example support verbs eg makedecision prepositional verbs eg relyupon idioms semantic relations eg partof and fixed expressions eg kick the bucket ,0,1,0
in calzolari and bindi 1990 church and hanks 1990 the significance of an association xy is measured by the mutual information ixy ie the probability of observing x and y together compared with the probability of observing x and y independently ,0,1,0
in particular mutual information church and hanks 1990 wu and su 1993 and other statistical methods such as smadja 1993 and frequencybased methods such as justeson and katz 1993 exclude infrequent phrases because they tend to introduce too much noise ,0,1,0
word association norms based on cooccurrence information have been proposed by church and hanks 1990 ,0,1,0
a large corpus is vahmble as a source of such nouns church and hanks 1990 brown et al 1992 ,0,1,0
as church and hanks 1990 we adopted an evaluation of mutual information as a cohesion measure of each cooccurrence ,0,1,0
a hindle and rooth 1991 and church and hanks 1990 used partial parses generated by fidditch to study word urrtnc patterns m syntactic contexts ,0,1,0
hindle uses the observed frequencies within a specific syntactic pattern subjectverb and verbobject to derive a cooccu rence score which is an estimate of mutual information church and hanks 1990 ,0,1,0
church hanks 1990p24 merkel nilsson ahrenberg 1994 have constructed a system that uses frequency of recurrent segments to determine long phrases ,0,1,0
pointwise mutual information church and hanks 1990 3 ,0,1,0
we measure this association using pointwise mutual information mi church and hanks 1990 ,0,1,0
this task is quite common in corpus linguistics and provides the starting point to many other algorithms eg for computing statistics such as pointwise mutual information church and hanks 1990 for unsupervised sense clustering schutze 1998 and more generally a large body of work in lexical semantics based on distributional profiles dating back to firth 1957 and harris 1968 ,0,1,0
 1989 eg lexicography church and hanks 1990 information retrieval salton 1986a text input yamashina and obashi 1988 etc this paper will touch on its feasibility in topic identification ,0,1,0
ic function is a derivative of fanos mutual information formula recently used by church and hanks 1990 to compute word cooccurrence patterns in a 44 million word corpus of associated press news stories ,0,1,0
using techniques described in church and hindle 1990 church and hanks 1990 and hindle and rooth 1991 below are some examples of the most frequent vo pairs from the ap corpus ,0,1,0
while we have observed reasonable results with both g 2 and fishers exact test we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information mi measure church and hanks 1990 ixy log 2 pxy 4 pxpy in 4 y is the seed term and x a potential target word ,0,1,0
orgpubscitations j ournalstoms1986122p154meht a mutual information given the definition of mutual information church and hanks 1990 ixy log 2 pxy pxpy we consider the distribution of a window word according to the contingency table a in table 4 ,0,1,0
equation 10 is of interest because the ratio pc v rpc r can be interpreted as a measure of association between the verb v and class c this ratio is similar to pointwise mutual information church and hanks 1990 and also forms part of resniks association score which will be introduced in section 6 ,0,1,0
this does not seem to be the case however for common feature weighting functions such as pointwise mutual information church and patrick 1990 hindle 1990 ,0,1,0
church and hanks 1990 smadja and mckeown 1990 ,0,1,0
7 this discussion could also be cast in an information theoretic framework using the notion of mutual information fano 1961 estimating the variance of the degree of match in order to find a frequencythreshold see church and hanks 1990 ,0,1,0
1 church and hanks 1990 church et al 1991 thus emphasize the importance of human judgment used in conjunction with these tools ,0,1,0
statistics on cooccurrence of words in a local context were used recently for monolingual word sense disambiguation gale church and yarowsky 1992b 1993 sch6tze 1992 1993 see section 7 for more details and church and hanks 1990 smadja 1993 for other applications of these statistics ,0,1,0
lexical collocation functions especially those determined statistically have recently attracted considerable attention in computational linguistics calzolari and bindi 1990 church and hanks 1990 sekine et al 1992 hindle and rooth 1993 mainly though not exclusively for use in disambiguation ,0,1,0
the window size may vary church and hanks 1990 used windows of size 2 and 5 ,0,1,0
211 pointwise mutual information this measure for word similarity was first used in this context by church and hanks 1990 ,0,1,0
strength of association between subject i and verb j is measured using mutual information church and hanks 1990 ln ji ij tftf tfnjimi here tfij is the maximum frequency of subjectverb pair ij in the reuters corpus tfi is the frequency of subject head noun i in the corpus tfj is the frequency of verb j in the corpus and n is the number of terms in the corpus ,0,1,0
hanks and church 1990 proposed using pointwise mutual information to identify collocations in lexicography however the method may result in unacceptable collocations for lowcount pairs ,0,0,1
the most widely used association weight function is pointwise mutual information mi church and hanks 1990 lin 1998 dagan 2000 weeds et al 2004 ,1,0,0
metric formula frequency guiliano 1964 x yf pointwise mutual information pmi church hanks 1990 xy x y2log p p p true mutual information tmi manning 1999 xy 2 xy x ylog p p p p chisquared 2 church and gale 1991 2 i x x y y i j i j i j j f tscore church hanks 1990 1 2 2 2 1 2 1 2 x x s s n n cvalues4 frantzi anadiou mima 2000 2 is not nested 2 log log 1 a a b t a f f f b p t where is the candidate string f is its frequency in the corpus t is the set of candidate terms that contain pt is the number of these candidate terms 609 1700 of the threeword phrases are attested in the lexile corpus ,0,1,0
to this end we follow the method introduced by church and hanks 1990 ie by sliding a window of a given size over some texts ,0,1,0
the information content of this set is defined as mutual information ifw church and hanks 1990 ,0,1,0
4 using vectorbased models of semantic representation to account for the systematic variances in neural activity 41 lexical semantic representation computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly cooccurs church and hanks 1990 ,0,1,0
the use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research church and hanks 1990 zernik and jacobs 1990 hindle 1990 ,0,1,0
three recent papers in this area are church and hanks 1990 hindle 1990 and smadja and mckeown 1990 ,0,1,0
in addition ic is stable even for relatively low frequency words which can be contrasted with fanos mutual information formula recently used by church and hanks 1990 to compute word cooccurrence patterns in a 44 million word corpus of associated press news stories ,0,1,0
researchers such as evans et al 1991 and church and hanks 1990 have applied robust grammars and statistical techniques over large corpora to extract interesting noun phrases and subjectverb verbobject pairs ,1,0,0
statistical data about these various cooccurrence relations is employed for a variety of applications such as speech recognition jelinek 1990 language generation smadja and mckeown 1990 lexicography church and hanks 1990 machine translation brown et al sadler 1989 information retrieval maarek and smadja 1989 and various disambiguation tasks dagan et al 1991 hindle and rooth 1991 grishman et al 1986 dagan and itai 1990 ,0,1,0
the mutual information of a cooccurrence pair which measures the degree of association between the two words church and hanks 1990 is defined as fano 1961 pxly ixy log 2 pxy log 2 1 pxpy px log 2 pyx py where px and py are the probabilities of the events x and y occurrences of words in our case and px y is the probability of the joint event a cooccurrence pair ,0,1,0
in the results we describe here we use mutual information fano 1961 2728 church and hanks 1990 as the metric for neighbourhood pruning pruning which occurs as the network is being generated ,0,1,0
thus given a hyponym definition o and a set of candidate hypernym definitions this method selects the candidate hypernym definition e which returns the maximum score given by formula 1 sco e e cwwi wj i wieoawj6e the cooccurrence weight cw between two words can be given by cooccurrence frequency mutual information church and hanks 1990 or association ratio resnik 1992 ,0,1,0
the cohesion between two words is measured as in church and hanks 1990 by an estimation of the mutual information based on their collocation frequency ,0,1,0
collocation collocations were extracted from a seven million word sample of the longman english language corpus using the association ratio church and hanks 1990 and outputted to a lexicon ,0,1,0
for instance church and hanks 1990 calculated sa in terms of mutual information between two words wl and w2 n fwlw2 iwl w2 log2 1 fwlfw2 here n is the size of the corpus used in the estimation fwl w2 is the frequency of the cooccurrence fwl and fw2 that of each word ,0,1,0
the cohesion between words has been evaluated with the mutual information measure as in church and hanks 1990 ,0,1,0
arguably the most widely used is the mutual information hindle 1990 church and hanks 1990 dagan et al 1995 luk 1995 d lin 1998a ,1,0,0
we then propose a relatively simple yet effective method for resolving translation disambiguation using mutual information mi church and hanks 1990 statistics obtained only from the target document collection ,0,1,0
the mutual information mlxy is defined as the following formula church and hanks 1990 ,0,1,0
we preferred the loglikelihood ratio to other statistical scores such as the association ratio church and hanks 1990 or 2 since it adequately takes into account the frequency of the cooccurring words and is less sensitive to rare events and corpussize dunning 1993 daille 1996 ,0,1,0
for mutual information mi we use two different equations one for twoelement compound nouns church and hanks 1990 and the other for threeelement compound nouns suet al 1994 ,0,1,0
the association relationship between two words can be indicated by their mutual information which can be further used to discover phrases church hanks 1990 ,0,1,0
there are several distance measures suitable for this purpose such as the mutual informationchurch and hanks 1990 the dice coefficientmanning and schueutze 85 1999 the phi coefficientmanning and schuetze 533 1999 the cosine measuremanning and schueutze 85 1999 and the confidencearrawal and srikant 1995 ,1,0,0
3 related work word collocation various collocation metrics have been proposed including mean and variance smadja 1994 the ttest church et al 1991 the chisquare test pointwise mutual information mi church and hanks 1990 and binomial loglikelihood ratio test blrt dunning 1993 ,0,1,0
study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction church and hanks 1990 smadja 1993 choueka 1993 lin 1998 ,0,1,0
church and hanks church and hanks 1990 employed mutual information to extract both adjacent and distant bigrams that tend to cooccur within a fixedsize window ,0,1,0
the typical problems like doctornurse church and hanks 1990 could be avoided by using such information ,0,1,0
there are several basic methods for evaluating associations between words based on frequency counts choueka 1988 wettler and rapp 1993 information theoretic church and hanks 1990 and statistical significance smadja 1993 ,0,1,0
in the following sections we will use 2 statistics to measure the the mutual translation likelihood church and hanks 1990 ,0,1,0
andw2 iscomputedusinganassociationscorebased on pointwise mutual information asdefinedbyfano 1961 and used for a similar purpose in church and hanks 1990 as well as in many other studies in corpus linguistics ,0,1,0
indeed as sinopalnikova and pavel 2004 note deese 1965 was the first to conduct linguistic analyses of word association norms such as measurements of semantic similarity based on his convictions that similar words evoke similar word association responsesan approach that is somewhat reminiscent of church and hanks 1990 notion of mutual information ,0,1,0
mutual informatio n church and hanks 1990 discussed the use of the mutual information statistics as a way to identify a variety of interesting linguistic phenomena ranging from semanti c relations of the doctornurse type content wordcontent word to lexicosyntactic cooccurrence preferences between verbs and prepositions content wordfunction word ,0,1,0
the tensor has been adapted with a straightforward extension of pointwise mutual information church and hanks 1990 for threeway cooccurrences following equation 4 ,0,1,0
the first adaptation includes theswapoperationwagnerandlowrance1975 whilethesecondadaptationincludesphoneticsegment distances which are generated by applying an iterative pointwise mutual information pmi procedurechurchand hanks 1990 ,0,1,0
we used pointwise mutual information pmi church and hanks 1990 to obtain these distances ,0,1,0
a broad view of the possible scope of lexical semantics would thus be one which tries to chart out the systematic generalizable aspects of word meanings and of the relations between words drawing on readily accessible sources of lexical knowledge such as machine readable dictionaries encyclopedias and representative corpora coupled with the kind of analytic apparatus that is needed to fruitfully explore such sources for instance custombuilt parsers to cope with dictionary definitions vossen 1990 statistical programs to deal with the distributional properties of lexical items in large corpora church hanks 1990 etc at the same time this kind of massive dataacquisition should be made sensitive to the borders between perceptual experience lexical knowledge and expert knowledge ,0,1,0
1989 wettler rapp 1989 and church hanks 1990 describe algorithms which do this ,0,1,0
other representative collocation research can be found in church and hanks 1990 and smadja 1993 ,0,1,0
unlike choueka 1988 church and hanks 1990 identify as collocations both interrupted and uninterrupted sequences of words ,1,0,0
unlike church and hanks 1990 smadja 1993 goes beyond the twoword limitation and deals with collocations of arbitrary length ,0,0,1
following church and hanks 1990 they use mutual information to select significant twoword patterns but at the same time a lexical inductive process is incorporated which as they claim can improve the collection of domainspecific terms ,0,1,0
most of the previously proposed methods to extract compounds or to measure word association using mutual information mi either ignore or penalize items with low cooccurrence counts church and hanks 1990 su wu and chang 1994 because mi becomes unstable when the cooccurrence counts are very small ,0,1,0
the classifier uses mutual information mi scores rather than the raw frequences of the occurring patterns church and hanks 1990 ,0,1,0
of the works of kuplec pedersen and chen 1995 and brandow mltze and ran 1995 and advances summarmatlon technology by applynag corpusbased statistical nlp teehmques robust information extraction and readily avaalable onhne resources our prehxmnary experiments with combining different summarization features have been reported and our current effort to learn to combine these features to produce the best summaries has been described the features derived by these robust nlp techmques were also utihzed m presentmg multiple summaryvtews to the user m a novel way references advanced research projects agency 1995 proceedrigs of szth message understanding conference muc6 morgan kanfmann pubhshers brandow ron karl mltze and lisa ran 1995 automatic condensation of electromc pubhcatlous by sentence selection information processing and management 31 forthcoming bull eric 1993 a compsbased approach to language learning ph d thesm umverslty of pennsylvania church kenneth and patrick hanks 1990 word aesoclatlon norrns mutual information and lexicography computational lmgmstscs 161 church kenneth w 1995 one term or two 9 in proceedings of the 17th annual international sigir conference on research and development in informatzon retrzeral pages 310318 edmundson h p 1969 new methods m automatic abstracting journal of the acm 162 264228 fum dando glovanm gmda and carlo tasso 1985 evalutatmg importance a step towards text surnmarlzatlon in i3cai85 pages 840844ijcai aaai hahn udo 1990 topic parsing accounting for text macro structures m fulltext analysm in formaton processing and management 261135170 harman donna 1991 how effective is suttixang journal of the amerlcan sotcry for informatwn scence 421 715 harman donna 1996 overview of the fifth text retrieval conference tree5 in trec5 conference proceedings jmg y and b croft 1994 an assocatwn thesaurns for informatzon retrseval umass techmcal report 94i7 center for intelligent information retrieval university of massachusetts johnson f c c d prate w j black and a p neal 1993 ,0,1,0
ridf is like mi but different references church k and p hanks 1990word association norms mutual information and lexicography computational linguistics 161 pp ,0,1,0
a standard solution is to use a weighted linear mixture of ngram models 1 n n brown et al 1992 ,0,1,0
previous studies have shed light on the predictability of the next unix command that a user will enter motoda and yoshida 1997 davison and hirsch 1998 the next keystrokes on a small input device such as a pda darragh and witten 1992 and of the translation that a human translator will choose for a given foreign sentence nepveu et al 2004 ,0,1,0
successful approaches aimed at trying to overcome the sparse data limitation include backoff katz 1987 turinggood variants good 1953 church and gale 1991 interpolation jelinek 1985 deleted estimation jelinek 1985 church and gale 1991 similaritybased models dagan pereira and lee 1994 essen and steinbiss 1992 poslanguage models derouault and merialdo 1986 and decision tree models bahl et al 1989 black garside and leech 1993 magerman 1994 ,0,1,0
much research has been carried out recently in this area hughes and atwell 1994 finch and chater 1994 redington chater and finch 1993 brill et al 1990 kiss 1973 pereira and tishby 1992 resnik 1993 ney essen and kneser 1994 matsukawa 1993 ,0,1,0
dependency models rosenfeld 2000 use the parsed dependency structure of sentences to build the language model as in grammatical trigrams lafferty et al 1992 structured language models chelba and jelinek 2000 and dependency language models chelba et al 1997 ,0,1,0
work at the university of dundee eg aim et al 1992 todman and alm this volume has shown that the extensive use of fixed text for sequences such as greetings and prestored narratives is beneficial in aac ,0,1,0
most approaches brown et al 1992 li abe 1997 inherently extract semantic knowledge in the abstracted form of semantic clusters ,0,1,0
previous approaches to processing lnetonymy have used handconstructed ontologies or semantic networks ass 1988 iverson and hehnreich 1992 bmaud et al 1996 fass 1997 ,0,1,0
1992 describe one application of mi to identify word collocations kashioka et al ,0,1,0
we believe the benefit to limiting the size of n is connected to brown et als 1992 470 observation that as n increases the accuracy of an ngram model increases but the reliability of our parameter estimates drawn as they must be from a limited training text decreases ,0,1,0
applications of word clustering include language modeling brown et al 1992 text classification baker and mccallum 1998 thesaurus construction lin 1998 and so on ,0,1,0
we have used a stateoftheart chinese handwriting recognizer li et al 1992 developed by atc ccl itri taiwan as the basis of our experiments ,0,1,0
for a class bigram model find v c to maximize t ili pwi iwlpwilwi1 alternatively perplexity jardino an d adda 1993 or average mutual information brown et al 1992 can be used as the characteristic value for optimization ,0,1,0
introduction classbased language models brown et al 1992have been proposed for dealing with two problems confronted by the wellknown word ngram language models 1 data sparseness the amount of training data is insufficient for estimating the huge number of parameters and 2 domain robustness the model is not adaptable to new application domains ,0,1,0
have been proposed hindle 1990 brown et al 1992 pereira et al 1993 tokunaga et al 1995 ,0,1,0
in future work we plan to experiment with richer representations eg including longrange ngrams rosenfeld 1996 class ngrams brown et al 1992 grammatical features amaya and benedy 2001 etc ,0,1,0
2008 who employ clusters of related words constructed by the brown clustering algorithm brown et al 1992 for syntactic processing of texts ,0,1,0
models of this type include brown et al 1992 zitouni 2007 which use semantic word clustering and bahl et al 1990 which uses variablelength context ,0,1,0
various clustering techniques have been proposed brown et al 1992 jardino and adda 1993 martin et al 1998 which perform automatic word clustering optimizing a maximumlikelihood criterion with iterative clustering algorithms ,0,1,0
the smoothing methods proposed in the literature overviews are provided by dagan lee and pereira 1999 and lee 1999 can be generally divided into three types discounting katz 1987 classbased smoothing resnik 1993 brown et al 1992 364 computational linguistics volume 28 number 3 pereira tishby and lee 1993 and distanceweighted averaging grishman and sterling 1994 dagan lee and pereira 1999 ,0,1,0
the distortion probabilities are classbased they depend on the word class ff of a covered source word f as well as on the word class ee of the previously generated target word e the classes are automatically trained brown et al 1992 ,0,1,0
similaritybased smoothing hindle 1990 brown et al 1992 dagan marcus and markovitch 1993 pereira tishby and lee 1993 dagan lee and pereira 1999 provides an intuitively appealing approach to language modeling ,1,0,0
previous work has demonstrated that this scoring function is able to provide high discrimination power for a variety of applications su chiang and lin 1992 chen et al 1991 su and chang 1990 ,0,1,0
successful approaches aimed at trying to overcome the sparse data limitation include backoff katz 1987 turinggood variants good 1953 church and gale 1991 interpolation jelinek 1985 deleted estimation jelinek 1985 church and gale 1991 similaritybased models dagan pereira and lee 1994 essen and steinbiss 1992 poslanguage models derouault and merialdo 1986 and decision tree models bahl et al 1989 black garside and leech 1993 magerman 1994 ,0,1,0
introduction many applications that process natural language can be enhanced by incorporating information about the probabilities of word strings that is by using statistical language model information church et al 1991 church and mercer 1993 gale church and yarowsky 1992 liddy and paik 1992 ,0,1,0
furthermore our model is not necessarily nativist these biases may be innate but they may also be the product of some other earlier learning algorithm as the results of ellison 1992 and brown et al ,0,1,0
several authors for example krovetz and croft 1989 guthrie et al 1991 slator 1992 cowie guthrie and guthrie 1992 janssen 1992 bradenharder 1993 liddy and paik 1993 have attempted to improve results by using supplementary fields of information in the electronic version of the longman dictionary of contemporary english ldoce in particular the box codes and subject codes provided for each sense ,0,1,0
since then supervised learning from sensetagged corpora has since been used by several researchers zernik 1990 1991 hearst 1991 leacock towell and voorhees 1993 gale church and yarowsky 1992d 1993 bruce and wiebe 1994 miller et al ,0,1,0
a similar view underlies the classbased methods cited in section 243 brown et al 1992 pereira and tishby 1992 pereira tishby and lee 1993 ,0,1,0
this set of context vectors is then clustered into a predetermined number of coherent clusters or context groups using buckshot cutting et al 1992 a combination of the em algorithm and agglomerative clustering ,0,1,0
regardless of whether it takes the form of dictionaries lesk 1986 guthrie et al 1991 dagan itai and schwall 1991 karov and edelman 1996 thesauri yarowsky 1992 walker and amsler 1986 bilingual corpora brown et al 1991 church and gale 1991 or handlabeled training sets hearst 1991 leacock towell and voorhees 1993 niwa and nitta 1994 bruce and wiebe 1994 providing information for sense definitions can be a considerable burden ,0,1,0
another body of related work is the literature on word clustering in computational linguistics brown et al 1992 finch 1993 pereira tishby and lee 1993 grefenstette 1994a and document clustering in information retrieval van rijsbergen 1979 willett 1988 sparckjones 1991 cutting et al 1992 ,0,1,0
the second approach sekine et al 1992 chang luo and su 1992 resnik 1993a grishman and sterling 1994 alshawi and carter 1994 takes triples verb prep noun2 and nounl prep noun2 like those in table 10 as training data for acquiring semantic knowledge and performs ppattachment disambiguation on quadruples ,0,1,0
3 24 intonation annotations for our intonation annotation we have annotated the intonational phrase boundaries using the tobi tones and break indices definition silverman et al 1992 ,0,1,0
1992 and magerman 1994 used the clustering algorithm of brown et al ,0,1,0
413 alternative paraphrasing techniques to investigate the effect of paraphrase quality on automatic evaluation we consider two alternative paraphrasing resources latent semantic analysis lsa and brown clustering brown et al 1992 ,0,1,0
this can also be interpreted as a generalization of standard classbased models brown et al 1992 ,0,1,0
4 can be used to motivate a novel classbased language model and a regularized version of minimum discrimination information mdi models della pietra et al 1992 ,0,1,0
the most popular nondatasplitting methods for predicting test set crossentropy or likelihood are aic and variants such as aicc quasiaic qaic and qaicc akaike 1973 hurvich and tsai 1989 lebreton et al 1992 ,0,1,0
1999 and lee 1999 can be generally divided into three types discounting katz 1987 classbased smoothing resnik 1993 brown et al 1992 pereira et al 1993 and distanceweighted averaging grishman and sterling 1994 dagan et al 1999 ,0,1,0
2 related work a large amount of previous research on clustering has been focused on how to find the best clusters brown et al 1992 kneser and ney 1993 yamamoto and sagisaka 1999 ueberla 1996 pereira et al 1993 bellegarda et al 1996 bai et al 1998 ,0,1,0
many traditional clustering techniques brown et al 1992 attempt to maximize the average mutual information of adjacent clusters 21 2 12 2121 log ww wp wwp wwpwwi 2 where the same clusters are used for both predicted and conditional words ,0,1,0
proceedings of the 40th annual meeting of the association for cently semantic resources have also been used in collocation discovery pearce 2001 smoothing and model estimation brown et al 1992 clark and weir 2001 and text classi cation baker and mccallum 1998 ,0,1,0
most systems extract cooccurrence and syntactic information from the words surrounding the target term which is then converted into a vectorspace representation of the contexts that each target term appears in brown et al 1992 pereira et al 1993 ruge 1997 lin 1998b ,0,1,0
in many applications it is natural and convenient to construct classbased language models that is models based on classes of words brown et al 1992 ,0,1,0
agglomerative clustering eg brown et al 1992 li 1996 can produce hierarchical word categories from an unannotated corpus ,0,1,0
for example we can use automatically extracted hyponymy relations hearst 1992 shinzato and torisawa 2004 or automatically induced mn clusters rooth et al 1999 torisawa 2001 ,0,1,0
in addition the clustering methods used such as hmms and browns algorithm brown et al 1992 seem unable to adequately capture the semantics of mns since they are based only on the information of adjacent words ,0,0,1
they constructed word clusters by using hmms or browns clustering algorithm brown et al 1992 which utilize only information from neighboring words ,0,1,0
to scale lms to larger corpora with higherorder dependencies researchers work completed while this author was at google inc have considered alternative parameterizations such as classbased models brown et al 1992 model reduction techniques such as entropybased pruning stolcke 1998 novel represention schemes such as suffix arrays emami et al 2007 golomb coding church et al 2007 and distributed language models that scale more readily brants et al 2007 ,0,1,0
22 brown clustering algorithm in order to provide word clusters for our experiments we used the brown clustering algorithm brown et al 1992 ,0,1,0
129 5 active learning whereas a passive supervised learning algorithm is provided with a collection of training examples that are typically drawn at random an active learner has control over the labelled data that it obtains cohn et al 1992 ,0,1,0
we have 11 hypernym patterns based on patterns proposed by hearst 1992 and snow et al 2005 12 sibling patterns which are basically conjunctions and 13 partof patterns based on patterns proposed by girju et al 2003 and cimiano and wenderoth 2007 ,0,1,0
53 performance of taxonomy induction in this section we compare the following automatic taxonomy induction systems he the system by hearst 1992 with 6 hypernym patterns gi the system by girju et al ,0,1,0
clusteringbased approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors brown et al 1992 lin 1998 ,0,1,0
agglomerative clustering brown et al 1992 caraballo 1999 rosenfeld and feldman 2007 yang and callan 2008 iteratively merges the most similar clusters into bigger clusters which need to be labeled ,0,1,0
previous approaches eg miller et al 2004 and koo et al 2008 have all used the brown algorithm for clustering brown et al 1992 ,0,1,0
52 a data recovery task in the second evaluation the estimation method had to distinguish between members of two sets of 8it should be emphasized that the tws method uses only a monolingual target corpus and not a bilingual corpus as in other methods brown et al 1991 gale et al 1992 ,0,1,0
however only recently has work been done on the automatic computation of such relationships from text quantifying similarity between words and clustering them brown et al 1992 pereira et al 1993 ,0,1,0
the notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in brown et al 1992 ,1,0,0
272 similaritybased estimation was first used for language modeling in the cooccurrence smoothing method of essen and steinbiss 1992 derived from work on acoustic model smoothing by sugawara et al ,0,1,0
the classbased approaches brown et al 1992 resnik 1992 pereira et al 1993 calculate cooccurrence data of words belonging to different classes rather than individual words to enhance the cooccurrence data collected and to cover words which have low occurrence frequencies ,0,1,0
on the other hand the thesaurusbased method of yarowsky 1992 may suffer from loss of information since it is semiclassbased as well as data sparseness since h classes used in resnik 1992 are based on the wordnet taxonomy while classes of brown et al ,0,1,0
these 30 questions are determined by growing a classification tree on the word vocabulary as described in brown et al 1992 ,0,1,0
finally inducing lexical semantics from distributional data eg brown et al 1992 church et al 1989 is also a form of surface cueing ,0,1,0
given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues brown et al 1992 pereira et al 1993 hatzivassiloglou and mckeown 1993 identifying the semantic orientation of words would allow a system to further refine the retrieved semantic similarity relationships extracting antonyms ,0,1,0
wang and hirschberg 1992 wightman and ostendorf 1994 stolcke and shriberg 1996a kompe et al 1994 mast et al 1996 and on speech repair detection and correction eg ,0,1,0
black et al 1992 materman 1995 we treat the word identities as a further refinement of the pos tags thus we build a word classification tree for each pos tag ,0,1,0
in our experiments the class assignment is performed by maximizing the mutual information between adjacent phrases following the line described in brown 301 et al 1992 with only the modification that candidates to clustering are phrases instead of words ,0,1,0
to cope with this problem we 898 use the concept of class proposed for a word ngram model brown et al 1992 ,0,1,0
to avoid this problem we use the concept of class proposed for a word ngram model brown et al 1992 ,0,1,0
syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts eg brown et al 1992 yarowsky 1992 as well as in similar predicateargument structure contexts eg grishman and sterling 1994 ,0,1,0
furthermore early work on classbased language models was inconclusive brown et al 1992 ,0,1,0
in this spirit we introduce a generalization of the classic kgram models widely used for string processing brown et al 1992 ney et al 1995 to the case of trees ,0,1,0
for better probability estimation the model was extended to work with hidden word classes brown et al 1992 ward and issar 1996 ,0,1,0
in the literature approaches to construction of taxonomies of concepts have been proposed brown et al 1992 mcmahon and smith 1996 sanderson and croft 1999 ,0,1,0
the second type has clear interpretation as a probability model but no criteria to determine the number of clusters brown et al 1992 kneser and ney 1993 ,0,1,0
the idea of word class brown et al 1992 gives a general solution to this problem ,0,1,0
examples have been classbased d2gram models brown et al 1992 kneser and ney 1993 smoothing techniques for structural disambiguation li and abe 1998 and word sense disambiguation shutze 1998 ,0,1,0
among various language modeling approaches ngram modeling has been widely used in many applications such as speech recognition machine translation katz 1987 jelinek 1989 gale and church 1990 brown et al 1992 yang et al 1996 bai et al 1998 zhou et al 1999 rosenfeld 2000 gao et al 2002 ,0,1,0
the corpus used for training our models was on the order of 100000 words whereas that used by brown et al 1992 was around 1000 times this size ,0,1,0
in this article we used the algorithm of brown et al 1992 to initialize the model ,0,1,0
since there is no practical way of determining the classification a0 which maximizes this quantity for a given corpus brown et al 1992 use a greedy algorithm which proceeds from the initial classification performing the merge which results in the least loss in mutual information at each stage ,0,1,0
1994 uses the mutual information clustering algorithm described in brown et al 1992 ,0,1,0
it has been known for some years that good performance can be realized with partial tagging and a hidden markov model cutting et al 1992 ,0,1,0
fortunately using distributional characteristics of term contexts it is feasible to induce partofspeech categories directly from a corpus of suf cient size as several papers have made clear brown et al 1992 schcurrency1utze 1993 clark 2000 ,0,1,0
our approach to inducing syntactic clusters is closely related to that described in brown et al 1992 which is one of the earliest papers on the subject ,0,1,0
clark 2000 reports results on a corpus containing 12 million terms schcurrency1utze 1993 on one containing 25 million terms and brown et al 1992 on one containing 365 million terms ,0,1,0
this paper is heavily indebted to prior work on unsupervised learning of position categories such as brown et al 1992 schtze 1997 higgins 2002 and others cited there ,1,0,0
a key example is that of classbased language models brown et al 1992 dagan et al 1999 where clustering approaches are used in order to partition words determined to be similar into sets ,0,1,0
this approach to term clustering is closely related to others from the literature brown et al 1992 clark 20002 recall that the mutual information between random variables a0 and a1 can be written a2a4a3a6a5a8a7a10a9a11a13a12a15a14a17a16a19a18a21a20a23a22a25a24a27a26a29a28 a14a17a16a19a18a21a20a23a22a25a24 a14a17a16a19a18a30a24a31a14a17a16a19a22a32a24 1 here a0 and a1 correspond to term and context clusters respectively each event a18 and a22 the observation of some term and contextual term in the corpus ,0,1,0
many approaches for pos tagging have been developed in the past including rulebased tagging brill 1995 hmm taggers brants 2000 cutting and others 1992 maximumentropy models rathnaparki 1996 cyclic dependency networks toutanova et al 2003 memorybased learning daelemans et al 1996 etc all of these approaches require either a large amount of annotated training data for supervised tagging or a lexicon listing all possible tags for each word for unsupervised tagging ,0,1,0
these problems include collocation discovery pearce 2001 smoothing and estimation brown et al 1992 clark and weir 2001 and question answering pasca and harabagiu 2001 ,0,1,0
8 related research classbased lms brown et al 1992 or factored lms bilmes and kirchhoff 2003 are very similar to our tc scenario ,0,1,0
the approach is related but not identical to distributional similarity for details see brown et al 1992 and liang 2005 ,0,1,0
a number of knowledgerich jacobs and rau 1990 calzolari and bindi 1990 mauldin 1991 and knowledgepoor brown et al 1992 hindle 1990 ruge 1991 grefenstette 1992 methods have been proposed for recognizing when words are similar ,0,1,0
other statistical systems that address word classification probleans do not emphasize the use of linguistic knowledge and do not deal with a specific word classbrown et al 1992 or do not exploit as much linguistic knowledge as we do pereira et al 1993 ,0,0,1
similarly the sense disambiguation problem is typically attacked by comparing the distribution of the neighbors of a words occurrence to prototypical distributions associated with each of the words senses gale et al 1992 schtltze 1992 ,0,1,0
5 conclusions and future work the results of the evaluation are exlremely encouraging especially considering that disambiguating word senses to the level of finegrainedness found in wordnet is quite a bit more difficult than disambiguation to the level of homographs hearst 1991 cowie et al 1992 ,0,1,0
for example the sets of tags and rule labels have been clustered by our team grmmtrian while a vocabulary of about 60000 words has been clustered by machine brown et al 1992 ushioda 1996a ushioda 1996b ,0,1,0
the concept of mutual information taken from information theory was proposed as a measure of word association church 1990 jelinek et al 19901992 dagan 1995 ,0,1,0
semantic classification programs brown et al 1992 hatzivassiloglou and mckeown 1993 pereira et al 1993 use statistical information based on cooccurrence with appropriate marker words to partition a set of words into semantic groups or classes ,0,1,0
many authors claim that classbased methods are more robust against data sparseness problems dagan1994 pereira 1993 brown et al 1992 ,0,1,0
brown et al 1992 ,0,1,0
cutting et al 1992 feldweg 1995 the tagger for grammatical functions works with lexical and contextual probability measures pq ,0,1,0
aggregate models based on higherorder ngrams brown et al 1992 might be able to capture multiword structures such as noun phrases ,0,1,0
82 2 aggregate markov models in this section we consider how to construct classbased bigram models brown et al 1992 ,0,1,0
though several algorithms brown et al 1992 pereira tishby and lee 1993 have been proposed 100 9o 80 4o 20 1000 goo 80 41111 2 5 10 15 20 25 30 5 10 15 20 25 30 iteration of em iteration of em a b figure 1 plots of a training and b test perplexity versus number of iterations of the em algorithm for the aggregate markov model with c 32 classes ,0,1,0
several authors have used mutual information and similar statistics as an objective function for word clustering dagan et al 1993 brown et al 1992 pereira et al 1993 wang et al 1996 for automatic determination of phonemic baseforms lucassen mercer 1984 and for language modeling for speech recognition ries ct al 1996 ,0,1,0
as with similar work eg brown et al 1992 the size of the corpus makes preprocessing such as lemmatization pos tagging or partial parsing too costly ,0,0,1
this is in contrast to work by researchers such as schiitze and pedersen 1992 brown et al 1992 and futrelle and gauch 1995 where it is often the most frequent words in the lexicon which are clustered predominantly with the purpose of determining their grammatical classes ,0,1,0
precursors to this work include pereira et al 1993 brown et al 1992 brill kapur 1993 jelinek 1990 and brill et al 1990 and as applied to child language acquisition finch chater 1992 ,0,1,0
in the link grammar framework lagerty et al 1992 della pietra et al 1994 strictly local contexts are naturally combined with longdistance information coming from longrange trigrams ,0,1,0
most clustering schemes etal 1992 kneser and ney 1993 pereira et al 1993 mccandless and glass 1993 bellegarda et al 1996 saul and pereira 1997 use the average entropy reduction to decide when two words fall into the same cluster ,0,1,0
cutting et al 1992 feldweg 1995 the tagger for grammatical functions works with lexical 1 selbst besucht adv vvpp himself visited hat peter sabine vafin ne ne has peter sabine peter never visited sabine himself l hie adv never figure 2 example sentence and contextual probability measures po depending on the category of a mother node q ,0,1,0
their weights are calculated by deleted interpolation brown et al 1992 ,0,1,0
toiletbathroom since the word facility is the subject of employ and is modified by new in 3 we retrieve other words that appeared in the same contexts and obtain the following two groups of selectors the log a column shows the likelihood ratios dunning 1993 of these words in the local contexts subjects of employ with top20 highest likelihood ratios word freq iogk word freq org 64 504 plant 14 310 company 27 286 operation 8 230 industry 9 146 firm 8 135 pirate 2 121 unit 9 932 shift 3 848 postal service 2 773 machine 3 656 corporation 3 647 manufacturer 3 621 insurance company 2 606 aerospace 2 581 memory device 1 579 department 3 555 foreign office 1 541 enterprise 2 539 pilot 2 537 org includes all proper names recognized as organizations 18 modifiees of new with top20 highest likelihood ratios word freq logk post 432 9529 issue 805 9028 product 675 8886 rule 459 8758 law 356 5415 technology 237 3827 generation 150 3232 model 207 3193 job 260 2692 system 318 2518 word freq log bonds 223 2454 capital 178 2418 order 228 2365 version 158 2237 position 236 2073 high 152 2012 contract 279 1981 bill 208 1949 venture 123 1937 program 283 1838 since the similarity between sense 1 of facility and the selectors is greater than that of other senses the word facility in 3 is tagged sense the key innovation of our algorithm is that a polysemous word is disambiguated with past usages of other words ,0,1,0
we adopted loglikelihood ratio danning 1993 which gave the best pertbrmance among crude noniterative methods in our test experiments 6 ,0,1,0
several techniques and results have been reported on learning subcategorization frames sfs from text corpora webster and marcus 1989 brent 1991 brent 1993 brent 1994 ushioda et al 1993 manning 1993 ersan and charniak 1996 briscoe and carroll 1997 carroll and minnen 1998 carroll and rooth 1998 ,0,1,0
however dunning 1993 pointed out that for the purpose of corpus statistics where the sparseness of data is an important issue it is better to use the loglikelihood ratio ,0,1,0
as the strength of relevance between a target compound noun t and its cooccurring word r the feature value of r wtr is deflned by the log likelihood ratio dunning 1993 1 as follows ,0,1,0
the value of distdt can be defined in various ways and they found that using loglikelihood ratio see dunning 1993 worked best which is represented as follows 0 log log d k k td k k i m ii i i m ii i where k i and k i are the frequency of a word w i in dw and d 0 respectively and w 1w m is the set of all words in d 0 as stated in introduction distdt is normalized by the baseline function which is referred as b dist here ,0,1,0
in order to avoid this problem we implemented a simple bootstrapping procedure in which a seed data set of 100 instances of each of the eight categories was hand tagged and used to generate a decision list classifier using the c45 algorithm quinlan 1993 with the word frequency and topic signature features described below ,0,1,0
the topic signatures are automatically generated for each specific term by computing the likelihood ratio score between two hypotheses dunning 1993 ,0,1,0
its distribution is asymptotic to a 2 distribution and can hence be used as a test statistic dunning 1993 ,0,1,0
3 0 log 2 log a lh lh 1 problems for an unscaled log approach although log identifies collocations much better than competing approaches dunning 1993 in terms of its recall it suffers from its relatively poor precision rates ,1,0,0
we have begun experimenting with log likelihood ratio dunning 1993 as a thresholding technique ,0,1,0
the measure of predictiveness we employed is log likelihood ratio with respect to the target variable dunning 1993 ,0,1,0
however while similarity measures such as wordnet distance or lins similarity metric only detect cases of semantic similarity association measures such as the ones used by poesio et al or by garera and yarowsky also find cases of associative bridg497 lin98 rff they theyg2 pl03 land countrystateland staat staat kemalismus regierung kontinent state state kemalism government continent stadt stadt bauernfamilie prasident region city city agricultural family president region region landesregierung bankgesellschaft dollar stadt region country government banking corporation dollar city bundesrepublik bundesregierung baht albanien staat federal republic federal government baht albania state republik gewerkschaft gasag hauptstadt bundesland republic trade union a gas company capital state medikament medical drug arzneimittel pille ru patient arzneimittel pharmaceutical pill a drug patient pharmaceutical praparat droge abtreibungspille arzt lebensmittel preparation drug nonmedical abortion pill doctor foodstuff pille praparat viagra pille praparat pill preparation viagra pill preparation hormon pestizid pharmakonzern behandlung behandlung hormone pesticide pharmaceutical company treatment treatment lebensmittel lebensmittel praparat abtreibungspille arznei foodstuff foodstuff preparation abortion pill drug highest ranked words with very rare words removed ru 486 an abortifacient drug lin98 lins distributional similarity measure lin 1998 rff geffet and dagans relative feature focus measure geffet and dagan 2004 they association measure introduced by garera and yarowsky 2006 theyg2 similar method using a loglikelihoodbased statistic see dunning 1993 this statistic has a preference for higherfrequency terms pl03 semantic space association measure proposed by pado and lapata 2003 table 1 similarity and association measures most similar items ing like 1ab the result of this can be seen in table 2 while the similarity measures lin98 rff list substitutable terms which behave like synonyms in many contexts the association measures garera and yarowskys they measure pado and lapatas association measure also find noncompatible associations such as countrycapital or drugtreatment which is why they are commonly called relationfree ,0,1,0
as association measure we apply loglikelihood ratio dunning 1993 to normalized frequency ,0,1,0
beside simple cooccurrence counts within sliding windows other soa measures include functions based on tfidf fung and yee 1998 mutual information pmi lin 1998 conditional probabilities schuetze and pedersen 1997 chisquare test and the loglikelihood ratio dunning 1993 ,0,1,0
many studies on collocation extraction are carried out based on cooccurring frequencies of the word pairs in texts choueka et al 1983 church and hanks 1990 smadja 1993 dunning 1993 pearce 2002 evert 2004 ,0,1,0
one popular and statistically appealing such measure is loglikelihood ll dunning 1993 ,1,0,0
for example in this work we use loglikelihood ratio dunning 1993 to determine the soa between a word sense and cooccurring words and cosine to determine the distance between two dpwss log likelihood vectors mcdonald 2000 ,0,1,0
we then scored each query pair q1q2 in this subset using the loglikelihood ratio llr dunning 1993 between q1 and q2 which measures the mutual dependence within the context of web search queries jones et al 2006a ,0,1,0
the significance values are obtained using the loglikelihood measure assuming a binomial distribution for the unrelatedness hypothesis dunning 1993 ,0,1,0
conditional probability the loglikelihood ratio and resniks 1993 selectional association measure were also significantly correlated with plausibility ratings ,0,1,0
the research presented in this paper is similar in motivation to resniks 1993 work on selectional restrictions ,0,1,0
several other measures like loglikelihood dunning 1993 pearsons a2a4a3 church et al 1991 zscore church et al 1991 cubic association ratio mi3 etc have been also proposed ,0,1,0
all the enumerated segment pairs are listed in the following table feature xy feature xy am11 c1 c0 am21 c2c1 c0 am12 c1 c0c1 am22 c2c1 c0c1 am13 c1 c0c1c2 am31 c3c2c1 c0 we use dunnings method dunning 1993 because it does not depend on the assumption of normality and it allows comparisons to be made between the signiflcance of the occurrences of both rare and common phenomenon ,1,0,0
until now translation models have been evaluated either subjectively eg white and oconnell 1993 or using relative metrics such as perplexity with respect to other models brown et al 1993b ,0,1,0
8 an alternative formula for g 2 is given in dunning 1993 but the two are equivalent ,0,1,0
there are good reasons for using such a handcrafted genrespecific verb lexicon instead of a general resource such as wordnet or levins 1993 classes many verbs used in the domain of scientific argumentation have assumed a specialized meaning which our lexicon readily encodes ,0,1,0
other corpusbased methods determine associations between words grefenstette 1992 dunning 1993 lin et al 1998 which yields a basis for computing thesauri or dictionaries of terminological expressions and multiword lexemes gaizauskas demetriou and humphreys 2000 grefenstette 2001 ,0,1,0
from multilingual texts translation lexica can be generated gale and church 1991 kupiec 1993 kumano and hirakawa 1994 boutsis piperidis and demiros 1999 grefenstette 1999 ,0,1,0
as has been pointed out by dunning 1993 the calculation of log assumes a binomial distribution ,0,1,0
a period should therefore be interpreted as an abbreviation marker and not as a sentence boundary marker if the two tokens surrounding it can indeed be considered as a collocation according to dunnings 1993 original loglikelihood ratio amended with the onesidedness constraint introduced in section 22 ,0,1,0
21 likelihood ratios in the typebased stage the loglikelihood ratio by dunning 1993 tests whether the probability of a word is dependent on the occurrence of the preceding word type ,0,1,0
the third function is an original variant of the second the fourth is original and the fifth is prompted by the arguments of dunning 1993 ,0,1,0
translation lexicon entries were scored according to the log likelihood ratio dunning 1993 cf ,0,1,0
dunning 1993 also used windows of size 2 which corresponds to word bigrams ,0,1,0
dunning 1993 used a likelihood ratio to test word similarity under the assumption that the words in text have a binomial distribution ,0,1,0
1 introduction many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts dunning 1993 church and hanks 1990 dagan et al 1999 ,0,1,0
the chunker is trained on the answer side of the training corpus in order to learn 2 and 3word collocations defined using the likelihood ratio of dunning 1993 ,0,1,0
babar uses the loglikelihood statistic dunning 1993 to evaluate the strength of a cooccurrence relationship ,0,1,0
 significant neighborbased cooccurrence as discussed in dunning 1993 it is possible to measure the amount of surprise to see two neighboring words in a corpus at a certain frequency under the assumption of independence ,0,1,0
throughout the likelihood ratio dunning 1993 is used as significance measure because of its stable performance in various evaluations yet many more measures are possible ,1,0,0
we then ranked the collected query pairs using loglikelihoodratiollrdunning 1993 whichmeasures the dependence between q1 and q2 within the context of web queries jones et al 2006b ,0,1,0
these methods often involve using a statistic such as 2 gale and church 1991 or the log likelihood ratio dunning 1993 to create a score to measure the strength of correlation between source and target words ,0,1,0
these constraints tie words in such a way that the space of alignments cannot be enumerated as in ibm models 1 and 2 brown et al 1993 ,0,1,0
our approach was to identify a parallel corpus of manually and automatically transcribed documents the tdt2 corpus and then use a statistical approach dunning 1993 to identify tokens with significantly table 5 impact of recall and precision enhancing devices ,0,1,0
second the significance of the ks distance in case of the null hypothesis data sets are drawn from same distribution can be calculated press et al 1993 ,0,1,0
smadja 1993 also detailed techniques for collocation extraction and developed a program called xtract which is capable of computing flexible collocations based on elaborated statistical calculation ,0,1,0
 m aj j m j aj l i i l i ii m j j mlajdeft en pp m ap 01 11 1 2 0 0 0 pr 00 eef 3 1 a cept is defined as the set of target words connected to a source word brown et al 1993 ,0,1,0
in order to filter the noise caused by the error alignment links we only retain those translation pairs whose loglikelihood ratio scores dunning 1993 are above a threshold ,0,1,0
22 using loglikelihoodratios to estimate word translation probabilities our method for computing the probabilistic translation lexicon llrlex is based on the the log2httpwwwfjochcomgizahtml likelihoodratio llr statistic dunning 1993 which has also been used by moore 2004a 2004b and melamed 2000 as a measure of word association ,0,1,0
to identify these termsweusetheloglikelihoodstatisticsuggested by dunning dunning 1993 and first used in summarization by lin and hovy hovy and lin 2000 ,0,1,0
partitioning 2 medium and low frequency words as noted in dunning 1993 loglikelihood statistics are able to capture word bigram regularities ,0,1,0
ii apply some statistical tests such as the binomial hypothesis test brent 1993 and loglikelihood ratio score dunning 1993 to sccs to filter out false sccs on the basis of their reliability and likelihood ,0,1,0
loglikelihood ratio g2 dunning 1993 with respect to a large reference corpus web 1t 5gram corpus brants and franz 2006 is used to capture the contextually relevant nouns ,0,1,0
many researchers smadja 1991 srihari baltus 1993 have suggested that the informationtheoretic notion of mutual information score mis directly captures the idea of context ,0,1,0
64 table 1 subjects of employ with highest likelihood ratio word freq loga word freq loga brg 64 504 plant 14 310 company 27 286 operation 8 230 industry 9 146 firm 8 135 pirate 2 121 unit 9 932 shift 3 848 postal service 2 773 machine 3 656 corporation 3 647 manufacturer 3 621 insurance company 2 606 aerospace 2 581 memory device 1 579 department 3 555 foreign office 1 541 enterprise 2 539 pilot 2 537 org includes all proper names recognized as organizations the loga column are their likelihood ratios dunning 1993 ,0,1,0
1990 1993 these models have nonuniform linguistically motivated structure at present coded by hand ,0,1,0
5 effectiveness comparison 51 englishchinese atis models both the transfer and transducer systems were trained and evaluated on englishtomandarin chinese translation of transcribed utterances from the atis corpus hirschman et al 1993 ,0,1,0
macklovitch 1994 melamed 1996b concordancing for bilingual lexicography catizone et al 1993 gale church 1991 computerassisted language learning corpus linguistics melby ,0,1,0
for each cooccurring pair of word types u and v these likelihoods are initially set proportional to their cooccurrence frequency nuv and inversely proportional to their marginal frequencies nu and nv z following dunning 1993 2 ,0,1,0
table lookup using an explicit translation lexicon is sufficient and preferable for many multilingual nlp applications including crummy mt on the world wide web church iiovy 1993 certain machineassisted translation tools eg ,0,1,0
1 introduction early works gale and church 1993 brown et al 1993 and to a certain extent kay and r6scheisen 1993 presented methods to exct biigua ,0,1,0
for the final ranking we chose the log likelihood statistic outlined in dunning 1993 which is based upon the cooccurrence counts of all nouns see dunning for details ,0,1,0
the frequency counts of dependency relationships are filtered with the loglikelihood ratio dunning 1993 ,0,1,0
not only many combinations are found in the corpus many of them have very similar mutual information values to that of 318 table 2 economic impact verb economic financial political social budgetary ecological economic economic economic economic economic economic economic economic economic object impact impact impact impact impact impact effect implication consequence significance fallout repercussion potential ramification risk mutual freq info 171 185 127 172 46 050 15 094 8 320 4 259 84 070 17 080 59 188 10 084 7 166 7 184 27 124 8 219 17 033 nomial distribution can be accurately approximated by a normal distribution dunning 1993 ,0,1,0
a total of 216 collocations were extracted shown in appendix a we compared the collocations in appendix a with the entries for the above 10 words in the ntcs english idioms dictionary henceforth ntceid spears and kirkpatrick 1993 which contains approximately 6000 definitions of idioms ,0,1,0
we preferred the loglikelihood ratio to other statistical scores such as the association ratio church and hanks 1990 or 2 since it adequately takes into account the frequency of the cooccurring words and is less sensitive to rare events and corpussize dunning 1993 daille 1996 ,1,0,0
it is faster and more mnemonic than the one in dunning 1993 ,0,0,1
however in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case see also grefenstette 1993 so we did not try to include these methods in our system ,0,1,0
they were based on mutual information church hanks 1989 conditional probabilities rapp 1996 or on some standard statistical tests such as the chisquare test or the loglikelihood ratio dunning 1993 ,1,0,0
dunning 1993 reports that we should not rely on the assumption of a normal distribution when performing statistical text analysis and suggests that parametric analysis based on the binomial or multinomial distributions is a better alternative for smaller texts ,0,1,0
adopting the scf acquisition system of briscoe and carroll we have experimented with an alternative hypothesis test the binomial loglikelihood ratio llr test dunning 1993 ,0,1,0
since we need knowledgepoor daille 1996 induction we cannot use humansuggested filtering chisquared g24 2 church and gale 1991 zscore smadja 1993 fontenelle et al 1994 students tscore church and hanks 1990 ngram list in accordance to each probabilistic algorithm ,0,1,0
method number of frames number of verbs linguistic resources fscore evaluation based on a gold standard coverage on a corpus c manning 1993 19 200 pos tagger simple finite state parser 58 t briscoe j carroll 1997 161 14 full parser 55 a sarkar d zeman 2000 137 914 annotated treebank 88 d kawahara et al ,0,1,0
one aspect of vpcs that makes them dicult to extract cited in eg smadja 1993 is that the verb and particle can be noncontiguous eg hand the paper in and battle right on ,0,1,0
neats computes the likelihood ratio dunning 1993 to identify key concepts in unigrams bigrams and trigrams and clusters these concepts in order to identify major subtopics within the main topic ,0,1,0
to make sense tagging more precise it is advisable to place constraint on the translation counterpart c of w swat considers only those translations c that has been linked with w based the competitive linking algorithm melamed 1997 and logarithmic likelihood ratio dunning 1993 ,0,1,0
there is potential of developing sense definition model to identify and represent semantic and stylistic differentiation reflected in the mrd glosses pointed out in dimarco hirst and stede 1993 ,0,1,0
the approach is in the spirit of smadja 1993 on retrieving collocations from text corpora but is more integrated with parsing ,0,1,0
3 related work word collocation various collocation metrics have been proposed including mean and variance smadja 1994 the ttest church et al 1991 the chisquare test pointwise mutual information mi church and hanks 1990 and binomial loglikelihood ratio test blrt dunning 1993 ,0,1,0
for our baseline we have selected the method based on binomial loglikelihood ratio test blrt described in dunning 1993 ,0,1,0
many statistical metrics have been proposed including pointwise mutual information mi church et al 1990 mean and variance hypothesis testing ttest chisquare test etc loglikelihood ratio lr dunning 1993 statistic language model tomokiyo et al 2003 and so on ,0,1,0
candidate term segment result of gpws for one sentence in which term appears table 2 examples of candidates eliminated by gpws 5 relative frequency ratio against background corpus relative frequency ratio rfr is a useful method to be used to discover characteristic linguistic phenomena of a corpus when compared with another damerau 1993 ,0,1,0
information extraction approaches that infer labeled relations either require substantial handcreated linguistic or domain knowledge eg craven and kumlien 1999 hull and gomez 1993 or require humanannotated training data with relation information for each domain craven et al 1998 ,0,1,0
we use the log likelihood ratio llr dunning 1993 given by 2log 2 h o pk 1n 1k 2n 2 h a p 1p 2 n 1k 1n 2k 2 llr measures the extent to which a hypothesized model of the distribution of cell counts h a differs from the null hypothesis h o namely that the percentage of documents containing this term is the same in both corpora ,0,1,0
the statistical significance often evaluate whether two words are independant using hypothesis tests such as tscore church et al 1991 the x2 the loglikelihood dunning 1993 and fishers exact test pedersen 1996 ,0,1,0
the various extraction measures have been discussed in great detail in the literature manning and schutze 1999 mckeown and radev 2000 their performance has been compared dunning 1993 pedersen 1996 evert and krenn 2001 and the methods have been combined to improve overall performance inkpen and hirst 2002 ,0,1,0
a second pass aligns the sentences in a way similar1 to the algorithm described by gale and church 1993 but where the search space is constrained to be close to the one delimited by the word alignment ,0,1,0
in our case we computed a likelihood ratio score dunning 1993 for all pairs of english tokens and inuktitut substrings of length ranging from 3 to 10 characters ,0,1,0
when efficient techniques have been proposed brown et al 1993 och and ney 2003 they have been mostly evaluated on safe pairs of languages where the notion of word is rather clear ,0,1,0
since in these lvcs the complement is a predicative noun in stem form identical to a verb we form development and test expressions by combining give or take with verbs from selected semantic classes of levin 1993 taken from stevenson et al ,0,1,0
breidt1993 alsopointedouta coupleof problemsthatmakes extractionfor germanmoredifficultthanfor english the stronginflectionfor verbsthe variable wordorderandthepositionalambiguityoftheargumentssheshowsthatevendistinguishingsubjectsfromobjectsisverydifficultwithoutparsing ,0,1,0
such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the british 3 linguist jr firth you shall know a word by the company it keeps firth 1957 p 11 context similarity has been used as a means of extracting collocations from corpora eg by church hanks 1990 and by dunning 1993 of identifying word senses eg by yarowski 1995 and by schutze 1998 of clustering verb classes eg by schulte im walde 2003 and of inducing selectional restrictions of verbs eg by resnik 1993 by abe li 1996 by rooth et al ,0,1,0
typicality was measured using the loglikelihood ratio test dunning 1993 ,0,1,0
the other 5 have been suggested for dutch by hollebrandse 1993 ,0,1,0
it is known that pmi gives undue importance to low frequency events dunning 1993 therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus ,0,1,0
in this case we use the loglikelihood measure as described in dunning 1993 ,0,1,0
the outcomes of cw resemble those of mincut wu leahy 1993 dense regions in the graph are grouped into one cluster while sparsely connected regions are separated ,0,1,0
it was later applied by dunning 1993 as a way to determine if a sequence of n words ngram came from an independently distributed sample ,0,1,0
in this study we have concentrated on the npsterm extraction which comprises the focus of interest in several studies jacquemin 2001 justeson katz 1995 voutanen 1993 ,0,1,0
generative word alignment models initially developed at ibm brown et al 1993 and then augmented by an hmmbased model vogel et al 1996 have provided powerful modeling capability for word alignment ,0,1,0
prcj1aj1ei1 pjii 1j jproductdisplay j1 pcjeaj 8 312 loglikelihood ratio the loglikelihood ratio statistic has been found to be accurate for modeling the associations between rare events dunning 1993 ,1,0,0
it can be expected that the loglikelihood ratio produces an accurate ranking of word pairs that highly correlates with human judgment dunning 1993 although there are other measures which come close in performance eg rapp 1998 ,1,0,0
corpus dunning 1993 scott 1997 rayson et al 2004 ,0,1,0
the most obvious comparison takes on the form of a keyword analysis which looks for the words that are significantly more frequent in the one corpus as compared to the other dunning 1993 scott 1997 rayson et al 2004 ,0,1,0
we worked with an implementation of the log likelihood ratio gscore as proposed by dunning 1993 and two variants of the tscore one considering all values tscore and one where only positive values tscore are kept following the results of curran and moens 2002 ,0,1,0
given a contextual word cw that occurs in the paragraphs of bc a loglikelihood ratio g2 test is employed dunning 1993 which checks if the distribution of cw in bc is similar to the distribution of cw in rc pcwbc pcwrc null hypothesis ,0,1,0
the interest reader is referred to basili et al 1993 b and c for a summary of ariosto an integrated tool for extensive acquisition of lexieal knowledge from corpora that we used to demonstrate and validate our approach ,0,1,0
this statistic is given by 2 log a 2log lp1 kl hi log lp2 k2 n2log lp kl r1log lp k2 n2 where log lco k n k logp n klog1 p and pl p2 p for a detailed description of the statistic used see dunning 1993 ,0,1,0
dunning 1993 and pedersen 1996 shows how some of the methods which have been used in the past particularly mutual information scores are invalid for rare events and introduce accurate measures of how surprising rare events are ,1,0,0
31 the genderanimaticity statistics after we have identified the correct antecedents it is a simple counting procedure to compute ppwa where wa is in the correct antecedent for the pronoun p note the pronouns are grouped by their gender wain the antecedent for p ppl o when there are multiple relevant words in the antecedent we apply the likelihood test designed by dunning 1993 on all the words in the candidate np ,0,1,0
c c c pcvr is just the probability of the disjunction of the concepts in c that is zpclv r cec in order to see how pclvr relates to the input data note that given a concept c verb v and argument position r a noun can be generated according to the distribution pnc v r where pnlc v r 1 nesync now we have a model for the input data pn v r pvrpnivr pvr pclv rlpntc vr cecnn note that for c cnn pnlc v r o the association norm and similar measures such as the mutual information score have been criticised dunning 1993 because these scores can be greatly overestimated when frequency counts are low ,0,1,0
although this approach can give inaccurate estimates the counts given to the incorrect senses will disperse randomly throughout the hierarchy as noise and by accumulating counts up the hierarchy we will tend to gather counts from the correct senses of related words yarowsky 1992 resnik 1993 ,0,1,0
some methods use sentence alignment and additional statistics to find candidate translations of terms smadja 1992 van der eijk 1993 ,0,1,0
for instance one might be interested in frequencies of cooccurences of a word with other words and phrases collocations smadja 1993 or one might be interested in inducing wordclasses from the text by collecting frequencies of the left and right context words for a word in focus finchchater 1993 ,0,1,0
smadja 1993 1 ,0,1,0
smadja 1993 which is the classic work on collocation extraction uses a twostage filtering model in which in the first step ngram statistics determine possible collocations and in the second step these candidates are submitted to a syntactic valida7of course lexical material is always at least partially dependent on the domain in question ,1,0,0
we propose a corpusbased method biber1993 nagao1993 smadja1993 which generates noun classifier associations nca to overcome the problems in classifier assignment and semantic construction of noun phrase ,0,1,0
the user can select characters by their frequencies ie f and g options the top or bottom n ie m and n options their ranks ie r and s options and by their frequencies above two standard deviations phls the mean smadja 1993 ie z option ,0,1,0
iegarling lhis lypu of olloeation the approaches till ilow could be diviled inlo two groups those thai do uo refer to sttbstrings of colloco l ions as a lartiular problem church and llanks t99 kim and cho 1993 nagao and mori 1994 and those that do kita et al t994 smadja 1993 lkchara et al 1995 kjelhner 11994 ,0,1,0
some papers fung wu 1994 wang et al 1994 based on smadjas paradigm 1993 learned an aided dictionary from a corpus to reduce the possibility of unknown words ,0,1,0
they first extract english collocations using the xtract systetn smadja 1993 and theu look for french coutlterparts ,0,1,0
there are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words church and hanks 1990 the distance between words smadja and makeown 1990 and the number of combined words and frequency of appearance kita 1993 1994 ,0,1,0
thus conventional methods had to introduce some kinds of restrictions such as the limitation of the kind of chains or the length of chains to be extracted smadja 1993 shinnou and isahara 1995 ,0,1,0
smadja 1993finds significant bigrams using an estimate of zscore deviation from an expected mean ,0,1,0
smadja 1993p168 kita al ,0,1,0
these measures have in fact been used previously in measuring term recognition smadja 1993 bourigault 1994 lauriston 1994 ,0,1,0
since odds p1 p we multiply both sides of definition 3 by 1pue1 to obtain pue 1pue peupu pe1pue 7 by substituting equation 6 in equation 7 and later applying the multiplication rule puepe peupu to it we will obtain pue pue peupu peupu 8 we proceed to take the log of the odds in equation 8 ie logit to get log peupeu log puepue log pupu 9 while it is obvious that certain words tend to cooccur more frequently than others ie idioms and collocations such phenomena are largely arbitrary smadja 1993 ,0,1,0
the use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research church and hanks 1990 zernik and jacobs 1990 hindle 1990 smadja 1993 ,0,1,0
as we remarked earlier however the input data required by our method triples could be generated automatically from unparsed corpora making use of existing heuristic rules brent 1993 smadja 1993 although for the experiments we report here we used a parsed corpus ,0,1,0
some studies have been done for acquiring collocation translations using parallel corpora smadja et al 1996 kupiec 1993 echizenya et al 2003 ,0,1,0
these range from twoword to multiword with or without syntactic structure smadja 1993 lin 1998 pearce 2001 seretan et al 2003 ,0,1,0
3 schone jurafskys results indicate similar results for loglikelihood tscore and strong parallelism among informationtheoretic measures such as chisquared selectional association resnik 1996 symmetric conditional probability ferreira and pereira lopes 1999 and the zscore smadja 1993 ,0,1,0
it is true that various term extraction systems have been developed such as xtract smadja 1993 termight dagan church 1994 and terms justeson katz 1995 among others cf ,0,1,0
this fact is being seriously challenged by current research and might not be true in the near future smadja 1993 151 ,0,1,0
4 related work algorithms for retrieving collocations has been described smadja 1993 haruno et al 1996 ,0,1,0
smadja 1993 proposed a method to retrieve collocations by combining bigrams whose cooccurrences are greater than a given threshold 3 ,0,1,0
we then replaced fi with its associated zscore ke ke is the strength of code frequency f at lt and represents the standard deviation above the average of frequency favet referring to smadjas definition smadja 1993 the standard deviation at at lt and strength kft of the code frequencies are defined as shown in formulas 1 and 2 ,0,1,0
in smadja 1993 automatically extracted collocations are judged by a lexicographer ,0,1,0
some examples of language reuse include collocation analysis smadja 1993 the use of entire factual sentences extracted from corpora eg toy story is the academy award winning animated film developed by pixar and summarization using sentence extraction paice 1990 kupiec et al 1995 ,0,1,0
it seems nevertheless that all 2church and hanks 1989 smadja 1993 use statistics in their algorithms to extract collocations from texts ,0,1,0
the recurrence property had been utilized to extract keywords or keyphrases from text chien 1999 fung 1998 smadja 1993 ,0,1,0
since we need knowledgepoor daille 1996 induction we cannot use humansuggested filtering chisquared g24 2 church and gale 1991 zscore smadja 1993 fontenelle et al 1994 students tscore church and hanks 1990 ngram list in accordance to each probabilistic algorithm ,0,1,0
one aspect of vpcs that makes them dicult to extract cited in eg smadja 1993 is that the verb and particle can be noncontiguous eg hand the paper in and battle right on ,0,1,0
related works generally speaking approaches to mwe extraction proposed so far can be divided into three categories a statistical approaches based on frequency and cooccurrence affinity b knowledgebased or symbolic approaches using parsers lexicons and language filters and c hybrid approaches combining different methods smadja 1993 dagan and church 1994 daille 1995 mcenery et al 1997 wu 1997 wermter et al 1997 michiels and dufour 1998 merkel and andersson 2000 piao and mcenery 2001 sag et al 2001a 2001b biber et al 2003 ,0,1,0
in his xtract system smadja 1993 first extracted significant pairs of words that consistently cooccur within a single syntactic structure using statistical scores called distance strength and spread and then examined concordances of the bigrams to find longer frequent multiword units ,0,1,0
the precision rate using the lexical statistics approach can reach around 60 if both word bigram extraction and ngram extractions are taking into account smadja 1993 lin 1997 and lu et al 2003 ,0,1,0
there are several basic methods for evaluating associations between words based on frequency counts choueka 1988 wettler and rapp 1993 information theoretic church and hanks 1990 and statistical significance smadja 1993 ,0,1,0
morphosyntacticinformationhas in fact been shown to significantlyimprove the extractionresults breidt 1993 smadja 1993 zajac et al 2003 ,0,1,0
morphologicaltoolssuch as lemmatizers andpostaggersarebeingcommonlyusedin extractionsystemsthey areemployedbothfordealingwithtext variationandfor validatingthe candidatepairs combinationsof functionwordsare typicallyruledout justesonand katz 1995as are the ungrammaticalcombinationsin the systemsthatmake useofparserschurchandhanks 1990smadja1993basilietal ,0,1,0
given the motivations for performing a linguisticallyinformedextraction whichwere also put forth among others by church and hanks199025 smadja1993151 and heid 1994 and given the recent developmentof linguisticanalysistoolsitseemsplausiblethatthe linguisticstructurewill be more and more taken intoaccountbycollocationextractionsystems ,0,1,0
2 related work the issue of mwe processing has attracted much attention from the natural language processing nlp community including smadja 1993 dagan and church 1994 daille 1995 1995 mcenery et al 1997 wu 1997 michiels and dufour 1998 maynard and ananiadou 2000 merkel and andersson 2000 piao and mcenery 2001 sag et al 2001 tanaka and baldwin 2003 dias 2003 baldwin et al 2003 nivre and nilsson 2004 pereira et al ,0,1,0
we argue that linguistic knowledge could not only improve results krenn 2000b smadja 1993 but is essential when extracting collocations from certain languages this knowledge provides other applications or a lexicon user respectively with a negrained description of how the extracted collocations are to be used in context ,0,1,0
lastly collocations are domaindependent smadja 1993 and languagedependent ,0,1,0
unlike church and hanks 1990 smadja 1993 goes beyond the twoword limitation and deals with collocations of arbitrary length ,0,1,0
now with the availability of largescale corpus automatic acquisition of word compositions especially word collocations from them have been extensively studiedeg choueka et al 1988 church and hanks 1989 smadja 1993 ,0,1,0
based on this assumption smadja 1993 stored all bigrams of words along with their relative position p 5 p 5 ,0,1,0
21 the evaluator the evaluator is a function ptt s which assigns to each targettext unit t an estimate of its probability given a source text s and the tokens t which precede t in the current translation of s 1 our approach to modeling this distribution is based to a large extent on that of the ibm group brown et al 1993 but it differs in one significant aspect whereas the ibm model involves a noisy channel decomposition we use a linear combination of separate predictions from a language model ptlt and a translation model ptls ,0,1,0
techniques for weakening the independence assumptions made by the ibm models 1 and 2 have been proposed in recent work brown et al 1993 berger et al 1996 och and weber 98 wang and waibel 98 wu and wong 98 ,0,1,0
3 bilingual task an application for word alignment 31 sentence and word alignment bilingual alignment methods warwick et al 1990 brown et al 1991a brown et al 1993 gale and church 1991b gale and church 1991a kay and roscheisen 1993 simard et al 1992 church 1993 kupiec 1993a matsumoto et al 1993 dagan et al 1993 ,0,1,0
algorithms for the more difficult task of word alignment were proposed in gale and church 1991a brown et al 1993 dagan et al 1993 and were applied for parameter estimation in the ibm statistical machine translation system brown et al 1993 ,0,1,0
we have been using the output of wordalign a robust alignment program that proved useful for bilingual concordancing of noisy texts dagan et al 1993 ,0,1,0
it also differs from previous proposals on lexical acquisition using statistical measures such as church et al 1991 brent 1991 brown et al 1993 which either deny the prior existence of linguistic knowledge or use linguistic knowledge in ad hoc ways ,0,0,1
the computation mechanism of gp and lp bears a resemblance to the em algorithmdempster et al 1977 brown et al 1993 which iteratively computes maximum likelihood estimates from incomplete data ,0,1,0
in statistical machine translation ibm 15 models brown et al 1993 based on the sourcechmmel model have been widely used and revised for many language donmins and applications ,1,0,0
thus a lot of alignment techniques have been suggested at the sentence gale et al 1993 phrase shin et al 1996 nomt thrase kupiec 1993 word brown et al 1993 berger et al 1996 melamed 1997 collocation smadja et al 1996 and terminology level ,0,1,0
in our machine anslation system transfer rules are generated automatically from parsed parallel text along the lines of matsulnoto el al 1993 meyers et al 1996 meyers et al 1998b ,0,1,0
these transtbr rules are pairs of corresponding rooted substructures where a substructure matsumoto et al 1993 is a connected set of arcs and nodes ,0,1,0
many existing systems tbr smt wang and waibel 1997 niefien et al 198 och and weber 1998 make use of a special way of structuring the string translation model brown et al 1993 lhe correspondence between the words in the source and the target string is described by aligmuents that assign one target word position to each source word position ,0,1,0
1087 model 3 of brown et al 1993 is a zeroorder alignment model like model 2 including in addition fertility paranmters ,0,1,0
model 4 of brown et al 1993 is also a firstorder alignment model along the source positions like the hmm trot includes also fertilities ,0,1,0
as in tile hmm we easily can extend the dependencies in the alignment model of model 4 easily using the word class of the previous english word e gci or the word class of the french word f gij brown et al 1993 ,0,1,0
most smt models brown et al 1993 vogel et al 1996 try to model wordtoword correslondences between source and target words using an alignment nmpling from source losition j to target position i aj ,0,1,0
related work 21 translation with nonparallel corpora a straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora eg brown et al 1993 ,0,1,0
for example the statistical word alignment in ibm translation models brown et al 1993 can only handle word to word and multiword to word alignments ,0,0,1
2 statistical word alignment statistical translation models brown et al 1993 only allow word to word and multiword to word alignments ,0,0,1
1 introduction bilingual word alignment is first introduced as an intermediate result in statistical machine translation smt brown et al 1993 ,0,1,0
besides being used in smt it is also used in translation lexicon building melamed 1996 transfer rule learning menezes and richardson 2001 examplebased machine translation somers 1999 etc in previous alignment methods some researches modeled the alignments as hidden parameters in a statistical translation model brown et al 1993 och and ney 2000 or directly modeled them given the sentence pairs cherry and lin 2003 ,0,1,0
word alignment models were first introduced in statistical machine translation brown et al 1993 ,0,1,0
6 related work the popular ibm models for statistical machine translation are described in brown et al 1993 ,1,0,0
estimated clues are derived from the parallel data using for example measures of cooccurrence eg the dice coefficient smadja et al 1996 statistical alignment models eg ibm models from statistical machine translation brown et al 1993 or string similarity measures eg the longest common subsequence ratio melamed 1995 ,0,1,0
word alignment models were first introduced in statistical machine translation brown et al 1993 ,0,1,0
using the ibm translation models ibm1 to ibm5 brown et al 1993 as well as the hiddenmarkov alignment model vogel et al 1996 we can produce alignments of good quality ,1,0,0
6 related work a description of the ibm models for statistical machine translation can be found in brown et al 1993 ,0,1,0
a detailed description of the popular translationalignment models ibm1 to ibm5 brown et al 1993 as well as the hiddenmarkov alignment model hmm vogel et al 1996 can be found in och and ney 2003 ,1,0,0
2 related work the popular ibm models for statistical machine translation are described in brown et al 1993 and the hmmbased alignment model was introduced in vogel et al 1996 ,1,0,0
detailed description of those models can be found in brown et al 1993 vogel et al 1996 and och and ney 2003 ,0,1,0
for scoring mt outputs the proposed rscm uses a score based on a translation model called ibm4 brown et al 1993 tmscore and a score based on a language model for the translation target language lmscore ,0,1,0
giza is a freely available implementation of ibm models 15 brown et al 1993 and the hmm alignment vogel et al 1996 along with various improvements and modifications motivated by experimentation by och ney 2000 ,0,1,0
statistical machine translation is based on the noisy channel model where the translation hypothesis is searched over the space defined by a translation model and a target language brown et al 1993 ,0,1,0
1 introduction decoding is one of the three fundamental problems in classical smt translation model and language model being the other two as proposed by ibm in the early 1990s brown et al 1993 ,0,1,0
according to the statistical machine translation formalism brown et al 1993 the translation process is to search for the best sentence be such that be arg max e pejj arg maxe pjjepe where pjje is a translation model characterizing the correspondence between e and j pe the english language model probability ,0,1,0
by introducing the hidden word alignment variable a brown et al 1993 the optimal translation can be searched for based on the following criterion 1 arg max m mm m ea eh efa 1 where is a string of phrases in the target language e f fa is the source language string of phrases he are feature functions weights m m are typically optimized to maximize the scoring function och 2003 ,0,1,0
given a manually compiled lexicon containing words and their relative frequencies psfprimej the best segmentationfj1 is the one that maximizes the joint probability of all words in the sentence with the assumption that words are independent of each other1 fj1 argmax fprimejprime1 prfprimejprime1 ck1 argmax fprimejprime1 jprimeproductdisplay j1 psfprimej where the maximization is taken over chinese word sequences whose character sequence is ck1 22 translation system once we have segmented the chinese sentences into words we train standard alignment models in both directions with giza och and ney 2002 using models of ibm1 brown et al 1993 hmm vogel et al 1996 and ibm4 brown et al 1993 ,0,1,0
based on these grammars a great number of smt models have been recently proposed including stringtostring model synchronous fsg brown et al 1993 koehn et al 2003 treetostring model tsgstring huang et al 2006 liu et al 2006 liu et al 2007 stringtotree model stringcfgtsg yamada and knight 2001 galley et al 2006 marcu et al 2006 treetotree model synchronous cfgtsg dataoriented translation chiang 2005 cowan et al 2006 eisner 2003 ding and palmer 2005 zhang et al 2007 bod 2007 quirk wt al 2005 poutsma 2000 hearne and way 2003 and so on ,0,1,0
for example sentence alignment of bilingual texts are performed just by measuring sentence lengths in words or in characters brown et al 1991 gale and church 1993 or by statistically estimating word level correspondences chen 1993 kay and rsscheisen 1993 ,0,1,0
then those structurally matched parallel sentences are used as a source for acquiring lexical knowledge snch as verbal case frames utsuro et al 1992 utsuro et al 1993 ,0,1,0
so fitr we have implemented the following sentence dignment btlsedon word correspondence information word correspondence estimation by cooccnlrenceffequencybased methods in gme mid church 19h and kay and r6scheisen 1993 structured imttehlng of parallel sentences matsumoto et a l 1993 and case dame acquisition of japanese verbs utsuro et al 1993 ,0,1,0
dynamic programming is applied to bilingual sentence alignment in most of previous works brown et al 1991 gate and church 1993 chen 1993 ,0,1,0
in previous work church et al 1993 we have reported some preliminary success in aligning the english and japanese versions of the awk manual aho kernighan weinberger 1980 using charalign church 1993 a method that looks for character sequences that are the same in both the source and target ,0,1,0
these tables were computed from a small fragment of the canadian hansards that has been used in a number of other studies church 1993 and simard et al 1992 ,0,1,0
results this algorithm was applied to a fragment of the canadian hansards that has been used in a number of other studies church 1993 and simard et al 1992 ,0,1,0
the resolution of alignment can vat3 from low to high section paragraph sentence phrase and word gale and church 1993 matsumoto et al 1993 ,0,1,0
mcarthur 1992 mei et al 1993 classification allows a word to align with a target word using the collective translation tendency of words in the same class ,0,1,0
 t he em algorit hnt brown et al 1993ietrttstcr et al 1977 ,0,1,0
this conclusion is supported by the fact that true imt is not to our knowledge used in most modern translators support environments eg eurolang 1995 irederking et al 1993 ibm 1995 kugler et al 1991 nirenburg 1992 liados 1995 ,0,1,0
some o1 lhis research has treated the sentenees as unstructured word sequences to be aligned this work has primarily involved the acquisition of bilingual lexical correspondences chen 1993 although there has also been an attempt to create a full mt system based on such trcat ment brown et al 1993 ,0,1,0
 1993 graham et al 1980 where k is the number of distinct nonternfinal symbols in the grammar g we can expect a very etfident parser tbr our patterns r the input string can also be scanned to reduce the number of relewmt grammar rules before parsing e the combined process is also known as offlineparsing in ltac ,0,1,0
this sort of problem can be solved in principle by conditional variants of the expectationmaximization algorithm baum et al 1970 dempster et al 1977 meng and rubin 1993 jebara and pentland 1999 ,0,1,0
similarly murdock and croft 2005 adopted a simple translation model from ibm model 1 brown et al 1990 brown et al 1993 and applied it to qa ,0,1,0
the tree is produced by a stateoftheart dependency parser mcdonald et al 2005 trained on the wall street journal penn treebank marcus et al 1993 ,0,1,0
2 word alignment framework a statistical translation model brown et al 1993 och and ney 2003 describes the relationship between a pair of sentences in the source and target languages f fj1e ei1 using a translation probability pfe ,0,1,0
given a sentencepair fe the most likely viterbi word alignment is found as brown et al 1993 a argmaxa pfae ,0,1,0
5 previous work the leaf model is inspired by the literature on generative modeling for statistical word alignment and particularly by model 4 brown et al 1993 ,0,1,0
22 unsupervised parameter estimation we can perform maximum likelihood estimation of the parameters of this model in a similar fashion to that of model 4 brown et al 1993 described thoroughly in och and ney 2003 ,0,1,0
brown et al 1993 defined two local search operations for their 1ton alignment models 3 4 and 5 ,0,1,0
smt has evolved from the original wordbased approach brown et al 1993 into phrasebased approaches koehn et al 2003 och and ney 2004 and syntaxbased approaches wu 1997 alshawi et al 2000 yamada and knignt 2001 chiang 2005 ,0,1,0
31 the traditional ibm alignment model ibm model 4 brown et al 1993 learns a set of 4 probability tables to compute pfe given a foreign sentence f and its target translation e via the following greatly simplified generative story 361 npc npb npb nnp taiwan pos s nn surplus pp in in npc npb nn trade pp in between npc npb dt the cd two nns shores ftd0 gr g4e7 dybg el didv taiwan in twoshores trade middle surplus r1 npc npb x0npb x1nn x2pp x0 x2el x1 r10 npc npb x0npb x1nn x2pp x0 x2 x1 r10 npc npb x0npb x1nn x2pp x0 x2 x1 r2 npb nnp taiwan pos s ftd0 r11 npb x0nnp pos s x0 r17 npb nnp taiwan x0pos x0 r12 nnp taiwan ftd0 r18 pos s ftd0 r3 pp x0in x1npc x0 x1 r13 pp in in x0npc gr x0el r19 pp in in x0npc x0 r4 in in gr r5 npc x0npb x1pp x1 x0 r5 npc x0npb x1pp x1 x0 r20 npc x0npb pp x1in x2npc x2 x0 x1 r6 pp in between npc npb dt the cd two nns shores g4e7 r14 pp in between x0npc x0 r21 in between el r15 npc x0npb x0 r15 npc x0npb x0 r16 npb dt the cd two nns shores g4e7 r22 npb x0dt cd two x1nns x0 x1 r23 nns shores g4e7 r24 dt the gr r7 npb x0nn x0 r7 npb x0nn x0 r7 npb x0nn x0 r8 nn trade dybg r9 nn surplus didv r8 nn trade dybg r9 nn surplus didv r8 nn trade dybg r9 nn surplus didv figure 2 a english tree chinese string pair and three different sets of multilevel treetostring rules that can explain it the first set is obtained from bootstrap alignments the second from this papers realignment procedure and the third is a viable if poor quality alternative that is not learned ,0,1,0
however searching the space of all possible alignments is intractable for em so in practice the procedure is bootstrapped by models with narrower search space such as ibm model 1 brown et al 1993 or aachen hmm vogel et al 1996 ,0,1,0
approaches include word substitution systems brown et al 1993 phrase substitution systems koehn et al 2003 och and ney 2004 and synchronous contextfree grammar systems wu and wong 1998 chiang 2005 all of which train on string pairs and seek to establish connections between source and target strings ,0,1,0
1 introduction given a sourcelanguage eg french sentence f the problem of machine translation is to automatically produce a targetlanguage eg english translation e the mathematics of the problem were formalized by brown et al 1993 and reformulated by och and ney 2004 in terms of the optimization e arg maxe msummationdisplay m1 mhmef 1 where fhmefg is a set of m feature functions and fmg a set of weights ,0,1,0
these joint counts are estimated using the phrase induction algorithm described in koehn et al 2003 with symmetrized word alignments generated using ibm model 2 brown et al 1993 ,0,1,0
42 base model ii using the translation model ii brown et al 1993 where alignments are dependent on wordentity positions and wordentity sequence lengths we have pwe mproductdisplay j1 lsummationdisplay i0 paj ijmlpwjei 2 where aj i means that wj is aligned with ei ,0,1,0
one of the simplest models that can be seen in the context of lexical triggers is the ibm model 1 brown et al 1993 which captures lexical dependencies between source and target words ,0,1,0
compared with clean parallel corpora such as hansard brown et al 1993 which consists of 505 frenchenglish translations of political debates in the canadian parliament texts from the web are far more diverse and noisy ,1,0,0
although we have argued section 2 that this is unlikely to succeed to our knowledge we are the first to investigate the matter empirically11 the bestknown mt aligner is undoubtedly giza och and ney 2003 which contains implementations of various ibm models brown et al 1993 as well as the hmm model of vogel et al ,0,1,0
we use giza och and ney 2003 to train generative directed alignment models hmm and ibm model4 brown et al 1993 from training recordtext pairs ,0,1,0
one of the simplest models in the context of lexical triggers is the ibm model 1 brown et al 1993 which captures lexical dependencies between source and target words ,0,1,0
however since we are interested in the word counts that correlate to w we adopt the concept of the translation model proposed by brown et al 1993 ,0,1,0
many studies on collocation extraction are carried out based on cooccurring frequencies of the word pairs in texts choueka et al 1983 church and hanks 1990 smadja 1993 dunning 1993 pearce 2002 evert 2004 ,0,1,0
thus the alignment set is denoted as 1 ialiaia ii we adapt the bilingual word alignment model ibm model 3 brown et al 1993 to monolingual word alignment ,0,1,0
in the early statistical translation model work at ibm these representations were called cepts short for concepts brown et al 1993 ,0,1,0
numbers in the table correspond to the percentage of experiments in which the condition at the head of the column was true for example figure in the first row and first column means that for 989 percent of the language pairs the bleu score for the bidirectional decoder was better than that of the forward decoder proach brown et al 1993 ,0,0,1
increasingly parallel corpora are becoming available for many language pairs and smt systems have been built for frenchenglish germanenglish arabicenglish chineseenglish hindienglish and other language pairs brown et al 1993 alonaizan et al 1999 udupa 2004 ,0,1,0
in the classic work on smtbrownandhiscolleagues atibmintroduced the notion of alignment between a sentence f and its translation e and used it in the development of translation models brown et al 1993 ,1,0,0
an open question in smt is whether there existsclosed formexpressions whoserepresentation is polynomial in the size of the input for p fe and the counts in the em iterations for models 35 brown et al 1993 ,0,1,0
for a detailed introduction to ibm translation models please see brown et al 1993 ,0,1,0
expectation evaluation is the soul of parameter estimation brown et al 1993 alonaizan et al 1999 ,0,1,0
in their seminal paper on smt brownand his colleagues highlighted the problems weface aswe go from ibm models 12 to 35brown et al 1993 3 asweprogress from model1tomodel5 evaluating the expectations that gives us counts becomes increasingly difficult ,1,0,0
1 introduction statistical machine translation is a data driven machine translation technique which uses probabilistic models of natural language for automatic translation brown et al 1993 alonaizan et al 1999 ,0,1,0
the parameters of the models are estimated by iterative maximumlikelihood training on a large parallel corpus of natural language texts using the em algorithm brown et al 1993 ,0,1,0
we use the ibm model 1 brown et al 1993 uniform distribution and the hidden markov model hmm firstorder dependency vogel et al 1996 to estimate the alignment model ,0,1,0
the ibm models brown et al 1993 search a version of permutation space with a onetomany constraint ,0,1,0
the task originally emerged as an intermediate result of training the ibm translation models brown et al 1993 ,0,1,0
287 system train base test base 1 baseline 8789 8789 2 contrastive 8870 082 8845 056 5 trialsfold 3 contrastive 8882 093 8855 066 greedy selection table 1 average f1 of 7way crossvalidation to generate the alignments we used model 4 brown et al 1993 as implemented in giza och and ney 2003 ,0,1,0
according to this model when translating a stringf in the source language into the target language a string e is chosen out of all target language strings e if it has the maximal probability given f brown et al 1993 e arg maxe pref arg maxe prfepre where prfe is the translation model and pre is the target language model ,0,1,0
the method uses a translation model based on ibm model 1 brown et al 1993 in which translation candidates of a phrase are generated by combining translations and transliterations of the phrase components and matching the result against a large corpus ,0,1,0
one approach to translate terms consists in using a domainspecific parallel corpus with standard alignment techniques brown et al 1993 to mine new translations ,0,1,0
alignment is often used in training both generative and discriminative models brown et al 1993 blunsom et al 2008 liang et al 2006 ,0,1,0
to model pfjle8t we assume the existence of an alignment a j we assume that every word fj is produced by the word ej at position aj in the training corpus with the probability pflei j pf lc 1 pl icon jl 7 the word alignment a j is trained automatically using statistical translation models as described in brown et al 1993 vogel et al 1996 ,0,1,0
various clustering techniques have been proposed brown et al 1992 jardino and adda 1993 martin et al 1998 which perform automatic word clustering optimizing a maximumlikelihood criterion with iterative clustering algorithms ,0,1,0
the 1000best lists are augmented with ibm model1 brown et al 1993 scores and then rescored with a second set of met parameters ,0,1,0
the ibm translation models brown et al 1993 describe word reordering via a distortion model defined over word positions within sentence pairs ,0,1,0
word alignments traditionally are based on ibm models 15 brown et al 1993 or on hmms vogel et al 1996 ,0,1,0
2 related work one of the major problems with the ibm models brown et al 1993 and the hmm models vogel et al 1996 is that they are restricted to the alignment of each sourcelanguage word to at most one targetlanguage word ,0,0,1
the ibm models have shown good performance in machine translation and especially so within certain families of languages for example in translating between french and english or between sinhalese and tamil brown et al 1993 weerasinghe 2004 ,1,0,0
this is a common technique in machine translation for which the ibm translation models are popular methods brown et al 1993 ,1,0,0
in the first of our methods we align manual transcripts and asr sentences using the ibm translation model brown et al 1993 to obtain a probabilistic dictionary ,0,1,0
one widely used model is the ibm model brown et al 1993 ,1,0,0
757 hbps strong tendency to overestimate the probability of rare biphrases it is computed as in equation 2 except that biphrase probabilities are computed based on individual word translation probabilities somewhat as in ibm model 1 brown et al 1993 prts 1st productdisplay tt summationdisplay ss prts the target language feature function htl this is based on a ngram language model of the target language ,0,1,0
while in traditional wordbased statistical models brown et al 1993 the atomic unit that translation operates on is the word phrasebased methods acknowledge the significant role played in language by multiword expressions thus incorporating in a statistical framework the insight behind examplebased machine translation somers 1999 ,0,0,1
therefore we determine the maximal translation probability of the target word e over the source sentence words pibm1efj1 maxj0j pefj 9 where f0 is the empty source word brown et al 1993 ,0,1,0
for the give source text s it finds the most probable alignment set a and target text t aa satpstp 1 brown brown et al 1993 proposed five alignment models called ibm model for an englishfrench alignment task based on equa68 tion 1 ,0,1,0
chen et al 1993 gale et al 1993 proposed sentence alignment techniques based on dynamic programming using sentence length and lexical mapping information ,0,1,0
haruno et al 1996 kay et al 1993 applied iterative refinement algorithms to sentence level alignment tasks ,0,1,0
2 overview 21 the word segmentation problem as statistical machine translation systems basically rely on the notion of words through their lexicon models brown et al 1993 they are usually capable of outputting sentences already segmented into words when they translate into languages like chinese or japanese ,0,1,0
large volumes of training data of this kind are indispensable for constructing statistical translation models brown et al 1993 melamed 2000 acquiring bilingual lexicon gale and church 1991 melamed 1997 and building examplebased machine translation ebmt systems nagao 1984 carl and way 2003 way and gough 2003 ,0,1,0
these probabilities are estimated with ibm model 1 brown et al 1993 on parallel corpora ,0,1,0
512 learning translation model according to the standard statistical translation model brown et al 1993 we can find the optimal model m by maximizing the probability of generating queries from documents or m argmax m ny i1 pqijdim 524 qw dw pqwjdwu journal kdd 00176 journal conference 00123 journal journal 00176 journal sigkdd 00088 journal discovery 00211 journal mining 00017 journal acm 00088 music music 00375 music purchase 00090 music mp3 00090 music listen 00180 music mp3com 00450 music free 00008 table 1 sample user profile to find the optimal word translation probabilities pqwjdwm we can use the em algorithm ,0,1,0
ibm model1 brown et al 1993 is a simplistic model which takes no account of the subtler aspects of language translation including the way word order tends to differ across languages ,0,0,1
most current transliteration systems use a generative model for transliteration such as freely available giza1 och and ney 2000an implementation of the ibm alignment models brown et al 1993 ,0,1,0
a word order correlation bias as well as the phrase structure biases in brown et als 1993b models 4 and 5 would be less beneficial with noisier training bitexts or for language pairs with less similar word order ,0,0,1
choosing the most advantageous hiemstra has published parts of the translational distributions of certain words induced using both his method and brown et als 1993b model 1 from the same training bitext ,0,1,0
due to the parameter interdependencies introduced by the onetoone assumption we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in brown et al 1993b equation 26 ,0,1,0
the fertility for the null word is treated specially for details see brown et al 1993 ,0,1,0
the translation models they presented in various papers between 1988 and 1993 brown et al 1988 brown et al 1990 brown della pietra della pietra and mercer 1993 are commonly referred to as ibm models 15 based on the numbering in brown della pietra della pietra and mercer 1993 ,0,1,0
these results were achieved using the statistical alignments provided by model 5 brown et al 1993 och and ney 2000 and smoothed 11grams and 6grams respectively ,0,1,0
yet the modeling training and search methods have also improved since the field of statistical machine translation was pioneered by ibm in the late 1980s and early 1990s brown et al 1990 brown et al 1993 berger et al 1994 ,1,0,0
we compare against several competing systems the first of which is based on the original ibm model 4 for machine translation brown et al 1993 and the hmm machine translation alignment model vogel ney and tillmann 1996 as implemented in the giza package och and ney 2003 ,0,1,0
one obvious first approach would be to run a simpler model for the first iteration for example model 1 from machine translation brown et al 1993 which tends to be very recall oriented and use this to see subsequent iterations of the more complex model ,0,1,0
in the context of headline generation simple statistical models are used for aligning documents and headlines banko mittal and witbrock 2000 berger and mittal 2000 schwartz zajic and dorr 2002 based on ibm model 1 brown et al 1993 ,0,1,0
for these first smt systems translationmodel probabilities at the sentence level were approximated from wordbased translation models that were trained by using bilingual corpora brown et al 1993 ,0,1,0
this feature is implemented by using the ibm1 lexical parameters brown et al 1993 och et al 2004 ,0,1,0
according to our experience the best performance is achieved when the union of the sourcetotarget and targettosource alignment sets ibm models brown et al 1993 is used for tuple extraction some experimental results regarding this issue are presented in section 422 ,1,0,0
the first smt systems were developed in the early nineties brown et al 1990 1993 ,0,1,0
therefore we determine the maximal translation probability of the target word e over the source sentence words p ibm1 ef j 1 max j0j pef j 18 where f 0 is the empty source word brown et al 1993 ,0,1,0
the basic phrasebased model is an instance of the noisychannel approach brown et al 1993 ,0,1,0
different approaches have been proposed for modeling prsat in equation 8 zeroorder models such as model 1 model 2andmodel 3 brown et al 1993 and the rstorder models such as model 4 model 5 brown et al 1993 hidden markov model ney et al 2000 and model 6 och and ney 2003 ,0,1,0
