CITATION,Positive,Objective,Negative
we analyzed a set of articles and identified six major operations that can be used for editing the extracted sentences including removing extraneous phrases from an extracted sentence combining a reduced sentence with other sentences syntactic transformation substituting phrases in an extracted sentence with their paraphrases substituting phrases with more general or specific descriptions and reordering the extracted sentences jing and mckeown 1999 jing and mckeown 2000 ,0,1,0
table 3 example compressions compression avglen rating baseline 970 193 bt2step 2206 321 spade 1909 310 humans 2007 383 table 4 mean ratings for automatic compressions nally we added a simple baseline compression algorithm proposed by jing and mckeown 2000 which removed all prepositional phrases clauses toinfinitives and gerunds ,0,1,0
53 related works and discussion our twostep model essentially belongs to the same category as the works of mani et al 1999 and jing and mckeown 2000 ,0,1,0
1999 proposed a summarization system based on the draft and revision jing and mckeown 2000 proposed a system based on extraction and cutandpaste generation our abstractors performed the same cutandpaste operations that jing and mckeown noted in their work and we think that our twostep model will be a reasonable starting point for our subsequent research ,0,1,0
we found that the deletion of lead parts did not occur very often in our summary unlike the case of jing and mckeown 2000 ,0,1,0
al 1994 compression of sentences with automatic translation approaches knight and marcu 2000 hidden markov model jing and mckeown 2000 topic signatures based methods lin and hovy 2000 lacatusu et al 2006 are among the most popular techniques that have been used in the summarization systems of this category ,1,0,0
because of this it is generally accepted that some kind of postprocessing should be performed to improve the final result by shortening fusing or otherwise revising the material grefenstette 1998 mani gates and bloedorn 1999 jing and mckeown 2000 barzilay et al 2000 knight and marcu 2000 ,0,1,0
in addition to sentence fusion compression algorithms chandrasekar doran and bangalore 1996 grefenstette 1998 mani gates and bloedorn 1999 knight and marcu 2002 jing and mckeown 2000 reizler et al 2003 and methods for expansion of a multiparallel corpus pang knight and marcu 2003 are other instances of such methods ,0,1,0
while earlier approaches for text compression were based on symbolic reduction rules grefenstette 1998 mani gates and bloedorn 1999 more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced knight and marcu 2002 jing and mckeown 2000 reizler et al 2003 ,0,1,0
while this approach exploits only syntactic and lexical information jing and mckeown 2000 also rely on cohesion information derived from word distribution in a text phrases that are linked to a local context are retained while phrases that have no such links are dropped ,0,1,0
in addition to reducing the original sentences jing and mckeown 2000 use a number of manually compiled rules to aggregate reduced sentences for example reduced clauses might be conjoined with and ,0,1,0
previous research has addressed revision in singledocument summaries jing mckeown 2000 mani et al 1999 and has suggested that revising summaries can make them more informative and correct errors ,0,1,0
1 introduction texttotext generation is an emerging area of research in nlp chandrasekar and bangalore 1997 caroll et al 1999 knight and marcu 2000 jing and mckeown 2000 ,0,1,0
the recent approach for editing extracted text spans jing and mckeown 2000 may also produce improvement for our algorithm ,1,0,0
1 introduction the task of sentence compression or sentence reduction can be defined as summarizing a single sentence by removing information from it jing and mckeown 2000 ,0,1,0
like the work of jing and mckeown 2000 and mani et al ,0,1,0
one of the most effective taggers based on a pure hmm is that developed at xerox cutting et al 1992 ,1,0,0
cutting et al 1992 ,0,1,0
cutting et al 1992 and feldweg 1995 ,0,1,0
41 complete ambiguity classes ambiguity classes capture the relevant property we are interested in words with the same category possibilities are grouped together4 and ambiguity classes have been shown to be successfully employed in a variety of ways to improve pos tagging eg cutting et al 1992 daelemans et al 1996 dickinson 2007 goldberg et al 2008 tseng et al 2005 ,1,0,0
in tabh 2 the accuracy rate of the nettagger is corollated to that of a trigram lmsed tagger kempe 1993 and a liidden markov model tagger cutting et al 1992 which were ,0,1,0
language models such as ngram class models brown et al 1992 and ergodic hidden markov models kuhn el al 1994 were proposed and used in applications such as syntactic class pos tagging for english cutting et al 1992 clustering and scoring of recognizer sentence hypotheses ,0,1,0
the tagger used is thus one that does not need tagged and disambiguated material to be trained on namely the xpost originally constructed at xerox parc cutting et al 1992 cutting and pedersen 1993 ,0,1,0
it is usedas tagging mode in english church 1988 cutting et al 1992 and morphological analysis nlodel word segmentation and tagging in japanese nagata 1994 ,0,1,0
this text was partofspeech tagged using the xerox hmm tagger cutting et al 1992 ,0,1,0
as a common strategy pos guessers examine the endings of unknown words cutting et al 1992 along with their capitalization or consider the distribution of unknown words over specific partsofspeech weischedel et al 1993 ,0,1,0
in the absence of an annotated corpus dependencies can be derived by other means eg part413 ofspeech probabilities can be approximated from a raw corpus as in cutting et al 1992 wordsense dependencies can be derived as definitionbased similarities etc label dependencies are set as weights on the arcs drawn between corresponding labels ,0,1,0
there are many pos taggers developed using different techniques for many major languages such as transformationbased errordriven learning brill 1995 decision trees black et al 1992 markov model cutting et al 1992 maximum entropy methods ratnaparkhi 1996 etc for english ,0,1,0
our statistical tagging model is modified from the standard bigrams cutting et al 1992 using viterbi search plus onthefly extra computing of lexical probabilities for unknown morphemes ,0,1,0
two main approaches have generally been considered rulebased klein and simmons 1963 brodda 1982 paulussen and martin 1992 brill et al 1990 probabilistic bahl and mercer 1976 debili 1977 stolz tannenbaum and carstensen 1965 marshall 1983 leech garside and atwell 1983 derouault and merialdo 1986 derose 1988 church 1989 beale 1988 marcken 1990 merialdo 1991 cutting et al 1992 ,0,1,0
stochastic taggers use both contextual and morphological information and the model parameters are usually defined or updated automatically from tagged texts cerfdanon and e1beze 1991 church 1988 cutting et al 1992 dermatas and kokkinakis 1988 1990 1993 1994 garside leech and sampson 1987 kupiec 1992 maltese department of electrical engineering wire communications laboratory wcl university of patras 265 00 patras greece ,0,1,0
unlike stochastic approaches to partofspeech tagging church 1988 kupiec 1992 cutting et al 1992 merialdo 1990 derose 1988 weischedel et al 1993 up to now the knowledge found in finitestate taggers has been handcrafted and was not automatically acquired ,0,1,0
7 independently cutting et al 1992 quote a performance of 800 words per second for their partofspeech tagger based on hidden markov models ,0,1,0
these methods have reported performance in the range of 9599 correct by word derose 1988 cutting et al 1992 jelinek mercer and roukos 1992 kupiec 1992 ,0,1,0
a number of partofspeech taggers are readily available and widely used all trained and retrainable on text corpora church 1988 cutting et al 1992 brill 1992 weischedel et al 1993 ,1,0,0
partofspeech tagging is an active area of research a great deal of work has been done in this area over the past few years eg jelinek 1985 church 1988 derose 1988 hindle 1989 demarcken 1990 merialdo 1994 brill 1992 black et al 1992 cutting et al 1992 kupiec 1992 charniak et al 1993 weischedel et al 1993 schutze and singer 1994 ,0,1,0
cutting et al 1992 reported very high results 96 on the brown corpus for unsupervised pos tagging using hidden markov models hmms by exploiting handbuilt tag dictionaries and equivalence classes ,1,0,0
it is also possible to train statistical models using unlabeled data with the expectation maximization algorithm cutting et al 1992 ,0,1,0
for english there are many pos taggers employing machine learning techniques like transformationbased errordriven learning brill 1995 decision trees black et al 1992 markov model cutting et al 1992 maximum entropy methods ratnaparkhi 1996 etc there are also taggers which are hybrid using both stochastic and rulebased approaches such as claws garside and smith 1997 ,0,1,0
this situation is very similar to that involved in training hmm text taggers where joint probabilities are computed that a particular word corresponds to a particular partofspeech and the rest of the words in the sentence are also generated eg cutting et al 1992 ,0,1,0
making such an assumption is reasonable since pos taggers that can achieve accuracy of 96 are readily available to assign pos to unrestricted english sentences brill 1992 cutting et al 1992 ,0,1,0
derose 1988 cutting et al 1992 church 1988 ,0,1,0
there has been a large number of studies in tagging and morphological disambiguation using various techniques such as statistical techniques eg church 1988 cutting et al 1992 derose 1988 constraintbased techniques karlsson et al 1995 voutilainen 1995b voutilainen heikkili and anttila 1992 voutilainen and tapanainen 1993 oflazer and kurusz 1994 oflazer and till 1996 and transformationbased techniques brilt 1992 brill 1994 brill 1995 ,0,1,0
second the automatic approach in which the model is automatically obtained from corpora either raw or annotated 1 and consists of ngrams garside et al 1987 cutting et ah 1992 rules hindle 1989 or neural nets schmid 1994 ,0,1,0
cutting et al 1992 local rules eg hindle 1989 and neural networks eg schmid 1994 ,0,1,0
the xerox tagger comes with a list of builtin ending guessing rules cutting et al 1992 ,0,1,0
it has been known for some years that good performance can be realized with partial tagging and a hidden markov model cutting et al 1992 ,1,0,0
this analysis depends on the specialist lexicon and the xerox partofspeech tagger cutting et al 1992 and provides simple noun phrases that are mapped to concepts in the umls metathesaurus using metamap aronson 2001 ,0,1,0
many approaches for pos tagging have been developed in the past including rulebased tagging brill 1995 hmm taggers brants 2000 cutting and others 1992 maximumentropy models rathnaparki 1996 cyclic dependency networks toutanova et al 2003 memorybased learning daelemans et al 1996 etc all of these approaches require either a large amount of annotated training data for supervised tagging or a lexicon listing all possible tags for each word for unsupervised tagging ,0,0,1
5 comparison with other approaches in some sense this approach is similar to the notion of ambiguity classes explained in kupiec 1992 and cutting et al 1992 where words that belong to the same partofspeech figure together ,0,1,0
most work on statistical methods has used ngram models or hidden markov modelbased taggers eg church 1988 derose 1988 cutting et al 1992 merialdo 1994 etc ,0,1,0
kupiec 1992 has proposed an estimation method for the ngram language model using the baumwelch reestimation algorithm rabiner et al 1994 from an untagged corpus and cutting et al ,0,1,0
the accuracy of the derived model depends heavily on the initial bias but with a good choice results are comparable to those of method three cutting et al 1992 ,0,1,0
our statistical tagging model is adjusted from standard bigrams using the viterbisearch cutting et al 1992 plus onthefly extra computing of lexical probabilities for unknown morphemes ,0,1,0
cutting et al 1992 feldweg 1995 the tagger for grammatical functions works with lexical 1 selbst besucht adv vvpp himself visited hat peter sabine vafin ne ne has peter sabine peter never visited sabine himself l hie adv never figure 2 example sentence and contextual probability measures po depending on the category of a mother node q ,0,1,0
examples of such affinities include synonyms terra and clarke 2003 verb similarities resnik and diab 2000 and word associations rapp 2002 ,0,1,0
several papers have looked at higherorder representations but have not examined the equivalence of synpara distributions when formalized as markov chains schutze and pedersen 1993 lund and burgess 1996 edmonds 1997 rapp 2002 biemann et al 2004 lemaire and denhiere 2006 ,0,0,1
then by using evaluations similar to those described in baroni et al 2008 and by rapp 2002 we show that the best distancebased measures correlate better overall with human association scores than do the best window based configurations see section 4 and that they also serve as better predictors of the strongest human associations see section 5 ,0,1,0
3 methodology similar to rapp 2002 baroni et al 2008 among others we use comparison to human assocation datasets as a test bed for the scores produced by computational association measures ,0,1,0
we use evaluations similar to those used before rapp 2002 pado and lapata 2007 baroni et al 2008 among others ,0,1,0
as rapp 2002 observes choosing a window size involves making a tradeoff between various qualities ,0,1,0
this is one manifestation of what is commonly referred to as the data sparseness problem and was discussed by rapp 2002 as a sideeffect of specificity ,0,1,0
we used the procedure described in rapp 2002 with the only modification being the multiplication of the loglikelihood values with a triangular function that depends on the logarithm of a words frequency ,0,1,0
ruge 1992 rapp 2002 ,0,1,0
even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments landauer et al 1998 rapp 2002 there has not been any research on the usage of a digital networkbased dictionary reflecting the organisation of the mental lexicon to our knowledge ,0,1,0
there are several other approaches such as ji and ploux 2003 and the already mentioned rapp 2002 ,0,1,0
this method was preferred against other related methods like the one introduced in mihalcea et al 2004 since it embeds all the available semantic information existing in wordnet even edges that cross pos thus offering a richer semantic representation ,0,0,1
ranking algorithms such as kleinbergs hits algorithm kleinberg 1999 or googles pagerank brin and page 1998 have been traditionally and successfully used in weblink analysis brin and page 1998 social networks and more recently in text processing applications mihalcea and tarau 2004 mihalcea et al 2004 erkan and radev 2004 ,0,1,0
although various approaches to smt system combination have been explored including enhanced combination model structure rosti et al 2007 better word alignment between translations ayan et al 2008 he et al 2008 and improved confusion network construction rosti et al 2008 most previous work simply used the ensemble of smt systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way ,0,1,0
most of the work focused on seeking better word alignment for consensusbased confusion network decoding matusov et al 2006 or wordlevel system combination he et al 2008 ayan et al 2008 ,0,1,0
however one of the major limitations of these advances is the structured syntactic knowledge which is important to global reordering li et al 2007 elming 2008 has not been well exploited ,0,1,0
please note that our approach is very different from other approaches to context dependent rule selection such as ittycheriah and roukos 2007 and he et al 2008 ,0,1,0
thus we can compute the source dependency lm score in the same way we compute the target side score using a procedure described in shen et al 2008 ,0,1,0
a remedy is to aggressively limit the feature space eg to syntactic labels or a small fraction of the bilingual features available as in chiang et al 2008 chiang et al 2009 but that reduces the benefit of lexical features ,0,1,0
carpuat and wu 2007 and he et al 2008 the specific technique we used by means of a context language model is rather different ,0,1,0
73 122 baseline system and experimental setup we take bbns hierdec a stringtodependency decoder as described in shen et al 2008 as our baseline for the following two reasons it provides a strong baseline which ensures the validity of the improvement we would obtain ,0,1,0
previously published approaches to reducing the rule set include enforcing a minimum span of two words per nonterminal lopez 2008 which would reduce our set to 115m rules or a minimum count mincount threshold zollmann et al 2008 which would reduce our set to 78m mincount2 or 57m mincount3 rules ,0,1,0
lopez 2008 explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrasebased systems ,0,1,0
2 models search spaces and errors a translation model consists of two distinct elements an unweighted ruleset and a parameterization lopez 2008a 2009 ,0,1,0
in a next step chunk information was added by a rulebased languageindependent chunker macken et al 2008 that contains distituency rules which implies that chunk boundaries are added between two pos codes that cannot occur in the same constituent ,0,1,0
one such relational reasoning task is the problem of compound noun interpretation which has received a great deal of attention in recent years girju et al 2005 turney 2006 butnariu and veale 2008 ,0,1,0
turney 2008 argues that many nlp tasks can be formulated in terms of analogical reasoning and he applies his pairclass algorithm to a number of problems including sat verbal analogy tests synonymantonym classification and distinction between semantically similar and semantically associated words ,0,1,0
analternativeembeddingisthatusedbyturney 2008 in his pairclass system see section 6 ,0,1,0
language modeling chen and goodman 1996 nounclustering ravichandran et al 2005 constructing syntactic rules for smt galley et al 2004 and finding analogies turney 2008 are examples of some of the problems where we need to compute relative frequencies ,0,1,0
in nlp community it has been shown that having more data results in better performance ravichandran et al 2005 brants et al 2007 turney 2008 ,0,1,0
2 related work turney 2008 recently advocated the need for a uniform approach to corpusbased semantic tasks ,0,1,0
such tasks will require an extension of the current framework of turney 2008 beyond evidence from the direct cooccurrence of target word pairs ,0,1,0
we adopt a similar approach to the one used in turney 2008 and consider each question as a separate binary classification problem with one positive training instance and 5 unknown pairs ,0,1,0
they are part of an effort to better integrate a linguistic rulebased system and the statistical correcting layer also illustrated in ueffing et al 2008 ,0,1,0
35 domain adaptation in machine translation within mt there has been a variety of approaches dealing with domain adaption for example wu et al 2008 koehn and schroeder 2007 ,0,1,0
optimal algorithms exist for minimising the size of rules in a synchronous contextfree grammar scfg uno and yagiura 2000 zhang et al 2008 ,0,1,0
following the broad shift in the field from finite state transducers to grammar transducers chiang 2007 recent approaches to phrasebased alignment have used synchronous grammar formalisms permitting polynomial time inference wu 1997 783 cherry and lin 2007 zhang et al 2008b blunsom et al 2008 ,0,1,0
4 training this section discusses how to extract our translation rules given a triple nullnullnull null nullnull as we know the traditional treetostring rules can be easily extracted from nullnullnull null nullnull using the algorithm of mi and huang 2008 2 we would like 2 mi and huang 2008 extend the treebased rule extraction algorithm galley et al 2004 to forestbased by introducing nondeterministic mechanism ,0,1,0
among these advances forestbased modeling mi et al 2008 mi and huang 2008 and tree sequencebased modeling liu et al 2007 zhang et al 2008a are two interesting modeling methods with promising results reported ,1,0,0
therefore structure divergence and parse errors are two of the major issues that may largely compromise the performance of syntaxbased smt zhang et al 2008a mi et al 2008 ,0,1,0
to address this issue many syntaxbased approaches yamada and knight 2001 eisner 2003 gildea 2003 ding and palmer 2005 quirk et al 2005 zhang et al 2007 2008a bod 2007 liu et al 2006 2007 hearne and way 2003 tend to integrate more syntactic information to enhance the noncontiguous phrase modeling ,0,1,0
nevertheless the generated rules are strictly required to be derived from the contiguous translational equivalences galley et al 2006 marcu et al 2006 zhang et al 2007 2008a 2008b liu et al 2006 2007 ,0,1,0
2 we illustrate the rule extraction with an example from the treetotree translation model based on tree sequence alignment zhang et al 2008a without losing of generality to most syntactic tree based models ,0,1,0
the proposed synchronous grammar is able to cover the previous proposed grammar based on tree stsg eisner 2003 zhang et al 2007 and tree sequence stssg zhang et al 2008a alignment ,0,1,0
word alignment is also a required first step in other algorithms such as for learning subsentential phrase pairs lavie et al 2008 or the generation of parallel treebanks zhechev and way 2002 ,0,1,0
previously published approaches to reducing the rule set include enforcing a minimum span of two words per nonterminal lopez 2008 which would reduce our set to 115m rules or a minimum count mincount threshold zollmann et al 2008 which would reduce our set to 78m mincount2 or 57m mincount3 rules ,0,1,0
various approaches to word sense division have been proposed in the literature on wsd including 1 sense numbers in everyday dictionaries lesk 1986 cowie guthrie and guthrie 1992 2 automatic or handcrafted clusters of dictionary senses dolan 1994 bruce and wiebe 1995 luk department of computer science national tsing hua university hsinchu 30043 taiwan roc ,0,1,0
furthermore as pointed out in dolan 1994 the sense division in an mrd is frequently too finegrained for the purpose of wsd ,0,1,0
however they do not elaborate on how the comparisons are done or on how effective the program is dolan 1994 describes a heuristic approach to forming unlabeled clusters of closely related senses in an mrd ,0,1,0
recently various approaches dolan 1994 luk 1995 yarowsky 1992 dagan et al 1991 dagan and itai 1994 to word sense division have been used in wsd research ,0,1,0
dolan 1994 described a heuristic approach to forming unlabeled clusters of closely related senses in a mrd ,0,1,0
on the british national corpus bnc using lins 1998 similarity method we retrieve the following neighbors for the first and second sense respectively 1 ,0,1,0
a potential caveat with lins 1998 distributional similarity measure is its reliance on syntactic information for obtaining dependency relations ,0,0,1
feature comparison measures to convert two feature sets into a scalar value several measures have been proposed such as cosine lins measure lin 1998 kullbackleibler kl divergence and its variants ,0,1,0
others proposed distributional similarity measures between words hindle 1990 lin 1998 lee 1999 weeds et al 2004 ,0,1,0
one is automatic thesaurus acquisition that is to identify synonyms or topically related words from corpora based on various measures of similarity eg riloff and shepherd 1997 lin 1998 caraballo 1999 thelen and riloff 2002 you and chen 2006 ,0,1,0
by no means an exhaustive list the most commonly cited ranking and scoring algorithms are hits kleinberg 1998 and pagerank page et al 1998 which rank hyperlinked documents using the concepts of hubs and authorities ,0,1,0
within the nlp community nbest list ranking has been looked at carefully in parsing extractive summarization barzilay et al 1999 hovy and lin 1998 and machine translation zhang et al 2006 to name a few ,0,1,0
given a wordq its set of featuresfq and feature weightswqf for f fq a common symmetric similarity measure is lin similarity lin 1998a linuv summationtext ffufvwufwvfsummationtext ffu wuf summationtext ffv wvf where the weight of each feature is the pointwise mutual information pmi between the word and the feature wqf logprfqprf ,0,1,0
among these measures the most important are wu palmers wu and palmer 1994 resniks resnik 1995 and lins lin 1998 ,1,0,0
where pantel and lin use lins 1998 measure we use wu and palmers 1994 measure ,0,1,0
4 experiments and results 41 set up we parsed the 3 gb aquaint corpus voorhees 2002 using minipar lin 1998b and collected verbobject and verbsubject frequencies building an empirical mi model from this data ,0,1,0
they have been successfully applied in several tasks such as information retrieval salton et al 1975 and harvesting thesauri lin 1998 ,0,1,0
distributional measures of distance such as those proposed by lin 1998 quantify how similar the two sets of contexts of a target word pair are ,0,1,0
accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation resnik 1995 synonym extraction lin 1998a and automatic thesauri generation curran 2002 ,0,1,0
strube and ponzetto 2006 019048 leacock chodrow 1998 036 lin 1998b 036 resnik 1995 037 proposed 0504 7 conclusion we proposed a relational model to measure the semantic similarity between two words ,0,1,0
wiebe 2000 uses lin 1998a style distributionally similar adjectives in a clusterandlabel process to generate sentiment lexicon of adjectives ,0,1,0
our approach to stc uses a thesaurus based on corpus statistics lin 1998 for realvalued similarity calculation ,0,1,0
some researchers hindle 1990 grefenstette 1994 lin 1998 classify terms by similarities based on their distributional syntactic patterns ,0,1,0
a wide range of contextual information such as surrounding words lowe and mcdonald 2000 curran and moens 2002a dependency or case structure hindle 1990 ruge 1997 lin 1998 and dependency path lin and pantel 2001 pado and lapata 2007 has been utilized for similarity calculation and achieved considerable success ,0,1,0
31 context extraction we adopted dependency structure as the context of words since it is the most widely used and wellperforming contextual information in the past studies ruge 1997 lin 1998 ,0,1,0
2 related work thisworkbuildsuponthatofmccarthyetal2004 which acquires predominant senses for target words from a large sample of text using distributional similarity lin 1998 to provide evidence for predominance ,0,1,0
in this approach we extend the denition overlap by considering the distributional similarity lin 1998 rather than identify of the words in the two denitions ,0,1,0
mccarthy et al use a distributional similarity thesaurus acquired from corpus data using the method of lin 1998 for nding the predominant sense of a word where the senses are dened by wordnet ,0,1,0
let w be a target word and nw fn1n2nkg be the ordered set of the top scoring k neighbours of w from the thesaurus with associated distributional similarity scores fdsswn1dsswn2dsswnkg using lin 1998 ,0,1,0
we use the similarity proposed by lin 1998 ,0,1,0
the common types of features include contextual lin 1998 cooccurrence yang and callan 2008 and syntactic dependency pantel and lin 2002 pantel and ravichandran 2004 ,0,1,0
inspired by the conjunction and appositive structures riloff and shepherd 1997 roark and charniak 1998 used cooccurrence statistics in local context to discover sibling relations ,0,1,0
clusteringbased approaches usually represent word contexts as vectors and cluster words based on similarities of the vectors brown et al 1992 lin 1998 ,0,1,0
the second uses lin dependency similarity a syntacticdependency based distributional word similarity resource described in lin 1998a9 ,0,1,0
while kazama and torisawa used a chunker we parsed the definition sentence using minipar lin 1998b ,0,1,0
the earliest work in this direction are those of hindle 1990 lin 1998 dagan et al 1999 chen and chen 2000 geffet and dagan 2004 and weeds and weir 2005 ,0,1,0
lin 1998 proposed a word similarity measure based on the distributio nal pattern of words which allows to construct a thesaurus using a parsed corpus ,0,1,0
for every pair of nouns where each noun had a total frequency in the triple data of 10 or more we computed their distributional similarity using the measure given by lin 1998 ,0,1,0
as a basis mapping function we used a generalisation of the one used by grefenstette 1994 and lin 1998 ,0,1,0
in particular this method has been used for word sense disambiguation lin 1997 and thesaurus construction lin 1998 ,0,1,0
we used an implementation of mcdonald 2006forcomparisonofresultsclarkeandlapata 2007 ,0,1,0
elhadad et al 2001 clarke and lapata 2007 madnani et al 2007 ,0,1,0
this framework is 211 commonly used in generation and summarization applications where the selection process is driven by multiple constraints marciniak and strube 2005 clarke and lapata 2007 ,0,1,0
in prior research ilp was used as a postprocessing step to remove redundancy and make other global decisions about parameters mcdonald 2007 marciniak and strube 2005 clarke and lapata 2007 ,0,1,0
41 corpora sentence compression systems have been tested on product review data from the ziffdavis zd henceforth corpus by knight and marcu 2000 general news articles by clarke and lapata cl henceforth corpus 2007 and biomedical articles lin and wilbur 2007 ,0,1,0
besides precision recall and balanced fmeasure we also include an fmeasure variant strongly biased towards recall 0b01 which fraser and marcu 2007 found to be best to tune their leaf aligner for maximum mt accuracy ,0,1,0
the training data is aligned using the leaf technique fraser and marcu 2007 ,0,1,0
12 related work recently discriminative methods for alignment have rivaled the quality of ibm model 4 alignments liu et al 2005 ittycheriah and roukos 2005 taskar et al 2005 moore et al 2006 fraser and marcu 2007b ,0,1,0
in contrast to the semisupervised leaf alignment algorithm of fraser and marcu 2007b which requires 15002000 cpu days per iteration to align 84m chineseenglish sentences anonymous pc link deletion requires only 450 cpu hours to realign such a corpus after initial alignment by giza which requires 2024 cpu days ,0,1,0
however fraser and marcu 2007a show that in phrasebased translation improvements in aer or fmeasure do not necessarily correlate with improvements in bleu score ,0,1,0
probabilistic generative models like ibm 15 brown et al 1993 hmm vogel et al 1996 itg wu 1997 and leaf fraser and marcu 2007 define formulas for pf e or pe f with okvoon ororok sprok atvoon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat okdrubel okvoon anok plok sprok atdrubel atvoon pippat rrat dat okvoon anok drok brok jok atvoon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok okyurp totat nnat quat oloat atyurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat figure 1 word alignment exercise knight 1997 ,0,1,0
carpuat and wu 2007b integrated a wsd system into a phrasebased smt system pharaoh koehn 2004a ,0,1,0
furthermore they extended wsd to phrase sense disambiguation psd carpuat and wu 2007a ,0,1,0
similar to wsd carpuat and wu 2007a used contextual information to solve the ambiguity problem for phrases ,1,0,0
recently wordsense disambiguation wsd methods have been shown to improve translation quality chan et al 2007 carpuat and wu 2007 ,1,0,0
in carpuat and wu 2007 anotherstateoftheartwsdengineacombination of naive bayes maximum entropy boosting and kernel pca models is used to dynamically determine the score of a phrase pair under consideration and thus let the phrase selection adapt to the context of the sentence ,1,0,0
wsd is one of the fundamental problems in natural language processing and is important for applications such as machine translation mt chan et al 2007a carpuat and wu 2007 information retrieval ir etc wsd is typically viewed as a classification problem where each ambiguous word is assigned a sense label from a predefined sense inventory during the disambiguation process ,1,0,0
second instead of disambiguating phrase senses as in carpuat and wu 2007 we model word selection independently of the phrases used in the mt models ,0,1,0
there has been considerable skepticism over whether wsd will actually improve performance of applications but we are now starting to see improvement in performance due to wsd in crosslingual information retrieval clough and stevenson 2004 vossen et al 2006 and machine translation carpuat and wu 2007 chan et al 2007 and we hope that other applications such as questionanswering text simplication and summarisation might also benet as wsd methods improve ,1,0,0
promising features might include those over source side reordering rules wang et al 2007 or source context features carpuat and wu 2007 ,1,0,0
on the other hand integrating an additional component into a baseline smt system is notoriously tricky as evident in the research on integrating word sense disambiguation wsd into smt systems different ways of integration lead to conflicting conclusions on whether wsd helps mt performance chan et al 2007 carpuat and wu 2007 ,1,0,0
carpuat and wu 2007 approached the issue as a word sense disambiguation problem ,0,1,0
carpuat and wu 2007 and chan et al ,0,1,0
we are starting to see the beginnings of a positive effect of wsd in nlp applications such as machine translation carpuat and wu 2007 chan et al 2007 ,1,0,0
unlike a full blown machine translation task carpuat and wu 2007 annotators and systems will not be required to translate the whole context but just the target word ,0,1,0
several studies have demonstrated that for instance statistical machine translation smt benefits from incorporating a dedicated wsd module chan et al 2007 carpuat and wu 2007 ,1,0,0
a simple example is shown in figure 1 where the arc between a and hat indicates that hat is the head of a current statistical dependency parsers perform better if the dependency lengthes are shorter mcdonald and nivre 2007 ,0,1,0
3 maltparser maltparser nivre et al 2007b is a languageindependent system for datadriven dependency parsing based on a transitionbased parsing model mcdonald and nivre 2007 ,0,1,0
we then describe the two main paradigms for learning and inference in this years shared task as well as in last years which we call transitionbased parsers section 52 and graphbased parsers section 53 adopting the terminology of mcdonald and nivre 20075 finally we give an overview of the domain adaptation methods that were used section 54 ,0,1,0
a solution that leverages the complementary strengths of these two approachesdescribed in detail by mcdonald and nivre 2007was recently and successfully explored by nivre and mcdonald 2008 ,1,0,0
2007 and nivre and mcdonald 2008 can be seen as methods to combine separately defined models ,0,1,0
the terms graphbased and transitionbased were used by mcdonald and nivre 2007 to describe the difference between mstparser mcdonald and pereira 2006 which is a graphbased parser with an exhaustive search decoder and maltparser nivre et al 2006 which is a transitionbased parser with a greedy search decoder ,0,1,0
mcdonald and nivre 2007 showed that the mstparser and maltparser produce different errors ,0,1,0
in the field of parsing mcdonald and nivre 2007 compared parsing errors between graphbased and transitionbased parsers ,0,1,0
in examining the combination of the two types of parsing mcdonald and nivre 2007 utilized similar approaches to our empirical analysis ,0,1,0
also mcdonald and nivre 2007 ,0,1,0
dependency lengths longdistance dependencies exhibit bad performance mcdonald and nivre 2007 ,0,1,0
c 2009 association for computational linguistics reverse revision and linear tree combination for dependency parsing giuseppe attardi dipartimento di informatica universita di pisa pisa italy attardidiunipiit felice dellorletta dipartimento di informatica universita di pisa pisa italy felicedellorlettadiunipiit 1 introduction deterministic transitionbased shiftreduce dependency parsers make often mistakes in the analysis of long span dependencies mcdonald nivre 2007 ,0,1,0
in order to get a better understanding of these matters we replicate parts of the error analysis presented by mcdonald and nivre 2007 where parsing errors are related to different structural properties of sentences and their dependency graphs ,0,1,0
first the graphbased models have better precision than the transitionbased models when predicting long arcs which is compatible with the results of mcdonald and nivre 2007 ,0,1,0
as expected we see that mst does better than malt for all categories except nouns and pronouns mcdonald and nivre 2007 ,0,1,0
both models have been used to achieve stateoftheart accuracy for a wide range of languages as shown in the conll shared tasks on dependency parsing buchholz and marsi 2006 nivre et al 2007 but mcdonald and nivre 2007 showed that a detailed error analysis reveals important differences in the distribution of errors associated with the two models ,0,1,0
this difference was highlighted in the 3httpw3msivxusejhamaltparser studyofmcdonaldandnivre2007 whichshowed that the difference is reflected directly in the error distributions of the parsers ,0,1,0
7an alternative framework that formally describes some dependency parsers is that of transition systems mcdonald and nivre 2007 ,0,1,0
4 dependency parsing baseline 41 learning model and features according to mcdonald and nivre 2007 all datadriven models for dependency parsing that have been proposed in recent years can be described as either graphbased or transitionbased ,0,1,0
kuhlmann and mohl 2007 mcdonald and nivre 2007 nivre et al 2007 hindi is a verb final flexible word order language and therefore has frequent occurrences of nonprojectivity in its dependency structures ,0,1,0
acknowledgments i want to thank my fellow organizers of the shared task johan hall sandra kubler ryan mcdonald jens nilsson sebastian riedel and deniz yuret whoarealsocoauthorsofthelongerpaperonwhich this paper is partly based nivre et al 2007 ,0,1,0
there are also attempts at a more finegrained analysis of accuracy targeting specific linguistic constructions or grammatical functions carroll and briscoe 2002 kubler and prokic 2006 mcdonald and nivre 2007 ,0,1,0
5 datadriven dependency parsing models for datadriven dependency parsing can be roughly divided into two paradigms graphbased and transitionbased models mcdonald and nivre 2007 ,0,1,0
for example both papers propose minimumrisk decoding and mcdonald and satta 2007 discuss unsupervised learning and language modeling while smith and smith 2007 define hiddenvariable models based on spanning trees ,0,1,0
we can sum over all nonprojective spanning trees by taking the determinant of the kirchhoff matrix of the graph defined above minus the row and column corresponding to the root node smith and smith 2007 ,0,1,0
the approach has been shown to give improvements over the map classifier in many areas of natural language processing including automatic speech recognition goel and byrne 2000 machine translation kumar and byrne 2004 zhang and gildea 2008 bilingual word alignment kumar and byrne 2002 andparsinggoodman 1996 titovandhenderson 2006 smith and smith 2007 ,1,0,0
it is often straightforward to obtain large amounts of unlabeled data making semisupervised approaches appealing previous work on semisupervised methods for dependency parsing includes smith and eisner 2007 koo et al 2008 wang et al 2008 ,0,1,0
then the method of smith and smith 2007 can be used to compute the probability of every possible edge conditioned on the presence of ki pyiprime kprimeyi kx using k1ki multiplying this probability by pyikx yields the desired two edge marginal ,0,1,0
this weak supervision has been encoded using priors and initializations klein and manning 2004 smith 2006 specialized models klein and manning 2004 seginer 2007 bod 2006 and implicit negative evidence smith 2006 ,0,1,0
smith and eisner 2007 apply entropy regularization to dependency parsing ,0,1,0
mcdonald and satta 2007 smith and smith 2007 ,0,1,0
smith and smith 2007 describe a more efficient algorithm that can compute all edge expectations in on3 time using the inverse of the kirchoff matrix k1 ,1,0,0
minimizing risk has been shown to improve performance for mt kumar and byrne 2004 as well as other language processing tasks goodman 1996 goel and byrne 2000 kumar and byrne 2002 titov and henderson 2006 smith and smith 2007 ,1,0,0
2007 and smith and smith 2007 showed that the matrixtree theorem can be used to train edgefactored loglinearmodelsofdependencyparsing ,0,1,0
for nonprojective parsing the analogy to the inside algorithm is the on3 matrixtree algorithm which is dominated asymptotically by a matrix determinant smith and smith 2007 koo et al 2007 mcdonald and satta 2007 ,0,1,0
we used a nonprojective model trained using an application of the matrixtree theorem koo et al 2007 smith and smith 2007 mcdonald and satta 2007 for the firstorder czech models and projective parsers for all other models ,0,1,0
for example the topics sport and education are important cues for differentiating mentions of michael jordan which may refer to a basketball player a computer science professor etc second as noted in the top weps run chen and martin 2007 feature development is important in achieving good coreference performance ,0,1,0
for more detail see chen martin 2007 ,0,1,0
followingjohnson2007iusevariational bayes em beal 2003 during the mstep for the transition distribution l1ji fenij ifen i ci 3 fv expv 4 60 v braceleftbigg gv 1 2 ifv 7 v 1 1v ow ,0,1,0
1 introduction there has been a great deal of recent interest in the unsupervised discovery of syntactic structure from text both partsofspeech johnson 2007 goldwater and griffiths 2007 biemann 2006 dasgupta and ng 2007 and deeper grammatical structure like constituency and dependency trees klein and manning 2004 smith 2006 bod 2006 seginer 2007 van zaanen 2001 ,0,1,0
for an hmm with a set of states t and a set of output symbols v t t t dir1t 1 t t t dir1v 2 titi1 ti1 multiti1 3 witi ti multiti 4 one advantage of the bayesian approach is that the prior allows us to bias learning toward sparser structures by setting the dirichlet hyperparameters to a value less than one johnson 2007 goldwater and griffiths 2007 ,0,1,0
johnson 2007 evaluates both estimation techniques on the bayesian bitag model goldwater and griffiths 2007 emphasize the advantage in the mcmc approach of integrating out the hmm parameters in a tritag model yielding a tagging supported by many different parameter settings ,0,1,0
finally following haghighi and klein 2006 and johnson 2007 we can instead insist that at most one hmm state can be mapped to any partofspeech tag ,0,1,0
goldwater and griffiths 2007 evaluated against the reduced tag set of 17 tags developed by smith and eisner 2005 while johnson 2007 evaluated against the full penn treebank tag set ,0,1,0
we ran each estimator with the eight different combinations of values for the hyperparameters and prime listed below which include the optimal values for the hyperparameters found by johnson 2007 and report results for the best combination for each estimator below 1 ,0,1,0
the samplers that goldwater and griffiths 2007 and johnson 2007 describe are pointwise collapsed gibbs samplers ,0,1,0
recent advances in these approaches include the use of a fully bayesian hmm johnson 2007 goldwater and griffiths 2007 ,0,1,0
recent work goldwater and griffiths 2007 johnson 2007 gao and johnson 2008 on this task explored a variety of methodologies to address this issue ,0,1,0
johnson 2007 and zhang et al ,0,1,0
similar to goldwater and griffiths 2007 and johnson 2007 toutanova and johnson 2007 also use bayesian inference for pos tagging ,0,1,0
nevertheless em sometimes fails to find good parameter values2 the reason is that em tries to assign roughly the same number of word tokens to each of the hidden states johnson 2007 ,0,1,0
they are most commonly used for parsing and linguistic analysis charniak and johnson 2005 collins 2003 but are now commonly seen in applications like machine translation wu 1997 and question answering wang et al 2007 ,0,1,0
for example if we make a meanfield assumption with respect to hidden structure and weights the variationalalgorithmforapproximatelyinferringthe distribution over and trees y resembles the traditional em algorithm very closely johnson 2007 ,0,1,0
for instance on unsupervised partofspeech tagging em requires over 100 iterations to reach its peak performance on the wallstreet journal johnson 2007 ,0,1,0
3 variational bayes for itg goldwater and griffiths 2007 and johnson 2007 show that modifying an hmm to include a sparse prior over its parameters and using bayesian estimation leads to improved accuracy for unsupervised partofspeech tagging ,0,1,0
as pointed out by johnson 2007 in effect this expression adds to c a small value that asymptotically approaches 05 as c approaches and 0 as c approaches 0 ,0,1,0
bayesian approaches can also improve performance goldwater and griffiths 2007 johnson 2007 kurihara and sato 2006 ,0,1,0
recent projects in semisupervised toutanova and johnson 2007 and unsupervised biemann et al 2007 smith and eisner 2005 tagging also show significant progress ,1,0,0
hmms have been used many times for pos tagging and chunking in supervised semisupervised and in unsupervised settings banko and moore 2004 goldwater and griffiths 2007 johnson 2007 zhou 2004 ,0,1,0
the overall pos tag distribution learnt by em is relatively uniform as noted by johnson 2007 and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed ,0,1,0
5since the test data of svore et al 2007 is not publicly available we were unable to carry out a more detailed comparison ,0,0,1
our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of svore et al 2007 without requiring their thirdparty data resources ,0,0,1
also related are the areas of word alignment for machine translation och and ney 2000 induction of translation lexicons schafer and yarowsky 2002 and crosslanguage annotation projections to a second language riloff et al 2002 hwa et al 2002 mohammad et al 2007 ,0,1,0
measures of crosslanguage relatedness are useful for a large number of applications including crosslanguage information retrieval nie et al 1999 monz and dorr 2005 crosslanguage text classification gliozzo and strapparava 2006 lexical choice in machine translation och and ney 2000 bangalore et al 2007 induction of translation lexicons schafer and yarowsky 2002 crosslanguage annotation and resource projections to a second language riloff et al 2002 hwa et al 2002 mohammad et al 2007 ,0,1,0
2 related work the most commonly used similarity measures are based on the wordnet lexical database eg budanitsky and hirst 2006 hughes and ramage 2007 and a number of such measures have been made publicly available pedersen etal 2004 ,0,1,0
et al 2004 collinsthompson and callan 2005 hughes and ramage 2007 ,0,1,0
for instance hughes and ramage 2007 constructed a graph which represented various types of word relations from wordnet and compared randomwalk similarity to similarity assessments from humansubject trials ,0,1,0
we want to note that our wordnetbased method outperforms that of hughes and ramage 2007 which uses a similar method ,0,0,1
our similarity method is similar but simpler to that used by hughes and ramage 2007 which report very good results on similarity datasets ,1,0,0
hughes and ramage 2007 present a lexical similarity model based on random walks on graphs derived from wordnet rao et al ,0,1,0
this is similartothegraphconstructionmethodofhughes and ramage 2007 and rao et al ,0,1,0
713 similarity via pagerank pagerank page et al 1998 is the celebrated citation ranking algorithm that has been applied to several natural language problems from summarization erkan and radev 2004 to opinion mining esuli and sebastiani 2007 to our task of lexical relatedness hughes and ramage 2007 ,0,1,0
4 semantic class induction from wikipedia wikipedia has recently been used as a knowledge source for various language processing tasks including taxonomy construction ponzetto and strube 2007a coreference resolution ponzetto and strube 2007b and english ner eg bunescu and pasca 2006 cucerzan 2007 kazama and torisawa 2007 watanabe et al ,0,1,0
note that it is straightforward to calculate these expected counts using a variant of the insideoutside algorithm baker 1979 applied to the eisner 1996 dependencyparsing data structures paskin 2001 for projective dependency structures or the matrixtree theorem koo et al 2007 smith and smith 2007 mcdonald and satta 2007 for nonprojective dependency structures ,0,1,0
one option would be to leverage unannotated text mcclosky et al 2006 smith and eisner 2007 ,0,1,0
this generates tens of millions features so we prune those features that occur fewer than 10 total times as in smith and eisner 2007 ,0,1,0
finally recent work has explored learning to map sentences to lambdacalculus meaning representations wong and mooney 2007 zettlemoyer and collins 2005 zettlemoyer and collins 2007 ,0,1,0
for example when applying their approach to a different domain with somewhat less rigid syntax zettlemoyer and collins 2007 need to introduce new combinators and new forms of candidate lexical entries ,0,1,0
it has been used for a variety of tasks such as widecoverage parsing hockenmaier and steedman 2002 clark and curran 2007 sentence realization white 2006 learning semantic parsers zettlemoyer and collins 2007 dialog systems kruijff et al 2007 grammar engineering beavers 2004 baldridge et al 2007 and modeling syntactic priming reitter et al 2006 ,0,1,0
available scissor ge and mooney 2005 an integrated syntacticsemantic parser krisp kate and mooney 2006 an svmbased parser using string kernels wasp wong and mooney 2006 wong and mooney 2007 a system based on synchronous grammars zc zettlemoyer and collins 20073 a probabilistic parser based on relaxed ccg grammars and lu lu et al 2008 a generative model with discriminative reranking ,0,1,0
1 introduction recently researchers have developed algorithms that learn to map natural language sentences to representations of their underlying meaning he and young 2006 wong and mooney 2007 zettlemoyer and collins 2005 ,0,1,0
albeit simple the algorithm has proven to be very efficient and accurate for the task of parse selection collins and roark 2004 collins 2004 zettlemoyer and collins 2005 zettlemoyer and collins 2007 ,1,0,0
the parameters of the refined productions ax by cz where ax is a subcategory of a by of b and cz of c can then be estimated in various ways past work has included both generative matsuzaki et al 2005 liang et al 2007 and discriminative approaches petrov and klein 2008 ,0,1,0
a different approach in evaluating nonparametric bayesian models for nlp is statesplitting finkel et al 2007 liang et al 2007 ,0,1,0
there has been an increased interest recently in employing bayesian modeling for probabilistic grammars in different settings ranging from putting priors over grammar probabilities johnson et al 2007 to putting nonparametric priors over derivations johnson et al 2006 to learning the set of states in a grammar finkel et al 2007 liang et al 2007 ,0,1,0
in addition to the block sampler used by bhattacharya and getoor 2006 we are investigating generalpurpose splitmerge samplers jain and neal 2000 and the permutation sampler liang et al 2007a ,0,1,0
wed like to learn the number of paradigm classes from the data but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive statesplitting found in the ihmm and ipcfg liang et al 2007 ,0,1,0
recently methods from nonparametric bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks including language modeling word and morpheme segmentation parsing and machine translation teh et al 2006 goldwater et al 2006a goldwater et al 2006b liang et al 2007 finkel et al 2007 denero et al 2008 ,0,1,0
4 semantic class induction from wikipedia wikipedia has recently been used as a knowledge source for various language processing tasks including taxonomy construction ponzetto and strube 2007a coreference resolution ponzetto and strube 2007b and english ner eg bunescu and pasca 2006 cucerzan 2007 kazama and torisawa 2007 watanabe et al ,0,1,0
kazama and torisawa 2007 improve their fscore by 3 by including a wikipediabased feature in their machine learner ,1,0,0
the most relevant to our work are kazama and torisawa 2007 toral and muoz 2006 and cucerzan 2007 ,0,1,0
similarly kazama and torisawa 2007 used wikipedia particularly the first sentence of each article to create lists of entities ,0,1,0
the small differences from their work are 1 we used characters as the unit as we described above 2 while kazama and torisawa 2007 checked only the word sequences that start with a capitalized word and thus exploitedthecharacteristicsofenglishlanguage we checked the matching at every character 3 we used a trie to make the lookup efcient ,0,1,0
for instance kazama and torisawa 2007 used the hyponymy relations extracted from wikipedia for the english ner and reported improved accuracies with such a gazetteer ,1,0,0
6 related work and discussion there are several studies that used automatically extracted gazetteers for ner shinzato et al 2006 talukdar et al 2006 nadeau et al 2006 kazama and torisawa 2007 ,0,1,0
on the other hand kazama and torisawa 2007 extracted hyponymy relations which are independent of the ne categories from wikipedia and utilized it as a gazetteer ,0,1,0
kazama and torisawa 2007 extracted hyponymyrelationsfromtherstsentencesie dening sentences of wikipedia articles and then used them as a gazetteer for ner ,0,1,0
the method described by kazama and torisawa 2007 is to rst extract the rst base noun phrase after the rst is was are or were in the rst sentence of a wikipedia article ,0,1,0
the second baseline is our implementation of the relevant part of the wikipedia extraction in kazama and torisawa 2007 taking the first noun after a be verb in the definition sentence denoted as wikibl ,0,1,0
becomp following the general idea in kazama and torisawa 2007 we identify the isa pattern in the definition sentence by extracting nominal complements of the verb be taking 451 no ,0,1,0
for example the entry about the microsoft in wikipedia has the following categories companies listed on nasdaq cloud computing vendors etc both toral and munoz 2006 and kazama and torisawa 2007a used the freetext description of the wikipedia entity to reason about the entity type ,0,1,0
systems based on perceptron have been shown to be competitive in ner and text chunking kazama and torisawa 2007b punyakanok and roth 2001 carreras et al 2003 we specify the model and the features with the lbj rizzolo and roth 2007 modeling language ,0,1,0
we perform term disambiguation on each document using an entity extractor cucerzan 2007 ,0,1,0
even if the idea of using wikipedia links for disambiguation is not novel cucerzan 2007 it is applied for the first time to framenet lexical units considering a frame as a sense definition ,0,1,0
some researchers cucerzan 2007 nguyen and cao 2008 have explored the use of wikipedia information to improve the disambiguation process ,0,1,0
however most of them do not build a nes resource but exploit external gazetteers bunescu and pasca 2006 cucerzan 2007 ,0,1,0
an important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature mann and yarowsky 2003 artiles et al 2007 cucerzan 2007 ,1,0,0
ca 2006 and cucerzan 2007 in mining relationships between named entities or in extracting useful facet terms from news articles eg dakka and ipeirotis 2008 ,0,1,0
much later work evans 2003 etzioni et al 2005 cucerzan 2007 pasca 2004 relies on the use of extremely large corpora which allow very precise but sparse features ,1,0,0
the most relevant to our work are kazama and torisawa 2007 toral and muoz 2006 and cucerzan 2007 ,0,1,0
cucerzan 2007 by contrast to the above used wikipedia primarily for named entity disambiguation following the path of bunescu and paca 2006 ,0,1,0
crf baseline 9718 9721 table 7 pos tagging results of the previous top systems for ptb iii data evaluated by label accuracy system test additional resources jesscm crfhmm 9515 1gword unlabeled data 9467 15mword unlabeled data ando and zhang 2005 9439 15mword unlabeled data suzuki et al 2007 9436 17mword unlabeled data zhang et al 2002 9417 full parser output kudo and matsumoto 2001 9391 supervised crf baseline 9388 table 8 syntactic chunking results of the previous top systems for conll00 shared task data f1 score 3031 aug 1996 and 67 dec 1996 reuters news articles respectively ,0,1,0
as our approach for incorporating unlabeled data we basically follow the idea proposed in suzuki et al 2007 ,0,1,0
following this idea there have been introduced a parameter estimation approach for nongenerative approaches that can effectively incorporate unlabeled data suzuki et al 2007 ,0,1,0
in addition the calculation cost for estimating parameters of embedded joint pms hmms is independent of the number of hmms j that we used suzuki et al 2007 ,0,1,0
surprisingly although jesscm is a simpler version of the hybrid model in terms of model structure and parameter estimation procedure jesscm provides fscores of 9445 and 8803 for conll00 and 03 data respectively which are 015 and 083 points higher than those reported in suzuki et al 2007 for the same configurations ,0,0,1
these records are also known as field books and reference sets in literature canisius and sporleder 2007 michelson and knoblock 2008 ,0,1,0
both agichtein and ganti 2004 and canisius and sporleder 2007 train a language model for each database column ,0,1,0
for instance word alignment models are often trained using the giza toolkit och and ney 2003 error minimizing training criteria such as the minimum error rate training och 2003 are employed in order to learn feature function weights for loglinear models and translation candidates are produced using phrasebased decoders koehn et al 2003 in combination with ngram language models brants et al 2007 ,0,1,0
of course many applications require smoothing of the estimated distributionsthis problem also has known solutions in mapreduce brants et al 2007 ,0,1,0
since that time however increasingly large amounts of language model training data have become available ranging from approximately one billion words the gigaword corpora from the linguistic data consortium to trillions of words brants et al 2007 ,0,1,0
all the tblms and orlms were unpruned 5gram models and used stupidbackoff smoothing brants et al 2007 2 with the backoff parameter set to 04 as suggested ,0,1,0
2007 looked at golomb coding and brants et al ,0,1,0
33 language model we estimate ps using ngram lms trained on data from the web using stupid backoff brants et al 2007 ,0,1,0
we build sentencespecific zerocutoff stupidbackoff brants et al 2007 5gram language models estimated using 47b words of english newswire text and apply them to rescore each 10000best list ,0,1,0
a recent trend is to store the lm in a distributed cluster of machines which are queried via network requests brants et al 2007 emami et al 2007 ,0,1,0
we build sentencespecific zerocutoff stupidbackoff brants et al 2007 5gram language models estimated using 47b words of english newswire text and apply them to rescore either 10000best lists generated by hcp or word lattices generated by hifst ,0,1,0
brants et al 2007 emami et al 2007 built 5gram lms over web using distributed cluster of machines and queried them via network requests ,0,1,0
1 introduction very large corpora obtained from the web have been successfully utilized for many natural languageprocessingnlpapplications suchasprepositional phrase pp attachment otheranaphora resolution spellingcorrection confusablewordsetdisambiguation and machine translation volk 2001 modjeska et al 2003 lapata and keller 2005 atterer and schutze 2006 brants et al 2007 ,1,0,0
here we choose to work with stupid backoff smoothing brants et al 2007 since this is significantly more efficient to train and deploy in a distributed framework than a contextdependent smoothing scheme such as kneserney ,1,0,0
table 2 shows the total space and number of bytes required per ngram to encode the model under different schemes ldc gzipd is the size of the files as delivered by ldc trie uses a compact trie representation eg clarkson et al 1997 church et al 2007 with 3 byte word ids 1 byte values and 3 byte indices block encoding is the encoding used in brants et al 2007 and randomized uses our novel randomized scheme with 12 error bits ,0,1,0
there is a vast literature on language modeling see eg rosenfeld 2000 chen and goodman 1999 brants et al 2007 roark et al 2007 ,0,1,0
indeed researchers have shown that gigantic language models are key to stateoftheart performance brants et al 2007 and the ability of phrasebased decoders to handle largesize highorder language models with no consequence on asymptotic running time during decoding presents a compelling advantage over ckydecoderswhosetimecomplexitygrowsprohibitively large with higherorder language models ,1,0,0
we implemented an ngram indexerestimator using mpi inspired by the mapreduce implementation of ngram language model indexingestimation pipeline brants et al 2007 ,0,1,0
the recent emphasis on improving these components of a translation system brants et al 2007 is likely due in part to the widespread availability of nlp tools for the language that is most frequently the target english ,0,1,0
it is therefore desirable to have dedicated servers to load parts of the lm3 an idea that has been exploited by zhang et al 2006 emami et al 2007 brants et al 2007 ,0,1,0
these findings are somehow surprising since it was eventually believed by the community that adding large amounts of bitexts should improve the translation model as it is usually observed for the language model brants et al 2007 ,0,1,0
we have also used tpts to encode ngram count databases such as the google 1t web ngram database brants and franz 2006 but are not able to provide detailed results within the space limitations of this paper4 51 perplexity computation with 5gram language models we compared the performance of tptencoded language models against three other language model implementations the sri language modeling toolkit stolcke 2002 irstlm federico and cettolo 2007 and the language model implementation currently used in the portage smt system badr et al 2007 which uses a pointerbased implementation but is able to perform fast lm filtering at load time ,0,1,0
in this study we use the google web 1t 5gram corpus brants et al 2007 ,0,1,0
2 related work there have been various efforts to integrate linguistic knowledge into smt systems either from the target side marcu et al 2006 hassan et al 2007 zollmann and venugopal 2006 the source side quirk et al 2005 liu et al 2006 huang et al 2006 or both sides eisner 2003 ding et al 2005 koehn and hoang 2007 just to name a few ,0,1,0
in koehn and hoang 2007 shallow syntactic analysis such as pos tagging and morphological analysis were incorporated in a phrasal decoder ,0,1,0
c2009 association for computational linguistics improving midrange reordering using templates of factors hieu hoang school of informatics university of edinburgh hhoangsmsedacuk philipp koehn school of informatics university of edinburgh pkoehninfedacuk abstract we extend the factored translation model koehn and hoang 2007 to allow translations of longer phrases composed of factors such as pos and morphological tags to act as templates for the selection and reordering of surface phrase translation ,0,1,0
koehn and hoang 2007 describes various strategies for the decomposition of the decoding into multiple translation models using the moses decoder ,0,1,0
in a factored translation model other factors than surface form can be used such as lemma or partofspeech koehn and hoang 2007 ,0,1,0
recent work by koehn and hoang 2007 pro514 poses factored translation models that combine feature functions to handle syntactic morphological and other linguistic information in a loglinear model ,0,1,0
in particular we adopt the approach of phrasebased statistical machine translation koehn et al 2003 koehn and hoang 2007 ,0,1,0
the publicly available moses4 decoder is used for training and decoding koehn and hoang 2007 ,0,1,0
factored models are introduced in koehn and hoang 2007 for better integration of morphosyntactic information ,0,1,0
in recent work koehn and hoang 2007 proposed a general framework for including morphological features in a phrasebased smt system by factoring the representation of words into a vector of morphological features and allowing a phrasebased mt system to work on any of the factored representations which is implemented in the moses system ,0,1,0
though our motivation is similar to that of koehn and hoang 2007 we chose to build an independent component for inflection prediction in isolation rather than folding morphological information into the main translation model ,0,1,0
koehn and hoang 2007 present factored translation models as an extension to phrasebased statistical machine translation models ,0,1,0
koehn and hoang 2007 propose factored translation models that combine feature functions to handle syntactic morphological and other linguistic information in a loglinear model ,0,1,0
41 overview in this work factored models koehn and hoang 2007 are experimented with three factors the surface form the lemma and the part of speech pos ,0,1,0
for example factored translation models koehn and hoang 2007 retain the simplicity of phrasebased smt while adding the ability to incorporate additional features ,1,0,0
43 adaptation for unknown word2 the unknown word problem is an important issue for domain adaptationdredze et al 2007 ,0,1,0
without specific knowledge of the target domains annotation standards significant improvement can not be madedredze et al 2007 ,0,1,0
2007 and dredze et al ,0,1,0
instead of assigning head and deprel in a single step some systems use a twostage approach for attaching and labeling dependencies chen et al 2007 dredze et al 2007 ,0,1,0
in order to calculate a global score or probability for a transition sequence two systems used a markov chain approach duan et al 2007 sagae and tsujii 2007 ,0,1,0
54 domain adaptation 541 featurebased approaches onewayofadaptingalearnertoanewdomainwithout using any unlabeled data is to only include features that are expected to transfer well dredze et al 2007 ,0,1,0
in contrast semisupervised domain adaptation blitzer et al 2006 mcclosky et al 2006 dredze et al 2007 is the scenario in which in addition to the labeled source data we only have unlabeled and no labeled target domain data ,0,1,0
however based on annotation differences in the datasets dredze et al 2007 and a bug in their system shimizu and nakagawa 2007 their results are inconclusive1 thus the effectiveness of scl is rather unexplored for parsing ,0,1,0
8412 only ptb baseline 8358 1st sagae and tsujii 2007 8342 2nd dredze et al 2007 8338 3rd attardi et al 2007 8308 third row lists the three highest scores of the domain adaptation track of the conll 2007 shared task ,0,1,0
this was a difcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data dredze et al 2007 ,0,1,0
as well as the sentiment expressions leading to evaluations there are many semantic aspects to be extracted from documents which contain writers opinions such as subjectivity wiebe and mihalcea 2006 comparative sentences jindal and liu 2006 or predictive expressions kim and hovy 2007 ,0,1,0
specifically kim and hovy 2007 identify which political candidate is predicted to win by an opinion posted on a message board and aggregate opinions to correctly predict an election result ,0,1,0
an application of the idea of alternative targets can be seen in kim and hovys 2007 work on election prediction ,0,1,0
kim and hovy 2007 predict the results of an election by analyzing forums discussing the elections ,0,1,0
examples of the latter include providing suggestions from a machine labeler and using extremely cheap human labelers eg with the amazon mechanical turk snow et al 2008 ,0,1,0
while this is certainly a daunting task it is possible that for annotation studies that do not require expert annotators and extensive annotator training the newly available access to a large pool of inexpensive annotators such as the amazon mechanical turk scheme snow et al 20084 or embedding the task in an online game played by volunteers poesio et al 2008 von ahn 2006 could provide some solutions ,1,0,0
previous work has shown that data collected through the mechanical turk service is reliable and comparable in quality with trusted sources snow et al 2008 ,0,1,0
several recent papers have studied the use of annotations obtained from amazon mechanical turk a marketplace for recruiting online workers su et al 2007 kaisser et al 2008 kittur et al 2008 sheng et al 2008 snow et al 2008 sorokin and forsyth 2008 ,0,1,0
2005 ponzetto and strube 2006 and the exploitation of advanced techniques that involve joint learning eg daume iii and marcu 2005 and joint inference eg denis and baldridge 2007 for coreference resolution and a related extraction task ,0,1,0
other similar work includes the mention detection md task florian et al 2006 and joint probabilistic model of coreference daume iii and marcu 2005 ,0,1,0
7 related work there has been a recent interest in training methods that enable the use of firstorder features paskin 2002 daume iii and marcu 2005b richardson and domingos 2006 ,0,1,0
perhaps the most related is 86 learning as search optimization laso daume iii and marcu 2005b daume iii and marcu 2005a ,0,1,0
we finally move on to present more complex models which attempt to model coreference as a global discourse phenomenon yang et al 2003 luo et al 2004 daume iii marcu 2005 inter alia ,0,1,0
in contrast globally optimized clustering decisions were reported in luo et al 2004 and daumeiii and marcu 2005a where all clustering possibilities are considered by searching on a bell tree representation or by using the learning as search optimization laso framework daumeiii and marcu 2005b respectively but the first search is partial and driven by heuristics and the second one only looks back in text ,0,1,0
meanwhile some learning algorithms like maximum likelihood for conditional loglinear models lafferty et al 2001 unsupervised models pereira and schabes 1992 and models with hidden variables koo and collins 2005 wang et al 2007 blunsom et al 2008 require summing over the scores of many structures to calculate marginals ,0,1,0
1 introduction state of the art statistical parsers collins 1999 charniak 2000 koo and collins 2005 charniak and johnson 2005 are trained on manually annotated treebanks that are highly expensive to create ,1,0,0
koo and collins 2005 matsuzaki et al 2005 riezler et al 2002 ,0,1,0
previous research in this area includes several models which incorporate hidden variables matsuzaki et al 2005 koo and collins 2005 petrov et al 2006 titov and henderson 2007 ,0,1,0
use of probability estimates is not a serious limitation of this approach because in practice candidates are normally provided by some probabilistic model and its probability estimates are used as additional features in the reranker collins and koo 2005 shen and joshi 2003 henderson and titov 2005 ,0,1,0
matsuzaki et al 2005 koo and collins 2005 ,0,1,0
recently several latent variable models for constituent parsing have been proposed koo and collins 2005 matsuzaki et al 2005 prescher 2005 riezler et al 2002 ,0,1,0
for example syntactic features ng and cardie 2002b luo and zitouni 2005 can be computed this way and are used in our system ,0,1,0
the coreference resolution system employs a variety of lexical semantic distance and syntactic featuresluo et al 2004 luo and zitouni 2005 ,0,1,0
these features are calculated by mining the parse trees and then could be used for resolution by using manually designed rules lappin and leass 1994 kennedy and boguraev 1996 mitkov 1998 or using machinelearning methods aone and bennett 1995 yang et al 2004 luo and zitouni 2005 ,0,1,0
luo and zitouni 2005 proposed a coreference resolution approach which also explores the information from the syntactic parse trees ,0,1,0
2 f 1 score maximization training of lrm we first review the f 1 score maximization training method for linear models using a logistic function described in jansche 2005 ,0,1,0
2006 and jansche 2005 who discuss maximum expected fscore training of decision trees and logistic regression models ,0,1,0
for example the constrained optimization method of mozer et al 2001 relies on approximations of sensitivity which they call ca and specificity2 their cr related techniques gao et al 2006 jansche 2005 rely on approximations of true positives false positives and false negatives and indirectly recall and precision ,0,1,0
2001 whose constrained optimization technique is similar to those in gao et al 2006 jansche 2005 ,0,1,0
abney 2004 presented a thorough discussion on the yarowsky algorithm ,0,1,0
3 the framework 31 the algorithm our transductive learning algorithm algorithm 1 is inspired by the yarowsky algorithm yarowsky 1995 abney 2004 ,0,1,0
under certain precise conditions as described in abney 2004 we can analyze algorithm 1 as minimizing the entropy of the distribution over translations of u however this is true only when the functions estimate score and select have very prescribed definitions ,0,1,0
more recently haffari and sarkar 2007 have extended the work of abney 2004 and given a better mathematical understanding of selftraining algorithms ,0,1,0
previous studies called the class of algorithms illustrated in figure 2 cautious or sequential because in each iteration they acquire 1 or a small set of rules abney 2004 collins and singer 1999 ,0,1,0
thispaperfocusesontheframeworkintroduced in figure 2 for two reasons a cautious al50 gorithms were shown to perform best for several nlp problems including acquisition of ie patterns and b it has nice theoretical properties abney 2004 showed that regardless of the selection procedure sequential bootstrapping algorithms converge to a local minimum of k where k is an upper bound of the negative log likelihood of the data ,0,1,0
the linear kernel derived from the l1 distance is the same as the differenceweighted tokenbased similarity measure of weeds and weir 2005 ,0,1,0
weeds and weir 2005 discuss the influence of bias towards highor lowfrequency items for different tasks correlation with wordnetderived neighbour sets and pseudoword disambiguation and it would not be surprising if the different highfrequency bias were leading to different results ,0,1,0
see weeds and weir 2005 for an overview of other measures ,0,1,0
a variety of other measures of semantic relatedness have been proposed including distributional similarity measures based on cooccurrence in a body of text see weeds and weir 2005 for a survey ,0,1,0
formally by distributional similarity or cooccurrence similarity of two words w 1 and w 2 we mean that they tend to occur in similar contexts for some definition of context or that the set of words that w 1 tends to cooccur with is similar to the set that w 2 tends to cooccur with or that if w 1 is substituted for w 2 in a context its plausibility weeds 2003 weeds and weir 2005 is unchanged ,0,1,0
further it has been shown weeds et al 2005 weeds and weir 2005 that performance of lins distributional similarity score decreases more significantly than other measures for low frequency nouns ,0,1,0
we then compute the weight of a context word w in context c ww c using mutual information and ttest which were reported by weeds and weir 2005 to perform the best on a pseudodisambiguation task ,0,1,0
2 evaluating sr measures various approaches for computing semantic relatedness of words or concepts have been proposed eg dictionarybased lesk 1986 ontologybased wu and palmer 1994 leacock and chodorow 1998 informationbased resnik 1995 jiang and conrath 1997 or distributional weeds and weir 2005 ,0,1,0
the closest work is that of jing and mckeown 1999 and daume iii and marcu 2005 in which multiple sentences are processed with fragments within them being recycled to generate the novel generated text ,0,1,0
the best previous result is an accuracy of 561 turney 2006 ,1,0,0
the average senior high school student achieves 57 correct turney 2006 ,0,1,0
turney 2006 later addressed the same problem using 8000 automatically generated patterns ,0,1,0
the distinction between lexical and relational similarity for word pair comparison is recognised byturney2006hecallstheformer attributional similarity though the methods he presents focus on relational similarity ,0,1,0
the attr cells summarize the performance of the 6 models on the wiki table that are based on attributional similarity only turney 2006 ,0,1,0
in particular we need to develop a backoff strategy for unseen pairs in the relational similarity tasks that following turney 2006 could be based on constructing surrogate pairs of taxonomically similar words found in the cxlc space ,0,1,0
in computational linguistics our pattern discovery procedure extends over previous approaches that use surface patterns as indicators of semantic relations between nouns or verbs hearst 1998 chklovski and pantel 2004 etzioni et al 2004 turney 2006 davidov and rappoport 2008 inter alia ,0,1,0
previous research in automatic acquisition focuses primarily on the use of statistical techniques such as bilingual alignment church and hanks 1990 klavans and tzoukermann 1995 wu and xia 1995 or extraction of syntactic constructions from online dictionaries and corpora brent 1993 ,0,1,0
church and hanks 1990 introduced a statistical measurement called mutual information for extracting strongly associated or collocated words ,0,1,0
213 correlation analysis as a correlation measure between terms we use mutual information church and hanks 1990 ,0,1,0
collocations were extracted according to the method described in church and hanks 1990 by moving a window on texts ,0,1,0
mutual information mixy is defined as following church and hanks 1990 log log 22 yfxf yxfn ypxp yxp yxmi 4 where fx and fy are frequency of term x and term y respectively ,0,1,0
one way of resolving query ambiguities is to use the statistics such as mutual information church and hanks 1990 to measure associations of query terms on the basis of existing corpora jang et al 1999 ,0,1,0
the mutual information of a pair of words is defined in terms of their cooccurrence frequency and respective occurrence frequencies church and hanks 1990 ,0,1,0
5 related work although there have been many studies on collocation extraction and mining using only statistical approaches church and hanks 1990 ikehara et al 1996 there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations ,0,1,0
the initial vectors to be clustered are adapted with pointwise mutual information church and hanks 1990 ,0,1,0
in zernik 1990 calzolari and bindi 1990 smadja 1989 church and hanks 1990 associations are detected in a 5 window ,0,1,0
in the field of eomputationa1 linguistics mutual information brown et al 1988 2 church and hanks 1990 or a likelihood ratio test dunning 199a are suggested ,0,1,0
previous research in automatic acquisition focuscs primarily on the use of statistical techniques such as bilingual alignment church and hanks 1990 klavans and tzoukermann 1996 wu and xia 1995 or extraction of syntactic constructions from online dictionaries and corpora brant 1993 dorr garman and weinberg 1995 ,0,1,0
in the past five years important research on the automatic acquisition of word classes based on lexical distribution has been published church and hanks 1990 hindle 1990 smadja 1993 greinstette 1994 grishman and sterling 1994 ,1,0,0
there are many method proposed to extract rigid expressions from corpora such as a method of focusing on the binding strength of two words church and hanks 1990 the distance between words smadja and makeown 1990 and the number of combined words and frequency of appearance kita 1993 1994 ,0,1,0
given this the mutual information ratio church hanks 1990 church mercer 1993 steier belew 1991 is expressed by formula 1 ,0,1,0
in the field of statistical analysis of natural language data it is common to use measures of lexical association such as the informationtheoretic measure of mutual information to extract useful relationships between words eg church and hanks 1990 ,0,1,0
more rare words rather than common words are found even in standard dictionaries church and hanks 1990 ,0,1,0
many studies on collocation extraction are carried out based on cooccurring frequencies of the word pairs in texts choueka et al 1983 church and hanks 1990 smadja 1993 dunning 1993 pearce 2002 evert 2004 ,0,1,0
2mutual information though potentially of interest as a measure of collocational status was not tested due to its wellknown property of overemphasising the significance of rare events church and hanks 1990 ,0,0,1
mutual information compares the probability of the cooccurence of words a and b with the independent probabilities of occurrence of a and b church and hanks 1990 ,0,1,0
church k and hanks p 1990 word association norms mutual information and lexicography computational linguistics vol ,0,1,0
2 related works some of the most common measures of unithood include pointwise mutual information mi church and hanks 1990 and loglikelihood ratio dunning 1994 ,0,1,0
church and hanks 1990 use mutual information to identify collocations a method they claim is reasonably effective for words with a frequency of not less than five ,1,0,0
303 wiebe wilson bruce bell and martin learning subjective language while it is common in studies of collocations to omit lowfrequency words and expressions from analysis because they give rise to invalid or unrealistic statistical measures church and hanks 1990 we are able to identify higherprecision collocations by including placeholders for unique words ie the ugenngrams ,0,1,0
probably the most widely used feature weighting function is pointwise mutual information mi church and patrick 1990 hindle 1990 luk 1995 lin 1998 gauch wang and rachakonda 1999 dagan 2000 baroni and vegnaduzzo 2004 chklovski and pantel 2004 pantel and ravichandran 2004 pantel ravichandran and hovy 2004 weeds weir and mccarthy 2004 dened by weight mi wflog 2 pwf pwpf 1 we calculate the mi weights by the following statistics in the space of cooccurrence instances s weight mi wflog 2 countwf nrels countw countf 2 where countwf is the frequency of the cooccurrence pair wf in s countwand countf are the independent frequencies of w and f in sandnrels is the size of shigh mi weights are assumed to correspond to strong wordfeature associations ,1,0,0
using techniques described in church and hindle 1990 church and hanks 1990 and hindle and rooth 1991 figure 4 shows some examples of the most frequent vo pairs from the ap corpus ,0,1,0
church and hanks 1990 klavans chodorow and wacholder 1990 wilks et al 1993 smadja 1991a 1991b calzolari and bindi 1990 ,0,1,0
for example church and hanks 1990 describe the use of the mutual information index for this purpose cf ,0,1,0
these tools are important in that the strongest collocational associations often represent different word senses and thus they provide a powerful set of suggestions to the lexicographer for what needs to be accounted for in choosing a set of semantic tags church and hanks 1990 p 28 ,0,1,0
the use of such relations mainly relations between verbs or nouns and their arguments and modifiers for various purposes has received growing attention in recent research church and hanks 1990 zernik and jacobs 1990 hindle 1990 smadja 1993 ,0,1,0
1 introduction many different statistical tests have been proposed to measure the strength of word similarity or word association in natural language texts dunning 1993 church and hanks 1990 dagan et al 1999 ,0,1,0
the value of fj is calculated by mutual information church and hanks 1990 between xi and fj ,0,1,0
the former extracts collocations within a fixed window church and hanks 1990 smadja 1993 ,0,1,0
concrete similarity measures compare a pair of weighted context feature vectors that characterize two words church and hanks 1990 ruge 1992 pereira et al 1993 grefenstette 1994 lee 1997 lin 1998 pantel and lin 2002 weeds and weir 2003 ,0,1,0
a variety of methods have been applied ranging from simple frequency justeson katz 1995 modified frequency measures such as cvalues frantzi anadiou mima 2000 maynard anadiou 2000 and standard statistical significance tests such as the ttest the chisquared test and loglikelihood church and hanks 1990 dunning 1993 and informationbased methods eg pointwise mutual information church hanks 1990 ,0,1,0
like church and hanks 1990 we used mutual information to measure the cohesion between two words ,0,1,0
one can also examine the distribution of character or word ngrams eg language modeling croft and lafferty 2003 phrases church and hanks 1990 lewis 1992 and so on ,0,1,0
to extract such word clusters we used suffix arrays proposed in yamamoto and church 2001 and the pointwise mutual information measure see church and hanks 1990 ,0,1,0
to examine the effects of including some known ams on the performance the following ams had a 50 chance of being included in the initial population pointwise mutual information church and hanks 1990 the dice coefficient and the heuristic measure defined in petrovic et al 2006 habc 2log fabcfafc if posb x log fabcfafbfc otherwise ,0,1,0
computational linguists have demonstrated that a words meaning is captured to some extent by the distribution of words and phrases with which it commonly cooccurs church hanks 1990 ,0,1,0
in this case it is possible to perform the correct selection if we used only statistics about the cooccurrences of corruption with either investigator or researcher without looking for any syntactic relation as in church and hanks 1990 ,0,1,0
introduction word associations have been studied for some time in the fields of psycholinguistics by testing human subjects on words linguistics where meaning is often based on how words cooccur with each other and more recently by researchers in natural language processing church and hanks 1990 hindle and rooth 1990 dagan 1990 mcdonald et al 1990 wilks et al 1990 using statistical measures to identify sets of associated words for use in various natural language processing tasks ,0,1,0
in each experiment performance imutud information provides an estimate of the magnitude of the ratio tctwn the joint prolability pverbnoun1reposition and the joint probability asuming indcpendcnce pverbnounpprclosition s church and hanks 1990 ,0,1,0
the collocations have been calculated according to the method described in church and hanks 1990 by moving a window on the texts ,0,1,0
we then rankorder the p xy mi xy m z pr zy mi zy g092log p x p y p x p y f y p xy p xy f xy p xy p xy f xy m ig13xx jg13yy f ij g09 ij 2 ij f xy g09 xy xy 1g09 xy n f xy g09 xy f xy 1g09f xy n table 1 probabilistic approaches method formula frequency guiliano 1964 f xy pointwise mutual information mi fano 1961 church and hanks 1990 log p pp 2xy xy selectional association resnik 1996 symmetric conditional probability ferreira and pereira 1999 p pp xy x y 2 dice formula dice 1945 2 f f f xy x y loglikelihood dunning 1993 daille 1996 ,0,1,0
since we need knowledgepoor daille 1996 induction we cannot use humansuggested filtering chisquared g24 2 church and gale 1991 zscore smadja 1993 fontenelle et al 1994 students tscore church and hanks 1990 ngram list in accordance to each probabilistic algorithm ,0,1,0
a11a29a9 thea13 thea15 a1a4a3a6a5 a11a29a9 thea13 thea15 a11a29a9 thea15 a11a29a9 thea15a1a0 a2 since a11a2a9 thea13 thea15a4a3 a11a29a9 thea15 a11a29a9 thea15 also note that in the case of phraseness of a bigram the equation looks similar to pointwise mutual information church and hanks 1990 but they are different ,0,1,0
after building the chunker students were asked to 4 choose a verb and then analyze verbargument structure they were provided with two relevant papers church and hanks 1990 chklovski and pantel 2004 ,0,1,0
pmi church and hanks 1990 between two phrases is de ned as log2 probph1 is near ph2probph 1 probph2 pmi is positive when two phrases tend to cooccur and negative when they tend to be in a complementary distribution ,0,1,0
such studies follow the empiricist approach to word meaning summarized best in the famous dictum of the british 3 linguist jr firth you shall know a word by the company it keeps firth 1957 p 11 context similarity has been used as a means of extracting collocations from corpora eg by church hanks 1990 and by dunning 1993 of identifying word senses eg by yarowski 1995 and by schutze 1998 of clustering verb classes eg by schulte im walde 2003 and of inducing selectional restrictions of verbs eg by resnik 1993 by abe li 1996 by rooth et al ,0,1,0
usually in 1 in our experiments we set negative pmi values to 0 because church and hanks 1990 in their seminal paper on word association ratio show that negative pmi values are not expected to be accurate unless cooccurrence counts are made from an extremely large corpus ,1,0,0
to compute the degree of interaction between two proteins d4 bd and d4 be we use the informationtheoretic measure of pointwise mutual information church and hanks 1990 manning and schutze 1999 which is computed based on the following quantities 1 ,0,1,0
pointwise mutual information fano 1961 was used to measure strength of selection restrictions for instance by church and hanks 1990 ,0,1,0
following church hanks 1990 rapp 2004 and wettler et al ,0,1,0
in our approach we take into account both the relative positions of the nearby context words as well as the mutual information church hanks 1990 associated with the occurrence of a particular context word ,0,1,0
we used th3 following a very rough rule of thumb used for wordbased mutual information in church and hanks 1990 ,0,1,0
previous research in automatic acquisition focuses primarily on the use of statistical techniques such as bilingual alignment church and hanks 1990 klavans and tzoukermann 1996 wu and xia 1995 or extraction of syntactic constructions from online dictionaries and corpora brent 1993 dorr garman and weinberg 1995 ,0,1,0
s e the window to consider when extracting words related to word w should span from postttuon w5 to w5 maarek also defines the resolwng power of a parr m a document d as p pd log pc where pd is the observed probabshty of appearance of the pan m document d pc the observed probabdny of the pmr recorpus and log pc the quantity of mformauon assocmted to the pmr it is easdy seen that p wall be hgher the higher the frequency of the pmr m the document and the lower sts frequency m the corpus which agrees wlth the sdea presented at the begmnmg of this sectton church and hanks 1990 propose the apphcatlon of the concept of mutual mformatton exy xy hog2 ecxey 51 to the retrieval ro a corpus of pairs of lextcally related words they alsoconslder a word span of e5 words and observe that roterestrog pmr s generally present a mutual mformatxon above 3 salton andallan 1995 focas on paragraph level each paragraph is represented by a weighed vector where each element is a term typically ,0,1,0
robust mforrmatlon extractlon and readllyavmlable onhne nlp resources these techtuques and resources allow us to create a richer indexed source of imgmstlc and domain knowledge than other frequency approaches our approach attempts to apprommate text dlscourse structure through these multlple layers of mformatlon ohtinned from automated methods m contrast to laborlntenslve discoursebased approaches moreover our planned training methodology will also allow us to explmt thin productlve infrastructure m ways whlch model human performance whde avoidmg handcrafting domaindependent rules of the knowledgebased approaches our ultlmate goal m to make our summarlzatlon system scalable and portable by learning summarization rules from easily extractable text features 2 system description our summarization system dlmsum consmts of the summarization server and the summarlzatzon chent the server extracts features the feature extractor from a document using various robust nlp techmques described in sectzon 2 1 and combines these features the feature combiner to basehne multiple combinations of features as described m section 2 2 our work m progress to automattcally tram the feature combiner based upon user and apphcatlon needs m presented in section 2 2 2 the javabased chent which wdl be dmcnssed in section 4 provides a graphical user interface gui for the end user to cnstomlze the summamzatlon preferences and see multiple views of generated suminarles 21 extracting stlmmarization features in this section we describe how we apply robust nlp technology to extract summarization features our goal is to add more mtelhgence to frequencybased approaches to acqmre domain knowledge in a more automated fashion and to apprommate text structure by recogmzing sources of dmcourse cohesion and coherence 211 going beyond a word frequencybased summarization systems typically use a single word stnng as a umt for counting frequencies whde such a method is very robust it ignores the semantic content of words and their potential membership m multiword phrases for example zt does not dmtmgumh between bill m bdl table 1 collocations with chlps potato tortdla corn chocolate bgle chips computer pentmm intel macroprocessor memory chips wood oak plastlc cchlps bsrgmmng clups blue clups mr chips clmton and bill in reform bill this may introduce noise m frequency counting as the same strmgs are treated umformly no matter how the context may have dmamblguated the sense or regardless of membership in multlword phrases for dlrnsum we use term frequency based on tfidf salton and mcgdl 1983 brandow mitze and rau 1995 to derive ssgnature words as one of the summarization features if single words were the sole basra of countmg for our summarization application nome would be introduced both m term frequency and reverse document frequency however recent advances in statmtlcal nlp and information extraction make it possible to utilize features which go beyond the single word level our approach is to extract multiword phrases automatlcally with high accuracy and use them as the basic unit in the summarization process including frequency calculation ftrst just as word association methods have proven effective m lemcal analysis e g church and hanks 1990 we are exploring whether frequently occurring collocatlonal reformation can improve on simple wordbased approaches we have preprocessed about 800 mb of la tlmeswastnngton post newspaper articles nsmg a pos tagger bnll 1993 and derived twoword noun collocations using mutual information the ,1,0,0
lxy log pxy exey mi has been used to identify a variety of interesting linguistic phenomena ranging from semantic relations of the doctornurse type to lexicosyntactic cooccurrence preferences of the savefrom type church and hanks 1990 ,0,1,0
this is due to the reason that telugu entropy15625 bits per character bharati et al 1998 is comparitively a high entropy language than english brown and pietra 1992 ,0,1,0
as a result the empirical approach has been adopted by almost all contemporary partofspeech programs bahl and mercer 1976 leech garside and atwell 1983 jelinek 1985 deroualt and merialdo 1986 garside leech and sampson 1987 church 1988 derose 1988 hindle 1989 kupiec 1989 1992 ayuso et al ,0,1,0
model bits character ascii huffman code each char lempelziv unix tm compress unigram huffman code each word trigram human performance 8 5 443 21 brown personal communication 176 brown et al 1992 125 shannon 1951 the cross entropy h of a code and a source is given by hsource code prs h i source log 2 prs i h code s h where prs h i source is the joint probability of a symbol s following a history h given the source ,0,1,0
illustrative clusterings of this type can also be found in pereira tishby and lee 1993 brown della pietra mercer della pietra and lai 1992 kneser and ney 1993 and brill et al ,0,1,0
introduction many applications that process natural language can be enhanced by incorporating information about the probabilities of word strings that is by using statistical language model information church et al 1991 church and mercer 1993 gale church and yarowsky 1992 liddy and paik 1992 ,0,1,0
in order to estimate the entropy of english brown et al 1992 approximated pkiunk by a poisson distribution whose parameter is the average word length a in the training corpus and pcz cklk unk by the product of character zerogram probabilities ,0,1,0
it is sometimes assumed that estimates of entropy eg shannons estimate that english is 75 redundant brown et als 1992 upper bound of 175 bits per character for printed english are directly 3there are some cases where words are deliberately misspelled in order to get better output from the synthesizer such as coyote spelled kiote ,0,1,0
farach et al 1995 wyner in press describe a novel algorithm for entropy estimation for which they claim very fast convergence time using no more than about five pages of text they can achieve nearly the same accuracy as brown et al 1992 ,0,1,0
methods that use bigrams brown et al 1992 or trigrams martin et al 1998 cluster words considering as a words context the one or two immediately adjacent words and employ as clustering criteria the minimal loss of average 836 nmtual information and the perplexity improvement respectively ,0,1,0
a large corpus is vahmble as a source of such nouns church and hanks 1990 brown et al 1992 ,0,1,0
brown et al 1992 put forward and discussed ngram models based on classes of words ,0,1,0
among all the language modeling approaches ngram models have been most widely used in speech recognition jelinek 1990 gale and church 1990 brown et al 1992 yang et al 1996 and other applications ,0,1,0
similaritybased smoothing brown et al 1992 dagan et al 1999 is an intuitively appealing approach to this problem where probabilities of unseen cooccurrences are estimated from probabilities of seen cooccurrences of distributionally similar events ,1,0,0
language models such as ngram class models brown et al 1992 and ergodic hidden markov models kuhn el al 1994 were proposed and used in applications such as syntactic class pos tagging for english cutting et al 1992 clustering and scoring of recognizer sentence hypotheses ,0,1,0
brown et al proposed a classbased ngram model which generalizes the ngram model to predict a word from previous words in a text brown et al 1992 ,0,1,0
of particular relevance are classbased language models eg saul and pereira 1997 brown et al 1992 ,0,1,0
this method was shown to outperform the class based model proposed in brown et al 1992 and can thus be expected to discover better clusters of words ,0,0,1
first hierarchical word clusters are derived from unlabeled data using the brown et al clustering algorithm brown et al 1992 ,0,1,0
many methods exist for clustering eg brown et al 1990 cutting et al 1992 pereira et al 1993 karypis et al 1999 ,0,1,0
introduction there has been considerable recent interest in the use of statistical methods for grouping words in large online corpora into categories which capture some of our intuitions about the reference of the words we use and the relationships between them eg brown et al 1992 schiitze 1993 ,0,1,0
decades like ngram backoff word models katz 1987 class models brown et al 1992 structured language models chelba and jelinek 2000 or maximum entropy language models rosenfeld 1996 ,0,1,0
more specifically we use a classbased bigram model from brown et al 1992 11 iiiiii ccpcwpwwp 3 in equation 3 c i is the class of the word w i which could be a syntactic class or a semantic class ,0,1,0
this is in contrast to purely statistical systems eg brown et al 1992 which are difficult to inspect and modify ,0,0,1
there has been considerable use in the nlp community of both wordnet eg lehman et al 1992 resnik 1992 and ldoce eg liddy et al 1992 wilks et al 1990 but no one has merged the two in order to combine their strengths ,0,1,0
classes can be induced directly from the corpus using distributional clustering pereira tishby and lee 1993 brown et al 1992 lee and pereira 1999 or taken from a manually crafted taxonomy resnik 1993 ,0,1,0
for this purpose we present a datadriven beam search algorithm similar to the one used in speech recognition search algorithms ney et al 1992 ,0,1,0
52 pseudodisambiguation task pseudodisambiguation tasks have become a standard evaluation technique gale church and yarowsky 1992 sch utze 1992 pereira tishby and lee 1993 sch utze 1998 lee 1999 dagan lee and pereira 1999 golding and roth 1999 rooth et al 1999 evenzohar and roth 2000 lee 2001 clark and weir 2002 and in the current setting we may use a nouns neighbors to decide which of two cooccurrences is the most likely ,0,1,0
in addition explicitly using the left context symbols allows easy use of smoothing techniques such as deleted interpolation bahl jelinek and mercer 1983 clustering techniques brown et al 1992 and model refinement techniques lin chiang and su 1994 to estimate the probabilities more reliably by changing the window sizes of the context and weighting the various estimates dynamically ,0,1,0
this scoring function has been successfully applied to resolve ambiguity problems in an englishtochinese machine translation system behaviortran chen et al 1991 and a spoken language processing system su chiang and lin 1991 1992 ,0,1,0
illustrative clusterings of this type can also be found in pereira tishby and lee 1993 brown della pietra mercer della pietra and lai 1992 kneser and ney 1993 and brill et al ,0,1,0
much research has been carried out recently in this area hughes and atwell 1994 finch and chater 1994 redington chater and finch 1993 brill et al 1990 kiss 1973 pereira and tishby 1992 resnik 1993 ney essen and kneser 1994 matsukawa 1993 ,0,1,0
notice that most incontext and dictionary translations of source words are bounded within the same category in a typical thesaurus such as the lloce mcarthur 1992 and cilin mei et al 1993 ,0,1,0
it is potentially useful in other natural language processing tasks such as the problem of estimating ngram models brown et al 1992 or the problem of semantic tagging cucchiarelli and velardi 1997 ,0,1,0
for handling word identities one could follow the approach used for handling the pos tags eg black et al 1992 magerman 1994 and view the pos tags and word identities as two separate sources of information ,0,1,0
in information retrieval word similarity can be used to identify terms for pseudorelevance feedback harman 1992 buckley et al 1995 xu and croft 2000 vechtomova and robertson 2000 ,0,1,0
a re nement of this model is the classbased ngram where the words are partitioned into equivalence classes brown et al 1992 ,0,1,0
in addition we developed a word clustering procedure based on a standard approach brown et al 1992 that optimizes conditional word clusters ,0,1,0
table 2 three types of classbased mslms on switchboardi swbd and icsi meeting mr corpora of swbd mr classes brown mmi mcmi brown mmi mcmi 100 689 03 684 03 682 03 789 30 773 28 768 28 500 689 03 683 03 679 03 787 31 771 28 767 28 1000 689 03 682 03 679 03 790 31 772 27 769 28 1500 690 03 682 03 680 03 796 31 774 27 774 27 2000 690 03 683 03 680 03 801 31 776 27 779 27 jv j 685 03 783 27 table 3 classbased mslm on switchboard eval2003 size 100 500 1000 1500 2000 jv j 3gram 4gram ppl 658 655 656 657 661 679 721 763 reduction 86 89 88 87 83 58 0 58 classbased language models brown et al 1992 whittaker and woodland 2003 yield great bene ts when data sparseness abounds ,0,1,0
srilm stolcke 2002 can produce classes to maximize the mutual information between the classes icwtcwt 1 as described in brown et al 1992 ,0,1,0
to compare different clustering algorithms results with the standard method of brown et al 1992 srilms ngramclass are also reported ,0,1,0
we consider three class models models s m and l defined as pscjc1cj1w1wj1pngcjcj2cj1 pswjc1cjw1wj1pngwjcj pmcjc1cj1w1wj1pngcjcj2cj1wj2wj1 pmwjc1cjw1wj1pngwjwj2wj1cj plcjc1cj1w1wj1pngcjwj2cj2wj1cj1 plwjc1cjw1wj1pngwjwj2cj2wj1cj1cj model s is an exponential version of the classbased ngram model from brown et al 1992 model m is a novel model introduced in chen 2009 and model l is an exponential version of the model indexpredict from goodman 2001 ,0,1,0
42 models with prior distributions minimum discrimination information models della pietra et al 1992 are exponential models with a prior distribution qyx pyx qyxexp summationtextf i1 ifixy zx 14 the central issue in performance prediction for mdi models is whether qyx needs to be accounted for ,0,1,0
we compare the following model types conventional ie nonexponential word ngram models conventional ibm class ngram models interpolated with conventional word ngram models brown et al 1992 and model m all conventional ngram models are smoothed with modified kneserney smoothing chen and goodman 1998 except we also evaluate word ngram models with katz smoothing katz 1987 ,0,1,0
while we can only compare class models with word models on the largest training set for this training set model m outperforms the baseline katzsmoothed word trigram model by 19 absolute6 4 domain adaptation in this section we introduce another heuristic for improving exponential models and show how this heuristic can be used to motivate a regularized version of minimum discrimination information mdi models della pietra et al 1992 ,0,1,0
classes can be induced directly from the corpus pereira et al 1993 brown et al 1992 or taken from a manually crafted taxonomy resnik 1993 ,0,1,0
and we consider that word pairs that have a small distance between vectors also have similar word neighboring characteristics brown et al 1992 bai et al 1998 ,0,1,0
words are encoded through an automatic clustering algorithm brown et al 1992 while tags labels and extensions are normally encoded using diagonal bits ,0,1,0
recent research yamamoto et al 2001 shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models which use the same clusters for both words brown et al 1992 ,0,1,0
for example we would like to know that if a jj jj 7we also tried using word clusters brown et al 1992 instead of pos but found that pos was more helpful ,0,0,1
in classbased ngram modeling brown et al 1992 for example classbased ngrams are used to determine the probability of occurrence of a pos class given its preceding classes and the probability of a particular word given its own pos class ,0,1,0
distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired schutze 1995 clark 2000 finch et al 1995 probabilistic models have been used to find classes that can improve smoothing and reduce perplexity brown et al 1992 saul and pereira 1997 ,0,1,0
to group the letters into classes we employ a hierarchical clustering algorithm brown et al 1992 ,0,1,0
patternbased approaches are known for their high accuracy in recognizing instances of relations if the patterns are carefully chosen either manually berland and charniak 1999 kozareva et al 2008 or via automatic bootstrapping hearst 1992 widdows and dorow 2002 girju et al 2003 ,0,1,0
class based models brown et al pereira et al 1993 hirschman 1986 resnik 1992 distinguish between unobserved cooccurrences using classes of similar words ,0,1,0
one other published model for grouping semantically related words brown et al 1992 is based on a statistical model of bigrams and trigrams and produces word groups using no linguistic knowledge but no evaluation of the results is reported ,0,1,0
brown et al 1992 where the same idea of improving generalization and accuracy by looking at word classes instead of individual words is used ,0,1,0
1 introduction previous corpusbased sense disambiguation methods require substantial amounts of sensetagged training data kelly and stone 1975 black 1988 and hearst 1991 or aligned bilingual corpora brown et al 1991 dagan 1991 and gale et al 1992 ,0,1,0
classbased methods brown et al 1992 pereira tishby and lee 1993 resnik 1992 cluster words into classes of similar words so that one can base the estimate of a word pairs probability on the averaged cooccurrence probability of the classes to which the two words belong ,0,1,0
since there is no wellagreed to definition of what an utterance is we instead focus on intonational phrases silverman et al 1992 which end with an acoustically signaled boundary lone ,0,1,0
an exception is the use of similarity for alleviating the sparse data problem in language modeling essen steinbiss 1992 brown et al 1992 dagan et al 1994 ,0,1,0
clustering can be done statistically by analyzing text corpora wilks et al 1989 brown et al 1992 pereira et al 1995 and usually results in a set of words or word senses ,0,1,0
in some cases class or part of speech ngrams are used instead of word ngramsbrown et al 1992 chang and chen 1996 ,0,1,0
there have been a number of methods proposed in the literature to address the word clustering problem eg brown et al 1992 pereira et al 1993 li and abe 1996 ,0,1,0
our method is a natural extension of those proposed in brown et al 1992 and li and abe 1996 and overcomes their drawbacks while retaining their advantages ,0,0,1
the mutual information clustering algorithmbrown et al 1992 were used for this ,0,1,0
brown et al 1992 ,0,1,0
these tasks include collocation discovery pearce 2001 smoothing and model estimation brown et al 1992 clark and weir 2001 and text classi cation baker and mccallum 1998 ,0,1,0
proceedings of the conference on empirical methods in natural 2 automatic thesaurus extraction the development of large thesauri and semantic resources such as wordnet fellbaum 1998 has allowed lexical semantic information to be leveraged to solve nlp tasks including collocation discovery pearce 2001 model estimation brown et al 1992 clark and weir 2001 and text classi cation baker and mccallum 1998 ,0,1,0
the clusters were found automatically by attempting to minimize perplexity brown et al 1992 ,0,1,0
brown et al 1992 is one of the first works to use statistical methods of distributional analysis to induce clusters of words ,0,1,0
while we have shown an increase in performance over a purely syntactic baseline model the algorithm of brown et al 1992 there are a number of avenues to pursue in extending this work ,0,0,1
the observation probabilities for a given state representing a certain word class are determined by the relative frequencies of words belonging to that class as determined by the algorithm of brown et al 1992 the probabilities of other words are set to a small initial value ,0,1,0
since these morphological generalizations are based on the initial categorization provided by the algorithm of brown et al 1992 we hope that they will foster speedy convergence of hnn training ,0,1,0
for example the classbased language model of brown et al 1992 is defined as pw2w1 pw2c2pc2c1 1 this helps solve the sparse data problem since the number of classes is usually much smaller than the number of words ,1,0,0
there are many choices for modeling cooccurrence data brown et al 1992 pereira et al 1993 blei et al 2003 ,0,1,0
this merging of contexts is different than clustering words eg clark 2000 brown et al 1992 but is applicable as word clustering relies on knowing which contexts identify the same category ,0,1,0
the technique is based on word class models pioneered by brown et al 1992 which hierarchically 151 conll03 conll03 muc7 muc7 web component test data dev data dev test pages 1 baseline 8365 8925 7472 7128 7141 2 1 gazetteer match 8722 9161 8583 8043 7446 3 1 word class model 8682 9085 8025 7988 7226 4 all external knowledge 8855 9249 8450 8323 7444 table 4 utility of external knowledge ,0,1,0
brown et al 1992 peter f brown vincent j della pietra petere v desouza jenifer c lai and robert l mercer ,0,1,0
other researchers have also reported similar problems of excessive resource demands with the collect all neighbors model gale et al 1992 ,0,1,0
it has been used for diverse problems such as machine translation and sense disambiguation gale et al 1992 schiltze 1992 ,0,1,0
31 distributionally derived groupings distributional cluster brown et al 1992 head body hands eye voice arm seat hair mouth word head 17 alternatives 00000 crown peak summit head top subconceptofupperbound 00000 principal school principal head teacher head educator who has executive authority 00000 head chief top dog subeoncept of leader 00000 head a user of usually soft drugs 01983 head the head of the page the head of the fist 01983 beginning head origin root source the point or place where something begins 00000 pass head straits a difficult juncture a pretty pass 00000 headway head subconcept of progress progression advance 00903 point hod a vshaped mark at one end of an arrow pointer 00000 heading head a line of text serving to indicate what the passage below it is about 00000 mind head intellect psyche that which is responsible for your thoughts and feelings 05428 head the upper or front part of the body that contains the faee and brains 00000 toilet lavatory can head facility john privy bathroom 00000 head the striking part of a tool hammerhead 01685 head a part that projects out from the rest the head of the nail pinhead 00000 drumhead head stretched taut 00000 oral sex head oralgenital stimulation word body 8 alternatives 00000 body an individual 3dimensional object that has mass 00000 gathering assemblage assembly body confluence group of people together in one place 00000 body people associated by some common tie or occupation 00000 body the centralmessage of a communication 09178 torso trunk body subconcept of body part member 00000 body organic structure the entire physical structure of an animal or human being 60 00822 consistency consistence body subeoncept of property 00000 fuselage body the central portion of an airplane word hands 00000 00653 00653 00000 00000 00000 02151 07196 00000 00000 10 alternatives hand subconeept of linear unit hired hand hand hired man a hired laborer on a farm or ranch bridge player hand we need a 4th hand for bridge hand deal the cards held in a card game by a given player at any given time hand a round of applause to signify approval give the little lady a great big hand handwriting cursive hand script something written by hand hand ability he wanted to try his hand at singing hand manus hook mauler mitt paw the distal extremity of the superior limb hand subconcept of pointer hand physical assistance give me a hand with the chores word eye 4 alternatives 01479 center centre middle heart eye approximately central within some region 01547 eye good judgment she has an eye for fresh talent 06432 eye eyeball oculus optic peeper organ of sight 00542 eye a sanall hole or loop as in a needle word voice 7 alternatives 00000 01414 01122 02029 03895 00000 01539 voice the relation of the subject of a verb to the action that the verb denotes spokesperson spokesman interpreter representative mouthpiece voice voice vocalization the sound made by the vibration of vocal folds articulation voice expressing in coherent verbal form i gave voice to my feelings part voice the melody carried by a particular voice or instrument in polyphonic music voice the ability to speak he lost his voice voice the distinctive sound of a persons speech i recognized her voice word arm 6 alternatives 00000 branch subdivision arm an administrative division a branch of congress 06131 arm eornrnonly used to refer to the whole superior limb 00346 weapon arm weapon system used in fighting or hunting 02265 sleeve arm attached at armhole 01950 arm any projtion that is thought to resemble an arm the arm of the record player 00346 arm the part of an armchair that supports the elbow and forearm of a seated person word seat 6 alternatives 00000 seat a city from which authority is exercised 00000 seat place a space reserved for sitting 07369 buttocks arse butt backside burn buns can 02631 seat covers the buttocks 00402 seat designed for sitting on 00402 seat where one sits word hair 5 00323 02313 10000 10000 10000 alternatives hair pilus threadlike keratinous filaments growing from the skin of mammals hair tomentum filamentous hairlike growth on a plant hair follicular growth subeoncept of externalbody part hair mane head of hair hair on the head hair hairy covering of an animal or body part word mouth 5 alternatives 00000 mouth the point where a stream issues into a larger body of water 00000 mouth an opening that resembles a mouth as of a cave or a gorge 00613 sass sassing baektalk lip mouth an impudent or insolent rejoinder 09387 mouth oral cavity subconcept of cavity body cavity bodily cavity 09387 mouth trap hole maw yap muzzle suout list includes informal terms for mouth this group was among classes handselected by brown et al as particularly interesting ,0,1,0
61 distributional cluster brown et al 1992 tie jacket suit word tie 7 alternatives 00000 00000 00000 10000 00000 00000 00000 draw standoff tie stalemate affiliation association tie tieup a social or business relationship tie crosstie sleeper subconcept of brace bracing necktie tie link linkup tie tiein something that serves to join or link drawstring string tie cord used as a fastener tie tie beam used to prevent two rafters eg from spreading apart word jacket 4 alternatives 00000 book jacket dust cover subeoncept of promotional material 00000 jacket crown jacket artificial crown fitted over a broken or decayed tooth 00000 jacket subconceptofwrapping wrap wrapper 10000 jacket a short coat word suit 4 alternatives 00000 suit suing subconcept of entreaty prayer appeal 10000 suit suit of clothes subconcept of garment 00000 suit any of four sets of13 cards in a paek 00000 legal action action case lawsuit suit a judicial proceeding this cluster was derived by brown et al using a modification of their algorithm designed to uncover semantically sticky clusters ,0,1,0
distributional cluster brown et al 1992 cost expense risk profitability deferral earmarks capstone cardinality mintage reseller word cost 2 alternatives 05426 cost price terms damage the amount of money paid for something 04574 monetary value price cost the amount of money it would bring if sold word expense 2 alternatives 10000 expense expenditure outlay outgo spending disbursal disbursement 00000 expense a detriment or sacrifice at the expense of word risk 2 alternatives 06267 hazard jeopardy peril risk subconeept of danger 03733 risk peril danger subeonceptofventure word profitability 1 alternatives 10000 profitableness profitability subconcept of advantage benefit usefulness word deferral 3 alternatives 06267 abeyance deferral recess subconcept of inaction inactivity inactiveness 03733 postponement deferment deferral moratorium an agreed suspension of activity 03733 deferral subconeeptofpause wait word earmarks 2 alternatives 02898 earmark identification mark on the ear of a domestic animal 07102 hallmak trademark earmark a distinguishing characteristic or attribute word capstone 1 alternatives 10000 capstone coping stone stretcher used at top of wall word eardinality not in wordnet word mintage 1 alternatives 62 10000 coinage mintage specie metal money subconcept of cash word reseller not in wordnet this cluster was one presented by brown et al as a randomlyselected class rather than one handpicked for its coherence ,0,1,0
bensch and savitch 1992 brill 1991 brown et al 1992 grefenstette 1994 mckcown and hatzivassiloglou 1993 pereira et al 1993 schtltze 1993 ,0,1,0
2 hierarchical clustering of words several algorithms have been proposed for automatically clustering words based on a large corpus jardino and adda 91 brown et al 1992 kneser and ney 1993 martin et al 1995 ueberla 1995 ,0,1,0
the reader is referred to ushioda 1996 and brown et al 1992 for details of mi clustering but we will first briefly summarize the mi clustering and then describe our hierarchical clustering algorithm ,0,1,0
however the aforementioned sdt techniques require word classesbrown et al 1992 to help prevent data fragmentation and a sophisticated smoothing algorithm to mitigate the effects of any fragmentation that occurs ,0,1,0
in all other respects our work departs from previous research on broadcoverage 16 i t i i i i i i i i i i i i i i i i i i i i i 1 i i i i i i i 1 i i i i probabilistic parsing which either attempts to learn to predict grrarntical structure of test data directly from a training treebank brill 1993 collins 1996 eisner 1996 jelinek et al 1994 magerman 1995 skine and orishman 1995 sharman et al 1990 or employs a grammar and sometimes a dictionary to capture linguistic expertise directly black et al 1993a grinberg et al 1995 schabes 1992 but arguably at a less detailed and informative level than in the research reported here ,0,1,0
in contrast approaches to wsd attempt to take advantage of many different sources of information eg see mcroy 1992 ng and lee 1996 bruce and wiebe 1994 it seems possible to obtain benefit from sources ranging from local collocational clues yarowsky 1993 to membership in semantically or topically related word classes arowsky 1992 resnik 1993 to consistency of word usages within a discourse gale et al 1992 and disambignation seems highly lexically sensitive in effect requiring specialized disamhignators for each polysemous word ,0,1,0
their weights are calculated by deleted interpolation brown et al 1992 ,0,1,0
in section 2 we examine aggregate markov models or classbased bigram models brown et al 1992 in which the mapping from words to classes 81 is probabilistic ,0,1,0
our approach differs in important ways from the use of hidden markov models hmms for classbased language modeling jelinek et al 1992 ,0,1,0
in practice texts contain an enormous number of word sequences brown et al 1992 only a tiny fraction of which are nccs and it takes considerable computational effort to induce each translation model ,0,1,0
various methods are based on mutual information between classes see brown et al 1992 mcmahon and smith 1996 kneser and ney 1993 jardino and adda 1993 martin liermann and ney 1995 ueberla 1995 ,0,1,0
another application of hard clustering methods in particular bottomup variants is that they can also produce a binary tree which can be used for decisiontree based systems such as the spatter parser magerman 1995 or the atr decisiontree partofspeech tagger black et al 1992 ushioda 1996 ,0,1,0
brown brown et al 1992 uses the same bigrams and by means of a greedy algorithm forms the hierarchical clusters of words ,0,1,0
while schiitze and pedersen 1993 brown et al 1992 and futrelle and gauch 1993 all demonstrate the ability of their systems to identify word similarity using clustering on the most frequently occurring words in their corpus only grefenstette 1992 demonstrates his system by generating word similarities with respect to a set of target words ,0,1,0
while previous researchers have used agglomerative nesting clustering eg brown et al 1992 futrelle and gauch 1993 comparisons with our work are difficult to draw due to their use of the 1000 commonest words from their respective corpora ,0,1,0
in brown et al 1992 the authors provide some sample subtrees resulting from such a 1000word clustering ,0,1,0
clustering algorithms have been previously shown to work fairly well for the classification of words into syntactic and semantic classes brown et al 1992 but determining the optimum number of classes for a hierarchical cluster tree is an ongoing difficult problem particularly without prior knowledge of the item classification ,0,0,1
the fact that information consisting of nothing more than bigrams can capture syntactic information about english has already been noted by brown et al 1992 ,0,1,0
black et al 1992 magerman 1994 and view the pos tags and word identities as two separate sources of information ,0,1,0
7another related measure is dunning 1993s likelihood ratio tests for binomial and multinomial distributions which are claimed to be effective even with very much smaller volumes of text than is necessary for other tests based on assumed normal distributions ,1,0,0
for example in the context of syntactic disambiguation black 1993 and magerman 1995 proposed statistical parsing models basedon decisiontree learning techniques which incorporated not only syntactic but also lexicalsemantic information in the decisiontrees ,0,1,0
e max fl f e 23 11 y now the problem of learning probabilistic subcategorization preference is stated as for every verbnoun collocation e in c estimating the probability distribution pfl 6resnik 1993 applys the idea of the kl distance to measuring the association of a verb v and its object noun class c our definition of ekt corresponds to an extension of resniks association score which considers dependencies of more than one casemarkers in a subcategorization frame ,0,1,0
the problem of choosing an appropriate level in the hierarchy at which to represent a particular noun sense given a predicate and argument position has been investigated by resnik 1993 li and abe 1998 and llibas 1995 ,0,1,0
for instance mutual information church ct al 1990 and the loglikelihood dunning 1993 methods for extracting word bigrams have been widely used ,1,0,0
mutual infornaation involves a problem in that it is overestimated for lowfrequency terms iunning 1993 ,0,1,0
using the values computed above pl 7tl k2 p2 77 2 kl k2 p 7z 1 it 2 taking these probabilities to be binomially distributed the log likelihood statistic dunning 1993 is given by 2 log a 2log lpt kl rtl log lp2 k2 rl2 log lp kl n2 log lp k2 n2 where log lp n k k logp z klog1 p according to this statistic tile greater the value of 2 log a for a particular pair of observed frame and verb the more likely that frame is to be valid sf of the verb ,0,1,0
5 comparison with related work preliminary work on sf extraction from coqora was done by brent 1991 brunt 1993 brent 1994 and webster and marcus 1989 ushioda et al 1993 ,0,1,0
in the first step the scores are initialized according to the g 2 statistic dunning 1993 ,0,1,0
we describe the experiment in greater detail 2the particular verbs selected were looked up in levin 1993 and the class for each verb in the classification system defined in stevenson and merlo 1997 was selected with some discussion with linguists ,0,1,0
for example in john saw mary yesterday at the station only john and mary are required arguments while the other constituents are optional adjuncts3 the problem of sf identification using statistical methods has had a rich discussion in the literature ushioda et al 1993 manning 1993 briscoe and carroll 1997 brent 1994 also see the refences cited in sarkar and zeman 2000 ,0,1,0
ther background on this method of hypothesis testing the reader is referred to bickel and doksum 1977 dunning 1993 ,0,1,0
using the values computed above p1 k1n 1 p2 k2n 2 p k1k2n 1n2 taking these probabilities to be binomially distributed the log likelihood statistic dunning 1993 is given by 2 log 2log lp1k1n1log lp2k2n2 log lpk1n2 log lpk2n2 where log lpnkk log pn k log1 p according to this statistic the greater the value of 2 log for a particular pair of observed frame and verb the more likely that frame is to be valid sf of the verb ,0,1,0
for instance the mutual information church et al 1990 and loglikelihood ratio dunning 1993 cohen 1995 have been widely used for extracting word bigrams ,1,0,0
the scores were then weighted by the inverse of their height in the tree and then summed together similarly to the procedure in resnik 1993 ,0,1,0
methods 41 experiment 1 held out data to examine the generalizability of classifiers trained on the automatically generated data a c45 decision tree classifier quinlan 1993 was trained and tested on the held out test set described above ,0,1,0
each word i in the context vector of w is then weighted with a measure of its association with w we chose the loglikelihood ratio test dunning 1993 to measure this association the context vectors of the target words are then translated with our general bilingual dictionary leaving the weights unchanged when several translations are proposed by the dictionary we consider all of them with the same weight the similarity of each source word s for each target word t is computed on the basis of the cosine measure the similarities are then normalized to yield a probabilistic translation lexicon pts ,0,1,0
31 the likelihood ratio we adopted a method for collocation discovery based on the likelihood ratio dunning 1993 ,0,1,0
the starting point is the log likelihood ratio log dunning 1993 ,0,1,0
in the ineast system leuski et al 2003 the identification of relevant terms is oriented towards multidocument summarization and they use a likelihood ratio dunning 1993 which favours terms which are representative of the set of documents as opposed to the full collection ,0,1,0
we apply the log likelihood principle dunning 1993 to compute this score ,0,1,0
the candidates were then ranked according to the scores assigned by four association measures the loglikelihood ratio g2 dunning 1993 pearsons chisquared statistic x2 manning and schutze 1999 169172 the tscore statistic t church et al 1991 and mere cooccurrence frequency f4 tps were identified according to the definition of krenn 2000 ,0,1,0
the evaluation results also confirm the argument of dunning 1993 who suggested g2 as a more robust alternative to x2 ,1,0,0
smadja 1993 which is the classic work on collocation extraction uses a twostage filtering model in which in the first step ngram statistics determine possible collocations and in the second step these candidates are submitted to a syntactic valida7of course lexical material is always at least partially dependent on the domain in question ,0,1,0
almost all of these measures can be grouped into one of the following three categories a0 frequencybased measures eg based on absolute and relative cooccurrence frequencies a0 informationtheoretic measures eg mutual information entropy a0 statistical measures eg chisquare ttest loglikelihood dices coefficient the corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties dunning 1993 manning and schutze 1999 and their suitability for the task of collocation extraction see evert and krenn 2001 and krenn and evert 2001 for recent evaluations ,0,1,0
22 the choice of cooccurrence qeasure and matrix distance there c many alternatives to measure cooccurrence between two words x and y church 1990 dunning 1993 ,0,1,0
1 introduction phrasebased systems flat and hierarchical alike koehn et al 2003 koehn 2004b koehn et al 2007 chiang 2005 chiang 2007 have achieved a much better translation coverage than wordbased ones brown et al 1993 but untranslated words remain a major problem in smt ,0,1,0
thus the alignment set is denoted as 1 ialiaia ii we adapt the bilingual word alignment model ibm model 3 brown et al 1993 to monolingual word alignment ,0,1,0
this further supports the claim by dunning 1993 that loglikelihood ratio is much less sensitive than pmi to low counts ,1,0,0
as a result we can use collocation measures like pointwise mutual information church and hanks 1989 or the loglikelihood ratio dunning 1993 to predict the strong association for a given cue ,0,1,0
by default the loglikelihood ratio measure llr is proposed since it was shown to be particularly suited to language data dunning 1993 ,1,0,0
collocation map that is first suggested in itan 1993 is a sigmoid belief network with words as probabilistic variables ,0,1,0
this results also agree with dunnings argument about overestimation on the infrequent occurrences in which many infrequent pairs tend to get higher estimation dunning 1993 ,1,0,0
the problem is due to the assumption of normality in naive frequency based statistics according to dunning 1993 ,0,1,0
proceedings of eacl 99 determinants of adjectivenoun plausibility maria lapata and scott mcdonald and frank keller school of cognitive science division of informatics university of edinburgh 2 buccleuch place edinburgh eh8 9lw uk mlap scottm keller cogsciedacuk abstract this paper explores the determinants of adjectivenoun plausibility by using correlation analysis to compare judgements elicited from human subjects with five corpusbased variables cooccurrence frequency of the adjectivenoun pair noun frequency conditional probability of the noun given the adjective the loglikelihood ratio and resniks 1993 selectional association measure ,0,1,0
we employ the loglikelihood ratio as a measure of the collocational status of the adjectivenoun pair dunning 1993 daille 1996 ,0,1,0
we estimated the probabilities pc i pi and pc similarly to resnik 1993 by using relative frequencies from the bnc together with wordnet miller et al 1990 as a source of taxonomic semantic class information ,0,1,0
the second attempts to instill knowledge of collocations in the data we use the technique described by dunning 1993 to compute multiword expressions and then mark words that are commonly used as such with a feature that expresses this fact ,0,1,0
1 introduction word associations cooccurrences have a wide range of applications including speech recognition optical character recognition and information retrieval ir church and hanks 1991 dunning 1993 manning and schutze 1999 ,0,1,0
many studies focus on rare words dunning 1993 moore 2004 butterflies are more interesting than moths ,0,1,0
5httpclcsokayamauacjprsc jacabit a4a6a5 which gathers the set of cooccurrence units a7 associated with the number of times that a7 and a2 occur together a8a6a9a10a9 a5 a11 in order to identify speci c words in the lexical context and to reduce wordfrequency effects we normalize context vectors using an association score such as mutual information fano 1961 or loglikelihood dunning 1993 ,0,1,0
4 pattern switching the compositional translation presents problems which have been reported by baldwin and tanaka 2004 brown et al 1993 fertility swts and mwts are not translated by a term of a same length ,0,1,0
many methods have been proposed to measure the cooccurrence relation between two words such as 2 church and mercer1993 mutual information church and hanks 1989 pantel and lin 2002 ttest church and hanks 1989 and loglikelihood dunning1993 ,0,1,0
before training the classifiers we perform feature ablation by imposing a count cutoff of 10 and by limiting the number of features to the top 75k features in terms of log likelihood ratio dunning 1993 ,0,1,0
choueka 1988 regarded mwe as connected collocations a sequence of neighboring words whose exact meaning cannot be derived from the meaning or connotation of its components which means that mwes also have low st as some pioneers provide mwe identiflcation methods which are based on association metrics am such as likelihood ratio dunning 1993 ,0,1,0
a boundarybased model of cooccurrence assumes that both halves of the bitext have been segmented into s segments so that segment ui in one half of the bitext and segment vi in the other half are mutual translations 1 i s under the boundarybased model of cooccurrence there are several ways to compute cooccurrence counts coocu v between word types u and v in the models of brown della pietra della pietra and mercer 1993 reviewed in section 43 s coocr v eiu jv 12 i1 where ei and j5 are the unigram frequencies of u and v respectively in each aligned text segment i for most translation models this method produces suboptimal results however when eiu 1 and v 1 ,0,1,0
due to the parameter interdependencies introduced by the onetoone assumption we are unlikely to find a method for decomposing the assignments into parameters that can be estimated independently of each other as in brown et al 1993b equation 26 ,0,1,0
in informal experiments described elsewhere melamed 1995 i found that the g 2 statistic suggested by dunning 1993 slightly outperforms 2 ,1,0,0
bilingual lexicographers can work with bilingual concordancing software that can point them to instances of any link type induced from a bitext and display these instances sorted by their contexts eg simard foster and perrault 1993 ,0,1,0
the performance of crosslanguage information retrieval with a uniform t is likely to be limited in the same way as the performance of conventional information retrieval without termfrequency information ie where the system knows which terms occur in which documents but not how often buckley 1993 ,0,1,0
1993b this model is symmetric because both word bags are generated together from a joint probability distribution ,0,1,0
dunning 1993 has called attention to the loglikelihood ratio g 2 as appropriate for the analysis of such contingency tables especially when such contingency tables concern very low frequency words ,0,1,0
dunning 1993 argues for the use of g 2 rather than x 2 based on an analysis of the sampling distributions of g 2 and x 2 and results obtained when using the statistics to acquire highly associated bigrams ,0,1,0
dunning 1993 argues for the use of g 2 rather than x 2 based on the claim that the sampling distribution of g 2 approaches the true chisquare distribution quicker than the sampling distribution of x 2 however agresti 1996 page 34 makes the opposite claim the sampling distributions of x 2 and g 2 get closer to chisquared as the sample size n increasesthe convergence is quicker for x 2 than g 2 in addition pedersen 2001 questions whether one statistic should be preferred over the other for the bigram acquisition task and cites cressie and read 1984 who argue that there are some cases where the pearson statistic is more reliable than the loglikelihood statistic ,0,1,0
alternative classbased estimation methods the approaches used for comparison are that of resnik 1993 1998 subsequently developed by ribas 1995 and that of li and abe 1998 which has been adopted by mccarthy 2000 ,0,1,0
the x 2 statistic is performing at least as well as g 2 and the results show that the average level of generalization is slightly higher for g 2 than x 2 this suggests a possible explanation for the results presented here and those in dunning 1993 that the x 2 statistic provides a less conservative test when counts in the contingency table are low ,0,1,0
the example paper we use throughout the article is f pereira n tishby and l lees distributional clustering of english words acl1993 cmp lg9408011 it was chosen because it is the paper most often cited within our collection ,0,1,0
we measured associations using the loglikelihood measure dunning 1993 for each combination of target category and semantic class by converting each cell of the contingency into a 22 contingency table ,0,1,0
their systems output was an ordered list of possible parts according to some statistical metrics eg the loglikelihood metric dunning 1993 ,0,1,0
5 the semcor collection miller et al 1993 is a subset of the brown corpus and consists of 352 news articles distributed into three sets in which the nouns verbs adverbs and adjectives have been manually tagged with their corresponding wordnet senses and partofspeech tags using brills tagger 1995 ,0,1,0
introduction the automated analysis of large corpora has many useful applications church and mercer 1993 ,0,1,0
in the usual case considered by dunning 1993 and discussed by manning and sch utze 1999 the righthand side of the equation is larger than the lefthand side ,0,1,0
for english we have used sections 0306 of the wsj portion of the penn treebank marcus santorini and marcinkiewicz 1993 distributed by the linguistic data consortium ldc which have frequently been used to evaluate sentence boundary detection systems before compare section 7 ,0,1,0
the usefulness of likelihood ratios for collocation detection has been made explicit by dunning 1993 and has been confirmed by an evaluation of various collocation detection methods carried out by evert and krenn 2001 ,0,1,0
in our experiments we follow lowe and mcdonald 2000 in using the wellknown loglikelihood ratio g 2 dunning 1993 ,1,0,0
one could use the estimated cooccurrences from a small sample to compute the test statistics most commonly pearsons chisquared test the likelihood ratio test fishers exact test cosine similarity or resemblance jaccard coefficient dunning 1993 manning and schutze 1999 agresti 2002 moore 2004 ,0,1,0
1 word associations cooccurrences or joint frequencies have a wide range of applications including speech recognition optical character recognition and information retrieval ir salton 1989 church and hanks 1991 dunning 1993 baezayates and ribeironeto 1999 manning and schutze 1999 ,0,1,0
lexical collocation functions especially those determined statistically have recently attracted considerable attention in computational linguistics calzolari and bindi 1990 church and hanks 1990 sekine et al 1992 hindle and rooth 1993 mainly though not exclusively for use in disambiguation ,0,1,0
33 syntax based approach an alternative to the window and documentoriented approach is to use syntactical information grefenstette 1993 ,0,1,0
for comparing the sentence generator sample to the english sample we compute loglikelihood statistics dunning 1993 on neighboring words that at least cooccur twice ,0,1,0
we use the loglikelihood ratio for determining significance as in dunning 1993 but other measures are possible as well ,0,1,0
schtze 1993 is not suited to highly skewed distributions omnipresent in natural language ,0,1,0
the measures2 mutual information a0a2a1 church and hanks 1989 the loglikelihood ratio test dunning 1993 two statistical tests ttest and a3a5a4 test and cooccurrence frequency are applied to two sets of data adjectivenoun adjn pairs and prepositionnounverb pnv triples where the ams are applied to pnv pairs ,0,1,0
the remarks on the a3 a4 measure in dunning 1993 ,0,1,0
substituting the probabilities in the pmi formula with the previously introduced web statistics we obtain a15a17a16a25a18a26a11a22a21 qspa49a6a50a22a51a6a52 aspa24 a15a17a16a25a18a26a11a22a21 qspa24a56a55a57a15a33a16a19a18a26a11a6a21 aspa24 a55 a38 a1a6a39a17a34a40a1a8a41a45a43a46a11 maximal likelihood ratio mlhr is also used for word cooccurrence mining dunning 1993 ,0,1,0
proceedings of the 40th annual meeting of the association for in a key step for locating important sentences neats computes the likelihood ratio dunning 1993 to identify key concepts in unigrams bigrams and trigrams1 using the ontopic document collection as the relevant set and the offtopic document collection as the irrelevant set ,0,1,0
each element of the resulting vector was replaced with its loglikelihood value see definition 10 in section 23 which can be considered as an estimate of how surprising or distinctive a cooccurrence pair is dunning 1993 ,0,1,0
neats computes the likelihood ratio dunning 1993 to identify key concepts in unigrams bigrams and trigrams and clusters these concepts in order to identify major subtopics within the main topic ,0,1,0
in addition to collocation translation there is also some related work in acquiring phrase or term translations from parallel corpus kupiec 1993 yamamoto and matsumoto 2000 ,0,1,0
we have 21 21 trictrictric trictritri erpercpercp ecrcpecp 6 assumption 2 for an english triple tri e assume that i c only depends on 12 i i e and c r only depends on e r equation 6 is rewritten as 2211 21 ec trietrictrictritri rrpecpecp erpercpercpecp 7 notice that 11 ecp and 22 ecp are translation probabilities within triples they are different from the unrestricted probabilities such as the ones in ibm models brown et al 1993 ,0,1,0
to test whether a better set of initial parameter estimates can improve model 1 alignment accuracy we use a heuristic model based on the loglikelihoodratio llr statistic recommended by dunning 1993 ,0,1,0
in order to filter some noise caused by the error alignment links we only retain those translation pairs whose translation probabilities are above a threshold 1 d 1 or cooccurring frequencies are above a threshold 2 when we train the ibm statistical word alignment model with a limited bilingual corpus in the specific domain we build another translation dictionary with the same method as for the dictionary but we adopt a different filtering strategy for the translation dictionary we use loglikelihood ratio to estimate the association strength of each translation pair because dunning 1993 proved that loglikelihood ratio performed very well on smallscale data ,1,0,0
moreover log likelihood ratios are regarded as a more effective method to identify collocations especially when the occurrence count is very low dunning 1993 ,1,0,0
2 statistical word alignment according to the ibm models brown et al 1993 the statistical word alignment model can be generally represented as in equation 1 ,0,1,0
a variety of methods have been applied ranging from simple frequency justeson katz 1995 modified frequency measures such as cvalues frantzi anadiou mima 2000 maynard anadiou 2000 and standard statistical significance tests such as the ttest the chisquared test and loglikelihood church and hanks 1990 dunning 1993 and informationbased methods eg pointwise mutual information church hanks 1990 ,0,1,0
dunning 1993 or else as with mutual information eschew significance testing in favor of a generic informationtheoretic approach ,0,1,0
finally the loglikelihood ratios test henceforth llr dunning 1993 is applied on each set of pairs ,0,1,0
this metric tests the hypothesis that the probability of phrase is the same whether phrase has been seen or not by calculating the likelihood of the observed data under a binomial distribution using probabilities derived using each hypothesis dunning 1993 ,0,1,0
this method uses mutual information and loglikelihood which dunning 1993 used to calculate the dependency value between words ,0,1,0
1 introduction robust statistical syntactic parsers made possible by new statistical techniques collins 1999 charniak 2000 bikel 2004 and by the availability of large handannotated training corpora such as wsj marcus et al 1993 and switchboard godefrey et al 1992 have had a major impact on the field of natural language processing ,0,1,0
such measures as mutual information turney 2001 latent semantic analysis landauer et al 1998 loglikelihood ratio dunning 1993 have been proposed to evaluate word semantic similarity based on the cooccurrence information on a large corpus ,0,1,0
we compute loglikelihood significance between features and target nouns as in dunning 1993 and keep only the most significant 200 features per target word ,0,1,0
it forms a baseline for performance evaluations but is prone to sparse data problems dunning 1993 ,0,1,0
the likelihood ratio is obtained by treating word and ic as a bigram and computed with the formula in dunning 1993 ,0,1,0
dunning 1993 make use of both positive and negative instances of performing a task ,0,1,0
the cooccurrence relation can also be based on distance in a bitext space which is a more general representations of bitext correspondence dagan et al 1993 resnik melamed 1997 or it can be restricted to words pairs that satisfy some matching predicate which can be extrinsic to the model melamed 1995 melamed 1997 ,0,1,0
1 introduction over the past decade researchers at ibm have developed a series of increasingly sophisticated statistical models for machine translation brown et al 1988 brown et al 1990 brown et al 1993a ,0,1,0
probabilities based on relative frequencies or derived flom the measure defined in dunning 1993 for example allow to take this fact into account ,0,1,0
in this work model fit is reported in terms of the likelihood ratio statistic g 2 and its significance read and cressie 1988 dunning 1993 ,0,1,0
it is clear that appendix b contains far fewer true noncompositional phrases than appendix a 7 related work there have been numerous previous research on extracting collocations from corpus eg choueka 1988 and smadja 1993 ,0,1,0
we parsed a 125million word newspaper corpus with minipar 1 a descendent of principar lin 1993 lin 1994 and extracted dependency relationships from the parsed corpus ,0,1,0
levin 1993 assumes that the syntactic realization of a verbs arguments is directly correlated with its meaning cf ,0,1,0
we also experimented with a method suggested by brent 1993 which applies the binomial test on frame frequency data ,0,1,0
for instance the topp frame is poorly represented in the syntactically annotated version of the penn treebank marcus et al 1993 ,0,1,0
the different approaches eg brent 991 1993 ushioda et al 1993 briscoe and carroll 1997 manning 1993 carroll and rooth 1998 gahl 1998 lapata 1999 sarkar and zeman 2000 vary largely according to the methods used and the number of scfs being extracted ,0,1,0
according to one account briscoe and carroll 1997 the majority of errors arise because of the statistical filtering process which is reported to be particularly unreliable for low frequency scfs brent 1993 briscoe and carroll 1997 manning 1993 manning and schiitze 1999 ,0,1,0
brent 1993 estimated the error probabilities for each scf experimentally from the behaviour of his scf extractor which detected simple morphosyntactic cues in the corpus data ,0,1,0
222 the binomial log likelihood ratio as a statistical filter dunning 1993 demonstrates the benefits of the llr statistic compared to pearsons chisquared on the task of ranking bigram data ,1,0,0
we then rankorder the p xy mi xy m z pr zy mi zy g092log p x p y p x p y f y p xy p xy f xy p xy p xy f xy m ig13xx jg13yy f ij g09 ij 2 ij f xy g09 xy xy 1g09 xy n f xy g09 xy f xy 1g09f xy n table 1 probabilistic approaches method formula frequency guiliano 1964 f xy pointwise mutual information mi fano 1961 church and hanks 1990 log p pp 2xy xy selectional association resnik 1996 symmetric conditional probability ferreira and pereira 1999 p pp xy x y 2 dice formula dice 1945 2 f f f xy x y loglikelihood dunning 1993 daille 1996 ,0,1,0
in particular we use a randomlyselected corpus the first five columns as informationlike consisting of a 67 million word subset of the trec similarly since the last four columns share databases darpa 19931997 ,0,1,0
as a measure of association we use the loglikelihoodratio statistic recommended by dunning 1993 which is the same statistic used by melamed to initialize his models ,0,1,0
for each cell in the contingency table the expected counts are mi j nin jn the measures are calculated as pedersen 1996 2 ij ni j mi j 2 mi j ll 2 ij log2 n 2i j mi j loglikelihood ratios dunning 1993 are more appropriate for sparse data than chisquare ,0,1,0
in the latter case we use an unsupervised attachment disambiguation method based on the loglikelihood ratio llr dunning 1993 ,0,1,0
one of the earliest attempts at extracting interrupted collocations ie noncontiguous collocations including vpcs was that of smadja 1993 ,0,1,0
22 corpus occurrence in order to get a feel for the relative frequency of vpcs in the corpus targeted for extraction namely 0 5 10 15 20 25 30 35 40 0 10 20 30 40 50 60 70 vpc types corpus frequency figure 1 frequency distribution of vpcs in the wsj tagger correctextracted prec rec ffl1 brill 135135 1000 0177 0301 penn 667800 0834 0565 0673 table 1 posbased extraction results the wsj section of the penn treebank we took a random sample of 200 vpcs from the alvey natural language tools grammar grover et al 1993 and did a manual corpus search for each ,0,1,0
4 method2 simple chunkbased extraction to overcome the shortcomings of the brill tagger in identifying particles we next look to full chunk 2note this is the same as the maximum span length of 5 used by smadja 1993 and above the maximum attested np length of 3 from our corpus study see section 22 ,0,1,0
likelihood ratios are particularly useful when comparing common and rare events dunning 1993 plaunt and norgard 1998 making them natural here given the rareness of most question categories and the frequency of contributions ,0,1,0
for this present work we use dunnings loglikelihood ratio statistics dunning 1993 defined as follows sim alogablogbclogcdlogd ablogabaclogac bdlogbdcdlogcd abcdlogabcd for each bilingual pattern eijj we compute its similarity score and qualify it as a bilingual sequencetosequence correspondence if no equally strong or stronger association for monolingual constituent is found ,0,1,0
first word frequencies context word frequencies in surrounding positions here threewords window are computed following a statisticsbased metrics the loglikelihood ratio dunning 1993 ,0,1,0
to prune away those pairs we used the loglikelihoodratio algorithm dunning 1993 to compute the degree of association between the verb and the noun in each pair ,0,1,0
a list of pilot terms ranked from the most representative of the corpus to the least thanks to the loglikelihood coefficient introduced by dunning 1993 ,1,0,0
1 minority report 2 box office 3 scooby doo 4 sixth sense 5 national guard 6 bourne identity 7 air national guard 8 united states 9 phantom menace 10 special effects 11 hotel room 12 comic book 13 blair witch project 14 short story 15 real life 16 jude law 17 iron giant 18 bin laden 19 black people 20 opening weekend 21 bad guy 22 country bears 23 mans man 24 long time 25 spoiler space 26 empire strikes back 27 top ten 28 politically correct 29 white people 30 tv show 31 bad guys 32 freddie prinze jr 33 monsters ball 34 good thing 35 evil minions 36 big screen 37 political correctness 38 martial arts 39 supreme court 40 beautiful mind figure 7 result of reranking output from the phrase extension module 64 revisiting unigram informativeness an alternative approach to calculate informativeness from the foreground lm and the background lm is just to take the ratio of likelihood scores a11 fga9a54a86 a15 a23 a11 bga9a54a86 a15 this is a smoothed version of relative frequency ratio which is commonly used to find subjectspecific terms damerau 1993 ,0,1,0
for that purpose syntactical didier bourigault 1993 statistical frank smadja 1993 ted dunning 1993 gal dias 2002 and hybrid syntaxicostatistical methodologies batrice daille 1996 jeanphilippe goldman et al 2001 have been proposed ,0,1,0
alpha 0 01 02 03 04 05 freq2 13555 13093 12235 11061 10803 10458 freq3 4203 3953 3616 3118 2753 2384 freq4 1952 1839 1649 1350 1166 960 freq5 1091 1019 917 743 608 511 freq2 2869 2699 2488 2070 1666 1307 total 23670 22603 20905 18342 16996 15620 alpha 06 07 08 09 10 freq2 10011 9631 9596 9554 9031 freq3 2088 1858 1730 1685 1678 freq4 766 617 524 485 468 freq5 392 276 232 202 189 freq2 1000 796 627 517 439 total 14257 13178 12709 12443 11805 table 7 number of extracted mwus by frequency 62 qualitative analysis as many authors assess frank smadja 1993 john justeson and slava katz 1995 deciding whether a sequence of words is a multiword unit or not is a tricky problem ,0,1,0
on the other hand purely statistical systems frank smadja 1993 ted dunning 1993 gal dias 2002 extract discriminating mwus from text corpora by means of association measure regularities ,0,1,0
presently many systems tan et al 1999 liu 2000 song 1993 luo et al 2001 focus on online recognition of proper nouns and have achieved inspiring results in newscorpus but will be deteriorated in special text such as spoken corpus novels ,0,1,0
relative frequency ratio rfr of terms between two different corpora can also be used to discover domainoriented multiword terms that are characteristic of a corpus when compared with another damerau 1993 ,0,1,0
3 candidates extraction on suffix array suffix array also known as string patarraymanber et al 1993 is a compact data structure to handle arbitrarylength strings and performs much powerful online string search operations such as the ones supported by pattree but has less space overhead ,0,1,0
3 the loglikelihoodratio association measure we base all our associationbased wordalignment methods on the loglikelihoodratio llr statistic introduced to the nlp community by dunning 1993 ,0,1,0
1993 sometimes augmented by an hmmbased model or och and neys model 6 och and ney 2003 ,0,1,0
first we considered single sentences as documents and tokens as sentences we define a token as a sequence of characters delimited by 1in our case the score we seek to globally maximize by dynamic programming is not only taking into account the length criteria described in gale and church 1993 but also a cognatebased one similar to simard et al 1992 ,0,1,0
1pmi is subject to overestimation for low frequency items dunning 1993 thus we require a minimum frequency of occurrence for the expressions under study ,0,1,0
the sets obtainedare then ranked usingthe loglikelihoodratiostestdunning1993 ,0,1,0
32 german germanis thesecondmostinvestigatedlanguage thanks to the early work of breidt 1993 and morerecently to thatof krennandevertsuch as krennand evert 2001 evert and krenn2001 evert2004centeredonevaluation ,0,1,0
2 related work the issue of mwe processing has attracted much attention from the natural language processing nlp community including smadja 1993 dagan and church 1994 daille 1995 1995 mcenery et al 1997 wu 1997 michiels and dufour 1998 maynard and ananiadou 2000 merkel and andersson 2000 piao and mcenery 2001 sag et al 2001 tanaka and baldwin 2003 dias 2003 baldwin et al 2003 nivre and nilsson 2004 pereira et al ,0,1,0
for each candidate triple the loglikelihood dunning 1993 and salience kilgarriff and tugwell 2001 scores were calculated ,0,1,0
we use the likelihood ratio for a binomial distribution dunning 1993 which tests the hypothesis whether the term occurs independently in texts of biographical nature given a large corpus of biographical and nonbiographical texts ,0,1,0
to model aspects of cooccurrence association that might be obscured by raw frequency the loglikelihood ratio g2 dunning 1993 was also used to transform the feature space ,0,1,0
for the current work the loglikelihood coefficient has been employed dunning 1993 as it is reported to perform well among other scoring methods daille 1995 ,1,0,0
many previous studies have shown that the loglikelihood ratio is well suited for this purpose dunning 1993 ,1,0,0
21 keywords as our starting point we calculated the keywords of the belgian corpus with respect to the netherlandic corpus both on the basis of a chisquare test with yates continuity correction scott 1997 and the loglikelihood ratio dunning 1993 ,0,1,0
rapp 1999 dunning 1993 but using cosine rather than cityblock distance to measure profile similarity ,0,1,0
the algorithm is based on the machine learning method for word categorisation inspired by the well known study on basiclevel categories rosch 1978 presented in basili et al 1993a ,0,1,0
the algorithm to acquire the lexicon implemented in the ariostqlex system has been extensively described in basili et al 1993c ,0,1,0
pustejovsky confronted with the problem of automatic acquisition more extensively in pustejovsky et al 1993 ,0,1,0
the statistical methods are based on distributional analysis we defined a measure called mutual conditioned plausibility a derivation of the well known mutual information and cluster analysis a cobweblike algorithm for word classification is presented in basili et al 1993a ,0,1,0
4 related work the automatic extraction of english subcategorization frames has been considered in brent 1991 brent 1993 where a procedure is presented that takes untamed text as input and generates a list of verbal subcategorization frames ,0,1,0
the logllkelihood ratio g 2 is a mathematically wellgrounded and accurate method for calculating how surprising an event is dunning 1993 ,0,1,0
its roots are the same as computational linguistics cl but it has been largely ignored in cl until recently dunning 1993 carletta 1996 kilgarriff 1996 ,0,1,0
this set of words rooted primarily in the verbs of the set corresponds to the levin 1993 characterize class 292 declare 294 admire 312 and judgment verbs 33 and hence may have particular syntactic and semantic patterning ,0,1,0
this example is adapted from resnik 1993 ,0,1,0
we use the loglikelihood x statistic rather than the pearsons x 2 statistic as this is thought to be more appropriate when the counts in the contingency table are low dunning 1993 ,0,1,0
smadja frank 1993 retrieving collocations from text computational linguistics 191143177 ,0,1,0
in particular mutual information church and hanks 1990 wu and su 1993 and other statistical methods such as smadja 1993 and frequencybased methods such as justeson and katz 1993 exclude infrequent phrases because they tend to introduce too much noise ,0,1,0
have been used in statistical machine translation brown et al 1990 terminology research and translation aids isabelle 1992 ogden and gonzales 1993 van der eijk 1993 bilingual lexicography klavans and tzoukermann 1990 smadja 1992 wordsense disambiguation brown et al 1991b gale et al 1992 and information retrieval in a multilingual environment landauer and littman 1990 ,0,1,0
34 related work and issues for future research smadja 1992 and van der eijk 1993 describe term translation methods that use bilingual texts that were aligned at the sentence level ,0,1,0
manual processes such as lexicon development could be automated in the future using standard contextbased word distribution methods smadja 1993 or other corpusbased techniques ,0,1,0
smadjafrank1993 ,0,1,0
tools like xtract smadja 1993 were based on the work of church and others but made a step forward by incorporating various statistical measurements like zscore and variance of distribution as well as shallow linguistic techniques like partofspeech tagging and lemmatization of input data and partial parsing of raw output ,1,0,0
daille 1996 smadja 1993 less prior work exists for bilingual acquisition of domainspecific translations ,0,1,0
for colnparison we refer here to smadjas method 1993 because this method and the proposed method have much in connnon ,0,1,0
algorithms for the computation of firstorder associations have been used in lexicography for the extraction of collocations smadja 1993 and in cognitive psychology for the simulation of associative learning wettler rapp 1993 ,0,1,0
sometimes the notion of collocation is defined in terms of syntax by possible partofspeech patterns or in terms of semantics requiring collocations to exhibit noncompositional meaning smadja 1993 ,0,1,0
future work will include i applying the method to retrieve other types of collocations smadja 1993 and ii evaluating the method using internet directories ,0,1,0
one example of the 450 latter problem is the following in smadja 1993 the nature of a syntactic link between two associated words is detected a posteriori ,0,1,0
of acl 1990 smadja 1993 f smadja retrieving collocations ficma text xtract 1993 ,0,1,0
unlike smadja 1993 the kevord rnay be part of a chinese word ,0,1,0
further enhancement of these utilities include compiling collocation statistics smadja 1993 and semiautomatic gloassary construction tong 1993 ,0,1,0
for instance there is a substantial body of papers on the extraction of frequently cooccurring words from corpora using statistical methods eg choueka et al 1983 church and hanks 1989 smadja 1993 to list only a few ,0,1,0
from the extracted ngrams those with a flequcncy of 3 or more were kept other approaches get rid of ngrams of such low frequencies smadja 1993 ,0,1,0
smadja 1993 extracts uninterrupted as well as interrupted collocations predicative relations rigid noun phrases and phrasal templates ,0,1,0
the colllilloil poinis regarding collocations appear to be as smadja 1993 suggestsl they are mbilrary it is nol clear why to bill through means to fail thy are domaindependent interest rate stock market they are recurrenl and cohesive loxical clusters the presence of one of the ,0,1,0
smadja 1993 kits et al 1994 ikehara et al 1995 mention about substrings of collocations ,0,1,0
in the past five years important research on the automatic acquisition of word classes based on lexical distribution has been published church and hanks 1990 hindle 1990 smadja 1993 greinstette 1994 grishman and sterling 1994 ,1,0,0
baron and hirst 2004 extracted collocations with xtract smadja 1993 and classified the collocations using the orientations of the words in the neighboring sentences ,0,1,0
most previous work on compositionality of mwes either treat them as collocations smadja 1993 or examine the distributional similarity between the expression and its constituents mccarthy et al 2003 baldwin et al 2003 bannard et al 2003 ,0,1,0
one of the best efforts to quantify the performance of a termrecognition system smadja 1993 does so only for one processing stage leaving unassessed the texttooutput performance of the system ,1,0,0
in smadjas collocation algorithm xtract the lowestfrequency words are effectively discarded as well smadja 1993 ,0,1,0
statistics on cooccurrence of words in a local context were used recently for monolingual word sense disambiguation gale church and yarowsky 1992b 1993 sch6tze 1992 1993 see section 7 for more details and church and hanks 1990 smadja 1993 for other applications of these statistics ,0,1,0
for the extraction problem there have been various methods proposed to date which are quite adequate hindle and rooth 1991 grishman and sterling 1992 manning 1992 utsuro matsumoto and nagao 1992 brent 1993 smadja 1993 grefenstette 1994 briscoe and carroll 1997 ,1,0,0
baron and hirst 2004 extracted collocations with xtract smadja 1993 and classified the collocations using the orientations of the words in the neighboring sentences ,0,1,0
the former extracts collocations within a fixed window church and hanks 1990 smadja 1993 ,0,1,0
smadja 1993 also detailed techniques for collocation extraction and developed a program called xtract which is capable of computing flexible collocations based on elaborated statistical calculation ,0,1,0
parsing has been also used after extraction smadja 1993 for filtering out invalid results ,0,1,0
similarly smadja 1993 uses a six content word window to extract significant collocations ,0,1,0
finally knowledge of polarity can be combined with corpusbased collocation extraction methods smadja 1993 to automatically produce entries for the lexical functions used in meaningtext theory meluk and pertsov 1987 for text generation ,0,1,0
there has been a growing interest in corpusbased approaches which retrieve collocations from large corpora nagao and mori 1994 ikehara et al 1996 kupiec 1993 fung 1995 kitamura and matsumoto 1996 smadja 1993 smadja et al 1996 haruno et al 1996 ,0,1,0
however morphosyntactic features alone cannot verify the terminological status of the units extracted since they can also select non terms see smadja 1993 ,0,1,0
other classes such as the ones below can be extracted using lexicostatistical tools such as in smadja 1993 and then checked by a human ,0,1,0
for the correct identification of phrases in a korean query it would help to identify the lexical relations and produce statistical information on pairs of words in a text corpus as in smadja 1993 ,0,1,0
it is clear that appendix b contains far fewer true noncompositional phrases than appendix a 7 related work there have been numerous previous research on extracting collocations from corpus eg choueka 1988 and smadja 1993 ,0,1,0
cooccurrence information between neighboring words and words in the same sentence has been used in phrase extraction smadja 1993 fung and wu 1994 phrasal translation smadja et al 1996 kupiec 1993 wu 1995 dagan and church 1994 target word selection liu and li 1997 tanaka and iwasaki 1996 domain word translation fung and lo 1998 fung 1998 sense disambiguation brown et al 1991 dagan et al 1991 dagan and itai 1994 gale et al 1992a gale et al 1992b gale et al 1992c shiitze 1992 gale et al 1993 yarowsky 1995 and even recently for query translation in crosslanguage ir as well ballesteros and croft 1998 ,0,1,0
cooccurrence statistics is collected from either bilingual parallel and 334 nonparallel corpora smadja et al 1996 kupiec 1993 wu 1995 tanaka and iwasaki 1996 fung and lo 1998 or monolingual corpora smadja 1993 fung and wu 1994 liu and li 1997 shiitze 1992 yarowsky 1995 ,0,1,0
the second method considers the means and variance of the distance between two words and can compute flexible collocations smadja 1993 ,0,1,0
to perform code generalization li adopted to smadjas work smadja 1993 and defined the code strength using a code frequency and a standard deviation in each level of the concept hierarchy ,0,1,0
one of the earliest attempts at extracting interrupted collocations ie noncontiguous collocations including vpcs was that of smadja 1993 ,0,1,0
4 method2 simple chunkbased extraction to overcome the shortcomings of the brill tagger in identifying particles we next look to full chunk 2note this is the same as the maximum span length of 5 used by smadja 1993 and above the maximum attested np length of 3 from our corpus study see section 22 ,0,1,0
there have been many statistical measures which estimate cooccurrence and the degree of association in previous researches such as mutual information church 1990 sporat 1990 tscore church 1991 dice matrix smadja 1993 1996 ,0,1,0
the approach is in the spirit of smadja 1993 on retrieving collocations from text corpora but is more integrated with parsing ,0,1,0
3 related work word collocation various collocation metrics have been proposed including mean and variance smadja 1994 the ttest church et al 1991 the chisquare test pointwise mutual information mi church and hanks 1990 and binomial loglikelihood ratio test blrt dunning 1993 ,0,1,0
for that purpose syntactical didier bourigault 1993 statistical frank smadja 1993 ted dunning 1993 gal dias 2002 and hybrid syntaxicostatistical methodologies batrice daille 1996 jeanphilippe goldman et al 2001 have been proposed ,0,1,0
alpha 0 01 02 03 04 05 freq2 13555 13093 12235 11061 10803 10458 freq3 4203 3953 3616 3118 2753 2384 freq4 1952 1839 1649 1350 1166 960 freq5 1091 1019 917 743 608 511 freq2 2869 2699 2488 2070 1666 1307 total 23670 22603 20905 18342 16996 15620 alpha 06 07 08 09 10 freq2 10011 9631 9596 9554 9031 freq3 2088 1858 1730 1685 1678 freq4 766 617 524 485 468 freq5 392 276 232 202 189 freq2 1000 796 627 517 439 total 14257 13178 12709 12443 11805 table 7 number of extracted mwus by frequency 62 qualitative analysis as many authors assess frank smadja 1993 john justeson and slava katz 1995 deciding whether a sequence of words is a multiword unit or not is a tricky problem ,0,1,0
on the other hand purely statistical systems frank smadja 1993 ted dunning 1993 gal dias 2002 extract discriminating mwus from text corpora by means of association measure regularities ,0,1,0
for example smadja 1993 suggests a basic characteristic of collocations and multiword units is recurrent domaindependent and cohesive lexical clusters ,0,1,0
the group of collocations and compounds should be delimited using statistical approaches such as xtract smadja 1993 or localmax silva et al 1999 so that only the most relevantthose of higher frequency are included in the database ,0,1,0
many efficient techniques exist to extract multiword expressions collocations lexical units and idioms church and hanks 1989 smadja 1993 dias et al 2000 dias 2003 ,1,0,0
study in collocation extraction using lexical statistics has gained some insights to the issues faced in collocation extraction church and hanks 1990 smadja 1993 choueka 1993 lin 1998 ,1,0,0
smadja smadja 1993 proposed a statistical model by measuring the spread of the distribution of cooccurring pairs of words with higher strength ,0,1,0
in the work of smadja 1993 on extracting collocations preference was given to constructions whose constituents appear in a fixed order a similar and more generally implemented version of our assumption here that asymmetric constructions are more idiomatic than symmetric ones ,0,1,0
we can mentionhere only part of this work berryrogghe 1973 church et al 1989 smadja1993lin1998krennandevert2001 for monolingualextraction and kupiec 1993 wu1994smadjaetal ,0,1,0
3 overviewofextractionwork 31 english as one mightexpectthe bulk of the collocation extractionwork concernsthe english language choueka1988churchet al 1989churchand hanks1990 smadja1993 justesonand katz 1995kjellmer 1994sinclair 1995lin1998 amongmany others1 ,0,1,0
smadja1993employsthezscoreinconjunction with several heuristicseg the systematic occurrenceof two lexical items at the same distanceintextandextractspredicativecollocations 1egfrantziet al 2000pearce2001goldmanet al 2001zaiuinkpenandhirst2002dias2003seretanetal ,0,1,0
2004appliedextractiontechniquessimilarto xtractsystem smadja1993 japaneseikeharaetal ,0,1,0
krenn 2000b smadja 1993 ,0,1,0
there are some existing corpus linguistic researches on automatic extraction of collocations from electronic text smadja 1993 lin 1998 xu and lu 2006 ,0,1,0
the problem is that with such a definition of collocations even when improved one identifies not only collocations but freecombining pairs frequently appearing together such as lawyerclient doctorhospital as pointed out by smadja 1993 ,0,1,0
other representative collocation research can be found in church and hanks 1990 and smadja 1993 ,0,1,0
while several methods have been proposed to automatically extract compounds smadja 1993 suet al 1994 we know of no successful attempt to automatically make classes of compounds ,0,0,1
therefore sublanguage techniques such as sager 1981 and smadja 1993 do not work ,0,0,1
mi is defined in general as follows y i ix y log2 px py we can use this definition to derive an estimate of the connectedness between words in terms of collocations smadja 1993 but also in terms of phrases and grammatical relations hindle 1990 ,0,1,0
while bound compositions are not predictable ie their reasonableness cannot be derived from the syntactic and semantic properties of the words in themsmadja 1993 ,0,1,0
f c r cr 2 continue explanations we begin by mentioning the xtrgct tool by smadja smadja 1993 ,0,1,0
a number of alignment techniques have been proposed varying from statistical methods brown et al 1991 gale and church 1991 to lexical methods kay and rsscheisen 1993 chen 1993 ,0,1,0
furthermore the underlying decoding strategies are too time consuming for our application we therefore use a translation model based on the simple linear interpolation given in equation 2 which combines predictions of two translation models ms and m both based on ibmlike model 2brown et al 1993 ,0,1,0
32 mapping mapping the identified units tokens or sequences to their equivalents in the other language was achieved by training a new translation model ibm 2 using the em algorithm as described in brown et al 1993 ,0,1,0
have been used in statistical machine translation brown et al 1990 terminology research and translation aids isabelle 1992 ogden and gonzales 1993 van der eijk 1993 bilingual lexicography klavans and tzoukermann 1990 smadja 1992 wordsense disambiguation brown et al 1991b gale et al 1992 and information retrieval in a multilingual environment landauer and littman 1990 ,0,1,0
partofspeech taggers are used in a few applications such as speech synthesis sproat et al 1992 and question answering kupiec 1993b ,0,1,0
word alignment is newer found only in a few places gale and church 1991a brown et al 1993 dagan et al 1993 ,0,1,0
unlike probabilistic parsing proposed by fujisaki et al 1989 briscoe and carroll 1993 also a staff member of matsushita electric industrial co ltd shinagawa tokyo japan ,0,1,0
on the other end of the spectrum characterbased bitext mapping algorithms church 1993 davis et al 1995 are limited to language pairs where cognates are common in addition they may easily be misled by superficial differences in formatting and page layout and must sacrifice precision to be computationally tractable ,0,1,0
the model is often further restricted so that each source word is assigned to exactly one target word brown et al 1993 ney et al 2000 ,0,1,0
in this paper we will describe extensions to tile hiddenmarkov alignment model froln vogel et al 1996 and compare tlmse to models 1 4 of brown et al 1993 ,0,1,0
3 model 1 and model 2 lcllacing the ltendence on ajl in the hmm alignment mom iy a delendence on j we olltain a model wlfich an lie seen as a zeroorder hidlmmarkov model which is similar to model 2 1rotoset ty brown et al 1993 ,0,1,0
tile full description of model 4 brown et al 1993 is rather complicated as there have to be considered tile cases that english words have fertility larger than one and that english words have fertility zero ,0,1,0
therefore the viterbi alignment is comlmted only approximately using the method described in brown et al 1993 ,0,1,0
the assumptions we made were the following a lexical token in one half of the translation unit tu corresponds to at most one nonempty lexical unit in the other half of the tu this is the 11 mapping assumption which underlines the work of many other researchers ahrenberg et al 2000 brew and mckelvie 1996 hiemstra 1996 kay and rscheisen 1993 tiedmann 1998 melamed 2001 etc a polysemous lexical token if used several times in the same tu is used with the same meaning this assumption is explicitly used by gale and church 1991 melamed 2001 and implicitly by all the previously mentioned authors a lexical token in one part of a tu can be aligned to a lexical token in the other part of the tu only if the two tokens have compatible types partofspeech in most cases compatibility reduces to the same pos but it is also possible to define other compatibility mappings eg participles or gerunds in english are quite often translated as adjectives or nouns in romanian and viceversa although the word order is not an invariant of translation it is not random either ahrenberg et al 2000 when two or more candidate translation pairs are equally scored the one containing tokens which are closer in relative position are preferred ,0,1,0
another kind of popular approaches to dealing with query translation based on corpusbased techniques uses a parallel corpus containing aligned sentences whose translation pairs are corresponding to each other brown et al 1993 dagan et al 1993 smadja et al 1996 ,1,0,0
according to the bayes rule the problem is transformed into the noisy channel model paradigm where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text brown et al 1993 ,0,1,0
using the ibm translation models ibm1 to ibm5 brown et al 1993 as well as the hiddenmarkov alignment model vogel et al 1996 we can produce alignments of good quality ,1,0,0
these alignment models stem from the sourcechannel approach to statistical machine translation brown et al 1993 ,0,1,0
a detailed description of the popular translation models ibm1 to ibm5 brown et al 1993 aswellasthehiddenmarkovalignmentmodel hmm vogel et al 1996 can be found in och and ney 2003 ,1,0,0
on the other hand statistical mt employing ibm models brown et al 1993 translates an input sentence by the combination of word transfer and word reordering ,0,1,0
brown et al 1993 vogel et al 1996 garcavarea et al 2002 ahrenberg et al 1998 tiedemann 1999 tufis and barbu 2002 melamed 2000 ,0,1,0
they are based on the sourcechannel approach to statistical machine translation brown et al 1993 ,0,1,0
so far most of the statistical machine translation systems are based on the singleword alignment models as described in brown et al 1993 as well as the hidden markov alignment model vogel et al 1996 ,0,1,0
in wordbased models such as ibm model 15 brown et al 1993 the probability pts is decomposed into statistical parameters involving words ,0,1,0
2 decoding the decoding problem in smt is one of finding the most probable translation e in the target language of a given source language sentence f in accordance with the fundamental equation of smt brown et al 1993 e argmaxe prfepre ,0,1,0
in each iteration of local search we look in the neighborhood of the current best alignment for a better alignment brown et al 1993 ,0,1,0
in the context of statistical machine translation brown et al 1993 we may interprete as an english sentence f its translation in french and a a representation of how the words correspond to each other in the two sentences ,0,1,0
the statistical approach involves the following alignment of bilingual texts at the sentence level nsing statistical techniques eg brown lai and mercer 1991 gale and church 1993 chen 1993 and kay and rsscheisen 1993 statistical machine translation models eg brown cooke pietra pietra et al ,0,1,0
this estimate could be used as a starting point for a more detailed alignment algorithm such as wordalign dagan et al 1993 ,0,1,0
motivation there have been quite a number of recent papers on parallel text brown et al 1990 1991 1993 chen 1993 church 1993 church et al 1993 dagan et al 1993 gale and church 1991 1993 isabelle 1992 kay and rgsenschein 1993 klavans and tzoukermann 1990 kupiec 1993 matsumoto 1991 ogden and gonzales 1993 shemtov 1993 simard et al 1992 warwickarmstrong and russell 1990 wu to appear ,0,1,0
machine translation brown et al 1993 but also in other applications such as word sense disanabiguation brown et al 1991 and bilingnal lexicography klavans and tzoukermann 1990 ,0,1,0
of the position infer marion of words at ltlathillg pairs of selltelces which turned out useful brown et al 1993 ,1,0,0
iiowever dagan et al 1993 have shown that knowledge of targettext length is not crucial to the models iertbrmanee ,0,1,0
a sinfilar approach has been chosen by dagan et al 1993 ,0,1,0
in the recent years there have been a number of papers considering this or similar problems brown et al 1990 dagan et al 1993 kay et al 1993 fung et al 1993 ,0,1,0
given any word alignment model posterior probabilities can be computed as brown et al 1993 paj ief summationdisplay a pafeiaj 1 where i 01i ,0,1,0
2 we note that these posterior probabilities can be computed efficiently for some alignment models such as the hmm vogel et al 1996 och and ney 2003 models 1 and 2 brown et al 1993 ,1,0,0
we use viterbi training brown et al 1993 but neighborhood estimation alonaizan et al 1999 och and ney 2003 or pegging brown et al 1993 could also be used ,0,1,0
therefore to make the phrasebased smt system robust against data sparseness for the ranking task we also make use of the ibm model 4 brown et al 1993 in both directions ,0,1,0
this approach is usually referred to as the noisy sourcechannel approach in statistical machine translation brown et al 1993 ,0,1,0
the words with the highest association probabilities are chosen as acquired words for entity e 41 base model i using the translation model i brown et al 1993 where each word is equally likely to be aligned with each entity we have pwe 1l 1m mproductdisplay j1 lsummationdisplay i0 pwjei 1 where l and m are the lengths of entity and word sequences respectively ,0,1,0
3 model as an extension to commonly used lexical word pair probabilities pfe as introduced in brown et al 1993 we define our model to operate on word triplets ,0,1,0
the resulting training procedure is analogous to the one presented in brown et al 1993 and tillmann and ney 1997 ,0,1,0
usually the ibm model 1 developed in the statistical machine translation field brown et al 1993 is used to construct translation models for retrieval purposes in practice ,0,1,0
1 introduction sentencealigned parallel bilingual corpora have been essential resources for statistical machine translation brown et al 1993 and many other multilingual natural language processing applications ,0,1,0
91 training methodology given a training set we first run a variant of ibm alignment model 1 brown et al 1993 for 100 iterations and then initialize model i with the learned parameter values ,0,1,0
it acquires a set of synchronous lexical entries by running the ibm alignment model brown et al 1993 and learns a loglinear model to weight parses ,0,1,0
the mt community has developed not only an extensive literature on alignment brown et al 1993 vogel et al 1996 marcu and wong 2002 denero et al 2006 but also standard proven alignment tools such as giza och and ney 2003 ,0,1,0
traditionally generative word alignment models have been trained on massive parallel corpora brown et al 1993 ,0,1,0
the structure of the graphical model resembles ibm model 1 brown et al 1993 in which each target record word is assigned one or more source text words ,0,1,0
furthermore we provide a 638 error reduction compared to ibm model 4 brown et al 1993 ,0,0,1
31 conditional random field for alignment our conditional random field crf for alignment has a graphical model structure that resembles that of ibm model 1 brown et al 1993 ,0,1,0
in this work we propose two models that can be categorized as extensions of standard word lexicons a discriminative word lexicon that uses global ie sentencelevel source information to predict the target words using a statistical classifier and a triggerbased lexicon model that extends the wellknown ibm model 1 brown et al 1993 with a second trigger allowing for a more finegrained lexical choice of target words ,1,0,0
there are three major types of models heuristic models as in melamed 2000 generative models as the ibm models brown et al 1993 and discriminative models varea et al 2001 bangalore et al 2006 ,0,1,0
this is a problem with other direct translation models such as ibm model 1 used as a direct model rather than a channel model brown et al 1993 ,0,0,1
the giza aligner is based on ibm model 4 brown et al 1993 ,0,1,0
1 introduction word alignment is a critical component in training statistical machine translation systems and has received a significant amount of research for example brown et al 1993 ittycheriah and roukos 2005 fraser and marcu 2007 including work leveraging syntactic parse trees eg cherry and lin 2006 denero and klein 2007 fossum et al 2008 ,0,1,0
yamada and knight 2001 follow brown et al 1993 in using the noisy channel model by decomposing the translation decisions modeled by the translation model into different types and inducing probability distributions via maximum likelihood estimation over each decision type ,0,1,0
previous smt systems eg brown et al 1993 used a wordbased translation model which assumes that a sentence can be translated into other languages by translating each word into one or more words in the target language ,0,1,0
the difference between mwa and bilingual word alignment brown et al 1993 is that the mwa method works on monolingual parallel corpus instead of bilingual corpus used by bilingual word alignment ,0,1,0
generative methods brown et al 1993 vogel and ney 1996 treat word alignment as a hidden process and maximize the likelihood of bilingual training corpus using the expectation maximization em algorithm ,0,1,0
their experiments were performed using a decoder based on ibm model 4 using the translation techniques developed at ibm brown et al 1993 ,0,1,0
becausesuchapproachesdirectly learn a generative model over phrase pairs they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the manytoone wordlevel alignments produced by the ibm series models brown et al 1993 or the hidden markov model hmm vogel et al 1996 ,0,1,0
we then built separate directed word alignments for englishx andxenglish xindonesian spanish using ibm model 4 brown et al 1993 combined them using the intersectgrow heuristic och and ney 2003 and extracted phraselevel translation pairs of maximum length seven using the alignment template approach och and ney 2004 ,0,1,0
exact decoding is the original decoding problem as defined in brown et al 1993 and relaxed decoding is the relaxation of the decoding problem typically used in practice ,0,1,0
alignment spaces can emerge from generative stories brown et al 1993 from syntactic notions wu 1997 or they can be imposed to create competition between links melamed 2000 ,0,1,0
the implementation of meba was strongly influenced by the notorious five ibm models described in brown et al 1993 ,1,0,0
first a parsingbased approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise in order to construct a partial semantic representation dowding et al 1993 allen et al 2001 ward 1991 ,0,1,0
to this end we adopt techniques from statistical machine translation brown et al 1993 och and ney 2003 and use statistical alignment to learn the edit patterns ,0,1,0
we adopt an approach similar to ciaramella 1993 boros et al 1996 in which the meaning representation in our case xml is transformed into a sorted flat list of attributevalue pairs indicating the core contentful concepts of each command ,0,1,0
by introducing the hidden word alignment variable a the following approximate optimization criterion can be applied for that purpose e argmaxe pre f argmaxe summationdisplay a prea f argmaxea prea f exploiting the maximum entropy berger et al 1996 framework the conditional distribution prea f can be determined through suitable real valued functions called features hrefar 1r and takes the parametric form pea f exp rsummationdisplay r1 rhrefa the itcirst system chen et al 2005 is based on a loglinear model which extends the original ibm model 4 brown et al 1993 to phrases koehn et al 2003 federico and bertoldi 2005 ,0,1,0
this approach is usually referred to as the noisy sourcechannel approach in smt brown et al 1993 ,0,1,0
while em has worked quite well for a few tasks notably machine translations starting with the ibm models 15 brown et al 1993 it has not had success in most others such as partofspeech tagging merialdo 1991 namedentity recognition collins and singer 1999 and contextfreegrammar induction numerous attempts too many to mention ,1,0,0
our test set is 3718 sentences from the english penn treebank marcus et al 1993 which were translated into german ,0,1,0
each item is associated with a stack whose signa12specifically a bhypergraph equivalent to an andor graph gallo et al 1993 or contextfree grammar nederhof 2003 ,0,1,0
logics for the ibm models brown et al 1993 would be similar to our logics for phrasebased models ,0,1,0
the ibm models 15 brown et al 1993 produce word alignments with increasing algorithmic complexity and performance ,1,0,0
2 the wfst reordering model the translation template model ttm is a generative model of phrasebased translation brown et al 1993 ,0,1,0
there are five different ibm translation models brown et al 1993 ,0,1,0
further details are in the original paper brown et al 1993 ,0,1,0
32 details to learn alignments translation probabilities etc in the first method we used work that has been done in statistical machine translation brown et al 1993 where the translation process is considered to be equivalent to a corruption of the source language text to the target language text due to a noisy channel ,0,1,0
for example in the ibm models brown et al 1993 each word ti independently generates 0 1 or more 2note that we refer to t as the target sentence even though in the sourcechannel model t is the source sentence which goes through the channel model pst to produce the observed sentence s words in the source language ,0,1,0
it was initially proposed by brown et al 1993 and more recently have been intensively studied by several research groups germann et al 2001 och et al 2003 ,1,0,0
in this paper we propose an alignment algorithm between english and korean conceptual units or between english and korean term constituents in englishkorean technical term pairs based on ibm model brown et al 1993 ,0,1,0
one promising approach extends standard statistical machine translation smt techniques eg brown et al 1993 och ney 2000 2003 to the problems of monolingual paraphrase identification and generation ,0,1,0
4 pattern switching the compositional translation presents problems which have been reported by baldwin and tanaka 2004 brown et al 1993 fertility swts and mwts are not translated by a term of a same length ,0,1,0
most existing methods treat word tokens as basic alignment units brown et al 1993 vogel et al 1996 deng and byrne 2005 however many languages have no explicit word boundary markers such as chinese and japanese ,0,1,0
the details of the algorithm can be found in the literature for statistical translation models such as brown et al 1993 ,0,1,0
in the proposed method the statistical machine translation smt brown et al 1993 is deeply incorporated into the question answering process instead of using the smt as the preprocessing before the monolingual qa process as in the previous work ,0,1,0
in this paper we use ibm model 1 brown et al 1993 in order to get the probability pqda as follows ,0,1,0
brown et al 1993 ,0,1,0
at the same time we believe our method has advantages over the approach developed initially at ibm brown et al 1990 brown et al 1993 for training translation systems automatically ,0,0,1
dagan church and gale 1993 expanded on this idea by replacing brown et als 1988 word alignment parameters which were based on absolute word positions in aligned segments with a much smaller set of relative offset parameters ,0,1,0
brown et al 1993 ,0,1,0
evaluation 61 evaluation at the token level this section compares translation model estimation methods a b and c to each other and to brown et als 1993b model 1 ,0,1,0
until now translation models have been evaluated either subjectively eg white and oconnell 1993 or using relative metrics such as perplexity with respect to other models brown et al 1993b ,0,1,0
an analysis of the alignments shows that smoothing the fertility probabilities significantly reduces the frequently occurring problem of rare words forming garbage collectors in that they tend to align with too many words in the other language brown della pietra della pietra goldsmith et al 1993 ,0,1,0
for placing the head the center function centeri brown et al 1993 uses the notation circledot i is used the average position of the source words with which the target word e i1 is aligned ,0,1,0
many existing systems for statistical machine translation garcavarea and casacuberta 2001 germann et al 2001 nieen et al 1998 och tillmann and ney 1999 implement models presented by brown della pietra della pietra and mercer 1993 the correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position ,0,1,0
in the following section we show how this drawback can be overcome using statistical alignments brown et al 1993 ,0,1,0
as an alternative to the often used sourcechannel approach brown et al 1993 we directly model the posterior probability pre i 1 f j 1 och and ney 2002 ,0,1,0
knight and marcu 2000 treat reduction as a translation process using a noisychannel model brown et al 1993 ,0,1,0
one such model is the ibm model 1 brown et al 1993 ,0,1,0
our system outperforms competing approaches including the standard machine translation alignment models brown et al 1993 vogel ney and tillmann 1996 and the stateoftheart cut and paste summary alignment technique jing 2002 ,0,0,1
more specifically the latter system uses the ibm1 lexical parameters brown et al 1993 for computing the translation probabilities of two possible new tuples the one resulting when the nullalignedword is attached to table 6 evaluation results for experiments on ngram size incidence ,0,1,0
introduction automatic word alignment brown et al 1993 is a vital component of all statistical machine translation smt approaches ,1,0,0
note that the translation direction is inverted from what would be normally expected correspondingly the models built around this equation are often called invertedtranslationmodels brown et al 1990 1993 ,0,1,0
four alternatives are proposed in these special issues 1 brent 1993 2 briscoe and carroll this issue 3 hindle and rooth this issue and 4 weischedel et al ,0,1,0
